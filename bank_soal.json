{
  "1deba56902ad8957bbf063c48d4bd3b33a8d2382e45d20a65ce510083303ffdb": {
    "soal": "The visualization in the Network Explorer Widget works similarly to the Scatter Plot Widget. To select a subset of nodes, draw a rectangle around the subset. Shift will add a new group. Ctrl-Shift (Cmd-Shift) will add to the existing group. Alt (Option) will remove it from the group. Clicking outside the network will remove the selection.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "e48622e1afd4dff6b9cb11eceba376ba48fa77894cc7210e82cc8736533b35ff": {
    "soal": "from googlesearch import search\nimport csv\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef google_scrape_bulk(keywords, num_results=10, output_file='results.csv'):\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'Rank', 'URL'])\nfor keyword in keywords:\nprint(f\"\ud83d\udd0d Searching for: \")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nwriter.writerow([keyword, i+1, url])\ntime.sleep(1) # Delay agar tidak dianggap bot\nexcept Exception as e:\nprint(f\"\u274c Error while searching '{keyword}': {e}\")\nprint(f\"\n\u2705 All results saved to '{output_file}'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\ngoogle_scrape_bulk(keywords, num_results=10)\nWhich columns are included in the output CSV file?\n{\n=Keyword, Rank, URL\n~Keyword, Title, Description\n~Search Term, Position, Link\n~Query, Result Number, Web Address\n~Term, Order, Hyperlink\n~Phrase, Index, Site\n}",
    "jawaban": "The correct answer is: keyword"
  },
  "108183e97bff172949f8d69d86186c8886993eba65371f653e8fad0ebe5c78e7": {
    "soal": "111 (binary) - 10 (binary) = ....... (binary)",
    "jawaban": "The correct answer is: 101"
  },
  "6caabbacd860edf4a5764de30415fffce06027633aa07271c2769b113a8ec759": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\n# xes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\nxes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nprocess_tree = pm4py.discover_bpmn_inductive(log)\nbpmn_model = pm4py.convert_to_bpmn(process_tree)\npm4py.view_bpmn(bpmn_model)\nWhich function converts the discovered process tree into BPMN?",
    "jawaban": "The correct answer is: convert_to_bpmn"
  },
  "90ed11c714cd320cdd8fb6a7f3350d44c9326d80d86766c13368233b4a094bdf": {
    "soal": "In the workflow below, the Iris data from the File widget is passed into the Select Columns widget, where we choose to display only two attributes (petal width and petal length). Then, we can view the original dataset and the dataset with the selected columns in the Data Transpose widget.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "3738c7b469d132fba2723fd72772d1b0e2388e9da2711ecab3d978914b42883d": {
    "soal": "The following ticker code used in the Yahoo Finance Stock Data Widget (ORANGE)\n\n^TNX\n\n\nBelongs to the company,\n\n\n",
    "jawaban": "The correct answer is: Tanzanian Royalty Exploration Corporation"
  },
  "6d97babe6fb929ca0800e64e4b5fbf5fda86ee9fa994a4ab07c32519c3aca088": {
    "soal": "The widget ...........(1)............. uses the Aggregation function to aggregate values in time series windows. Available options include: mean (average), sum, max, min, median, mode, ..................(2)................, variance, product, linearly-weighted moving average, exponential moving average, harmonic mean, geometric mean, non-zero count, cumulative sum, and cumulative product.\n",
    "jawaban": "The correct answer is: (2) \u2192 standard deviation, (1) \u2192 Moving Transform"
  },
  "3e388b6aa80d0498d1a656e16b0916d099d909caaa0ff94fc1c29955fc2a1347": {
    "soal": "To group based on similarity, we can measure the distance between images. The numerical representation of the images obtained from the Image Embedding widget can be used to measure the distance using the Distances widget. Typically, cosine distance works best for images, but you can experiment on your own. We then send the distance matrix to Hierarchical Clustering to visualize the similar pair in the dendrogram.\n\nThe result is that all animals are correctly grouped together. In the dendrogram of the clustering results, we see the animal images. We can use the Image Viewer widget to view the images.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "2f50deb905c450868f6b07769ae33429a6446954fbc8690ecc795acbb2ab8f0c": {
    "soal": "In the example workflow below, using the zoo dataset and creating a clustering workflow with Widget ................... and Widget Hierarchical Clustering. Now define the threshold for selecting the cluster (click on the ruler above). Connect the Box Plot to Hierarchical Clustering, check Order by relevance, and select Cluster as a subgroup. This will sort the attributes by how well they define the selected subgroup, in our case a cluster.\n\n\n\n\n",
    "jawaban": "The correct answer is: Distances"
  },
  "957b107d1295a855cc469e2b2280b6a537669c5d4c4fb9f0d12ec50c22fd414d": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\n# xes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\nxes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nprocess_tree = pm4py.discover_bpmn_inductive(log)\nbpmn_model = pm4py.convert_to_bpmn(process_tree)\npm4py.view_bpmn(bpmn_model)\nWhat is the purpose of '__name__ == \"__main__\"'?",
    "jawaban": "The correct answer is: Ensures code runs only if script executed directly"
  },
  "0ad6d7eee2c838166cca9fd2e5019a307aa0a70881b273c04800b30c584cfc86": {
    "soal": "The Image Embedding widget in Orange is very easy to use. Technically, Orange will send the image to the server, where the server will push the image through a pre-trained deep neural network, such as Google ........................... v3. Deep networks are often trained for a specific purpose. Inception v3, for example, can classify images into one of 1,000 image classes. We can ignore the suggested classification and instead consider the penultimate layer of the network with 2048 nodes (numbers) and use them for the image\u2019s vector-based representation.\n",
    "jawaban": "The correct answer is: Inception"
  },
  "bcc99be4113e0858b471406e7fd2e944b043b97490ac03c0df34a6562e536497": {
    "soal": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n# Contoh: Memuat dataset dari file CSV\n# Gantilah 'data.csv' dengan path ke dataset Anda\ndf = pd.read_csv('data.csv')\n# Memisahkan fitur (X) dan target \n# Asumsikan kolom terakhir adalah target\nX = df.iloc[:, :-1]\ny = df.iloc[:, -1]\n# Membagi dataset menjadi data latih dan data uji (80% : 20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model Decision Tree\nmodel = DecisionTreeClassifier()\n# Melatih model dengan data latih\nmodel.fit(X_train, y_train)\n# Melakukan prediksi pada data uji\ny_pred = model.predict(X_test)\n# Menghitung akurasi\nakurasi = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model: ')\n# Menampilkan laporan klasifikasi yang mencakup presisi, recall, dan F1-score\nprint(classification_report(y_test, y_pred))\nBefore running the provided code, what must be ensured regarding the CSV file?\n{\n~That it is empty to prevent errors.\n~That it contains only numerical data.\n=That the file path 'data.csv' correctly points to the dataset location.\n~That it is stored in a database.\n~That it has no missing values.\n~That it is in JSON format.\n}",
    "jawaban": "The correct answer is: akurasi:.2f"
  },
  "797a083462d23e946c352af28c92fcda61a306d70681371b33e5cbe235d1c7d5": {
    "soal": "The Distance Transformation widget is used for normalization and ....................... of the distance matrix. Normalizing the data is required to bring all variables into proportion with one another.\n",
    "jawaban": "The correct answer is: inversion"
  },
  "93791e32d9f6719f31972b5f9d8c184ba01fa8bf9a36736943fadac7d28cd98d": {
    "soal": "Column attribute names can be preceded by a label followed by a hash. Use c for class and m for meta attributes, i for ignoring columns, w for weight (of a column), and C , D , T , S for continuous, discrete, time , and ........... attribute types. Example: C#mph, mS#name, i#dummy.\n\nWhere ........ is,\n",
    "jawaban": "The correct answer is: string"
  },
  "88631c5adfccffcdf419f2af87fed62598505d426a5296c43a1ec67cb614f4ad": {
    "soal": "Contoh sederhana dengan Calibrated Learner. Kita menggunakan dataset titanic karena widget ini membutuhkan nilai binary class (dalam hal ini mereka adalah 'survived\u2019 atau \u2018not survived\u2019).\n\nKita menggunakan Logistic Regression sebagai base learner yang akan dikalibrasi dengan nilai setting default, yaitu dengan sigmoid optimization dari distribusi nilai dan di optimasi dengan CA.\n\nMembandingkan hasil dari uncalibrated Logistic Regression model kita akan melihat dengan jelas bahwa logistic regression model lebih baik.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "2763ed1a351d9ee22295ef3267d17cc33be15f11e16ef0b461599ceb7681abbb": {
    "soal": "Similar to R-programming, Apache Hadoop is open source. It is a framework tool created by \nAnswer Question 14\n and Apache. The Hadoop framework allows for the processing of larger amounts of data, storing heterogeneous data, and accelerating the processing time.\nAccording to AWS, Hadoop is an open-source framework that is highly effective for storing extremely large datasets. In addition to storing, this framework can also efficiently process data from gigabytes to petabytes.",
    "jawaban": "The correct answer is: Google"
  },
  "46c0ba5839c7c9719f50ed913538c69ccd9f3e7f7e484ae9fea40fbafb76643e": {
    "soal": "The CN2 Rule Induction widget will induce (induce) rules from the data using the CNNN algorithm.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "74626921f9d89c507ac53687c1bca1293846d83f33ac9085a4db135e119875d9": {
    "soal": "In the workflow below, we collect data using the Twitter widget. We collect tweets from users @HillaryClinton and @realDonaldTrump over two weeks, totaling 242 tweets.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "94ab2cd0f6acf9c2bbd59b55d4466dcf95a726916a9bb12ead726b078f20d141": {
    "soal": "import os\nimport requests\nfrom googlesearch import search\nfrom bs4 import BeautifulSoup\nimport re\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef clean_filename(text):\n# Hilangkan karakter ilegal untuk nama file\nreturn re.sub(r'[\\/*?:\"<>|]', , text).strip().replace(' ', '_')[:50]<%1\u001f%\u001f0%> def get_page_content(url):\ntry:\nheaders =\nresponse = requests.get(url, headers=headers, timeout=10)\nsoup = BeautifulSoup(response.content, 'html.parser')\ntitle = soup.title.string if soup.title else 'No Title'\nparagraphs = soup.find_all('p')\ncontent = '\n\n'.join([p.get_text() for p in paragraphs[:5]])\nreturn title.strip(), content.strip()\nexcept Exception as e:\nreturn 'Error', f\"Gagal mengambil konten: {e}\"\ndef save_to_txt(keyword, rank, title, url, content, folder='outputs'):\nos.makedirs(folder, exist_ok=True)\nfilename = f\"{clean_filename(keyword)}_{rank}_{clean_filename(title)}.txt\"\nfilepath = os.path.join(folder, filename)\nwith open(filepath, 'w', encoding='utf-8') as f:\nf.write(f\"Keyword : {keyword}\n\")\nf.write(f\"Peringkat : {rank}\n\")\nf.write(f\"Judul : {title}\n\")\nf.write(f\"URL : {url}\n\n\")\nf.write(content)\ndef scrape_and_save_txt(keywords, num_results=5):\nfor keyword in keywords:\nprint(f\"\n\ud83d\udd0d Searching: {keyword}\")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nprint(f\" \u2192 ({i+1}) Fetching: {url}\")\ntitle, content = get_page_content(url)\nsave_to_txt(keyword, i+1, title, url, content)\ntime.sleep(2)\nexcept Exception as e:\nprint(f\"\u274c Error saat mencari '{keyword}': {e}\")\nprint(\"\n\u2705 Semua konten telah disimpan di folder 'outputs/'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\nscrape_and_save_txt(keywords, num_results=5)\nWhich Python libraries are required to run the script?\n{\n=googlesearch-python, requests, beautifulsoup4\n~selenium, pandas, numpy\n~urllib, json, lxml\n~scrapy, flask, sqlalchemy\n~mechanize, xml, argparse\n~pyquery, chardet, configparser\n}",
    "jawaban": "The correct answer is: \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\""
  },
  "c9cad97b1dd1df3f6c2cb716143fec092d988c284993e3b5be2bf2b16e56da92": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nWhat is the dropout rate specified in the Dropout layer of the model?",
    "jawaban": "The correct answer is: 0.2"
  },
  "d38230f06233af9355e01bed6b58d2012fcf37172f743c5dae373aaea733265d": {
    "soal": "The Save Data widget can consider the dataset provided in the input channel and save it to a data file with a specified name. The ........................ widget can save the data as a file with data separated by tabs or separated by commas.\n",
    "jawaban": "The correct answer is: Save Data"
  },
  "066d3f67c8197edbc1f1aa80b4f5cdaae46fd9fae0219bc6cd238417960f8dff": {
    "soal": "\n\n\n\n\n\nWorkflow di atas melakukan test / evaluasi CN2 Rule Induction dan ..................\u00a0 di Widget Test & Score. Terlihat CN2 Rule Induction lebih baik\n",
    "jawaban": "The correct answer is: tree"
  },
  "bcc113698624170b80201a8374416eca5437933e5ec12e6ce1dc3df634418c3a": {
    "soal": "In the PCA Widget in ORANGE2 - the number of transformation components can be selected from the Components Selection input box or by dragging the vertical cutoff line on the graph.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "bd25401e8c8ad9f88fa073bcd109d42abcff04060ce95a0b40bb41f4a364f8fc": {
    "soal": "For PM4Py visualization, what must be installed first?",
    "jawaban": "The correct answer is: PM4Py and dependencies"
  },
  "a953efd0f56b719c89bfb08139e48bcc66719a33aff4899fdcc7e717419def15": {
    "soal": "Which validation method involves dividing the dataset into K parts and training the model K times, each time using a different part as the validation set?",
    "jawaban": "The correct answer is: K-Fold Cross Validation"
  },
  "86935bb54029be28fc9d56473f281830baff401c5079dc2d4f350cf872753ab7": {
    "soal": "Why is the context window critical in conversations?",
    "jawaban": "The correct answer is: It helps maintain relevance in replies"
  },
  "826e03dd8749f3766a0ea1bef6dfd0e1e6d88d8e4d6faddee9e32663f0e74a1a": {
    "soal": "For classic Machine Learning models like Random Forest or SVM, what is the ideal dataset size?",
    "jawaban": "The correct answer is: Thousands of data points"
  },
  "bb0e934c31b04d180bce91e740f5481b8ac6d7c63609ac01714e2906f44dfe5d": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nHow many classes are present in the CIFAR-100 dataset?",
    "jawaban": "The correct answer is: 100"
  },
  "ef39eca806afad9bdc339cfe2b547569214b94cafe5ad31cd2f6fc4b54b108c3": {
    "soal": "1010 (binary) + 10 (binary) = ....... (binary)",
    "jawaban": "The correct answer is: 1100"
  },
  "f408140f9dd57bc4bd21f7f370bd58d649611286a2532baa1d82a3d1a54c027e": {
    "soal": "In the following workflow example, using the zoo dataset and creating a clustering workflow with Distances and Hierarchical Clustering. Now set the threshold for cluster selection (click on the ruler above). Connect the Box Plot to Hierarchical Clustering, check Order by relevance, and select Cluster as a subgroup. This will sort the attributes by how well they define the selected subgroup, in our case, a cluster.\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "7c736f33aa3b353b1e1728e119d7f181b62dfb0ad996a82c5ecb6cfb1f1e4f16": {
    "soal": "What is the main advantage of using decision trees?",
    "jawaban": "The correct answer is: They are easy to interpret and visualize"
  },
  "9bf784ffda4ffbfaf4f13a9bb1069d5dc4f5f3d46c43e1ed1ebea033b4fdd325": {
    "soal": "import os\nimport string\nfrom collections import Counter\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\n# Buat daftar stopwords Bahasa Indonesia\nstop_words = set(stopwords.words('indonesian'))\ndef bersihkan_teks(teks):\n# Lowercase + hilangkan tanda baca\nteks = teks.lower()\nteks = teks.translate(str.maketrans('', '', string.punctuation))\nreturn teks\ndef analisis_folder(folder='outputs', top_n=20):\nsemua_konten = ''\nfor filename in os.listdir(folder):\nif filename.endswith('.txt'):\nfilepath = os.path.join(folder, filename)\nwith open(filepath, 'r', encoding='utf-8') as f:\nisi = f.read()\n# Ambil hanya bagian isi, setelah header\nif '\n\n' in isi:\nisi = isi.split('\n\n', 1)[1]\nsemua_konten += isi + ' '\n# Bersihkan teks dan tokenisasi\nteks_bersih = bersihkan_teks(semua_konten)\nkata_kata = teks_bersih.split()\n# Hapus stopwords\nkata_kata = [kata for kata in kata_kata if kata not in stop_words and kata.isalpha()]\n# Hitung frekuensi\ncounter = Counter(kata_kata)\ntop_kata = counter.most_common(top_n)\nprint(f\"\n\ud83d\udd0d Top Kata Paling Sering Muncul:\")\nfor kata, jumlah in top_kata:\nprint(f\"{kata}: {jumlah}\")\n# Buat Word Cloud\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(counter)\n# Tampilkan\nplt.figure(figsize=(12, 6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud dari Semua Konten')\nplt.tight_layout()\nplt.savefig('wordcloud.png')\nplt.show()\nprint(\"\n\u2705 Word Cloud disimpan sebagai 'wordcloud.png'\")\n# Jalankan analisis\nif __name__ == '__main__':\nanalisis_folder('outputs', top_n=30)\nWhich Python libraries are required to run the script?\n{\n=nltk, matplotlib, wordcloud\n~pandas, numpy, seaborn\n~requests, beautifulsoup4, lxml\n~tensorflow, keras, scikit-learn\n~flask, django, sqlalchemy\n~opencv, pillow, scikit-image\n}",
    "jawaban": "The correct answer is: top_n"
  },
  "365054df8dc33e29059bb0a2efe2394315e9f8a6618178ae1db211bf0cec1f41": {
    "soal": "1110111 (binary) AND 00001111 (binary)",
    "jawaban": "The correct answer is: 0000111"
  },
  "b034d9a7672c8765a95ee8dd4e006856a92ae622e2b32c890348c9420bc42751": {
    "soal": "The Tree algorithm is a simple algorithm that can separate data into nodes based on class purity (class/category purity). The Tree algorithm is the predecessor of the Random Forest algorithm. The Tree widget in Orange is designed in-house and can handle discrete and continuous datasets.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "33ee9efea059ba14fa46a22715ec65d990f22423041bcd28be5d0fd8051d64c8": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nx_train = np.random.rand(1000, 10)\ny_train = np.random.randint(2, size=(1000, 1))\nmodel = keras.Sequential([\nkeras.layers.Dense(64, activation='relu', input_shape=(10,)),\nkeras.layers.Dense(32, activation='relu'),\nkeras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer='adam',\nloss='binary_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=10, batch_size=32)\nWhich of the following layers is not used in the model?",
    "jawaban": "The correct answer is: Dropout"
  },
  "ecd75d574db619df1037f72ec8ca89c5a281196841e2bdc4e5b98237afa85480": {
    "soal": "Data Mining is essential for supporting the success of modern organizations driven by data. An IDG survey of 70 IT and business leaders found that 92% of respondents want to implement advanced analytics more widely across their organizations. The same survey found that the benefits of data mining are profound and vast.",
    "jawaban": "The correct answer is 'True'."
  },
  "cedf4629e6f40b8c64999885b953fa39fa711810bf5a417dcb92cb19d40bdc2a": {
    "soal": "The CN2 Rule Induction widget will induce (induce) rules from the data using the CN3 algorithm.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "bf437d55068196cb481fb3d0a5b2e704c5e337a4a054bbc73a4e1d9b4c0e5126": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#-xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\ndfg, start_activities, end_activities = pm4py.discover_dfg(log)\npm4py.view_dfg(dfg, start_activities, end_activities)\nHow many filenames start with 'training_log'?",
    "jawaban": "The correct answer is: 3 -2 -1 -4 -5 -0"
  },
  "e8c3d8d529bfffece994a656200d3906ce1b60d298149956de774af152e3132a": {
    "soal": "1011010 (binary) + 100101 (binary) + 1 =",
    "jawaban": "The correct answer is: 10000000"
  },
  "21b51eaf873afbb15628e85815396de97f2242fd33c59b848626830f30f46727": {
    "soal": "from google.colab import drive\ndrive.mount('/content/drive')\nWhich function in Google Colab is used to mount Google Drive?",
    "jawaban": "The correct answer is: drive.mount()"
  },
  "4200dfd903d5b548ca9e108fdba332ccd37c56eafd473239aca7bc39fa48ecdf": {
    "soal": "Widget Network Clustering tries to find clusters in a network. Network Clustering works with two algorithms, one from ................... et al. (2007), which uses label propagation to find the appropriate clusters, and one from Leung et al. (2009), which builds on the work of Raghavan and adds hop attenuation as a parameter for cluster formation.\n",
    "jawaban": "The correct answer is: Raghavan"
  },
  "83707e7c0ad7795fbe2fd6a84deceb0eda5d6e756b6322dddafa9abb097239b2": {
    "soal": "Orange uses its own data format but can also handle Excel, comma- or tab-delimited data files. Input datasets are usually in the form of tables, with data instances (samples) in rows and data attributes in columns. Attributes can consist of various types except ....... and have designated roles (input features, meta attributes, and class). The type and role of data attributes can be specified in the data table header.\n\nSelect ..... below,\n",
    "jawaban": "The correct answer is: file"
  },
  "61d1c1b4c6720681b97c4188267fdfde25bcf1f8cca6670050f78fd682bb57a9": {
    "soal": "The Import Documents widget takes text files from a folder and creates a corpus. The Import Documents widget can read .txt, .docx, ........................., .pdf, and .xml. If there are subfolders in the folder, they can be used to label the class.\n",
    "jawaban": "The correct answer is: odt"
  },
  "303f9ff9ed555c11bb487bb3d9d0012f81fb77a3cd6da14fe7f6db673dc8fcef": {
    "soal": "Network Analysis Widget calculates statistical summaries from node-level and graph-level for network. The Network Analysis Widget will output the network with its statistical computation results and an extended item data table (only node-level index).\n",
    "jawaban": "The correct answer is 'True'."
  },
  "d719890038413082993281377c35048450a038c233c1ffb84cf3e1ec38680d25": {
    "soal": "The Periodogram widget can visualize cycle, .........................., periodicity, and important periods in time series.\n",
    "jawaban": "The correct answer is: seasonality"
  },
  "ccd5178e3f7bde35cb6524b27ff3ea397b144789971a4969f1e65a7b6251f963": {
    "soal": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n# Contoh: Memuat dataset dari file CSV\n# Gantilah 'data.csv' dengan path ke dataset Anda\ndf = pd.read_csv('data.csv')\n# Memisahkan fitur (X) dan target \n# Asumsikan kolom terakhir adalah target\nX = df.iloc[:, :-1]\ny = df.iloc[:, -1]\n# Membagi dataset menjadi data latih dan data uji (80% : 20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model Decision Tree\nmodel = DecisionTreeClassifier()\n# Melatih model dengan data latih\nmodel.fit(X_train, y_train)\n# Melakukan prediksi pada data uji\ny_pred = model.predict(X_test)\n# Menghitung akurasi\nakurasi = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model: ')\n# Menampilkan laporan klasifikasi yang mencakup presisi, recall, dan F1-score\nprint(classification_report(y_test, y_pred))\nWhat does the `accuracy_score` function compute?\n{\n~The proportion of true positive predictions.\n~The harmonic mean of precision and recall.\n=The ratio of correctly predicted instances to the total instances.\n~The area under the ROC curve.\n~The mean absolute error.\n~The root mean squared error.\n}",
    "jawaban": "The correct answer is: akurasi:.2f"
  },
  "8b1c873cf04b4046e2cb1ab7742c82df882814d9da442e75af5c259b11958018": {
    "soal": "Data attributes in Orange have the type .............(1)............, continuous or character string. The attribute type is marked with a symbol that appears in front of the attribute name (............(2)..............., C, S).\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 discrete, (2) \u2192 D"
  },
  "a68cc8e2ace43a8b4725139c9c8df1f9c390e229613586d6bac64b6a414d7931": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.rand(100) # 100 nilai target\n# Inisialisasi model regresi linier\nmodel = LinearRegression()\n# Tentukan jumlah fold (misalnya, 5-fold cross-validation)\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n# Evaluasi model menggunakan cross-validation\nscores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n# Menampilkan skor R^2 untuk setiap fold dan rata-ratanya\nprint(\"Skor R^2 untuk setiap fold:\", scores)\nprint(\"Rata-rata skor R^2:\", np.mean(scores))\nWhat does the `print(\"Skor R^2 untuk setiap fold:\", scores)` statement do?",
    "jawaban": "The correct answer is: Prints the R\u00b2 scores for each individual fold."
  },
  "5ac60e3a15a8577df39be94614d1d65b8c8a1cbcb6207348720db3111ec47e46": {
    "soal": "In the Preprocess Text widget, we can do several things, such as:\n\nConvert all letters to lowercase.\nRemove stop words, words that are not useful, such as connecting words like \"and,\" \"in,\" \"to,\" \"from,\" etc.\nSet stopword processing in the Indonesian language.\nRemove JavaScript tags.\n\nRemove URLs.\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "b4d60cafa2d2eb282f905208b0b9795481ed2effdb00b3be73133a01d644a7fa": {
    "soal": "from googlesearch import search\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef get_page_content(url):\ntry:\nheaders =\nresponse = requests.get(url, headers=headers, timeout=10)\nsoup = BeautifulSoup(response.content, 'html.parser')\n# Ambil judul halaman\ntitle = soup.title.string if soup.title else 'No Title'\n# Ambil konten paragraf utama\nparagraphs = soup.find_all('p')\ntext_content = ' '.join([p.get_text() for p in paragraphs[:5]]) # Batasi 5 paragraf pertama\nreturn title.strip(), text_content.strip()\nexcept Exception as e:\nreturn 'Error', f\"Failed to fetch content: {e}\"\ndef google_scrape_with_content(keywords, num_results=5, output_file='scraped_results.csv'):\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'Rank', 'Title', 'URL', 'Content'])\nfor keyword in keywords:\nprint(f\"\n\ud83d\udd0d Searching for: {keyword}\")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nprint(f\" \u2192 Fetching: {url}\")\ntitle, content = get_page_content(url)\nwriter.writerow([keyword, i+1, title, url, content])\ntime.sleep(2) # Delay biar aman\nexcept Exception as e:\nprint(f\"\u274c Error while searching '{keyword}': {e}\")\nprint(f\"\n\u2705 All results + content saved to '{output_file}'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\ngoogle_scrape_with_content(keywords, num_results=5)\nWhat is the reason for including `time.sleep(2)` after fetching each URL's content?\n{\n=To prevent being flagged as a bot by introducing a delay between requests.\n~To allow the CSV file to save data properly.\n~To synchronize with the rate limits of the Google API.\n~To ensure that the search results are accurate.\n~To give the system time to process each search query.\n~To reduce the load on the local machine's CPU.\n}",
    "jawaban": "The correct answer is: \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\""
  },
  "482ba134d74f58e7ef4acbf71ba484e9b043b9f0512e803575fc01a3e7e87a8d": {
    "soal": "Data Science is an interdisciplinary field that does not use scientific methods, processes, algorithms, and systems to extract knowledge and insights from data in various forms, both structured and unstructured, similar to data mining.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "52d390e0a4d45a667b372eb4c9b612a94d5c52a32347881c04653d2dac3004bc": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(2, size=100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi training (80%) dan testing (20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model, misalnya RandomForestClassifier\nmodel = RandomForestClassifier(random_state=42)\n# Melatih model pada data training\nmodel.fit(X_train, y_train)\n# Memprediksi data testing\ny_pred = model.predict(X_test)\n# Mengevaluasi akurasi model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model pada data testing: ')\nHow many features does each sample in the dummy dataset have?\n{\n~5\n~8\n=10\n~12\n~15\n~20\n}",
    "jawaban": "The correct answer is: accuracy:.2f"
  },
  "2768bae7c2432434a50362bd67b359cad0b972758323f1417ad8c17ec81e6218": {
    "soal": "In the following example workflow, we use the zoo dataset and create a clustering workflow with Distances and Hierarchical Clustering. Now set the threshold for selection ......................... (click on the ruler above). Connect the Box Plot to Hierarchical Clustering, check Order by relevance, and select Cluster as a subgroup. This will sort the attributes by how well they define the selected subgroup, in our case, a cluster.\n\n\n\n\n",
    "jawaban": "The correct answer is: cluster"
  },
  "ae8db09b3729a5cf02c28b67f2e1baf04850034aa0fad112ff4e08ae032453e8": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nx_train = np.random.rand(1000, 10)\ny_train = np.random.randint(2, size=(1000, 1))\nmodel = keras.Sequential([\nkeras.layers.Dense(64, activation='relu', input_shape=(10,)),\nkeras.layers.Dense(32, activation='relu'),\nkeras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer='adam',\nloss='binary_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=10, batch_size=32)\nWhat kind of values does `y_train` contain?",
    "jawaban": "The correct answer is: Binary integers (0 or 1)"
  },
  "230c6220a6a3fd85db55d8fe6e0f7775a97cf887e675771456b8ffe732c03404": {
    "soal": "In the Orange workflow below, we can predict the classification of a text. The classification model is obtained using a logistic regression learner with training data from the Grimms Corpus. Text predictions from the Andersen Corpus are made by the Predictions widget.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "bf7c4b8a53e5f8792807092c3c482fdda4123c9149f1a0f43abd0ba86dd7a8b2": {
    "soal": "1110 (binary) + 1 (binary) =",
    "jawaban": "The correct answer is: 1111"
  },
  "468be76f066e66078e9920e811e749153fc7681722c7c2c69463d4bd47175933": {
    "soal": "import pandas as pd\nfrom pm4py.objects.conversion.log import converter as log_converter\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom pm4py.visualization.petrinet import visualizer as pn_visualizer\nfrom pm4py.objects.log.util import dataframe_utils\n# csv_filename='Insurance_claims_event_log.csv'\n# csv_filename='Business Travel Permits.csv'\n# csv_filename='Parent-Child Union Output.csv'\n# csv_filename='PM4PY_Dummy_Event_Log_100_Cases_pm4py.csv'\n# csv_filename='PM4Py_Dummy_Event_Log-congested-business-process.csv'\n# 1. Load CSV event log\ndf = pd.read_csv(csv_filename)\n# df = pd.read_csv(csv_filename,sep=';')\n# print(df.head())\n# 2. Format timestamps properly\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n# 3. Ensure column names match PM4Py expectations\ndf = dataframe_utils.convert_timestamp_columns_in_df(df)\n# 4. Convert to event log object\nevent_log = log_converter.apply(df, parameters=)\n# 5. Discover process model using Alpha Miner\nnet, initial_marking, final_marking = alpha_miner.apply(event_log)\n# 6. Visualize Petri Net\ngviz = pn_visualizer.apply(net, initial_marking, final_marking)\npn_visualizer.view(gviz)\nWhich PM4Py component transforms DataFrames into event logs?\n{\n~pm4py.dataframe\n~pm4py.events\n=log_converter\n~dataframe_utils\n~pm4py.log\n~alpha_miner\n}",
    "jawaban": "The correct answer is: log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case_id'"
  },
  "52d6a10fb21690187490902db2e06dcaeb680c4b53844ad0ceba6306946bf3c9": {
    "soal": "In the workflow below, the Tree widget will train a model and evaluate it against .............(1)........... . The accuracy comparison between the Tree widget and Logistic Regression widget is done through the Test & Score widget. The results from the ...............(2)............... widget can be seen through the Confusion Matrix widget to compare the classification errors.\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 Logistic Regression, (2) \u2192 Test & Score"
  },
  "edfa419050563a4b892ab540fb6a8bd038e90b40f9be5665b1df14e032b1ed50": {
    "soal": "from googlesearch import search\nimport csv\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef google_scrape_bulk(keywords, num_results=10, output_file='results.csv'):\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'Rank', 'URL'])\nfor keyword in keywords:\nprint(f\"\ud83d\udd0d Searching for: \")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nwriter.writerow([keyword, i+1, url])\ntime.sleep(1) # Delay agar tidak dianggap bot\nexcept Exception as e:\nprint(f\"\u274c Error while searching '{keyword}': {e}\")\nprint(f\"\n\u2705 All results saved to '{output_file}'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\ngoogle_scrape_bulk(keywords, num_results=10)\nWhat is the purpose of the `load_keywords` function in the script?\n{\n=To read search keywords from a file and return them as a list.\n~To perform Google searches for each keyword.\n~To write search results to a CSV file.\n~To clean and preprocess the keywords.\n~To validate the format of the keywords.\n~To remove duplicate keywords from the list.\n}",
    "jawaban": "The correct answer is: keyword"
  },
  "8f39e567eec32955feda3f1bef1df57fbf26ccbb1e29e9b0526f597f3e414722": {
    "soal": "Output from the Yahoo Finance Widget is a time series table of open, high, low, close (OHLC), volume, and adjusted close prices.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "c747811133a5af1f45592288caef8be7cc6f29f42785eba87a8cbf960cf27406": {
    "soal": "In the Scatter Plot widget, the default tool is Select, which selects data instances within the selected rectangular area. Pan allows us to move the scatter plot around with zoom. With Zoom, we can zoom in and out by moving the mouse, while Reset zoom resets the visualization to the optimal size.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "654ff57e4300156feb6594e142fa8b624199b698c5a47af275c88e55ce4f7d1c": {
    "soal": "Data mining is the process of extracting valuable information or patterns from large and complex datasets. Data mining uses techniques and algorithms to analyze data and identify hidden patterns, relationships, correlations, anomalies, or trends in the data. The main goal of data mining is to generate information that is not valuable and usable from large and complex data.\nSome common data mining techniques include clustering, classification, association, regression, and anomaly detection.\n- Clustering is a technique used to group data based on similarities in features or characteristics, so that similar data is grouped together.\n- Classification is a technique used to classify data into known categories or classes.\n- Association is a technique used to find relationships between features or items in the dataset, helping with analysis and recommendations.\n- Regression is a technique used to predict continuous variable values based on other variables.\n- Anomaly detection is a technique used to identify data that differs from existing patterns and can help in detecting fraud or errors.\nData mining can be used in various fields, including business, healthcare, education, science, and technology. Examples of data mining applications are sales prediction, credit analysis, health risk analysis, text mining, and social media analysis.",
    "jawaban": "The correct answer is 'False'."
  },
  "4242b57af48b3d368312812112e0cbf0ac85ff4d053c124275f4440326fdb83d": {
    "soal": "The ................... widget creates a distance matrix, which is a two-dimensional array containing distances, taken pairwise, between elements of the dataset. The number of elements in the dataset determines the size of the matrix. Data matrix is essential for the Hierarchical Clustering Widget.\n\n\n\n",
    "jawaban": "The correct answer is: Distance Matrix"
  },
  "3b668f69c56010fa779c07a5784520e826310da8be75f44c7fdee26d282d2fb3": {
    "soal": "import pm4py\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\n# Import XES log\nlog = xes_importer.apply(xes_filename)\n# Cek jumlah kasus dan aktivitas\nprint(f\"Jumlah kasus: \")\nactivities = set()\nfor trace in log:\nfor event in trace:\nactivities.add(event['concept:name'])\nprint(f\"Jumlah aktivitas unik: {len(activities)}\")\nfrom pm4py.algo.discovery.dfg import algorithm as dfg_discovery\nfrom pm4py.visualization.dfg import visualizer as dfg_visualization\n# DFG berdasarkan frekuensi\ndfg = dfg_discovery.apply(log,\nvariant=dfg_discovery.Variants.FREQUENCY)\n# Tampilkan visualisasi\ndfg_vis = dfg_visualization.apply(dfg, log=log,\nvariant=dfg_visualization.Variants.FREQUENCY)\ndfg_visualization.view(dfg_vis)\nHow does PM4Py identify bottlenecks?\n{\n=By analyzing durations between activities\n-Counting frequency of events\n-Exporting data to JSON\n-Converting data to XML\n-Counting rows in CSV files\n-Checking pandas summary statistics\n}",
    "jawaban": "The correct answer is: len(log)"
  },
  "162326e49e290c2c2cae47c72613c0751aa12d33873906020d6ce0a8cb05dc58": {
    "soal": "The easiest way to use the Venn Diagram widget is to select a data subset and find matching examples in the visualization. We use the breast-cancer dataset to select two subsets with the Select Rows widget - the first subset is breast cancer patients aged between 40 and 49 years, and the second is patients with tumor sizes between 20 and 29. The Venn diagram helps us find examples that fit both criteria, which can be found at the intersection of ................. circles.\n\n\n",
    "jawaban": "The correct answer is: two"
  },
  "b968517a3868f465c08734aab58ee0e530bb8afcb8e7dd9d5263adb6ac8befb5": {
    "soal": "from google.colab import drive\ndrive.mount('/content/drive')\nWhat is the default mount point for Google Drive in Google Colab?",
    "jawaban": "The correct answer is: '/content/drive'"
  },
  "76a9a68fbe010a28782c698d8851b0d06574cea9be99d3d47bf361a94bcca592": {
    "soal": "Since the output data type from the Time Series Widget ORANGE is a table, we cannot connect this data to other widgets/modules that can accept tables. Of course, we cannot use the output from the Time Series Widget ORANGE to test various functions available in other time series add-ons in ORANGE.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "17ef2d6dca80f1c853c4e99f84e90a87947877dafc6612811f0a697890a31499": {
    "soal": "The Line Plot widget is a standard visualization widget, which displays the ............ profile, usually in the form of ordered numeric data. In this simple example, we will display the iris data in a line plot, grouped by iris attributes. The plot shows how petal length separates the class values well.\n\nIf we observe this in the Scatter Plot Widget, we can confirm this is indeed true. Petal length is an interesting attribute for class separation, especially when enhanced with petal width, which is also well separated in the line plot.\n\n\n\n\n",
    "jawaban": "The correct answer is: numeric"
  },
  "92fe027aba9f3d61953b696a00c8621d0b3af4ca1bb7188ce5ea6c5d7b2795a5": {
    "soal": "Network Generator Widget can create an example Intranet network.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "d8605b0d43fec800462e89b3ba9667fd69ebd6080d632915a1d064681a30e1fb": {
    "soal": "What is a loss function in machine learning?",
    "jawaban": "The correct answer is: A metric to measure the difference between predicted and actual values"
  },
  "2e4e09dd4bf328821fe5b454d1b52378cbc052f0bc3e3ddbf8e18d0d8dfd07bb": {
    "soal": "The Tree widget can work for regression tasks. In the workflow below, we use the housing dataset and give the dataset to the Tree. The selected Tree node in the Tree Viewer will be displayed in the Line Plot, and we can see that the selected example has the same feature.\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "4d5a388bf71cb1a37617202820599e863ce210f4cc060f004e3a0ae8fb46f4d2": {
    "soal": "The .............. widget shows probabilities and the final decision of the prediction model. The output of the .............. widget is another dataset, where the predictions are added as a new meta attribute. We can choose which features we want to remove (original data, predictions, probabilities). The results can be observed in the Data Table. If the predicted data includes the actual class value, the prediction results can also be observed in the Confusion Matrix Widget.\n",
    "jawaban": "The correct answer is: Predictions"
  },
  "c79f798f5db6a546c86ad9188bfb89924344b51d284dbe2411cc7e040a0310f3": {
    "soal": "from google.colab import drive\ndrive.mount('/content/drive')\nHow can you save a file from Colab to your Google Drive after mounting?",
    "jawaban": "The correct answer is: By specifying the path within '/content/drive/MyDrive/' when saving the file."
  },
  "39964cdadf15735d231daae5715dd8a96b4308da417af24fff7524aa9b877d01": {
    "soal": "What is the primary task this TensorFlow model is designed for?",
    "jawaban": "The correct answer is: Classifying 28x28 pixel images, likely from the MNIST dataset"
  },
  "a300745c54a3f661f3e490e7c6b173d410c8822618eb855e639604b7b5e506c6": {
    "soal": "Data attributes in Orange have types ....................., continuous, or character string. The attribute type is marked by a symbol that appears before the attribute name (D, C, S).\n\n\n",
    "jawaban": "The correct answer is: discrete"
  },
  "242db0c65034c000a74ca12fe5645092165712d29db5922013c66af9e146d49b": {
    "soal": "CN2 Rule Induction hanya bisa jalan untuk .................\u00a0 saja.\n",
    "jawaban": "The correct answer is: klasifikasi"
  },
  "c45613f89b5550daa4f9ad1afe572453c56462b26d53944b404b693982fc41c9": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(2, size=100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi training (80%) dan testing (20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model, misalnya RandomForestClassifier\nmodel = RandomForestClassifier(random_state=42)\n# Melatih model pada data training\nmodel.fit(X_train, y_train)\n# Memprediksi data testing\ny_pred = model.predict(X_test)\n# Mengevaluasi akurasi model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model pada data testing: ')\nWhat does the `test_size=0.2` parameter specify in the `train_test_split` function?\n{\n~That 20% of the data is used for training.\n=That 20% of the data is used for testing.\n~That the data is split into 2 equal parts.\n~That 20% of the features are selected.\n~That the function should run 20 times.\n~That the random state is set to 20.\n}",
    "jawaban": "The correct answer is: accuracy:.2f"
  },
  "82b861a20114e2daa9077c863400bb66b44cddb81465172f9261651cc2e006b6": {
    "soal": "What message appears if GPU is available in Colab?",
    "jawaban": "The correct answer is: GPU is available!"
  },
  "7c1a94a85f1edf461b240b9ab005b3d4f8f55f3d19a5717be691448be9e9efac": {
    "soal": "Contoh penggunaan Widget ............... untuk task regresi di perlihatkan pada workflow di bawah ini. Workflow ini menunjukkan cara menggunakan Learner output. Untuk tujuan contoh ini, kita menggunakan dataset housing. Kita memasukkan model prediksi kNN ke dalam Predictions dan mengamati nilai yang diprediksi.\n\n\n\n",
    "jawaban": "The correct answer is: kNN"
  },
  "32d21ae29a98aea45c493d9e13d47b1d84e63c047fcc70518d170401da31fdb6": {
    "soal": "111 (binary) AND 101 (binary) = ................. (binary)",
    "jawaban": "The correct answer is: 101"
  },
  "5d872ad0086e84931eba62f9ff67b2d330397da86912a99dbd5e6f017cd0baae": {
    "soal": "The Tree algorithm is a simple algorithm that can separate data into nodes based on class purity (category/class). The Tree algorithm is the predecessor of the Random Forest algorithm. The Tree widget in Orange is designed in-house and can handle .................... and continuous datasets.\n",
    "jawaban": "The correct answer is: discrete"
  },
  "ae632d38796391de8d1094a84d68b6ba93e0833413b3d7e46efa80a0f26033a4": {
    "soal": "According to the official Zoho website, Zoho Analytics is a comprehensive, reliable, and scalable e-learning platform. Developers and system integrators (SI) can use this platform to develop and deploy custom analytic applications and integrations.\nAnother advantage of Zoho Analytics is that it is user-friendly, making it easy for users to upload and control data. Using Zoho Analytics, data practitioners can create multifaceted and custom dashboards. The platform is easy to use and implement.",
    "jawaban": "The correct answer is 'False'."
  },
  "d8f9038dac0a1f95af295451e4f7cb116181f5ded2867f724d7c45c17026f4ca": {
    "soal": "In K-Fold Cross Validation, how many times is each data point used for validation?",
    "jawaban": "The correct answer is: Exactly once"
  },
  "c9f40a04706286f225a96f4dea8dc0e5388bb82b5398d59941c84b392013ae0b": {
    "soal": "Using the workflow above, if we input purchase data from an e-commerce site, we will not be able to find customer segmentation and discover which groups of items are frequently bought together.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "0c23eaa5587bbed8e79be5d94b72eb378a2ea7a9f6753f8d131efb9166c6cb8b": {
    "soal": "The Confusion Matrix widget shows the proportion between predicted and actual class.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "ad14167c4e747dffe0fa2d551a763fb9976c04208e66b2d484d0e99db63b7f39": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.reuters.load_data(num_words=10000)\nWhich machine learning task is the Reuters Newswire dataset commonly used for?",
    "jawaban": "The correct answer is: Multi-class text classification"
  },
  "ac07f13708906d571d1c69b92c4bf797ad592d9b89ec1325a222f8c7b2b415f8": {
    "soal": "\n\n\nIn the process of building a machine learning model, model performance evaluation using classification is usually measured by accuracy, sensitivity, specificity, and MCC2.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "5eab15731a20ffa850375c153888a14b16962a5e7abc1b6d00b3f32cbcb32041": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nWhat is the primary function of the Flatten layer in the given model?",
    "jawaban": "The correct answer is: It converts the 28x28 input images into a 784-element vector."
  },
  "47cbe9a829c31f9f4401c16094047ae68211e5b521c57b28a48d4f6c491a44df": {
    "soal": "Which command lists all available commands in Ollama?",
    "jawaban": "The correct answer is: ollama help"
  },
  "b1128b09136377dcc6eb55bdd18531489e319ddb296918dcc22feef9dd0ba7ce": {
    "soal": "The ......................... widget can load a collection of text documents, optionally tagged with categories, or convert input data to a corpus.\n",
    "jawaban": "The correct answer is: Corpus"
  },
  "239de44c60497d2c28b3abf8591876700fe289702453374cbae2a4e79fa92a41": {
    "soal": "The following regular expression (regex)\n\n\nmatches words ending with the letter c\nis\n",
    "jawaban": "The correct answer is: \\b\\w+(C|c)\\b"
  },
  "afacdc4553e6c02e6f2f76cda5ad230465487714a405044ec205e03674868c6d": {
    "soal": "Column attribute names can be preceded by a label followed by a hash. Use c for ..... and m for meta attributes, i for ignoring columns, w for weight (of a column), and C, D, T, S for continuous, discrete, time, and string attribute types. Example: C#mph, mS#name, i#dummy.\n\nWhere ........ is,\n",
    "jawaban": "The correct answer is: class / class"
  },
  "ff1ea3c300aa1a4a8587a922700ad5c6671a62e7b6350dce6b98d2593557e843": {
    "soal": "import pandas as pd\ndf = pd.read_excel('path_ke_file.xlsx', sheet_name=0)\ndf_selected = df[['Kolom1', 'Kolom2']]\nprint(df_selected.head())\nWhich command is used to install the Pandas library in Python?",
    "jawaban": "The correct answer is: pip install pandas"
  },
  "ecd6ea90cad7effac4b4977c30e28ed3cb97eb17b93be5a08cdead8c2775c520": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\n# xes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\nxes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nprocess_tree = pm4py.discover_bpmn_inductive(log)\nbpmn_model = pm4py.convert_to_bpmn(process_tree)\npm4py.view_bpmn(bpmn_model)\nWhat is the entry point condition in the script?",
    "jawaban": "The correct answers are: if __name__, \"__main__\":, \"main\":, True:, __name__():"
  },
  "47c444f5bef3b271dfc3ead6b4ba561f32331d2376a8209c6ec91066956675f6": {
    "soal": "Which port on the host is mapped to port 8080 in the Open WebUI service?",
    "jawaban": "The correct answer is: 3000"
  },
  "951a4bb49b11e3fe8af92e922e7c4899ecfc4ba99aeafef9d94924bf50b29091": {
    "soal": "The Distance Map widget can visualize .................. between objects. The visualization is the same as if we were printing a table of numbers, except the numbers are replaced by colored dots.\n\n\n",
    "jawaban": "The correct answer is: distance"
  },
  "f828d0fe656c37ecc4975ff1ddde7b3e3fcbca3784d51c0a187a79810f2ce295": {
    "soal": "\n\n\n\n\n\nClassification is performed using the linear regression algorithm/module, and the results are immediately assessed for accuracy by the Test & Score module.\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "c768ca54361c396657164af5a9c0591b2db0c23b07f2a0ef68059c3355050272": {
    "soal": "If the Import Documents widget fails to read a particular file for one reason or another, that file will be ......................... skipped. The successfully read files will be sent to the output.\n",
    "jawaban": "The correct answer is: loncat"
  },
  "2a740c573f29f7ed634eec2eda3624bb1c4b896b5a4937e99ae4118ac60367f5": {
    "soal": "001 (binary) x 100 (decimal) = ....... (decimal)",
    "jawaban": "The correct answer is: 100"
  },
  "fc7b930126d90e0b1a93791e931463ef5bbbdd52e90564991013a567984491ef": {
    "soal": "Column attribute names can be preceded by a label followed by a hash. Use c for class and m for meta attributes, i for ignoring columns, w for weight (of a column), and C , D , .......... , S for continuous, discrete, time, and string attribute types. Example: C#mph, mS#name, i#dummy.\n\nWhere ........ is,\n",
    "jawaban": "The correct answer is: T"
  },
  "e2e96a211ad6a5a5c158ebe3fede0c5a6c2df73030fb8a8b87e56efb927eb57b": {
    "soal": "Network Generator Widget creates an example network. It is intended for teaching/learning about intranet.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "99b5316704dc5e3f2f167821628a2f9963b92b9e5cd6b20b2bfd621f1ea9a40b": {
    "soal": "The Tree algorithm is a simple algorithm that can separate data into nodes based on class purity (category/class). The Tree algorithm is the predecessor of the ....................... algorithm. The Tree widget in Orange is designed in-house and can handle both discrete and continuous datasets.\n",
    "jawaban": "The correct answer is: Random Forest"
  },
  "0a2d23d3ccd8aa316607a7b2715e022f5aff6da44b978888dad5b1acb8b55db6": {
    "soal": "Widget SQL Table accesses data stored in an SQL database. The SQL widget can connect to PostgreSQL (requires the psycopg2 module) or SQL Server (requires the pymssql module).\n",
    "jawaban": "The correct answer is 'True'."
  },
  "378cce5862484478eb646020be425468358478b791e51653b3052fa2bad2bbfd": {
    "soal": "The Inductive Visual Miner algorithm tends to be slower on:",
    "jawaban": "The correct answer is: Large datasets"
  },
  "b6992dcbd2b6b68195266245eaac96ba52619d4ffdd9e5013e05b972068e2157": {
    "soal": "Data mining is the process of extracting valuable information or patterns from large and complex datasets. Data mining uses techniques and algorithms to analyze data and identify hidden patterns, relationships, correlations, anomalies, or trends in the data. The main goal of data mining is to generate valuable and usable information from large and complex data.\nSome data mining techniques that are not commonly used include clustering, classification, association, regression, and anomaly detection.\n- Clustering is a technique used to group data based on similarities in features or characteristics, so that similar data is grouped together.\n- Classification is a technique used to classify data into known categories or classes.\n- Association is a technique used to find relationships between features or items in the dataset, helping with analysis and recommendations.\n- Regression is a technique used to predict continuous variable values based on other variables.\n- Anomaly detection is a technique used to identify data that differs from existing patterns and can help in detecting fraud or errors.\nData mining can be used in various fields, including business, healthcare, education, science, and technology. Examples of data mining applications are sales prediction, credit analysis, health risk analysis, text mining, and social media analysis.",
    "jawaban": "The correct answer is 'False'."
  },
  "3028fa7e4a2a1e6cb5cfe16dd8c2e4c7ad2cc8f441533ea923e77ef1c33ab474": {
    "soal": "In the Interpolate widget, which is part of the Time Series add-on, the interpolation type - Mean interpolation replaces missing values with the minimum values of the series.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "48102d56058bc0fada850ae6da8509a3e1b2fa2ea43166a475036db16b018d48": {
    "soal": "Widget Import Images is likely the first widget we will use in image analysis. Widget Import Images loads images and creates class values from the folder. In this example, we are using Widget Import Images to load 26 paintings by Monet or Manet.\n\n\nWe can observe the results in the Data Table widget. It is clearly visible how Orange adds an additional class attribute with values Monet and Manet.\n\n\nNext, we can proceed to the standard machine learning method. We send the image to the Image Embedding widget, where we will use the Painters embedder to receive the image vector.\n\n\nThen we will use the Test & Score widget and the Linear Regression widget to create a model to predict the painter of a painting (in this case, Monet or Manet). We get a very good score. How is that possible? It turns out these images are images that have already been trained with the Painters embedder, so the accuracy obtained will be high.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "954b839d7c826c3ea94090f9a58d2e4d85200849abbfdbdc2713662419dafaba": {
    "soal": ":\nWhat does the F1-Score represent in a classification model?",
    "jawaban": "The correct answer is: The harmonic mean of Precision and Recall."
  },
  "2d8a2a087eb2f1317f68186edb41ad4a3573fe01fd720d013e6a3a6a58e98f89": {
    "soal": "What is a recurrent neural network (RNN) primarily used for?",
    "jawaban": "The correct answer is: Time series prediction and sequential data processing"
  },
  "e6b7084e9e31c29e4e126093dfeb44f73ed153f45e17d729e1f48d1e9fad66be": {
    "soal": "Word Cloud can be created from the Documents we have. In the text mining toolbox, there is an Import Documents widget to read document files. Before displaying them as a word cloud, it is advisable to pass them through the ............................ widget first, to reduce unnecessary words like conjunctions, etc. Then we can pass it through the Bag of Words widget first, or directly to the Word Cloud widget to display.\n\n\n\n\n",
    "jawaban": "The correct answer is: Preprocess Text"
  },
  "cee19e32fad9f9f483f183b31321caf35b91ee3dc856ea58d0cd9756e69d2632": {
    "soal": "instalasi orange3 yang baik bisa menggunakan perintah\n\npip3 install numpy scipy mkl nose sphinx pydot-ng pandas-datareader\npip3 install parameterized Theano dodol matplotlib sklearn seaborn h5py\npip3 install matplotlib pandas scipy sklearn seaborn features\npip3 install orange3\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "b63b1d6ced677c008bdd6d241cd858b1fbc4562bca436b3ed750471c543a6b25": {
    "soal": "The Data Info widget is a simple widget that presents information about the dataset size, ..................., target, meta attributes, and location.\n\n\n",
    "jawaban": "The correct answer is: feature"
  },
  "ced13652d3f45a5821024fab413d1019f43278dc0e36a28d62451a640929283c": {
    "soal": "Orange uses its own data format, but it can also handle Excel, comma- or tab-delimited data files.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "384145ed8050977c357af35786e9f6288cc2b63ab7899a9f7bdd4ca04fe77b94": {
    "soal": "Which command installs 'virtualenv' on Ubuntu 24.04?",
    "jawaban": "The correct answer is: sudo apt install python3-virtualenv"
  },
  "2446619be4ffd06bcb0892e737fbcefe5e39295a74af00b6b6c5abef450dda26": {
    "soal": "URL that is NOT a place for Orange data mining examples/tutorials is\n",
    "jawaban": "The correct answer is: ORKA0hBJ80k"
  },
  "6a74d156a0d92f222b7b8b2c3a4110f38f0f66d6e64453bf30e60ac92b991452": {
    "soal": "Widget Save Data can consider the dataset provided in the input channel and save it to a data file with the specified name. Widget Save Data can save data as a file with underscore-separated or comma-separated data.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "643f1f103d0d8c28086a5ce8fdff9e4eee5a380dc9d2a94d6418a2338d1b40ef": {
    "soal": "!pip install googlesearch-python nltk matplotlib wordcloud\nfrom googlesearch import search\nimport csv\ndef google_scrape(keyword, num_results=40,\noutput_file='results.csv'):\nprint(f\"Searching Google for: \")\n# Lakukan pencarian dan ubah hasil ke dalam list\nresults = list(search(keyword, num_results=num_results))\n# Simpan ke file CSV\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'URL'])\nfor url in results:\nwriter.writerow([keyword, url])\nprint(f\"Saved {len(results)} results to '{output_file}'\")\n# Contoh penggunaan\nkeyword = \"berita terbaru teknologi Indonesia\"\ngoogle_scrape(keyword, num_results=40)\nWhat is the role of 'csv.writer' in the provided script?\n{\n=To write the search results into a CSV file.\n~To read data from an existing CSV file.\n~To format the search results as a CSV string.\n~To append new data to an existing CSV file.\n~To convert the search results into a dictionary.\n~To parse CSV data from a web response.\n}",
    "jawaban": "The correct answer is: keyword"
  },
  "203326ab03a060b932ca82f75a57c08898b643ff0f2b71b4a31538a33f60e437": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.rand(100) # 100 nilai target\n# Inisialisasi model regresi linier\nmodel = LinearRegression()\n# Tentukan jumlah fold (misalnya, 5-fold cross-validation)\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n# Evaluasi model menggunakan cross-validation\nscores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n# Menampilkan skor R^2 untuk setiap fold dan rata-ratanya\nprint(\"Skor R^2 untuk setiap fold:\", scores)\nprint(\"Rata-rata skor R^2:\", np.mean(scores))\nWhat does the `scoring='r2'` parameter specify in the `cross_val_score` function?",
    "jawaban": "The correct answer is: That the coefficient of determination (R\u00b2) is used as the evaluation metric."
  },
  "ccd7e62d947d7e0216a129f440e1ec5fbd45bd284e531a35401910e7b7337d95": {
    "soal": "Which metric is commonly used to evaluate classification models?",
    "jawaban": "The correct answer is: Accuracy"
  },
  "fcc5f178a32686d0e2e647d2c5663ff330ae7e8050402d17b20a0d8d936f1fa9": {
    "soal": "CN2 Rule Induction bisa jalan untuk klasifikasi maupun regresi.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "c1de2a8d3ec28ed1fa9d4875b7a6c7bf8c97e993715355b335b81ddd3b467ec6": {
    "soal": "In the image below, we use the Network Generator Widget to create a Grid graph with a height of 4 and a width of 5 and send it to Network .................. . We can calculate node degrees and send the data to Network Explorer. Finally, we observe the resulting graph in the visualization and set the node size and color to Degree. Network Explorer is a good tool for observing and explaining network properties.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Analysis"
  },
  "a5008bcc97bb46b060992021b23c5d37a837e4c7cde85dd1fdc0779ac36de809": {
    "soal": "This workflow example shows how Network Analysis can enrich the workflow. We have used the lastfm.net data as input network from Data File and sent it to Network Analysis. We can decide to calculate the degree, degree centrality, and closeness centrality at node-level.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "663169610a08ae558e1d7ba574357b506d3f26354c8102546ad4ba493abd3ab9": {
    "soal": "The output data type from the Time Series Widget ORANGE is a table, so we cannot connect this data to other widgets/modules that can accept tables.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "912a4c0912d796c7c18a25df9a726160dda971465d3fb1c5c94e6e0152a2278d": {
    "soal": "Data Mining has the power to transform businesses; however, implementing processes that meet the needs of all stakeholders often hinders successful data mining investments \u2014 78% of respondents say they struggle to find the right data mining strategies or solutions.",
    "jawaban": "The correct answer is 'True'."
  },
  "5abbc42d701010994b1d6095ea9fd1e04c30cf618f856bd77aabef360002c1e8": {
    "soal": "The basic framework of Apache Hadoop consists of the following modules:\n- Hadoop Common - contains libraries and utilities required by other Hadoop modules;\n- Hadoop Distributed File System (HDFS) - a distributed file system that stores data on commodity machines, providing very high aggregate bandwidth in clusters;\n- Hadoop \nAnswer Question 26\n - a resource management platform responsible for managing computing resources in the cluster and using them for user application scheduling; and\n- Hadoop MapReduce - a programming model for large-scale data processing.",
    "jawaban": "The correct answer is: YARN"
  },
  "a46ed5a344ecdb97ec990206cb292e077697ee8d9928c2ee6affff9f3cc4e957": {
    "soal": "Just like R-programming, Apache Hadoop is open source. It is a tool framework created by Yahoo and Apache. The Hadoop framework allows for processing more data, storing heterogeneous data, and accelerating its processing.\nAccording to AWS, Hadoop is an open-source framework that is very effective for storing very large datasets. In addition to storing, this framework can also process data ranging from gigabytes to petabytes efficiently.",
    "jawaban": "The correct answer is 'False'."
  },
  "bcd8fb249366c593182382cc3ac676a3bee7949d58c1ee78880618d1a703a8c1": {
    "soal": "In the Constant Widget, the Learner will produce a model that always predicts ............ for classification or the mean value for regression.\n",
    "jawaban": "The correct answer is: majority"
  },
  "044c2aed28a2223a4c6e6804188b41d778d8ef67beebf1a80af2e14c3d39d2f3": {
    "soal": "Which command runs a model with a predefined prompt?",
    "jawaban": "The correct answer is: ollama run model \"Your prompt here\""
  },
  "4663bcdb6edaf38878d7a007161949faa83761b0c4112e3f0dbeb278b0ce77fe": {
    "soal": "import pandas as pd\nfrom pm4py.objects.conversion.log import converter as log_converter\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom pm4py.visualization.petrinet import visualizer as pn_visualizer\nfrom pm4py.objects.log.util import dataframe_utils\n# csv_filename='Insurance_claims_event_log.csv'\n# csv_filename='Business Travel Permits.csv'\n# csv_filename='Parent-Child Union Output.csv'\n# csv_filename='PM4PY_Dummy_Event_Log_100_Cases_pm4py.csv'\n# csv_filename='PM4Py_Dummy_Event_Log-congested-business-process.csv'\n# 1. Load CSV event log\ndf = pd.read_csv(csv_filename)\n# df = pd.read_csv(csv_filename,sep=';')\n# print(df.head())\n# 2. Format timestamps properly\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n# 3. Ensure column names match PM4Py expectations\ndf = dataframe_utils.convert_timestamp_columns_in_df(df)\n# 4. Convert to event log object\nevent_log = log_converter.apply(df, parameters=)\n# 5. Discover process model using Alpha Miner\nnet, initial_marking, final_marking = alpha_miner.apply(event_log)\n# 6. Visualize Petri Net\ngviz = pn_visualizer.apply(net, initial_marking, final_marking)\npn_visualizer.view(gviz)\nWhat's the main role of Graphviz in PM4Py?\n{\n=Visualize Petri Nets\n~Load CSV files\n~Convert DataFrames\n~Analyze events\n~Process data\n~Install packages\n}",
    "jawaban": "The correct answer is: log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case_id'"
  },
  "09a80e21fa7c948c80cac3cc917e4beb3728ac053109336f350cf1fae3d212c6": {
    "soal": "What is the primary purpose of 'one-hot encoding'?",
    "jawaban": "The correct answer is: To convert categorical variables into a binary vector representation"
  },
  "d68b7d1d2546ca45a4cbc95424ac5e149f7919fdb3fa4d81134bca563ac61e82": {
    "soal": "What should you do first to use ProM?",
    "jawaban": "The correct answer is: Download and install ProM"
  },
  "2336e8ef0d975157fe0559f2bf3b03cd3020b99a1e99233953f13eabd48e58ac": {
    "soal": "The Confusion Matrix widget provides the count/proportion of instances between predicted and actual ............... . Selecting elements within the matrix will provide the corresponding instances to the output. In this way, we can observe which specific instances were misclassified and how.\n",
    "jawaban": "The correct answer is: class"
  },
  "9a05401948b11bca1a70553aa034d1897b2801fb1ecba9f2e489b11a8ddffd07": {
    "soal": "100 (binary) x 100 (decimal) = ....... (decimal)",
    "jawaban": "The correct answer is: 400"
  },
  "92b62484b698ccdeef724e602e11f1f369f5421b9b9d9542aaf3d477100a9d02": {
    "soal": "In the Import Documents Widget, to fetch data, select the folder icon on the right side of the widget. Choose the folder that we want to use as ..............(1)............. After loading is complete, we will see how many documents were successfully retrieved by the Import Documents Widget. To observe the obtained corpus, connect ..............(2)............... to the Corpus Viewer Widget. In this workflow, we are using a collection of President Kennedy's speeches in plain text format.\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 Corpus, (2) \u2192 Import Documents"
  },
  "21539dcad9448a4350b96d3cacd7449ccbbe3db3b131556936af8d490ed7f099": {
    "soal": "Network Clustering Widget can help us reveal clusters and highly connected groups in a network. First, we will use the Network File Widget to load the lastfm.net dataset. Then we will send the network to the Network Clustering Widget. The Network Clustering Widget finds 79 clusters in the network. To visualize the results, we use the Network Explorer Widget and set the Line Size attribute to Cluster. This will color the network nodes with the corresponding cluster color - this is a good way to visualize highly connected groups in a dense network.\n\nNote that the Network Explorer Widget here will color the 10 largest clusters and color the rest as 'Other'.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "2fda36736cf266f68ecf16edd2f66a2665b71c875a171932fdab0739ebe0833a": {
    "soal": "RapidMiner, formerly known as YALE (Yet Another Learning Environment), is open-source software. This software serves as a solution for performing data mining, text mining, and predictive analysis.\nRapidMiner uses various descriptive and predictive techniques to provide insights to users, enabling them to make the best decisions. RapidMiner is written in Java, allowing it to work only on Windows.",
    "jawaban": "The correct answer is 'False'."
  },
  "cddd0df647093767b89ee036ded25d35c013873d6d113f75f4fb9381b7b72937": {
    "soal": "To cluster based on similarity, we can measure the distance between images. The numerical representation of the images obtained from the Image Embedding widget, we measure the distance using the .......................... widget. Typically, cosine distance works best for images, but you can experiment yourself. We then send the distance matrix to Hierarchical Clustering to visualize similar pairs in the dendrogram.\n\nThe result is that all the animals are correctly grouped. In the dendrogram, we do not see the images of the animals. We can use the Image Viewer widget to view the images.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Distances"
  },
  "e1771d33881ba36ccd1e4b851e2b94332a7648389b79789a31aab3ee429559d7": {
    "soal": "In the workflow below, we use the heart disease data and select the women subset from the Scatter Plot. Then, we visualize the distances between columns in the Distance Map. Since the subset also contains some discrete data, the Distances widget warns us that it will ignore the discrete features, so we will only see continuous instances/attributes in the map.\u00a0\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "b718efb33732f3aeb958f93275a0673db7a59984e364bcb21759b6ea4eda9371": {
    "soal": "In this example, we will just check what a bag of words model looks like. Load book-excerpts.tab using the .................(1)................ widget and connect it to the Bag of Words widget. Here, we intentionally use the default parameter - the simplest count is term frequency. Check what is output by the widget .................(2).............. using the Data Table widget. The last column represents the term frequency of each document.\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 Corpus, (2) \u2192 bag of words"
  },
  "d250c884872920acd82f0ab11757161f497240bb10001afca6bb81fdbb5ba134": {
    "soal": "Word Cloud data can be built from the text files (ASCII) we have, as in the workflow below. First, data from the ...................... Widget must be segmented into words using the Segment Widget. Then, the output segmented data needs to be converted from segmented data into a corpus so that it can be processed by the Widget Interchange in the text mining toolbox. Before displaying it as a word cloud, it is recommended to do preprocessing first to reduce unnecessary words, such as conjunctions, etc., using the Preprocess Text Widget.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Text Files"
  },
  "d056a5e28e1b8b5aa6b8d6519ea44341d3b81495b0d13ce1b2d111e357d910ec": {
    "soal": "\n\n\nThe workflow above tests/evaluates CN2 Rule Induction and Tree in the Widget Test & Score.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "a00407404b064c897dd09e9a12e813b97a608aa62d0eb0472399fb086f3de2f3": {
    "soal": "The MapReduce and HDFS components of Apache Hadoop were inspired by Google's papers on MapReduce and the Google File System.",
    "jawaban": "The correct answer is 'True'."
  },
  "1d7dcbca48837cfbfb077aad5b4ef0cdb6b692ce350a96541f13fd512130dc97": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#-xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\ndfg, start_activities, end_activities = pm4py.discover_dfg(log)\npm4py.view_dfg(dfg, start_activities, end_activities)\nWhat will the statement 'log = pm4py.read_xes(xes_filename)' return?",
    "jawaban": "The correct answer is: An event log data structure -A string filename -A numeric data type -A Boolean variable -An image object -A CSV object"
  },
  "24735a40b5aed494763eea79872b9feba704186a3b8f8016e5125589e8068994": {
    "soal": "The Sentiment Analysis widget allows basic sentiment analysis of corpora. It currently works for English and uses two techniques supported by NLTK - Liu Hu and Vader. Both techniques are lexicon-based. Liu Hu calculates a single normal score for sentiment in text (negative score for negative sentiment, positive for positive sentiment, and 0 for neutral), while Vader generates scores for each category (positive, negative, neutral) and adds a total sentiment score called the compound score.\n\n\n\n\nLiu Hu score\nVader score\n",
    "jawaban": "The correct answer is 'True'."
  },
  "9e53c07620a530d9db5b6b0b7e40508e40218e597ec769edfd9bc67aba374759": {
    "soal": "Widget ....(1)..... shows ....(2).... and the final decision of the prediction model. The output of Widget ....(1)..... is another dataset, where predictions are added as a new meta attribute. We can choose which features we want to remove (original data, predictions, ....(2)....). The results can be observed in the Data Table. If the predicted data includes actual class values, the prediction results can also be observed in Widget ....(3)......\n",
    "jawaban": "The correct answer is: (3) \u2192 Confusion Matrix, (2) \u2192 Probability, (1) \u2192 Predictions"
  },
  "1f2bca311fbc4c91043ad95ba4bd4ce86c70a271cb830c9d32c8533f6fcd9057": {
    "soal": "In the next example, we will try to predict the category of documents. We use the book-excerpts.tab dataset, which we send through the Preprocess Text widget with default parameters. Then we connect the Preprocess Text widget to the Bag of Words widget to obtain term frequency. With the term frequency information, we will calculate the model.\n\nConnect the Bag of Words widget to the Test & Score widget for predictive modeling. Connect the SVM widget or another classifier widget to the Test & Score widget (all on the left). The Test & Score widget will calculate performance scores for each learner input. Here, we obtain excellent results for the SVM widget.\n\nNext, we need to check where the model made errors. Add the ................. widget to the Test & Score widget. The Confusion Matrix widget will display the documents that were classified correctly and misclassified. Select \"misclassified\" to output the misclassified documents, which we will further examine using the Corpus Viewer.\n\n\n",
    "jawaban": "The correct answer is: Confusion Matrix"
  },
  "e33d049369593f1225a894a997993db3abbd4554172470d54aa3a750a543b13f": {
    "soal": "Which platform provides a wide range of datasets for various Machine Learning and Deep Learning purposes?",
    "jawaban": "The correct answer is: Kaggle Datasets"
  },
  "7fded83428928594edab5c56f435a79f843f33e77b8df418273b46e95a358c98": {
    "soal": "Pada Widget Constant ORANGE - untuk classification, ketika memprediksi nilai class dengan Prediction, widget ini akan menghasilkan frekuensi relative dari class yang ada di training set. Jika ada dua atau lebih majority class, classifier akan memilih secara random dari predicted class, tapi akan selalu menghasilkan class yang sama untuk contoh tersebut.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "e33066117320a9b3276f947aafc2acb79166c6f2fc69a9f163a91335ab2572f0": {
    "soal": "import pandas as pd\n# Contoh: Membuat DataFrame dengan missing values\ndata =\ndf = pd.DataFrame(data)\n# Menampilkan jumlah missing values di setiap kolom\nprint(\"Jumlah missing values per kolom:\")\nprint(df.isnull().sum())\n# Menghapus baris yang memiliki missing values\ndf_cleaned = df.dropna()\nprint(df_cleaned)\n# Mengisi missing values pada kolom 'Usia' dengan mean\ndf['Usia'].fillna(df['Usia'].mean(), inplace=True)\n# Mengisi missing values pada kolom 'Usia' dengan median\ndf['Usia'].fillna(df['Usia'].median(), inplace=True)\n# Mengisi missing values pada kolom 'Kota' dengan modus\ndf['Kota'].fillna(df['Kota'].mode()[0], inplace=True)\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n# Inisialisasi imputer untuk mengisi missing values dengan mean\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n# Mengimputasi kolom 'Usia'\ndf[['Usia']] = imputer.fit_transform(df[['Usia']])\nWhich of the following functions can also be used to detect missing values in pandas?\n{\n=isna()\n~dropna()\n~fillna()\n~notna()\n~replace()\n~interpolate()\n}",
    "jawaban": "The correct answer is: 'Nama': ['Andi', 'Budi', 'Citra', 'Dewi'], 'Usia': [25, 30, None, 22], 'Kota': ['Jakarta', None, 'Bandung', 'Surabaya']"
  },
  "7055009c2abbbfd3d298666119c2cbb0604783ad7c6a3cbabb02b6ab2e760318": {
    "soal": "import numpy as np\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n# Contoh data: nilai aktual dan prediksi\ny_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0]) # Nilai aktual\ny_pred = np.array([1, 0, 1, 0, 0, 1, 0, 1, 1, 0]) # Prediksi model\n# Menghitung confusion matrix\ncm = confusion_matrix(y_true, y_pred)\n# Menampilkan confusion matrix dalam bentuk teks\nprint(\"Confusion Matrix:\n\", cm)\n# Menampilkan confusion matrix dalam bentuk grafik\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.show()\nApa tujuan dari parameter display_labels dalam fungsi ConfusionMatrixDisplay?",
    "jawaban": "The correct answer is: Untuk menetapkan label kelas yang ditampilkan pada sumbu plot."
  },
  "655910db7cf625900f3f848e85feb10b2968c4bb574a45fd5136c4b6675c530e": {
    "soal": "Which of the following is a TensorFlow operation for element-wise multiplication of two tensors?",
    "jawaban": "The correct answer is: multiply()"
  },
  "6e531e82435ef3ab3bfb476b42eb185ef643ac2e9e9999fd7ce5d010d01a6f72": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nHow many neurons are present in the first Dense layer of the model?",
    "jawaban": "The correct answer is: 128"
  },
  "5ad688bc3a118e8dc3f381504bd59b6802d7a223ea3cad22938171245c1ce9dd": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#_xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nmap = pm4py.discover_heuristics_net(log)\npm4py.view_heuristics_net(map)\nThe script is an example of what kind of process mining analysis?",
    "jawaban": "The correct answer is: Heuristic mining -Conformance checking -Predictive analysis -Clustering -Classification -Sequence mining"
  },
  "38fc34808df8d87c7734acfa924c8f68371f84c1f8537a5b2a6387d1b42182d4": {
    "soal": "\n\n\n\n\n\nThe workflow above performs testing/evaluation of CN2 Rule Induction and .................. in the Test & Score Widget. It appears that CN2 Rule Induction is better\n",
    "jawaban": "The correct answer is: tree"
  },
  "76af47177ec8b11e515599c8c32146ee4ceec2e4f84e5d2b783804dbfa5a916d": {
    "soal": "In the following example, we will look at how to properly use Preprocess with the Predictions widget or the ................... widget\n\nThis time, we are using the heart disease.tab data from the File widget. We can access the data through the dropdown menu. This is a dataset with 303 patients who visited the doctor suffering from chest pain. After tests were conducted, some patients were found to have narrowed arteries, while others did not (this is our class variable).\n\nThe heart disease data has some missing values, and we want to handle that. First, we will split the dataset into training and test data using the Data Sampler.\n\nNext, we will send the Data Sample into Preprocess. We will use the Impute Widget to address the missing values, but we can try combinations of preprocessors on our data. We will send the processed data to the Logistic Regression Widget and the built model to the Predictions Widget.\n\nFinally, the Predictions Widget also needs data to predict. We will use the output of the Data Sampler Widget for predictions, but this time not the Data Sample, but the Remaining Data (the remaining data), which is data not used for training the model.\n\nNotice how we send the remaining data directly to the Predictions Widget without applying any preprocessing. This is because Orange handles preprocessing on new data internally to avoid errors in model construction. The exact same preprocessor used on training data will be used for predictions. The same process applies to the ...................\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Test & Score"
  },
  "66bffe6c4caeeb163fb91eb903d5ee4f18601ed88edf5bf840d083c218d448eb": {
    "soal": "Widget CSV File Import can import data tables from TXT formatted files.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "3c7b5e46a08ba10e5f860e4596318a5d66199e1983236dbfdcb0ec358add1340": {
    "soal": "111 (binary) + 11 (binary) = ........ (binary)",
    "jawaban": "The correct answer is: 1010"
  },
  "1e8f444aef757e8f7c4a0bbcd98a8bf39111b4286a5f83ab8b9bb203ba9c36b3": {
    "soal": "Convert the binary number 11011 to decimal...",
    "jawaban": "The correct answer is: 27"
  },
  "e30c7edfc5dabea189be5c83f54abc6814495853c7a1fd9390e1f6ef5a747bc2": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nHow many images are designated for training in the Fashion-MNIST dataset?",
    "jawaban": "The correct answer is: 60,000"
  },
  "1239be9e3f5c82fac36987b500727d80180a7affe38335a6cf72dd8d070e8e78": {
    "soal": "What does the 'fit()' method do in Keras?",
    "jawaban": "The correct answer is: Trains the model on training data"
  },
  "03fa28247d770bf029b9dccc71ac97018a41cc6af380e7390a2c5d9db80957f9": {
    "soal": "Big data is a term that refers to the very large and complex amount of data obtained from various sources, including business transactions, social data, census data, medical data, scientific data, and others. The main characteristics of big data are volume, velocity, and variety.\n- Volume refers to the very small amount of data created constantly.\n- Velocity (Velocity) refers to the speed at which the data is created and updated, as well as the ability to process data in real-time.\n- Variety (Variety) refers to the different sources and types of data that can be collected.\nBig data usually requires special technologies and techniques to process, analyze, and interpret the data, including machine learning techniques, data mining, and statistical analysis. The goal of big data analysis is to gain valuable insights and information, such as trends and patterns useful for business, scientific, or social decision-making.",
    "jawaban": "The correct answer is 'False'."
  },
  "8b999852ae20671db97a529dc2d20acb6e2d0b6bc08cb9373d9f7638b22cffc4": {
    "soal": "The Constant widget will predict the most frequent class or mean .............. of a training set.\n",
    "jawaban": "The correct answer is: value"
  },
  "1735622aa98d15d29ab605a2beb83f63d3929a536ca7e20f0198b81a8af297fc": {
    "soal": "import pandas as pd\nfrom pm4py.objects.conversion.log import converter as log_converter\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom pm4py.visualization.petrinet import visualizer as pn_visualizer\nfrom pm4py.objects.log.util import dataframe_utils\n# csv_filename='Insurance_claims_event_log.csv'\n# csv_filename='Business Travel Permits.csv'\n# csv_filename='Parent-Child Union Output.csv'\n# csv_filename='PM4PY_Dummy_Event_Log_100_Cases_pm4py.csv'\n# csv_filename='PM4Py_Dummy_Event_Log-congested-business-process.csv'\n# 1. Load CSV event log\ndf = pd.read_csv(csv_filename)\n# df = pd.read_csv(csv_filename,sep=';')\n# print(df.head())\n# 2. Format timestamps properly\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n# 3. Ensure column names match PM4Py expectations\ndf = dataframe_utils.convert_timestamp_columns_in_df(df)\n# 4. Convert to event log object\nevent_log = log_converter.apply(df, parameters=)\n# 5. Discover process model using Alpha Miner\nnet, initial_marking, final_marking = alpha_miner.apply(event_log)\n# 6. Visualize Petri Net\ngviz = pn_visualizer.apply(net, initial_marking, final_marking)\npn_visualizer.view(gviz)\nIf using an XES file instead of CSV, which line should you use?\n{\n=pd.read_csv \u2192 xes_importer.apply(\"file.xes\")\n~pd.load(\"file.xes\")\n~import_csv(\"file.xes\")\n~load.xes(\"file.xes\")\n~xes.read(\"file.xes\")\n~df = xes.load(\"file.xes\")\n}",
    "jawaban": "The correct answer is: log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case_id'"
  },
  "f93a5600e1967c38856dc4280e5392d145dce4368fc176017cf90f5de359e1ba": {
    "soal": "Which of the following is an example of a dataset's column?",
    "jawaban": "The correct answer is: The 'Date' attribute in a sales record."
  },
  "8bc7802d071d40763b46f7013623d3639f86a56d9ba6f655f347f2604394089e": {
    "soal": "The Distances Transformation widget is used for normalization and inversion of distance matrices. Normalizing the data is necessary to bring all material into proportion with one another.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "573c4a4d170a897da8592e9af58426363161fdef3d840da8e92da46e4b9e0011": {
    "soal": "For supervised problems, where data instances are described without class labels, we want to know which feature is the most informative. The Rank widget provides a table of features and their informativeness scores, and supports manual feature selection. In the workflow, we use it to find the top two features (out of 79 initial features from the selected dataset) and display their scatter plot.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "fe32ff0822d9723c5e24bbe2c7f50b03543902aebc120ce4c1103bee001dce87": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nHow many neurons are there in the first Dense layer of the model?",
    "jawaban": "The correct answer is: 128"
  },
  "2c48e1c7b27d832d5a02b0ddb906be78b9b535a65c39d279192ac0fbf4bf8ea7": {
    "soal": "101 (binary) + 10 (binary) = ........ (binary)",
    "jawaban": "The correct answer is: 111"
  },
  "a00dc37e17899a6a7d047638f65d1f5d84e73167a7a0cc44bc55a06ecad8c710": {
    "soal": "The following workflow is an example where we compare three (3) classifiers (Naive Bayes, Tree, and Constant) and input them into the Test & Score widget. We are using the Titanic dataset. The Test & Score widget will show the evaluation results for each classifier. Then, we can plot through the Calibration Plot widget and the ............... widget from the Test & Score output for further analysis of the performance of each classifier. The Calibration Plot widget allows us to visualize the predicted accuracy of class probabilities in a plot/image form.\n\n\n",
    "jawaban": "The correct answer is: ROC Analysis"
  },
  "102e3a3fc79c851783eea10454dbd61a310fe9332ce3af8d9dc0960595b2f144": {
    "soal": "The Text Preprocessing widget in ORANGE can process a Corpus based on the method we choose.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "c12b742594b43bb1c787713de4f385fd1d482b33b6fbe940f01da7edbd266025": {
    "soal": "The ...................... widget can show the match between predicted probability from the classifier and the actual class probability.\n",
    "jawaban": "The correct answer is: Calibration Plot"
  },
  "b765771000a52869b0d292b54a82378f844c971b7128f318d83780b4a110d806": {
    "soal": "The Venn Diagram widget can also be used to explore different prediction models. In the following example, we analyze 3 prediction methods, namely Naive Bayes, SVM, and Random Forest, based on their classification errors on data instances.\n\nBy selecting .......................... in the three Confusion Matrix widgets and sending them to the Venn Diagram widget, we can see all misclassification examples visualized by the method used. We then open the Venn Diagram widget and select, for example, misclassified instances identified by all three methods. This is represented as the intersection of the three circles. Click on the intersection to view these two examples highlighted in the Scatter Plot widget. Try selecting different parts of the diagram to see how the scatter plot visualization changes.\n\n\n\n\n",
    "jawaban": "The correct answer is: misclassification"
  },
  "f61678f00f49584fa64a53050ab0bb54637ea66a6d25383732e42271c77e1cad": {
    "soal": "If labels are one-hot encoded, which loss function would be appropriate?",
    "jawaban": "The correct answer is: CategoricalCrossentropy"
  },
  "9545fb040fc1ecb4d65968240931116fa516b7390fdac1fb4ddb20ab8b5d2111": {
    "soal": "result 8 (decimal) OR 8 (decimal) is",
    "jawaban": "The correct answer is: 8"
  },
  "e032f9b9ca262cdd2ca631f88cbb61ba35f9fa55c388c2c43d212ad1904da486": {
    "soal": "If the Import Documents widget fails to read a particular file for one reason or another, that file will be skipped / .......................... The successfully read files will be sent to the output.\n",
    "jawaban": "The correct answer is: skipped"
  },
  "f8427ad2a94686ac30e0df3e86fc4d024723fc7d83cce24404f3bbb45527bc8b": {
    "soal": "The Image Viewer widget can be used to compare images, for example when looking at similarities or differences between selected data instances (e.g., bacterial growth or bitmap representations of handwriting).\n",
    "jawaban": "The correct answer is 'True'."
  },
  "13193c52941e4cd33e2ed54663024fe925d77531cc5df91886177cf332d6a59c": {
    "soal": "How good is the supervised data mining method for classifying our dataset? Below is a workflow that evaluates various regression techniques on a dataset (the example here is iris). The main widget used here is the Test & Score widget, which receives the data and a learner set, performs cross-validation, calculates prediction accuracy, and generates scores for further inspection.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "a099674a712ba083e5719a24bd25a4114661d9fb6904e525bb16904cf191f4d2": {
    "soal": "What kind of model is LLM?",
    "jawaban": "The correct answer is: Neural network"
  },
  "56f96c2326930cda88104508540a3012713408894422164759320842e9422710": {
    "soal": "The Moving Transform widget uses the Aggregation function to aggregate values in time series windows. The available options are: mean, sum, max, ........................, median, mode, standard deviation, variance, product, linearly-weighted moving average, exponential moving average, harmonic mean, geometric mean, non-zero count, cumulative sum, and cumulative product\n",
    "jawaban": "The correct answer is: min"
  },
  "3de83413a71a4ae885dc6d4eb557ff8e96ef62c3d42d5ed0f1aff1f01cf35922": {
    "soal": "The Box Plot widget can show/describe the distribution of personal values.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "b5b2346d79b81fbf37ef62b5f666a9791a68f9664c0b3fe25456f8aa3199ed32": {
    "soal": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n# Contoh: Memuat dataset dari file CSV\n# Gantilah 'data.csv' dengan path ke dataset Anda\ndf = pd.read_csv('data.csv')\n# Memisahkan fitur (X) dan target \n# Asumsikan kolom terakhir adalah target\nX = df.iloc[:, :-1]\ny = df.iloc[:, -1]\n# Membagi dataset menjadi data latih dan data uji (80% : 20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model Decision Tree\nmodel = DecisionTreeClassifier()\n# Melatih model dengan data latih\nmodel.fit(X_train, y_train)\n# Melakukan prediksi pada data uji\ny_pred = model.predict(X_test)\n# Menghitung akurasi\nakurasi = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model: ')\n# Menampilkan laporan klasifikasi yang mencakup presisi, recall, dan F1-score\nprint(classification_report(y_test, y_pred))\nWhat additional information does the `classification_report` provide beyond accuracy?\n{\n~Only the overall accuracy of the model.\n~The confusion matrix of predictions.\n=Precision, recall, and F1-score for each class.\n~The ROC curve analysis.\n~The mean squared error of predictions.\n~The feature importance rankings.\n}",
    "jawaban": "The correct answer is: akurasi:.2f"
  },
  "888ac2a3bfb40ab6eb6553d4ebda2df520c56b9f9dcb4488a48052654ef34bee": {
    "soal": "Word Cloud data can be created from text files (ASCII) that we have, as seen in the workflow below. First, the data from the Text Files widget must be segmented into words using the Segment widget. Then, the segmented data output needs to be converted from segmented data into text so it can be processed by the text mining toolbox using the Interchange widget. Before displaying it as a word cloud, it is better to preprocess it first, to reduce unnecessary words like connecting words, etc., using the Preprocess Text widget.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "fcb415b527dc97547a7eabf830e6577e5f35110e77991593d653b32862a269cc": {
    "soal": "001 (binary) = ...... (decimal)",
    "jawaban": "The correct answer is: 1"
  },
  "2424ab7bb49760df059ed5245fd3da593c8b599cdb09a812b2370a7651cf58ab": {
    "soal": "Pivot Table can help us to collect and transform data. This workflow takes Kickstarter projects and aggregates them by month. We can check the frequency of projects published per month and observe the differences between funded and ................... unfunded projects.\n\n\n",
    "jawaban": "The correct answer is: tidak"
  },
  "dd682a5bf58237b315b25986c3c0c1ed6c74d79a3424e3b11fcc8d7a9aa7b7bc": {
    "soal": "Only two inputs are suitable for the Distance Matrix Widget: the Distances widget and the Distance Transformation widget. The output from the Distance Matrix widget is a data table containing the distance matrix. The user can decide how to label the table and the distance matrix (or examples in the distance matrix) and then visualize or display them in a separate data table.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "3f82c0200c120858bfcb1177bf52341124bb2506055d0e5d05d3187726e34c38": {
    "soal": "In the Interpolate widget, which is part of the Time Series add-on, the interpolation type - Spline interpolation fits a cubic polynomial to the values around the missing values. Therefore, this technique will be very fast but will provide the best results.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "6d0da1650b44dd1cd209751293b49abd0799bbc738c446b28ef8b93c8b9efdd0": {
    "soal": "The Interpolate widget is part of the Time Series add-on. In the Interpolate Widget, we can choose between linear, cubic spline, parabolic, or mean interpolation.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "49a6c579c0226007e31a55469d2fde2f279d133c8a83b816c4eb8437a707613e": {
    "soal": "\n\n\nIn the workflow above, cross-validation happens on the ......................... widget\n",
    "jawaban": "The correct answer is: Test & Score"
  },
  "0303da02cb99dafbf909437ad8354472025f2efbd36732d38caf139277300f38": {
    "soal": "\n\n\nCocokan ....\n",
    "jawaban": "The correct answer is: Visualisasi Kesalahan Klasifikasi di perlihatkan \u2192 Scatter Plot, Kesalahan Klasifikasi di perlihatkan \u2192 Confusion Matrix, Metoda Klasifikasi yang digunakan \u2192 Linear Regression"
  },
  "9a6154269bad78ca225e28750be75492c8059c4ba3d7e301fbd2aa16039a32fa": {
    "soal": "The Predictions widget shows the probability and final decision of the prediction model. The output of the Predictions widget is another dataset, where predictions are added as a new meta attribute. We can choose which features we want to output (original data, predictions, analysis). The results can be observed in the Data Table. If the predicted data includes the actual class values, the prediction results can also be observed in the Confusion Matrix widget.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "2d7e20259f99ff322592c02ff5a12cfbe5618d251c59f3ba04c977ae6ee1c099": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nApplying a Dropout rate of 0.2 during training affects the neurons in which way?",
    "jawaban": "The correct answer is: Randomly deactivates 20% of neurons in the previous layer during training"
  },
  "9e546d748355aae81a3715019fe2b7892e194169bc703e674d55fd608f84f48f": {
    "soal": "import pm4py\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\n# Import XES log\nlog = xes_importer.apply(xes_filename)\n# Cek jumlah kasus dan aktivitas\nprint(f\"Jumlah kasus: \")\nactivities = set()\nfor trace in log:\nfor event in trace:\nactivities.add(event['concept:name'])\nprint(f\"Jumlah aktivitas unik: {len(activities)}\")\nfrom pm4py.algo.discovery.dfg import algorithm as dfg_discovery\nfrom pm4py.visualization.dfg import visualizer as dfg_visualization\n# DFG berdasarkan frekuensi\ndfg = dfg_discovery.apply(log,\nvariant=dfg_discovery.Variants.FREQUENCY)\n# Tampilkan visualisasi\ndfg_vis = dfg_visualization.apply(dfg, log=log,\nvariant=dfg_visualization.Variants.FREQUENCY)\ndfg_visualization.view(dfg_vis)\nWhich of the following visualization methods can also enhance bottleneck analysis besides Performance Spectrum?\n{\n=Gantt Chart\n-Line Chart\n-Scatter Plot\n-Pie Chart\n-Bar Chart\n-Table View\n}",
    "jawaban": "The correct answer is: len(log)"
  },
  "a3d97b6bccd3228a4a6695d53b99a01df3025142098028c6d28d333a301c63ba": {
    "soal": "Data attributes in Orange have the type discrete, ............(1)............ or character string. The attribute type is marked with a symbol that appears in front of the attribute name (D, ............(2).............., S).\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 continuous, (2) \u2192 C"
  },
  "9a162fad858b8ad6acd89456b5150d98cc55305183b92da5bee955f7dfa38894": {
    "soal": "Widget Network Clustering tries to find clusters in a network. Network Clustering works with two algorithms, one from Raghavan et al. (2007), which uses ....................... to find the appropriate clusters, and one from Leung et al. (2009), which builds on the work of Raghavan and adds hop attenuation as a parameter for cluster formation.\n",
    "jawaban": "The correct answer is: label propagation"
  },
  "e4be2a8d06e8749bc617fa8021b692e00e4d9fb80b800ebf2ea8e91f9ca47619": {
    "soal": "Contoh penggunaan Widget kNN untuk task ..................... di perlihatkan pada workflow di bawah ini. Workflow ini menunjukkan cara menggunakan Learner output. Untuk tujuan contoh ini, kita menggunakan dataset housing. Kita memasukkan model prediksi kNN ke dalam Predictions dan mengamati nilai yang diprediksi.\n\n\n\n\n\n",
    "jawaban": "The correct answers are: regresi, regression"
  },
  "fe3bd71467249f9236f57c0c4e118ca18452cd80720101320296d2aa0bd87775": {
    "soal": "What is a convolutional neural network (CNN) typically used for?",
    "jawaban": "The correct answer is: Image and video recognition"
  },
  "a8c2cbebe9ba122b55208a6e54b32d873ae12f756a9a0bc518515f63e814ec85": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(2, size=100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi training (80%) dan testing (20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model, misalnya RandomForestClassifier\nmodel = RandomForestClassifier(random_state=42)\n# Melatih model pada data training\nmodel.fit(X_train, y_train)\n# Memprediksi data testing\ny_pred = model.predict(X_test)\n# Mengevaluasi akurasi model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model pada data testing: ')\nWhich module from `sklearn` provides the `train_test_split` function?\n{\n~sklearn.preprocessing\n=sklearn.model_selection\n~sklearn.metrics\n~sklearn.ensemble\n~sklearn.datasets\n~sklearn.utils\n}",
    "jawaban": "The correct answer is: accuracy:.2f"
  },
  "b975149039f572b1cd78758b509d5d39e2d75ef278910641040773b920c8a734": {
    "soal": "What is the purpose of the 'regularization' technique?",
    "jawaban": "The correct answer is: To prevent overfitting by adding a penalty for model complexity"
  },
  "1ef9d55b92a4217169ad4ddd3ace96413d1dd346659a10300ea2386c2aee5d88": {
    "soal": "result 8 (decimal) AND 2 (decimal) is",
    "jawaban": "The correct answer is: 0"
  },
  "15ea63262e6efd9d15221af45ee1967275df6052f0571a1fc92322b5abeaec66": {
    "soal": "import pandas as pd\nfile_path = \"/content/drive/My Drive/NamaFolder/NamaFile.xlsx\"\ndf = pd.read_excel(file_path)\nprint(df.head())\nWhich function in pandas can be used to check for missing values in a DataFrame?",
    "jawaban": "The correct answer is: df.isnull()"
  },
  "b47830a5e000153ca7d0d820da0ad2a39be3c37f8acbe8b5d02d83423aa0892a": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#_xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nmap = pm4py.discover_heuristics_net(log)\npm4py.view_heuristics_net(map)\nWhat does the variable \"map\" contain?",
    "jawaban": "The correct answer is: Heuristic net model -Text data -Image file -CSV data -Database connection -Web API"
  },
  "8f4d08c62a4ae1568ab5f0297fc673a27aae3e21af280ffedbd8115717f8ef69": {
    "soal": "The visualization in the Network Explorer Widget functions similarly to the one used in the Scatter Plot widget. To select a subset of nodes, draw a rectangle around ..................... . Shift will add a new group. Ctrl-Shift (Cmd-Shift) will add to the existing group. Alt (Option) will remove from the group. Clicking outside the network will deselect the selection.\n\n\n\n",
    "jawaban": "The correct answer is: subnet"
  },
  "2dc7b862d99d57f80164261bfae9272aa98ff44e2cc7113f1c9e4ccac53c08b2": {
    "soal": "The Scatter Plot widget, like other Orange widgets, supports zooming in and out of the plot area and manual selection of data instances. These functions are available at the bottom right corner of the widget.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "39f47e6c7aa338572645859f1de0fc2a1fc88ac5a22109dcc67008b2a3314905": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=10000)\nIn the IMDB dataset, what labels are used to classify the sentiment of reviews?",
    "jawaban": "The correct answer is: Positive and Negative"
  },
  "8c848473b8f85532df92c6c7fec8075753567d6aef5f1958d5aec907ca8f37d8": {
    "soal": "result 8 (decimal) AND 5 (decimal) is",
    "jawaban": "The correct answer is: 0"
  },
  "4935f3b7a11002121314dad778065a8f668bfaf5ec349cac69596b48df49f8eb": {
    "soal": "The Venn Diagram widget can plot a Venn diagram for .................. or more data subsets.\n",
    "jawaban": "The correct answer is: two"
  },
  "a6c746300278fde7ebcfe3d8f00a5ef3be5478d864cc7c0d17e0bf3bd4f7d8ae": {
    "soal": "We use the zoo dataset in combination with Hierarchical Clustering to find animal groups. Now we have the clusters that we want to find, and what is significant for each cluster! Give the cluster to the Box Plot and use \u2018Order by relevance\u2019 to find what defines the cluster. It seems they are separated by type, even though the grouping is done without any class labels! This is an example of unsupervised learning.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "f12b5924a6b8b46f82b42f5d82271a56d91f95e8b1da5c5a316746ab76d44693": {
    "soal": "If the widget ....................... fails to read a particular file for one reason or another, that file will be skipped. The successfully read files will be sent to the output.\n",
    "jawaban": "The correct answer is: Import Documents"
  },
  "2eedc41ea959c9c9c035f99650c5fb4563fd6e2c5fb24fb9c1ed51cc33b8ed1b": {
    "soal": "What is the main advantage of using the ReLU activation function in neural networks?",
    "jawaban": "The correct answer is: Introduces non-linearity and helps in learning complex representations"
  },
  "848d97e4c6a91a4112a6c029f1feb2df97d33e883edec976ba2e2bb518e69667": {
    "soal": "In the workflow below, we compare the statistics of two Data Info widgets - one with information from the entire dataset and the other with information from a subset selected automatically from the Scatter Plot widget. Here, we are using the Iris dataset.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "1c22011cc72e1e9846a19af905a8ee2ee3f00fac1b1cc75d816934319a0ed564": {
    "soal": "Column attribute names can be preceded by a label followed by a hash. Use c for class and m for meta attributes, i for ignoring columns, w for weight (of a column), and C , D , T , S for continuous attribute types ............ , discrete , time, and string. Example: C#mph, mS#name, i#dummy.\n\nWhere ........ is,\n",
    "jawaban": "The correct answer is: time"
  },
  "d8e7927fc6d157cba7743ede7134c70c3fe2a70366cfd7d2ae8e26c1964859fa": {
    "soal": "import os\nimport string\nfrom collections import Counter\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\n# Buat daftar stopwords Bahasa Indonesia\nstop_words = set(stopwords.words('indonesian'))\ndef bersihkan_teks(teks):\n# Lowercase + hilangkan tanda baca\nteks = teks.lower()\nteks = teks.translate(str.maketrans('', '', string.punctuation))\nreturn teks\ndef analisis_folder(folder='outputs', top_n=20):\nsemua_konten = ''\nfor filename in os.listdir(folder):\nif filename.endswith('.txt'):\nfilepath = os.path.join(folder, filename)\nwith open(filepath, 'r', encoding='utf-8') as f:\nisi = f.read()\n# Ambil hanya bagian isi, setelah header\nif '\n\n' in isi:\nisi = isi.split('\n\n', 1)[1]\nsemua_konten += isi + ' '\n# Bersihkan teks dan tokenisasi\nteks_bersih = bersihkan_teks(semua_konten)\nkata_kata = teks_bersih.split()\n# Hapus stopwords\nkata_kata = [kata for kata in kata_kata if kata not in stop_words and kata.isalpha()]\n# Hitung frekuensi\ncounter = Counter(kata_kata)\ntop_kata = counter.most_common(top_n)\nprint(f\"\n\ud83d\udd0d Top Kata Paling Sering Muncul:\")\nfor kata, jumlah in top_kata:\nprint(f\"{kata}: {jumlah}\")\n# Buat Word Cloud\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(counter)\n# Tampilkan\nplt.figure(figsize=(12, 6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud dari Semua Konten')\nplt.tight_layout()\nplt.savefig('wordcloud.png')\nplt.show()\nprint(\"\n\u2705 Word Cloud disimpan sebagai 'wordcloud.png'\")\n# Jalankan analisis\nif __name__ == '__main__':\nanalisis_folder('outputs', top_n=30)\nWhat does the command 'plt.imshow(wordcloud)' do in the script?\n{\n=It displays the generated word cloud image in the matplotlib figure.\n~It creates a histogram of word frequencies.\n~It saves the word cloud image to a file.\n~It clears the current figure in matplotlib.\n~It sets the colormap for the word cloud visualization.\n~It overlays the word cloud on an existing image.\n}",
    "jawaban": "The correct answer is: top_n"
  },
  "2b7460abc6da1b559b1c34c33035039469044fc5ed12b50a2169d54ad6c48f48": {
    "soal": "In the Correlogram widget, we will visualize the autocorrelation coefficient for the time series we select.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "2b977ff7834ec426fff635a8923c62c854acc43fea878542cc7ddb61e5d958ac": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.reuters.load_data(num_words=10000)\nWhich function is used to load the Reuters Newswire dataset in TensorFlow?",
    "jawaban": "The correct answer is: tf.keras.datasets.reuters.load_data()"
  },
  "ffddbe01271e2826ef8b7a6f2ad6f7525ce9cbf3aac9936f3ef66c6f791b7498": {
    "soal": "import pandas as pd\nfile_path = \"/content/drive/MyDrive/your_file.csv\"\ndf = pd.read_csv(file_path)\nprint(df.head())\nBefore accessing files in Google Drive using the given 'file_path', what step must be performed in Google Colab?",
    "jawaban": "The correct answer is: Mount Google Drive using 'drive.mount('/content/drive')'."
  },
  "8c6b4da66381a9f18351e4e5af3b5cc65541ab52b7d7d4d3f24d833c8245a029": {
    "soal": "The ...................... widget can visualize the distance measurement results in a distance matrix.\n\n",
    "jawaban": "The correct answer is: Distance Matrix"
  },
  "bb988e331eb544f71aa908085c3549b44adff36779082e90eddce9403a943d22": {
    "soal": "The Calibrated Learner widget wraps / continues the work of another learner with probability calibration and ......................................... optimization.\n",
    "jawaban": "The correct answer is: decision threshold"
  },
  "e60cee35539d45924b87ce6c2fde078e5f2bdc58c5fff75fcfb5aae427dd51f3": {
    "soal": "In the workflow below, the use of the Moving Transform widget to obtain a 5-day moving average is shown, we can use a rolling window with mean aggregation.\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "1eb333950d5a04748e88bf72b2fe9a8e4494b862d0b14a8f043918a16c63fd13": {
    "soal": "import pm4py\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\n# Import XES log\nlog = xes_importer.apply(xes_filename)\n# Cek jumlah kasus dan aktivitas\nprint(f\"Jumlah kasus: \")\nactivities = set()\nfor trace in log:\nfor event in trace:\nactivities.add(event['concept:name'])\nprint(f\"Jumlah aktivitas unik: {len(activities)}\")\nfrom pm4py.algo.discovery.dfg import algorithm as dfg_discovery\nfrom pm4py.visualization.dfg import visualizer as dfg_visualization\n# DFG berdasarkan frekuensi\ndfg = dfg_discovery.apply(log,\nvariant=dfg_discovery.Variants.FREQUENCY)\n# Tampilkan visualisasi\ndfg_vis = dfg_visualization.apply(dfg, log=log,\nvariant=dfg_visualization.Variants.FREQUENCY)\ndfg_visualization.view(dfg_vis)\nWhat format does the file analyzed by PM4Py typically have?\n{\n=XES\n-CSV\n-JSON\n-XML\n-PDF\n-DOCX\n}",
    "jawaban": "The correct answer is: len(log)"
  },
  "41f91e516efc7f5d8f8c6e4d8bf1c2d0b73de000987def82f66dc2e90a8db560": {
    "soal": "What kind of loop is created by the LLM process?",
    "jawaban": "The correct answer is: Continuous interaction cycle"
  },
  "4b3be8c1aaa79b8491edcfd4d8ee6885a41b356a33f76917b43db845edf1745f": {
    "soal": "1111 1111 (binary) = ....... (decimal)",
    "jawaban": "The correct answer is: 255"
  },
  "c7d9409bb00d9a0b45d3a71d6adca967a63a5f831008ffa2813e6ad3c2a78dfe": {
    "soal": "In the workflow below, the usage of the Moving Transform widget is shown to obtain a 5-day moving average, we can use rolling window with ..........................\n\n\n\n",
    "jawaban": "The correct answer is: mean aggregation"
  },
  "6c0dbb77aa10b2e3a28ac295be835a1e11f76aa65536483fc93ad567db9f3b11": {
    "soal": "The Line Plot widget cannot be used to visualize data profiles (e.g., Time Series).\n",
    "jawaban": "The correct answer is 'False'."
  },
  "91c8cf2bfd55bd6de7eb3ea68cf09d99da887c3533424fae0df3178b081c3fd4": {
    "soal": "110 (binary) = ...... (decimal)",
    "jawaban": "The correct answer is: 6"
  },
  "7de7656d7280a3003293bc13a4dc199b3681c26fb396290e35265e0716ee9e3c": {
    "soal": "Widget File can read data from Excel (.doc), tab-delimited (.txt) simple, comma-separated file (.csv), or URL.\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "f1099e7e6c84ec80ed92af2fd59c70bfe4ebc0ca900b4015316792fcfc0ed5ab": {
    "soal": "Some visualization widgets, such as ................... and several data projection widgets, can expose data instances in a data subset. In this workflow, the Scatter Plot visualizes data from the input data file, but also highlights the data points selected in the Data Table (selected rows).\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Scatter Plot"
  },
  "0d7af1f008c16bc8932fd307ab3cb2f9e8864ddb3ee9c564ec9d781160dd2cf0": {
    "soal": "The abbreviation of Bit is:",
    "jawaban": "The correct answer is: Binary Digit"
  },
  "64fc46ffdaf11fb4cc12beea1a83caa6986afeb71d71bbf41acc8b9772a1bb8a": {
    "soal": "Which TensorFlow function is used to save and restore models?",
    "jawaban": "The correct answer is: Saver()"
  },
  "1600e654d045077d5f0c75eb75deb11ccbdb71870920d7cabf63a5343812ffed": {
    "soal": "In the workflow below, the Iris data from the File widget is entered into the .............(1)............. widget, where we choose to display only two attributes (i.e. flower petal .................(2).............) and flower petal length). Then, we can see the original dataset and the dataset with the selected columns in the Data Table widget.\n\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 Select Columns, (2) \u2192 petal width"
  },
  "11e55c52b91b31ead9b991baef0e0a9e22078af1a27a9f6e642aa318a1411bc2": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nChoosing a Dropout rate of 0.5 means that:",
    "jawaban": "The correct answer is: 50% of the neurons will be randomly deactivated during training"
  },
  "c773a09986ae494cca50c5804ea0b41bf2581df2f0ee9f9c007d3960b0570c4d": {
    "soal": "What does the recall metric measure in a model's performance?",
    "jawaban": "The correct answer is: The proportion of true positive predictions among all actual positives."
  },
  "2bb67e056c41356e1996c99333b0ba9c62f051b1364a166105a1e4053a905bea": {
    "soal": "Widget Import Documents takes text files from a folder and creates a corpus. The Import Documents widget can read .txt, .docx, ...............(1).............., .pdf, and .xml. If there are ..............(2)............, they can be used to label classes.\n",
    "jawaban": "The correct answer is: (2) \u2192 subfolder, (1) \u2192 odt"
  },
  "cd7b1e70d5d9d45a53ec8273b6150d58522fc96b7a26d9d631b3802fe06013aa": {
    "soal": "The Import Documents widget takes text files from a folder and creates a corpus. The Import Documents widget can read .txt, ........................., .odt, .pdf, and .xml. If there are subfolders in the folder, they can be used to label the class.\n",
    "jawaban": "The correct answer is: docx"
  },
  "6f77b26774cb6e98b241acd942694e31c5d64f62ef33acd5cc6e771e46ba6a20": {
    "soal": "import pandas as pd\nfrom pm4py.objects.conversion.log import converter as log_converter\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom pm4py.visualization.petrinet import visualizer as pn_visualizer\nfrom pm4py.objects.log.util import dataframe_utils\n# csv_filename='Insurance_claims_event_log.csv'\n# csv_filename='Business Travel Permits.csv'\n# csv_filename='Parent-Child Union Output.csv'\n# csv_filename='PM4PY_Dummy_Event_Log_100_Cases_pm4py.csv'\n# csv_filename='PM4Py_Dummy_Event_Log-congested-business-process.csv'\n# 1. Load CSV event log\ndf = pd.read_csv(csv_filename)\n# df = pd.read_csv(csv_filename,sep=';')\n# print(df.head())\n# 2. Format timestamps properly\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n# 3. Ensure column names match PM4Py expectations\ndf = dataframe_utils.convert_timestamp_columns_in_df(df)\n# 4. Convert to event log object\nevent_log = log_converter.apply(df, parameters=)\n# 5. Discover process model using Alpha Miner\nnet, initial_marking, final_marking = alpha_miner.apply(event_log)\n# 6. Visualize Petri Net\ngviz = pn_visualizer.apply(net, initial_marking, final_marking)\npn_visualizer.view(gviz)\nWhat file extension is common for PM4Py's default event log format?\n{\n~.csv\n~.parquet\n=.xes\n~.log\n~.xml\n~.json\n}",
    "jawaban": "The correct answer is: log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case_id'"
  },
  "f5d218a1925c9cc1eae1ab24f0f9ee9256e1ef4099ae687f67134ffa35df0f96": {
    "soal": "\n\n\nThe image above shows the sequence of processes in data science, which are:\n\nASK - Ask, interesting questions.\n.............................. - Obtain data.\nEXPLORE - Explore data, whether there are any anomalies / interesting patterns, etc.\nMODEL - Build, fit, and validate the model.\nVISUALIZATION - Communicate and visualize the data.\n\n",
    "jawaban": "The correct answer is: GET"
  },
  "4a6e99bdb4414b7c8d010e4f281b4450617068518600ad1471aa8e304ea2cb7c": {
    "soal": "In the workflow below, we use the Zoo dataset. We load data into the Line Plot widget, which allows us to select a subset of data instances. Then, we can send the selected data instances to the Save Data widget to save them into a file.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "5cd42bf2f4da58b65ef9a24f1e2b03b4a18a00484ad8031723d6293a106891d3": {
    "soal": "What is a hyperparameter in machine learning?",
    "jawaban": "The correct answer is: A parameter whose value is set before the learning process begins"
  },
  "262b500306c94862449555b9e120904c2af77d582b90f7f2ee329b05bedc78bc": {
    "soal": "In this example, we will just check what a .................... model looks like. Load book-excerpts.tab using the Corpus widget and connect it to the Bag of Words widget. Here, we deliberately use the default parameters - the simplest count is term frequency. Check the output of the Bag of Words widget using the Data Table widget. The last column represents the term frequency of each document.\n\n\n",
    "jawaban": "The correct answer is: bag of words"
  },
  "fa1b29bc006bb967c86071fea4cc3df29b881a17f0efefa1807ab13f2c24c075": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nx_train = np.random.rand(1000, 10)\ny_train = np.random.randint(2, size=(1000, 1))\nmodel = keras.Sequential([\nkeras.layers.Dense(64, activation='relu', input_shape=(10,)),\nkeras.layers.Dense(32, activation='relu'),\nkeras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer='adam',\nloss='binary_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=10, batch_size=32)\nWhat does `np.random.rand(1000, 10)` generate?",
    "jawaban": "The correct answer is: Random float numbers between 0 and 1"
  },
  "d042bd504293bcf35d2fc9fac48fc6526ed5f730157476fe8a384a520e3e1502": {
    "soal": "The Venn Diagram widget can also be used to explore different prediction models. In the following example, we analyze 3 prediction methods, namely Naive Bayes, SVM, and Random Forest, based on their classification errors on data instances.\n\nBy selecting misclassification in the three Confusion Matrix widgets and sending them to the ..............., we can see all misclassification examples visualized by the method used. We then open the Venn Diagram widget and select, for example, misclassified instances identified by all three methods. This is represented as the intersection of the three circles. Click on the intersection to view these two examples highlighted in the ............ widget.\n\n\n\n\n",
    "jawaban": "The correct answer is: Scatter Plot"
  },
  "cc23483f70d4559ccbc0838ece9fa9f79657e7ae7708252dd6764a0960804f9f": {
    "soal": "To extract numbers from structured data (such as text and images), Orange can use a deep network embedder.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "f3e38bbdcf17de595f7ac0138e7347963ec0a52482b4080ea0eba63676117754": {
    "soal": "1 (binary) x 101 (decimal) = ....... (decimal)",
    "jawaban": "The correct answer is: 101"
  },
  "4bc72689b4902e9694db5efacdd0b062b8dcda6685b9232d3dca48ab0a9634d2": {
    "soal": "The following workflow is an example where we compare three (3) classifiers (Naive Bayes, Tree, and Constant) and input them into the Test & Score widget. We are using the Titanic dataset. The Test & Score widget will display the evaluation results for each classifier. Then, we can plot through the Calibration Plot widget and the ROC Analysis widget from the Test & Score output for further analysis of the performance of each classifier. The Calibration Plot widget allows us to see the predicted accuracy of class probabilities in a plot/image form.\n\n\n",
    "jawaban": "The correct answer is: Test & Score"
  },
  "051548d831714423aa87bbfae8df122b3af51e2209194aee5b71ec0df3b5f787": {
    "soal": "The example workflow below shows the very standard use of the Distance Matrix widget. We calculate the distance between rows in a sample from the Iris dataset and output it into the Distance Matrix. It is not surprising that Iris Virginica and Iris Setosa are the farthest apart.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "ac8222abec2740f0686aecced4fa159f9f04ed839ca8d03e81622360fc3460aa": {
    "soal": "Which of the following is TRUE about TensorFlow in Colab?",
    "jawaban": "The correct answer is: It does not require manual installation"
  },
  "5f654d7eb9bf9b71ddd6b698ece0ab621479d50377389ae85938aec738338a2c": {
    "soal": "There are three main challenges faced by businesses with Big Data:\n- Protecting sensitive and personal information\n- Data rights and ownership of houses and buildings\n- Lack of talent (such as data scientists) to analyze data",
    "jawaban": "The correct answer is 'False'."
  },
  "3951c7f5cabef674117a4cdfaa038b56cadaa8c6f91f49404f0d335907439baa": {
    "soal": "The Interpolate widget is part of the Time Series add-on. In the Interpolate Widget, we can choose between linear, cubic spline, nearest, or mean interpolation.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "dc21cac2231ad956b821476fc6b5e3673cc16eb1d75b95d9f8780f085a863830": {
    "soal": "What is the default behavior of the `restart: unless-stopped` policy in Docker Compose?",
    "jawaban": "The correct answer is: Restarts containers automatically unless they are explicitly stopped"
  },
  "63d0d00b493a6589c85bfe7ede2fbfd163009f5384aa4c479105d69aaf80ac20": {
    "soal": "The Test & Score widget will test the learning algorithm. Different sampling schemes are available, including using separate test data. The widget does two things. First, it will display a table with performance metrics for different classifiers, such as classification accuracy and area under the curve. Second, the evaluation output can be used by other widgets to analyze classifier performance, such as ROC Analysis or Confusion Matrix.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "ab103e70fd8032bac09f6849a2508864a7c64f255e56ae4009287142d487791e": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\n# xes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\nxes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nprocess_tree = pm4py.discover_bpmn_inductive(log)\nbpmn_model = pm4py.convert_to_bpmn(process_tree)\npm4py.view_bpmn(bpmn_model)\nWhat is the primary format of the log file used in this script?",
    "jawaban": "The correct answer is: XES"
  },
  "4ba95ce062e83b2289e3839ec3df18789dff4f64f2044410e84292e72c07b311": {
    "soal": "The Predictions widget shows probabilities and the final decision of the prediction model. The output of the Predictions widget is another dataset, where the predictions are added as a new meta attribute. We can choose which features we want to remove (original data, .................... , probabilities). The results can be observed in the Data Table. If the predicted data includes the actual class value, the prediction results can also be observed in the Confusion Matrix Widget.\n",
    "jawaban": "The correct answer is: predictions"
  },
  "4b79d583294242cd935bc3045f1b5ea964178615ad66d1d5a6704d150d6b70e9": {
    "soal": "How do you start all services defined in the `docker-compose.yml` file?",
    "jawaban": "The correct answer is: docker-compose up -d"
  },
  "78640c0d8924ef7f04802e8d96568f7c71af3f22442e4bb8b1e9503c768e855f": {
    "soal": "Widget File reads input data files (data tables with data instances) and sends the dataset to its output channel. The history of the last opened file is stored in the widget. This widget also includes a directory with pre-installed sample datasets in Orange.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "019f56798c70f6dc1ebc5856b2f75ef8d78e1ccced7bb61362c86b9c84aef647": {
    "soal": "When there is no data in the input, the Corpus widget reads the text corpus from a file and sends the corpus instance to the output channel. The history of the last opened file is saved in the Import Files widget. The Corpus widget also contains a directory with sample corpora that were previously installed with the add-on. The widget can read data from Excel files (.xlsx), comma-separated (.csv), and tab-delimited (.tab).\n",
    "jawaban": "The correct answer is 'False'."
  },
  "9ea71f0e10a1caa6bb77438630dad170b384e0100de5ca659572c82767a9ae19": {
    "soal": "Widget ...................... menggunakan algoritma Tree dengan kemampuan untuk melakukan forward pruning (pemangkasan ke depan).\n",
    "jawaban": "The correct answer is: Tree"
  },
  "5a6dee07baa64aab347e703ea24eb855a0f484891893f3d0973511f19c02c7d7": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nWhich activation function is used in the first Dense layer?",
    "jawaban": "The correct answer is: relu"
  },
  "fb91d489b51bd00945b9f10d64e7c69ca5503b3f09790e5f62491be643619586": {
    "soal": "To perform machine learning, we need numbers. To obtain numerical representations of the images, we send the images to the ..................................\n\nThe output data from the Image Embedding widget can be examined using the Data Table widget. Now we have numerical/pixel data for the images. Each image has 2048 numerical representations (columns n0 to n2047). With this numeric feature representation, we can apply all standard machine learning techniques, such as clustering.\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Image Embedding"
  },
  "1fbfef19ad223be250af8073f10ded2de5fff64056e9045ba8fb74ab01ac7e85": {
    "soal": "Data attributes in Orange have types discrete, continuous, or character string. Attribute types are marked with symbols that appear before the attribute name (D, C, S).\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "8fa521ff69794dbadb2a23c1ee3976b99cf01ad56aad509fd574e2c68af73298": {
    "soal": "One URL that contains many examples of Orange workflows for performing data analysis is\n\nhttps://orange.biolab.si/categories/examples/\n",
    "jawaban": "The correct answer is 'True'."
  },
  "aee0b537d24e155ebadb48e021957687b88476af12f082dc0123938848a2a02f": {
    "soal": "In the context of Big Data, what does 'Velocity' refer to?",
    "jawaban": "The correct answer is: The speed at which data is generated and processed"
  },
  "694822e6d1b369d9608d88a2422b8df86222c232c0f9c3dc91e3b4170a7d65a7": {
    "soal": "The ................... widget uses the Aggregation function to aggregate values in time series windows. The available options are: mean, sum, max, min, median, mode, standard deviation, variance, product, linearly-weighted moving average, exponential moving average, harmonic mean, geometric mean, non-zero count, cumulative sum, and cumulative product\n",
    "jawaban": "The correct answer is: Moving Transform"
  },
  "d92cf36065d0cd1eb3b7efe94a989e3070ad94b01c748338615e37715c4f769b": {
    "soal": "When using Deep Learning models like LSTM or BERT for TikTok comment analysis, how many comments are recommended?",
    "jawaban": "The correct answer is: More than 10,000 comments"
  },
  "d95c71646d50ac821b5dd4dbd4438388bf3d986be461cb737d32531d39c2e2b8": {
    "soal": "To obtain numerical representations of the images from Image Import, we need to send the images to the Image Viewer widget as shown in the workflow below.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "ad1d09dbd833350debd569b41f7dd516ad19c6a9671ef04f84672e2f6e793de8": {
    "soal": "Which command updates the package list on Ubuntu 24.04?",
    "jawaban": "The correct answer is: sudo apt update"
  },
  "c765c1c3572b2abd4ef4df57fb3bc58413fb460f67df5fd6b6b9d73e62f4f675": {
    "soal": "\n\n\nWorkflow di atas melakukan test / evaluasi CN2 Rule Induction dan Tree di Widget Test & Score.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "a3eba4732153329d21b6e6286953a670eb0c0b3684edcdfca6f7a361d03fca04": {
    "soal": "import pandas as pd\ndf = pd.read_excel('path_ke_file.xlsx', sheet_name=0)\ndf_selected = df[['Kolom1', 'Kolom2']]\nprint(df_selected.head())\nHow do you read multiple sheets, 'Sheet1' and 'Sheet2', from an Excel file?",
    "jawaban": "The correct answers are: pd.read_excel('file.xlsx', sheet_name, ['Sheet1', 'Sheet2']), pd.read_excel('file.xlsx', sheets, ['Sheet1', 'Sheet2']), dfs, pd.read_excel('file.xlsx', sheet_name, ['Sheet1', 'Sheet2']), pd.read_excel('file.xlsx', sheet_name, 'Sheet1,Sheet2'), pd.read_excel('file.xlsx', sheet_names, ['Sheet1', 'Sheet2']), pd.read_excel('file.xlsx', sheet, ['Sheet1', 'Sheet2'])"
  },
  "17994aa6e401a85675e086e4c45c9077db8c488f6296a0b077821f039a5567a8": {
    "soal": "Network Generator Widget can create an example Internet network.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "1b1c321deba19b9fc77b4cfd41504a756a43ad2d67db289c4d335c56c4f30ac9": {
    "soal": "In the Distributions widget, for continuous attributes, attribute values are displayed as a function graph. Statistics for continuous attributes are obtained using Gaussian kernel density estimation, while the curve display is adjusted with the Precision bar (smooth or precise). As an example, below we use the Iris dataset.\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "4c091a20de2e80647adc0db347739b8992e0d140856e2ec6ff596a895e865454": {
    "soal": "How does data quality affect model performance?",
    "jawaban": "The correct answer is: High-quality data leads to more accurate and reliable models."
  },
  "de7cfd2d85d4b21cd1f0b954ed9701f4a369e460f2a8f0eb8799c7cdc520ba9e": {
    "soal": "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target biner\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(0, 2, 100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi data latih dan data uji (80% latih, 20% uji)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi dan pelatihan model klasifikasi (misalnya RandomForest)\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n# Prediksi pada data uji\ny_pred = model.predict(X_test)\n# Menghitung Precision, Recall, dan F1-Score\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nprint(f'Precision: ')\nprint(f'Recall: {recall:.2f}')\nprint(f'F1-Score: {f1:.2f}')\n# Alternatif: Menggunakan classification_report untuk ringkasan metrik\nprint('\nClassification Report:')\nprint(classification_report(y_test, y_pred))\nWhich Python library provides functions like `precision_score`, `recall_score`, and `f1_score`?\n{\n~numpy\n~pandas\n=scikit-learn\n~matplotlib\n~tensorflow\n~keras\n}",
    "jawaban": "The correct answer is: precision:.2f"
  },
  "b0e6cbbf77f744b9e2f5b84201b6860c045685fef25199b6a66ab75141908975": {
    "soal": "In recurrent neural networks (RNNs), what is the primary purpose of the hidden state?",
    "jawaban": "The correct answer is: To maintain information about previous inputs in the sequence."
  },
  "39722701d19be2c77063f0d5711a23e2078433f0e888291a9fadd78dda7a7380": {
    "soal": "Network Analysis Widget calculates performance summaries from node-level and graph-level for network. The Network Analysis Widget will output the network with its statistical computation results and an extended item data table (only node-level index).\n",
    "jawaban": "The correct answer is 'False'."
  },
  "d5476ad81ba8d923bf119bbb8debb98b8cc869ac798b6a18a4405e62f7834dc1": {
    "soal": "In the Preprocess Text widget, we can do several things, such as:\n\nConvert all letters to lowercase.\nRemove stop words, words that are not useful, such as connecting words like \"and,\" \"in,\" \"to,\" \"from,\" etc.\nSet stopword processing in the Indonesian language.\nRemove HTML tags.\nRemove URLs.\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "196b5d058ba157ff5adc87ced516c774403330c90565550b2b182eeb6e3e9c0e": {
    "soal": "What is the main role of the Validation Set in model development?",
    "jawaban": "The correct answer is: To optimize the model and tune hyperparameters"
  },
  "5f1e763ce9730ab16479296d751419d1daa12db3dcdaa0f71549d08c59bf24ee": {
    "soal": "The Distance Transformation widget is used for ........................ and inversion of the distance matrix. Normalizing the data is required to bring all variables into proportion with one another.\n",
    "jawaban": "The correct answer is: normalization"
  },
  "32010f13056d2cb0969f61478cdf9b1b77af885a89c6245ea1cea86282e6b629": {
    "soal": "To animate a process using ProM, what is the second step after installation?",
    "jawaban": "The correct answer is: Import a XES file"
  },
  "43ca13f430a67904adb0e147623a55a7192c5c7bdace42d86dda2e53c586f1fa": {
    "soal": "!pip install googlesearch-python nltk matplotlib wordcloud\nfrom googlesearch import search\nimport csv\ndef google_scrape(keyword, num_results=40,\noutput_file='results.csv'):\nprint(f\"Searching Google for: \")\n# Lakukan pencarian dan ubah hasil ke dalam list\nresults = list(search(keyword, num_results=num_results))\n# Simpan ke file CSV\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'URL'])\nfor url in results:\nwriter.writerow([keyword, url])\nprint(f\"Saved {len(results)} results to '{output_file}'\")\n# Contoh penggunaan\nkeyword = \"berita terbaru teknologi Indonesia\"\ngoogle_scrape(keyword, num_results=40)\nBy default, how many search results does the 'search' function retrieve if 'num_results' is not specified?\n{\n=10\n~5\n~20\n~30\n~40\n~50\n}",
    "jawaban": "The correct answer is: keyword"
  },
  "99cd9abeb2aba4dcf0827e1d4749baf3b59b302ac22c41a1336aa378723d7575": {
    "soal": "The Box Plot widget is most commonly used right after the File widget to observe the statistical properties of a dataset. In this example, we used heart-disease data to inspect our variables.\n\n\n\n\nBox Plot is also useful for finding properties of a specific dataset, for example, a set of instances manually defined in another widget (e.g., Scatter Plot, or instances from multiple clusters, or nodes in a classification tree).\n",
    "jawaban": "The correct answer is 'True'."
  },
  "8021bffee74c7b895d2af8952238039118575342d5f0546992cdfa31eafdc59d": {
    "soal": "Data attributes in Orange have types discrete, continuous, or character string. Attribute types are marked with symbols that appear before the attribute name (D, D, S).\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "1f04a3345c1dd1f3fda0a1264c86f844351928db57d3ff5e05608065961b3e2e": {
    "soal": "111 (binary) + 111 (binary) = ?? (binary)",
    "jawaban": "The correct answer is: 1110"
  },
  "1e155865f4905f6cf72a04a9816cdef0089821f19f3a76757af3cfdfea8667b2": {
    "soal": "An example of using the kNN Widget for regression task is shown in the workflow below. This workflow demonstrates how to use the Learner output. For this example, we use the housing dataset. We input the kNN prediction model into Predictions and observe the predicted values.\n\n\n\n",
    "jawaban": "The correct answer is: kNN"
  },
  "dbe47a1f7767bbc51cae6664747d175eaed67457c75f4548a554b586135c248a": {
    "soal": "\n\n\nWorkflow di atas melakukan test / evaluasi CN2 Rule Induction dan Tree di Widget Test & Score. Terlihat CN2 Rule Induction lebih baik.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "f5626531a1253f843edf6e75e381f567bccbbdd3e43a85e6654f6cbf9d1366d1": {
    "soal": "For complex problems involving multiple categories, imbalanced data, or noisy data, what is the recommended dataset size?",
    "jawaban": "The correct answer is: Larger datasets are necessary"
  },
  "d6265d9ae00f71fb61ab27edec4596010e43b57fa1e85d182967aa6cce89b215": {
    "soal": "The ................................ widget can consider the dataset provided in the input channel and save it to a data file with a specified name. The Save Data widget can save the data as a file with data separated by tabs or separated by commas.\n",
    "jawaban": "The correct answer is: Save Data"
  },
  "722c272aea6cdbc230c88b4564ac25c8e5a9fb5db065be1b518e2978224bce3f": {
    "soal": "What kind of assistant benefits from LLMs?",
    "jawaban": "The correct answer is: Virtual assistant"
  },
  "6ee66ed0e652ff90d606e0cbbf486cafb7524f32a217b9b886c83e8766f4a1d8": {
    "soal": "The .......................... widget calculates the distance between rows/columns in the dataset.\n",
    "jawaban": "The correct answer is: Distances"
  },
  "a335461ce24ea6dba4e7d99c367295ffe5346e161df68efc751c82c44b807b16": {
    "soal": "Instalasi Orange3 di Ubuntu 18.04 dapat menggunakan perintah\n\n\u00a0apt -y install orange3\n",
    "jawaban": "The correct answer is 'False'."
  },
  "15421da2c248905821aad4f87a2a2e8d19cc9fbc0bfc9d02ac47cf265168c039": {
    "soal": "Select URLs that contain examples of data mining people\n",
    "jawaban": "The correct answer is: JdSpuTi9d8A"
  },
  "e181ae2ad230d1df2909fbf549e79946134fc9ee18291abd28afcb76b22417ba": {
    "soal": "In the snapshot below, we can see how transformation affects the distance matrix. We load the Iris dataset and calculate the distance between rows with the help of the Distances widget. To show how Distance Transformation affects the Distance Matrix, we create the workflow below and compare the transformed distance matrix with the \"original\".\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "e65dd55b64f616db9d4c1b856dab113b408bb2474c412f712b2c7df02d93bf20": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\n# xes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\nxes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nprocess_tree = pm4py.discover_bpmn_inductive(log)\nbpmn_model = pm4py.convert_to_bpmn(process_tree)\npm4py.view_bpmn(bpmn_model)\nHow is the BPMN model finally visualized?",
    "jawaban": "The correct answer is: pm4py.view_bpmn(bpmn_model)"
  },
  "707dac18ab041b504e56729081e7cb6667417d6ea5b24cd755ccff7c4e4f6ab3": {
    "soal": "In the Import Documents Widget, to retrieve data, select (click) the folder icon on the right side of the widget. Select (choose) the folder that we want to make a corpus. After loading is complete, we will see how many documents were successfully retrieved by the Import Documents Widget. To observe the obtained corpus, connect the Import Networks widget to the Corpus Viewer Widget. In this workflow, we are using a set of speeches by President Kennedy in plain text format.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "dbd813c3996ac5151af15eb589b3f22f3af7fb9e2c6a0b89089261cf5f94b446": {
    "soal": "What is a potential optional attribute in an event log?",
    "jawaban": "The correct answer is: Resource"
  },
  "74ed3a1ef16da1132be9d8a6265fcd19d311f685c0860374ebcdadb433c42aaa": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.reuters.load_data(num_words=10000)\nWhen loading the Reuters Newswire dataset using TensorFlow, which parameter limits the number of words to the most frequent ones?",
    "jawaban": "The correct answer is: num_words"
  },
  "0f8238d6371b1c8b70a1d7011aedf2d74c397fc7f13b2dae6d4f37af15fd8074": {
    "soal": "Algoritma CN2 adalah teknik klasifikasi yang di rancang agar secara effisien melakukan induksi dari rules yang simple & komprehesif dalam bentuk \u201cif cond then predict class\u201d, meskipun dalam domain dimana mungkin ada noise.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "55f9240f67e09746759bca47a7dbe2bdf76ba57d51f80e860887c47853c7aaf0": {
    "soal": "As an example of using kNN for classification, we use the iris dataset. We compare the results of k-Nearest Neighbors with the default Constant model, which will predict the class ............. It seems that kNN is better.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: majority"
  },
  "02ab3ab5828d5fb038d97a0d7cb74223147f7487244cac9ddd4c3ae829d9612f": {
    "soal": "The ................... widget can save data to a file.\n\n\n\n",
    "jawaban": "The correct answer is: Save Data"
  },
  "49f12417cf29f81a5f281247707ed290b8306b0d773704e887801ce4d327ab40": {
    "soal": "Network Analysis Widget calculates statistical summaries from node-level and graph-level for network. The Network Analysis Widget will output the network with its statistical computation results and an extended item data table (only graph-level index).\n",
    "jawaban": "The correct answer is 'False'."
  },
  "ca83ed44af29927cc4e92ab93d64f86e7239ae845a9f1524d6cf172939026892": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nWhat type of images does the MNIST dataset contain?",
    "jawaban": "The correct answer is: Grayscale"
  },
  "69bb39cde1f0737a9ae06cfd22c9c3bff4bd0033703f22e90d00c5eb3c4913b0": {
    "soal": "What is data augmentation in machine learning?",
    "jawaban": "The correct answer is: Creating modified versions of training data to improve model generalization"
  },
  "1ab5289c962e7b3aec7d0b247690ddd3fab0a799d44b740e1fccfe7079ef4577": {
    "soal": "The Moving Transform widget can apply the rolling window function to time series. Use the ............................... widget to get the average value of the series.\n",
    "jawaban": "The correct answer is: Moving Transform"
  },
  "0a5e132cde32188957c3da0b46fa50d4a406787b34674363e68e8f1aad2a030e": {
    "soal": "The Line Chart widget can visualize the sequence of time series and its movement in the most basic time series integration that is quite easy to imagine.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "3b29636709ba996e053222b9354a97a26e490c4440536a0c30361b5858190fde": {
    "soal": "import pandas as pd\ndf = pd.read_excel('path_ke_file.xlsx', sheet_name=0)\ndf_selected = df[['Kolom1', 'Kolom2']]\nprint(df_selected.head())\nWhich parameter allows you to skip the first 10 rows when reading an Excel file?",
    "jawaban": "The correct answers are: 10, 10, skiprows, 10, 10, 10, 10"
  },
  "069230d65856315f245e764e86c91c15f76b39dcf7f8491025bc6e3a324cf945": {
    "soal": "Only two inputs are suitable for the Distance Matrix Widget: the Distances widget and the Distance Transformation widget. The output of the Distance Matrix widget is a data table containing the distance matrix. The user can decide how to label the table and the distance matrix (or instances in the distance matrix), and then it can be visualized or displayed in a separate data table.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "23512fefc631ee4fac6192b30670a4daae8878c1bd87219cc8eb714a4dbc40b8": {
    "soal": "Word Cloud can be built from documents that we have. In the text mining toolbox, there is an Import Documents widget to read document files. Before displaying it as a word cloud, it is better to pass it through the Preprocess Text widget first, to reduce unnecessary words, such as connecting words, etc. Next, we can pass it through the Bag of Words widget first, or go directly to the Word Cloud widget to display it.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "96b53ff96ec0cfebe7d151eead116c30b0653fd4d632076eeb67438d6e0a2ca0": {
    "soal": "Why is it important to evaluate a model's accuracy on test data?",
    "jawaban": "The correct answer is: To assess how well the model generalizes to unseen data."
  },
  "48f99ea36b8b6060cc8e7bc655d9213a65dea40585613764f7db40e15b8038db": {
    "soal": "Data science is \"a concept to unify statistics, data analysis, machine learning, and related methods\" to \"understand and analyze real-world phenomena\" with data. It does not use techniques and theories drawn from many fields in the context of mathematics, statistics, information science, and computer science.",
    "jawaban": "The correct answer is 'False'."
  },
  "4dff887dd94b455a3c9c7d6615bb277b33a02276fdbcec9a28e09f38fbe29e79": {
    "soal": "Data doesn't always come in a nice table format. Data can also be in the form of text, audio recordings, video material, or even images. However, computers can only work with images, so for every data mining, we need to convert this unstructured data into a vector representation.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "fd4bded05ef961dfb02eb5a759888f8006b03f46fc2d756825426c8e8d7f57ce": {
    "soal": "What is the purpose of the command 'conda config --set auto_activate_base false'?",
    "jawaban": "The correct answer is: To prevent the base Anaconda environment from activating automatically"
  },
  "055462fcae6b6fed1e8f8a5b0d9d9863e3388ccbd4ab8b220505aec5f10599c5": {
    "soal": "Machine learning is a branch of computer science that focuses on developing algorithms and techniques to create computer systems that can learn from data. In machine learning, computers are trained to recognize patterns and rules in data so that they can make intelligent and accurate predictions or decisions based on that data.\nThe learning process in machine learning can be done with three main methods: supervised learning, unsupervised learning, and reinforcement learning.\n- Supervised learning involves using labeled or annotated data to train the algorithm so that the computer can recognize patterns or relationships between data features and labels. Examples of supervised learning applications are image classification, stock price prediction, and spam email classification.\n- Unsupervised learning, on the other hand, involves using data with labels to identify hidden patterns or structures in the data. Examples of unsupervised learning applications are data clustering and dimensionality reduction.\n- Reinforcement learning involves computer systems learning not through experience but by taking actions and receiving feedback on whether the actions were right or wrong. The goal is to find the optimal decision or action that results in maximum benefit.\nMachine learning has various applications, including in facial recognition, image analysis, speech recognition, forecasting, anomaly detection, and more.",
    "jawaban": "The correct answer is 'False'."
  },
  "b06de46be7639937bbb91b596b47918d8af7551d9b2842d5bbcf792c61ab2507": {
    "soal": "\n\n\n\n\n\n\n\n\nSupervised Learning techniques are generally divided into:\n\nClassification.\nRegression.",
    "jawaban": "The correct answer is 'True'."
  },
  "e4407bf7c86cd2278867082c0bd277f2ac2cf15b10d7a58c6b6383f709a7394a": {
    "soal": "\n\n\n\n\n\nUnsupervised Learning uses labeled data.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "2281b25cf70cd7517198ebb362dca25f0b8a0b4813049ca05a827dd43e8917c9": {
    "soal": "Big data is a term that refers to the very large and complex amount of data collected from various sources, including business transactions, social data, census data, medical data, scientific data, and more. The main characteristics of big data are volume, velocity, and variety.\n- \nAnswer Question 16\n refers to the very large amount of data created every moment.\n- Velocity refers to the speed at which this data is created and updated, as well as the ability to process data in real-time.\n- Variety refers to the different data sources and types of data that can be collected.\nBig data typically requires special technologies and techniques to process, analyze, and interpret the data, including machine learning, data mining, and statistical analysis techniques. The goal of big data analysis is to gain valuable insights and information, such as trends and patterns useful for business, scientific, or social decision-making.",
    "jawaban": "The correct answer is: Volume"
  },
  "b988ce06afeda93c6a5ef34f0b0dade972c761f5e8fdb386e819be0999c7b247": {
    "soal": "If the dataset has many attributes, it is not feasible to manually scan all pairs to find interesting or useful scatter plots. Orange implements intelligent data visualization with the Search Informative Projections option in the widget.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "f7816bce387caee5c49c3a88a5e166a08dee54106b247dad0374d92b4a41264e": {
    "soal": "This workflow example shows how Network Analysis can enrich the workflow. We have used the lastfm.net data as the input network from the Widget ................. and sent it to the Widget Network Analysis. We can decide to compute the degree, degree centrality, and closeness centrality at the node-level.\n\nWe can then visualize the network in the Widget Network Explorer. In the Network Explorer widget, it colors with the best tags, like the default for this dataset. But now we can also adjust the node size to fit the degree centrality results. This is a good way to visualize network properties.\n\n\n\n",
    "jawaban": "The correct answer is: Network File"
  },
  "1a47fded154c2b710fbcc1332956f09df1b1ef71b5fbaf9f4bf8f52681705216": {
    "soal": "The CSV File Import widget reads comma-separated files and sends the dataset to the output channel. The separator can be comma, .................. , space, tab, or manually-defined delimiters. The history of the recently opened files is maintained in the widget.\n",
    "jawaban": "The correct answer is: semicolon"
  },
  "c6fc26a2c49fa8324f7b540f51a125b79fc1289a7677deac1c1f23c343cc6db5": {
    "soal": "Which visualization method is ideal when your process includes high levels of noise?",
    "jawaban": "The correct answer is: Heuristic Net"
  },
  "6ab34b760dc074d56205eaabdf5dd83fbecee9099d15d2ced87b1a8feb3a8b92": {
    "soal": "What is used by LLM to process the prompt?",
    "jawaban": "The correct answer is: Large neural network"
  },
  "6357e4bb4811c8a76ac2ba0c09687c59a90681f9027a674a26d4c9720d753fa0": {
    "soal": "The Predictions widget shows the probability and final decision of the prediction model. The output of the Predictions widget is another dataset, where predictions are added as a new meta attribute. We can choose which features we want to output (original data, predictions, probabilities). The results can be observed in the Data Table. If the predicted data includes the actual class values, the prediction results can also be observed in the Confusion Matrix widget.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "5e3ada5b88c2549ae6c4b5cfc447c0e9b8a2eee3ebef2f1ac7ff489fa61cecbd": {
    "soal": "The Import Documents widget can import text documents from a folder to become a corpus.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "281ad488b24ae622483e028ed66ca65814121d004818891525e2afba459ee84d": {
    "soal": "In the Interpolate widget, which is part of the Time Series add-on, the interpolation type - ................. interpolation replaces missing values with previously defined values.\n",
    "jawaban": "The correct answer is: Nearest"
  },
  "c2ee48c3bf93d9e3ab3a88547dd64e7f15d0ab6f70aeda49bd13abfadcef1c24": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#_xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nmap = pm4py.discover_heuristics_net(log)\npm4py.view_heuristics_net(map)\nWhich variable contains the name of the XES file?",
    "jawaban": "The correct answer is: xes_filename -filename -log_file -log_name -input_file -file_xes"
  },
  "1e77b96a1954f1c2cb6c7b3219710c606aec9e6794e3ec8fb1368b78beada4ed": {
    "soal": "Which of the following is a common activation function used in hidden layers of deep neural networks?",
    "jawaban": "The correct answer is: ReLU (Rectified Linear Unit)"
  },
  "7cf2f99d2ffd93e3ab346e55060d18248a37cea8a1c5b50354dd10822d1a70e8": {
    "soal": "We use the zoo data set in combination with the Hierarchical Clustering widget to find animal groups. Now we have clusters that we want to identify and what is significant for each cluster! Provide the cluster to the ...................... widget and use \u2018Order by relevance\u2019 to find what defines the cluster. It seems they are separated by their type, although clustering was done without class labels! This is an example of unsupervised learning.\n\n\n\n",
    "jawaban": "The correct answer is: Box Plot"
  },
  "f5f0cbd25798cb0f38043ebf00814116e444ab61467a460bfd667ff940d755ca": {
    "soal": "Widget Network Clustering can detect ...................... in a network.\n",
    "jawaban": "The correct answer is: cluster"
  },
  "cc72977e6d2f3998bea1e768a3c8554843b7a8ab73a935bd188d2af6f14bffab": {
    "soal": "The workflow example below shows the use of the Distance Matrix widget, which is quite standard. We calculate ..................... between rows in a sample of the Iris dataset and output it in the Distance Matrix. Unsurprisingly, Iris Virginica and Iris Setosa are the farthest apart.\n\n\n\n\n",
    "jawaban": "The correct answer is: distance"
  },
  "bb75d0c8fd3f29e0ac0bd4f0213ed5463d31761a96a433ce080799e2e6b213a1": {
    "soal": "Similar to R-programming, Apache Hadoop is \nAnswer Question 31\n source. It is a framework tool created by Google and Apache. The Hadoop framework allows for the processing of larger amounts of data, storing heterogeneous data, and accelerating the processing time.\nAccording to AWS, Hadoop is an open-source framework that is highly effective for storing extremely large datasets. In addition to storing, this framework can also efficiently process data from gigabytes to petabytes.",
    "jawaban": "The correct answer is: open"
  },
  "b39b8ed35f799c48cf059366a2d548968740a936a6c6cd5e5a3714e78381061e": {
    "soal": "Network Analysis Widget can be used for ROC analysis of network data.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "fb5b6ad2a3ed67ca3fa1863dfc5c0e711deb9ac6ebf42d6e8ac1242c919e207e": {
    "soal": "When there is no data in the input, the Corpus widget reads the text corpus from a file and sends the corpus instance to the input channel. The history of the last opened file is saved in the Corpus widget. The Corpus widget also contains a directory with sample corpora that were previously installed with the add-on. The widget can read data from Excel files (.xlsx), comma-separated (.csv), and tab-delimited (.tab).\n",
    "jawaban": "The correct answer is 'False'."
  },
  "9908604e3eda915a91775d4101563b56264e91657ae8e52a10531e8f6e35c86e": {
    "soal": "from googlesearch import search\nimport csv\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef google_scrape_bulk(keywords, num_results=10, output_file='results.csv'):\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'Rank', 'URL'])\nfor keyword in keywords:\nprint(f\"\ud83d\udd0d Searching for: \")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nwriter.writerow([keyword, i+1, url])\ntime.sleep(1) # Delay agar tidak dianggap bot\nexcept Exception as e:\nprint(f\"\u274c Error while searching '{keyword}': {e}\")\nprint(f\"\n\u2705 All results saved to '{output_file}'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\ngoogle_scrape_bulk(keywords, num_results=10)\nWhat does the `if __name__ == '__main__':` construct do in the script?\n{\n=It ensures that the script runs the main function only when executed directly.\n~It imports the main function from another module.\n~It defines the main function of the script.\n~It checks if the main function has been defined.\n~It prevents the script from running if imported as a module.\n~It initializes global variables for the script.\n}",
    "jawaban": "The correct answer is: keyword"
  },
  "1cda8ed8fb92543c0b462d223699105b232c8aedff2089a139506104f0e5b31b": {
    "soal": "from googlesearch import search\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef get_page_content(url):\ntry:\nheaders =\nresponse = requests.get(url, headers=headers, timeout=10)\nsoup = BeautifulSoup(response.content, 'html.parser')\n# Ambil judul halaman\ntitle = soup.title.string if soup.title else 'No Title'\n# Ambil konten paragraf utama\nparagraphs = soup.find_all('p')\ntext_content = ' '.join([p.get_text() for p in paragraphs[:5]]) # Batasi 5 paragraf pertama\nreturn title.strip(), text_content.strip()\nexcept Exception as e:\nreturn 'Error', f\"Failed to fetch content: {e}\"\ndef google_scrape_with_content(keywords, num_results=5, output_file='scraped_results.csv'):\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'Rank', 'Title', 'URL', 'Content'])\nfor keyword in keywords:\nprint(f\"\n\ud83d\udd0d Searching for: {keyword}\")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nprint(f\" \u2192 Fetching: {url}\")\ntitle, content = get_page_content(url)\nwriter.writerow([keyword, i+1, title, url, content])\ntime.sleep(2) # Delay biar aman\nexcept Exception as e:\nprint(f\"\u274c Error while searching '{keyword}': {e}\")\nprint(f\"\n\u2705 All results + content saved to '{output_file}'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\ngoogle_scrape_with_content(keywords, num_results=5)\nBy default, how many search results does the script retrieve for each keyword?\n{\n=5\n~10\n~15\n~20\n~25\n~30\n}",
    "jawaban": "The correct answer is: \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\""
  },
  "bc33775517e808e14f0a80bd890bdcfd137cc34049c06db5eec2d23680045595": {
    "soal": "101 (binary) AND 111 (binary) =",
    "jawaban": "The correct answer is: 101"
  },
  "8f47bb9f75fa6fa3de55cf605f39cd5de5e1e1235d6420d421bae23f63f0a2b0": {
    "soal": "The Distance Matrix widget creates a distance matrix, which is a four-dimensional array containing distances, taken pairwise, between elements of the dataset. The number of elements in the dataset determines the size of the matrix. Data matrices are very important for the Hierarchical Clustering Widget.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "28fb48e58eecbcd2ea398635ed94557b38d1fe7657f619d77c3285489ac1f28f": {
    "soal": "import os\nimport requests\nfrom googlesearch import search\nfrom bs4 import BeautifulSoup\nimport re\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef clean_filename(text):\n# Hilangkan karakter ilegal untuk nama file\nreturn re.sub(r'[\\/*?:\"<>|]', , text).strip().replace(' ', '_')[:50]<%1\u001f%\u001f0%> def get_page_content(url):\ntry:\nheaders =\nresponse = requests.get(url, headers=headers, timeout=10)\nsoup = BeautifulSoup(response.content, 'html.parser')\ntitle = soup.title.string if soup.title else 'No Title'\nparagraphs = soup.find_all('p')\ncontent = '\n\n'.join([p.get_text() for p in paragraphs[:5]])\nreturn title.strip(), content.strip()\nexcept Exception as e:\nreturn 'Error', f\"Gagal mengambil konten: {e}\"\ndef save_to_txt(keyword, rank, title, url, content, folder='outputs'):\nos.makedirs(folder, exist_ok=True)\nfilename = f\"{clean_filename(keyword)}_{rank}_{clean_filename(title)}.txt\"\nfilepath = os.path.join(folder, filename)\nwith open(filepath, 'w', encoding='utf-8') as f:\nf.write(f\"Keyword : {keyword}\n\")\nf.write(f\"Peringkat : {rank}\n\")\nf.write(f\"Judul : {title}\n\")\nf.write(f\"URL : {url}\n\n\")\nf.write(content)\ndef scrape_and_save_txt(keywords, num_results=5):\nfor keyword in keywords:\nprint(f\"\n\ud83d\udd0d Searching: {keyword}\")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nprint(f\" \u2192 ({i+1}) Fetching: {url}\")\ntitle, content = get_page_content(url)\nsave_to_txt(keyword, i+1, title, url, content)\ntime.sleep(2)\nexcept Exception as e:\nprint(f\"\u274c Error saat mencari '{keyword}': {e}\")\nprint(\"\n\u2705 Semua konten telah disimpan di folder 'outputs/'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\nscrape_and_save_txt(keywords, num_results=5)\nHow does the script ensure that the 'outputs/' directory exists before saving files?\n{\n=By using 'os.makedirs(folder, exist_ok=True)' to create the directory if it doesn't exist.\n~By checking manually and creating the directory using command-line commands.\n~By prompting the user to create the directory before running the script.\n~By using 'shutil.copytree' to copy an existing directory structure.\n~By using 'os.mkdir' without checking for existing directories.\n~By assuming the directory exists and handling errors if it doesn't.\n}",
    "jawaban": "The correct answer is: \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\""
  },
  "74dd679fb5495261e94e3961d4770795470622bb25a561fb3e21a33e2b7066e1": {
    "soal": "Widget kNN menggunakan algoritma kNN yang akan mencari k instance training terdekat di ruang feature dan menggunakan rata-rata feature terdekat tersebut untuk mem-prediksi\n",
    "jawaban": "The correct answer is: kNN"
  },
  "88a5438a68879ead463cd3b1e2491e8188329d5e215c968e518d388cfdad66c9": {
    "soal": "Word Cloud can be built from documents we have. In the text mining toolbox, there is a widget Import Documents to read document files. Before displaying it as a word cloud, it is advisable to pass it through the .................(1)........... widget first, to reduce unnecessary words such as conjunctions, etc. Then we can pass it to the Bag of Words widget first, or directly to the ..................(2)............... widget to display it.\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 Preprocess Text, (2) \u2192 Word Cloud"
  },
  "2f7ed07d2785ee80e330d96952898001bece288084bbeba86d81dc0bba6e19dc": {
    "soal": "Where can the softmax activation function be applied in a model?",
    "jawaban": "The correct answer is: Either in the output layer or within the loss function computation"
  },
  "3877429acf2a47c03ae0a465c6a038e97ef71fe602be21a994be26f6957732b5": {
    "soal": "Widget Select Columns can manually select data attributes and composition of the data domain.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "265233212e10f7b474f26a3c0377f2bf54ef55ab5ff0fa4eee9657d02fb720e1": {
    "soal": "When there is no data in the input, the Corpus widget reads the text corpus from a file and sends the corpus instance to the output channel. The history of the last opened file is stored in the Corpus widget. In the Corpus widget, there is also a directory with sample corpora pre-installed with the add-on. The widget can read data from Excel (.xlsx), comma-separated (................) and tab-delimited (.tab).\n\n\n",
    "jawaban": "The correct answer is: .csv"
  },
  "710d271ba31d998cc40105fcf65f7f292257d23a48ca9214db8bd32b2f21880a": {
    "soal": "What kind of response can LLMs produce?",
    "jawaban": "The correct answer is: Human-like text"
  },
  "a287c3e3213431af9659037733f5f1ca6d20d7a72074af9aad126a18b8dff160": {
    "soal": "from googlesearch import search\nimport csv\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef google_scrape_bulk(keywords, num_results=10, output_file='results.csv'):\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'Rank', 'URL'])\nfor keyword in keywords:\nprint(f\"\ud83d\udd0d Searching for: \")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nwriter.writerow([keyword, i+1, url])\ntime.sleep(1) # Delay agar tidak dianggap bot\nexcept Exception as e:\nprint(f\"\u274c Error while searching '{keyword}': {e}\")\nprint(f\"\n\u2705 All results saved to '{output_file}'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\ngoogle_scrape_bulk(keywords, num_results=10)\nWhat is the reason for including `time.sleep(1)` after writing each URL to the CSV file?\n{\n=To prevent being flagged as a bot by introducing a delay between requests.\n~To allow the CSV file to save data properly.\n~To synchronize with the rate limits of the Google API.\n~To ensure that the search results are accurate.\n~To give the system time to process each search query.\n~To reduce the load on the local machine's CPU.\n}",
    "jawaban": "The correct answer is: keyword"
  },
  "9ceafa697401250decdc622ec5f68df42590ae27e222d91ba09edca0c7986744": {
    "soal": ".................. Science is an interdisciplinary field that uses methods, processes, algorithms, and scientific systems to extract knowledge and insights from data in various forms, both structured and unstructured, similar to data mining.\n",
    "jawaban": "The correct answer is: Data"
  },
  "4922a03a3f65a2886bc2b14141ca9c596e97d34612e929cbabb56ef83bb2d790": {
    "soal": "Data Mining is critical to supporting the success of modern data-driven organizations. An IDG survey of 70 IT and business leaders found that 92% of respondents want to apply advanced analytics more broadly across their organizations. The same survey found that the benefits of data mining are deep and wide.\nIn fact, respondents identified no fewer than 30 different ways in which data mining positively impacts their business. Here are the top 10:\n- Improving decision-making processes\n- Improving security risk posture\n- Improving Planning and Forecasting\n- Competitive advantage\n- Adding costs\n- Customer acquisition\n- New revenue streams\n- Acquisition/retention of new customers\n- Strengthening customer relationships\n- New product development",
    "jawaban": "The correct answer is 'False'."
  },
  "30f30c34a69655ef5f55518142ca6c454c2162857a9c93ffa4666bc6e07b11de": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nplt.imshow(x_train[0], cmap='gray')\nplt.title(f\"Label: \")\nplt.show()\nmodel = keras.Sequential([\nkeras.layers.Flatten(input_shape=(28, 28)),\nkeras.layers.Dense(128, activation='relu'),\nkeras.layers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nWhat does model.evaluate() return?\n{\n= Test loss and test accuracy\n~ Training history\n~ Model weights\n~ Prediction probabilities\n~ Confusion matrix\n~ Only test loss\n}",
    "jawaban": "The correct answer is: y_train[0]"
  },
  "5dc5c4ab7d8c00a9b7de34527a3d6e7aec876b3adcfeaab16462854daf0bc748": {
    "soal": "import pandas as pd\nfile_path = \"/content/drive/My Drive/NamaFolder/NamaFile.xlsx\"\ndf = pd.read_excel(file_path)\nprint(df.head())\nWhich method is used to write a DataFrame to an Excel file?",
    "jawaban": "The correct answer is: df.to_excel()"
  },
  "de0d057873778b6aa23dbed65a7723831598333ed92e37c3fe0aab3013d52dcd": {
    "soal": "Widget Line Plot is a standard visualization widget, displaying the data profile, usually in the form of ordered numeric data. In this simple example, we will display the iris data in a line plot, grouped by iris attributes. The plot shows how petal length separates the ..........(1)........... values well.\n\nIf we observe this in the Scatter Plot Widget, we can confirm this is indeed true. .............(2)........... petal length is an interesting attribute for class separation, especially when enhanced with petal width, which is also well separated in the line plot.\n\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 class, (2) \u2192 length"
  },
  "848f3fe6aaad70d5590877c13383e50334a4dc64d088573d235bb46b8f811af0": {
    "soal": "There are three main challenges faced by businesses with Big Data:\n- Protecting sensitive and personal information\n- Data rights and ownership\n- Many talents (such as data scientists) to analyze data",
    "jawaban": "The correct answer is 'False'."
  },
  "e5a6ccd4c53ef86d0915fa6b16eec8e99cc85b2699e5f84ae4cb3d6e06976aaa": {
    "soal": "import os\nimport requests\nfrom googlesearch import search\nfrom bs4 import BeautifulSoup\nimport re\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef clean_filename(text):\n# Hilangkan karakter ilegal untuk nama file\nreturn re.sub(r'[\\/*?:\"<>|]', , text).strip().replace(' ', '_')[:50]<%1\u001f%\u001f0%> def get_page_content(url):\ntry:\nheaders =\nresponse = requests.get(url, headers=headers, timeout=10)\nsoup = BeautifulSoup(response.content, 'html.parser')\ntitle = soup.title.string if soup.title else 'No Title'\nparagraphs = soup.find_all('p')\ncontent = '\n\n'.join([p.get_text() for p in paragraphs[:5]])\nreturn title.strip(), content.strip()\nexcept Exception as e:\nreturn 'Error', f\"Gagal mengambil konten: {e}\"\ndef save_to_txt(keyword, rank, title, url, content, folder='outputs'):\nos.makedirs(folder, exist_ok=True)\nfilename = f\"{clean_filename(keyword)}_{rank}_{clean_filename(title)}.txt\"\nfilepath = os.path.join(folder, filename)\nwith open(filepath, 'w', encoding='utf-8') as f:\nf.write(f\"Keyword : {keyword}\n\")\nf.write(f\"Peringkat : {rank}\n\")\nf.write(f\"Judul : {title}\n\")\nf.write(f\"URL : {url}\n\n\")\nf.write(content)\ndef scrape_and_save_txt(keywords, num_results=5):\nfor keyword in keywords:\nprint(f\"\n\ud83d\udd0d Searching: {keyword}\")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nprint(f\" \u2192 ({i+1}) Fetching: {url}\")\ntitle, content = get_page_content(url)\nsave_to_txt(keyword, i+1, title, url, content)\ntime.sleep(2)\nexcept Exception as e:\nprint(f\"\u274c Error saat mencari '{keyword}': {e}\")\nprint(\"\n\u2705 Semua konten telah disimpan di folder 'outputs/'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\nscrape_and_save_txt(keywords, num_results=5)\nWhat is the role of the 'clean_filename' function in the script?\n{\n=To sanitize and format strings to create valid filenames by removing illegal characters.\n~To clean the extracted content by removing HTML tags.\n~To validate URLs before making HTTP requests.\n~To normalize the text encoding of the extracted content.\n~To remove stopwords from the extracted text.\n~To convert all text to lowercase for uniformity.\n}",
    "jawaban": "The correct answer is: \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\""
  },
  "d383384961be3ebbf6b3ec342ec48f51f548549ba1490ca022558b17bfc9f5ea": {
    "soal": "The workflow clusters data items in the iris dataset by first checking the distances between data instances using the Distances widget. The distance matrix is passed to the Hierarchical Clustering widget, which creates a dendrogram. We can select different parts of the dendrogram to analyze related data further.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "e30f3ceea7f06534d33fa6fc912c454b3014b6c396fe148c87bfb7d1b6e4216c": {
    "soal": "Big data is a term referring to extremely large and complex datasets obtained from various sources, including business transactions, social data, census data, medical data, scientific data, and others. The main characteristics of big data are volume, velocity, and variety.\n- Volume refers to the enormous amount of data created constantly.\n- Velocity refers to the speed at which data is created and updated, as well as the ability to process data in real-time.\n- Variety refers to the different sources and types of data that can be collected.\nBig data typically requires special technology and techniques to process, analyze, and interpret the data, including machine learning, data mining, and statistical analysis techniques. The goal of big data analysis is to gain valuable insights and information, such as trends and patterns useful for business, scientific, or social decision-making.",
    "jawaban": "The correct answer is 'True'."
  },
  "1d215cfd0d5a0254e4ea011b17e515d1708e47759df1e83d4624fa171c1c1421": {
    "soal": "Which of the following is a characteristic of NoSQL databases?",
    "jawaban": "The correct answer is: Schema-less data models"
  },
  "8544fcfd5bf4365442ba2667e12988d78083d73048b64c1595761550ecaa80ec": {
    "soal": "\n\n\nThe image above shows the sequence of processes in data science, which are:\n\nASK - Ask interesting questions (such as, What do we want to achieve / scientific goal? What would we do if we had all the images? What do we want to predict / estimate?\nGET - Obtain data.\nEXPLORE - Explore data, looking for anomalies, patterns, etc.\nMODEL - Create, fit, and validate models.\nVISUALIZATION - Communicate and visualize the data.",
    "jawaban": "The correct answer is 'False'."
  },
  "c790e250826415f45a26b0b05de8f05f0f86cf0a48d5f34f5fa5145fda0bc14d": {
    "soal": "The Text Preprocessing widget in ORANGE cannot process the Corpus based on the method we choose.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "b1862a4f27b3c318d272ee6acf41385650001fa265fdb8a5415e6f5b1f263024": {
    "soal": "import os\nimport requests\nfrom googlesearch import search\nfrom bs4 import BeautifulSoup\nimport re\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef clean_filename(text):\n# Hilangkan karakter ilegal untuk nama file\nreturn re.sub(r'[\\/*?:\"<>|]', , text).strip().replace(' ', '_')[:50]<%1\u001f%\u001f0%> def get_page_content(url):\ntry:\nheaders =\nresponse = requests.get(url, headers=headers, timeout=10)\nsoup = BeautifulSoup(response.content, 'html.parser')\ntitle = soup.title.string if soup.title else 'No Title'\nparagraphs = soup.find_all('p')\ncontent = '\n\n'.join([p.get_text() for p in paragraphs[:5]])\nreturn title.strip(), content.strip()\nexcept Exception as e:\nreturn 'Error', f\"Gagal mengambil konten: {e}\"\ndef save_to_txt(keyword, rank, title, url, content, folder='outputs'):\nos.makedirs(folder, exist_ok=True)\nfilename = f\"{clean_filename(keyword)}_{rank}_{clean_filename(title)}.txt\"\nfilepath = os.path.join(folder, filename)\nwith open(filepath, 'w', encoding='utf-8') as f:\nf.write(f\"Keyword : {keyword}\n\")\nf.write(f\"Peringkat : {rank}\n\")\nf.write(f\"Judul : {title}\n\")\nf.write(f\"URL : {url}\n\n\")\nf.write(content)\ndef scrape_and_save_txt(keywords, num_results=5):\nfor keyword in keywords:\nprint(f\"\n\ud83d\udd0d Searching: {keyword}\")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nprint(f\" \u2192 ({i+1}) Fetching: {url}\")\ntitle, content = get_page_content(url)\nsave_to_txt(keyword, i+1, title, url, content)\ntime.sleep(2)\nexcept Exception as e:\nprint(f\"\u274c Error saat mencari '{keyword}': {e}\")\nprint(\"\n\u2705 Semua konten telah disimpan di folder 'outputs/'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\nscrape_and_save_txt(keywords, num_results=5)\nHow does the script handle empty lines in the 'keywords.txt' file?\n{\n=It ignores them and does not include them in the list of keywords.\n~It raises an error and stops execution.\n~It includes them as empty strings in the list of keywords.\n~It replaces them with a default keyword.\n~It logs a warning message but continues execution.\n~It prompts the user to enter a replacement keyword.\n}",
    "jawaban": "The correct answer is: \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\""
  },
  "9fb84d6e9ee4fcc4a97de8ee0bcc33a8ce61530873200194ccedcbb3e64583bc": {
    "soal": "\n\n\n\n\n\nPada confusion matrix di WorkFlow di atas kita bisa melihat kesalahan klasifikasi yang terjadi\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "9ebb4597710b07ae23cdf2b99f634963965d438cbf1ce53a5f5de9dc0a391da2": {
    "soal": "The Venn Diagram widget can also be used to explore different prediction models. In the following example, we analyze 3 prediction methods, namely Naive Bayes, SVM, and Random Forest, according to their classification errors on data instances.\n\nBy selecting misclassification in three Confusion Matrix widgets and sending them to the Venn diagram widget, we can see all misclassification examples visualized per method used. Then we open the Venn Diagram widget and select, for example, misclassified instances identified by all four methods. This is represented as the intersection of the three circles. Click on the intersection to see these two examples marked in the Scatter Plot widget. Try selecting different parts of the diagram to see how the scatter plot visualization changes.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "06cffae9fd728a358e664d515c871f5aa9f3cd05aab6050336394f3f624c0169": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nx_train = np.random.rand(1000, 10)\ny_train = np.random.randint(2, size=(1000, 1))\nmodel = keras.Sequential([\nkeras.layers.Dense(64, activation='relu', input_shape=(10,)),\nkeras.layers.Dense(32, activation='relu'),\nkeras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer='adam',\nloss='binary_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=10, batch_size=32)\nWhich module is used to access `Sequential`?",
    "jawaban": "The correct answer is: keras"
  },
  "a0bdee9c62c73f6839437660fecb20f13c5f4efaa0c94310bf3b423dfcb42563": {
    "soal": "The Distances widget ignores discrete values and calculates distances only for ................. data, so the Distance Map widget can only display the distance map for distance map if we prepare the data ............... first.\n",
    "jawaban": "The correct answer is: continuous"
  },
  "9a37c76ca4cf3f7d6b93041f755f15f705d254bdf7c8dde0ee2fd90c2f8e3b1d": {
    "soal": "The Data Info widget is a simple widget that presents information about the dataset size, features, target, attributes ........................, and location.\n\n\n",
    "jawaban": "The correct answer is: meta"
  },
  "9c509586b494cac39033992a96115d02e67a2c0967943b5d4f55d7179ba8c6bc": {
    "soal": "The Distance Matrix widget creates a distance matrix, which is a two-dimensional array containing distances, taken pairwise, between elements of ................. The number of elements in the dataset determines the size of the matrix. Data matrix is essential for the Hierarchical Clustering Widget.\n\n\n",
    "jawaban": "The correct answer is: dataset"
  },
  "299faf5038368a2f51de78dcfd84d34a47d992fe378f7295093355efafc33dcb": {
    "soal": "The Bag of Words model creates a .................. with word counts for each data instance (document). The count can be absolute, binary (present or not) or sublinear (logarithmic of term frequency). The Bag of Words model is required in combination with the Word Enrichment widget and can be used for predictive modelling.\n\n\n",
    "jawaban": "The correct answer is: corpus"
  },
  "d0093826aee8f22a54c5cc8aad569723617d46ab7bc41af2ec0470f29f3d7bb4": {
    "soal": "Which service in the `docker-compose.yml` file depends on both Ollama and PostgreSQL services?",
    "jawaban": "The correct answer is: Open-WebUI"
  },
  "95f2521509873859be141ee3313f6042682585302ae00bcefa960b63437c9176": {
    "soal": "The Corpus widget can load a collection of text documents, (optional) tagged with categories, or convert input data into text.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "c99941dd9719fd74e62db7f4a9a60d8b4e2c139df1690e07f2fc99674deb8584": {
    "soal": "Contoh sederhana dengan Calibrated Learner. Kita menggunakan dataset titanic karena widget ini membutuhkan nilai binary class (dalam hal ini mereka adalah 'survived\u2019 atau \u2018not survived\u2019).\n\nKita menggunakan Logistic Regression sebagai base learner yang akan dikalibrasi dengan nilai setting default, yaitu dengan sigmoid optimization dari distribusi nilai dan di optimasi dengan PCA.\n\nMembandingkan hasil dari uncalibrated Logistic Regression model kita akan melihat dengan jelas bahwa calibrated model lebih baik.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "6a9198e8d49d01938a392858311c2bdcf7efa7e40eb27bdda0bcb37f66329450": {
    "soal": "Which PM4Py visualization method is known for handling noisy data effectively?",
    "jawaban": "The correct answer is: Heuristic Net"
  },
  "baf95dce3338185121df98fc5d6ed69e30ecb66fd9e9a460af0db14630d1f5e5": {
    "soal": "A simple example with Calibrated Learner. We use the Titanic dataset because this widget requires binary class values (in this case, they are 'survived' or 'not survived').\n\nWe use Logistic Regression as the base learner, which will be calibrated with the default settings, with sigmoid optimization from the value distribution and optimized with PCA.\n\nBy comparing the results of the uncalibrated Logistic Regression model, we will clearly see that the calibrated model is better.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "81606870b5bab107de70b5c37c95f5c6cac621840b8a51912b22b9ffa10475d4": {
    "soal": "Apache Hadoop is an open-source software framework written exclusively in Python for distributed storage and processing of very large data sets on clusters of computers built from commodity hardware. All modules in Hadoop are designed with the fundamental assumption that hardware failures (individual machines or racks of machines) are common and should thus be handled automatically in software by the framework.",
    "jawaban": "The correct answer is 'False'."
  },
  "9373b38c8b71c8d1710a92da8c0a213b8068e95b3b05767733ff097403ad71f1": {
    "soal": "In the Scatter Plot widget, the default tool is Select, which selects data instances within the selected rectangular area. Pan allows us to move the scatter plot around. With Zoom, we can zoom in and out by moving the keyboard, while Reset zoom resets the visualization to the optimal size.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "3a74ad431269b356dd291aaf61bf4d6304a92ac56ca672c65305f3509e852ae0": {
    "soal": "USB flash drive 1 Mbyte can store 8000000 bits of information",
    "jawaban": "The correct answer is 'True'."
  },
  "a34a2084cdab4ca3884dae2dd19702661aa97b0db37579ac26e69c86a2a72560": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nHow many color channels do the images in the CIFAR-100 dataset have?",
    "jawaban": "The correct answer is: 3"
  },
  "25387a08d7b6687341b30010443afe828793feb39715682c5255834be5dffccf": {
    "soal": "\n\n\nExamples of business intelligence (BI) usage/applications include,\n\n\n\n",
    "jawaban": "The correct answers are: Price Optimization, Inventory Management"
  },
  "2c606531d9f3b2c2da59645655b35b76563cb63b4375ca6b0a92bd8e69a59ff9": {
    "soal": "import pandas as pd\nfile_path = \"/content/drive/My Drive/NamaFolder/NamaFile.xlsx\"\ndf = pd.read_excel(file_path)\nprint(df.head())\nWhat is the purpose of the 'import pandas as pd' statement in the given code?",
    "jawaban": "The correct answer is: It imports the pandas library and assigns it the alias 'pd'."
  },
  "0a0ea7d59b9ae9f9c2e3e9e5a3ac7016d061cbd3e0aa5605ca022adcf43d6717": {
    "soal": "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target biner\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(0, 2, 100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi data latih dan data uji (80% latih, 20% uji)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi dan pelatihan model klasifikasi (misalnya RandomForest)\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n# Prediksi pada data uji\ny_pred = model.predict(X_test)\n# Menghitung Precision, Recall, dan F1-Score\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nprint(f'Precision: ')\nprint(f'Recall: {recall:.2f}')\nprint(f'F1-Score: {f1:.2f}')\n# Alternatif: Menggunakan classification_report untuk ringkasan metrik\nprint('\nClassification Report:')\nprint(classification_report(y_test, y_pred))\nWhich class in scikit-learn is used to create a Random Forest classifier?\n{\n~DecisionTreeClassifier\n~SVC\n=RandomForestClassifier\n~GradientBoostingClassifier\n~KNeighborsClassifier\n~LogisticRegression\n}",
    "jawaban": "The correct answer is: precision:.2f"
  },
  "28298475fa875d26a05e2b5dcee545577d187d09013eee1788f1de618dbe76b7": {
    "soal": "In the Interpolate widget, which is part of the Time Series add-on, the interpolation type - Spline interpolation fits a cubic polynomial to the values around the missing values. Therefore, this technique will be very slow but will provide the best results.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "30935b771baa774064aee953c791450862475d8881aea2cf1410ccf103fda14a": {
    "soal": "import pandas as pd\ndf = pd.read_excel('path_ke_file.xlsx', sheet_name=0)\ndf_selected = df[['Kolom1', 'Kolom2']]\nprint(df_selected.head())\nWhich parameter is used to skip the last 5 rows when reading an Excel file?",
    "jawaban": "The correct answers are: 5, 5, skipfooter, 5, 5, 5, 5"
  },
  "f95f3cea61eb55f5544d6ea91800b63ff8d7a81ee77590a0fd70e7728993af10": {
    "soal": "Widget ............................ can detect clusters in a network.\n",
    "jawaban": "The correct answer is: Network Clustering"
  },
  "dbba3ed27d37ae33f427d554d041af7dd415fe714480f92fe98a534e8ceadc44": {
    "soal": "The workflow clusters data items in the iris dataset by first checking the ..................... between data instances using the Distances widget. The Distance Matrix is passed to the Hierarchical Clustering widget, which creates a dendrogram. We can select different parts of the dendrogram to further analyze the related data.\n",
    "jawaban": "The correct answer is: Distances"
  },
  "ca8bfce718f960fab19164ad1ee96b1ea72110406987a2ea3e8a15a359e70aa3": {
    "soal": "Frequent activity paths are explicitly modeled by:",
    "jawaban": "The correct answer is: Heuristic Miner"
  },
  "ea7eabb931ba90bb41522b687dafcabd4cf1db43e9b3e5091934517acfe6731e": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#_xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nmap = pm4py.discover_heuristics_net(log)\npm4py.view_heuristics_net(map)\nWhat library is imported in the given Python script?",
    "jawaban": "The correct answer is: pm4py -python -pandas -numpy -scikit-learn -matplotlib"
  },
  "7508c777e8e55e2a0d8824219105ef848bc678db2b522201cc350176df893c78": {
    "soal": "The output data type from the Time Series Widget ORANGE is a table, so we can connect this data to other widgets/modules that can accept tables.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "9b3e1b741f324d133d88a624246d038d3a585ffe2800868df6fb6aa9e6fd9579": {
    "soal": "from sklearn.model_selection import train_test_split\nimport pandas as pd\n# Contoh: Membuat DataFrame dengan fitur 'X' dan target 'y'\ndata =\ndf = pd.DataFrame(data)\n# Memisahkan fitur dan target\nX = df[['X1', 'X2']]\ny = df['y']\n# Membagi dataset menjadi data pelatihan dan pengujian dengan stratifikasi\nX_train, X_test, y_train, y_test = train_test_split(\nX, y, test_size=0.3, stratify=y, random_state=42\n)\n# Menampilkan proporsi kelas di setiap subset\nprint(\"Proporsi kelas di y_train:\n\", y_train.value_counts(normalize=True))\nprint(\"\nProporsi kelas di y_test:\n\", y_test.value_counts(normalize=True))\nAfter performing a stratified split, how can you verify that the class proportions are maintained?\n{\n~By plotting a histogram of the feature distributions.\n~By checking the mean and standard deviation of the features.\n=By using `value_counts(normalize=True)` on the target variable in both training and test sets.\n~By calculating the correlation between features.\n~By performing a chi-square test on the subsets.\n~By visualizing the data using a scatter plot.\n}",
    "jawaban": "The correct answer is: 'X1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'X2': [11, 12, 13, 14, 15, 16, 17, 18, 19, 20], 'y': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]"
  },
  "798b89ea47c26fdbf9e993dc037e59db81c8820e7205a2bb00e9c1e5f8d2c724": {
    "soal": "Orange adalah Machine Learning open source dan visualisasi data untuk para ahli saja.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "e31d3958283521884e120f166053ea9a596aaa77487a2f744e7c56fd644992e1": {
    "soal": "import pandas as pd\n# Contoh: Membuat DataFrame dengan missing values\ndata =\ndf = pd.DataFrame(data)\n# Menampilkan jumlah missing values di setiap kolom\nprint(\"Jumlah missing values per kolom:\")\nprint(df.isnull().sum())\n# Menghapus baris yang memiliki missing values\ndf_cleaned = df.dropna()\nprint(df_cleaned)\n# Mengisi missing values pada kolom 'Usia' dengan mean\ndf['Usia'].fillna(df['Usia'].mean(), inplace=True)\n# Mengisi missing values pada kolom 'Usia' dengan median\ndf['Usia'].fillna(df['Usia'].median(), inplace=True)\n# Mengisi missing values pada kolom 'Kota' dengan modus\ndf['Kota'].fillna(df['Kota'].mode()[0], inplace=True)\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n# Inisialisasi imputer untuk mengisi missing values dengan mean\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n# Mengimputasi kolom 'Usia'\ndf[['Usia']] = imputer.fit_transform(df[['Usia']])\nWhich method combines both fitting the SimpleImputer and transforming the data in one step?\n{\n=fit_transform()\n~fit()\n~transform()\n~apply()\n~impute()\n~replace()\n}",
    "jawaban": "The correct answer is: 'Nama': ['Andi', 'Budi', 'Citra', 'Dewi'], 'Usia': [25, 30, None, 22], 'Kota': ['Jakarta', None, 'Bandung', 'Surabaya']"
  },
  "9a5e31403b29b1dace5018468bd35bead5c4b1c050a3cb7c4250df8f275c12a3": {
    "soal": "What is recommended to use when scraping Google's HTML pages directly to reduce the risk of being blocked?",
    "jawaban": "The correct answer is: Using a proxy or an API like SerpApi."
  },
  "15e9d272ecf60f613e66e3966285b90834d3b3540da99c8e3dd2bd6f68e170f1": {
    "soal": "Data doesn't always come in a nice table format. Data can also be in the form of text, audio recordings, video material, or even images. However, computers can only work with numbers, so for every data mining, we need to convert this unstructured data into numerical representation.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "b9643f23e87a107ee58092eacbb3a98386507d806a66ac4996cd4574af1a4089": {
    "soal": "Which elements are included in a confusion matrix?",
    "jawaban": "The correct answer is: True positives, true negatives, false positives, and false negatives."
  },
  "5d0ba710fa6a23daffb3d672c14200623644af9a80f47f15532344669c6182c7": {
    "soal": "11011 (binary) + 110 (binary) = ..............................\u00a0 (decimal)\n",
    "jawaban": "The correct answer is: 33"
  },
  "031bf86490ba37b9ddf818c5f9db1b812c6cd857e599377c21e45f3ce97c6819": {
    "soal": "To obtain numerical representations of the images from Image Import, we need to send the images to the Image Embedding widget as shown in the workflow below.\n\nNow we have the numbers we want. There are 2048 of them (columns n0 to n255). From now on, we can apply all standard machine learning techniques, such as clustering.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "ef8245f16bc9156fa5ee9da8465f0038974aff0b01873767f129ce034cab4e0b": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#-xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\ndfg, start_activities, end_activities = pm4py.discover_dfg(log)\npm4py.view_dfg(dfg, start_activities, end_activities)\nWhat format is used for visualizing the DFG?",
    "jawaban": "The correct answer is: Graphical diagram -Tabular format -JSON format -CSV format -Text file -Excel format"
  },
  "6e7a22fa6faec43700330a9cef4d8613196509800fb966f64c3c250655d82468": {
    "soal": "This workflow example shows how Network Analysis can enrich ..........(1)............ We used the lastfm.net data as the input network for the Network File Widget and sent it to the ..............(2)............. Widget. We can decide to compute the degree, degree centrality, and closeness centrality at the ...........(3)..............-level.\n\nWe can then visualize the network in the Network Explorer widget. In the Network Explorer widget, it colors with the best tags, like the default for this dataset. But now we can also adjust the node size to fit the degree centrality results. This is a good way to visualize network properties.\n\n\n\n",
    "jawaban": "The correct answer is: (3) \u2192 node, (1) \u2192 workflow, (2) \u2192 Network Analysis"
  },
  "b4de1c50d0e9c4809d36af1ab7d24d87efdb35e8a3586a885b89a34c9054fe66": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nx_train = np.random.rand(1000, 10)\ny_train = np.random.randint(2, size=(1000, 1))\nmodel = keras.Sequential([\nkeras.layers.Dense(64, activation='relu', input_shape=(10,)),\nkeras.layers.Dense(32, activation='relu'),\nkeras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer='adam',\nloss='binary_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=10, batch_size=32)\nWhich Python library is used to build the neural network in the example?",
    "jawaban": "The correct answer is: keras"
  },
  "fbcce49a7f28025f575f17f38c808e099d671aa5ae66a21c93467a8938b45120": {
    "soal": "What does the term 'feature engineering' refer to?",
    "jawaban": "The correct answer is: Creating new input features from existing ones to improve model performance"
  },
  "5a080d45680e6163c43cbcfa9f55a7a94b7ef00ce435e9518b8cebec47098762": {
    "soal": "The Sieve Diagram widget plots the class probability against the probability predicted by the classifier.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "7be4dc371b02aeab9a32770d7fcc51bca945956f15103a03455eb0861995db16": {
    "soal": "\n\n\n\n\n\n\n\n\nIn general, machine learning techniques are divided into three (3) categories:\n\nSupervised Learning.\nUnsupervised Learning.\nReinforced / Reinforcement Learning\n",
    "jawaban": "The correct answer is 'True'."
  },
  "203f5ad6507a4492a50152059a9d9fb729b44ddb8888f201a6901b0f96652649": {
    "soal": "from googlesearch import search\nimport csv\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef google_scrape_bulk(keywords, num_results=10, output_file='results.csv'):\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'Rank', 'URL'])\nfor keyword in keywords:\nprint(f\"\ud83d\udd0d Searching for: \")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nwriter.writerow([keyword, i+1, url])\ntime.sleep(1) # Delay agar tidak dianggap bot\nexcept Exception as e:\nprint(f\"\u274c Error while searching '{keyword}': {e}\")\nprint(f\"\n\u2705 All results saved to '{output_file}'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\ngoogle_scrape_bulk(keywords, num_results=10)\nWhat are the headers written to the output CSV file?\n{\n=Keyword, Rank, URL\n~Search Term, Position, Link\n~Query, Result Number, Web Address\n~Term, Order, Hyperlink\n~Phrase, Index, Site\n~Keyword, Title, Snippet\n}",
    "jawaban": "The correct answer is: keyword"
  },
  "a06668ae1b6a57b387ae462e9ac2ae17ed99a5f560c3b07c6927ade0884d7773": {
    "soal": "The Line Plot widget is a standard visualization widget that displays data profiles, typically numeric data arranged in order. In this simple example, we will display iris data in a line plot, grouped by iris attributes. The plot shows how petal length separates class values well.\n\n\nIf we observe this in the Scatter Plot widget, we can confirm this is true. Petal length is an interesting attribute for class separation, especially when enhanced with petal width, which is also well separated in the line plot.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "7a64478427adcdd9fb2f745c26e91c185258bfeec1c6d126e3edb2c5a8376f33": {
    "soal": "Widget Network Clustering can help us uncover clusters and highly connected groups in a network. First, we will use the Widget ...........(1)............... to load the lastfm.net dataset. Then we will send this network to the Network Clustering Widget. The ..............(2)............. widget finds 79 clusters in the network. To visualize the results, we use the Network Explorer Widget and set the attribute Color to Cluster. This will color ...........(3)............... nodes in the network with the appropriate cluster color - this is a good way to visualize highly connected groups in a dense network.\n\nKeep in mind that the Network Explorer Widget here will color the 10 largest clusters and color the rest as 'Other'.\n\n\n\n",
    "jawaban": "The correct answer is: (2) \u2192 Network Clustering, (1) \u2192 Network File, (3) \u2192 node"
  },
  "2d6db6a82b86fa5a3fe5c55817f0b82d01c6c3eb0773108c6b32ac4cfb8e6bcd": {
    "soal": "In the workflow below, the Import Documents widget can load .................... We store President Kennedy's speeches in two (2) subfolders, pre-1962 and post-1962. If we load the parent folder, both subfolders will be used as class labels. Check the output of the Import Documents widget in the Data Table widget.\n\n\n\n\n",
    "jawaban": "The correct answer is: subfolder"
  },
  "65216d32fc3596684b2e2ec4bd6445b0605bc4c72c6ea7076663490e9e2ec560": {
    "soal": "A sample use of the kNN widget for regression task is shown in the workflow below. This workflow shows how to use the Learner output. For this example, we use the housing dataset. We input the kNN prediction model into Predictions and observe the predicted values.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "cd33effc0c8f9772536dafd286e81c7b8ffc1fc64608bf97f4131ebbbca1b773": {
    "soal": "Word Cloud can be built from documents that we have. In the text mining toolbox, there is an Import Documents widget to read document files. Before displaying it as a word cloud, it is better to pass it through the Preprocess Text widget first, to reduce unnecessary words, such as connecting words, etc. Next, we must pass it through the Bag of Words widget first, and cannot go directly to the Word Cloud widget to display it.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "0e74ac97c70cfa592eaa7d4bfff7095d83712b37d6929386c99e892f0b508385": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#-xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\ndfg, start_activities, end_activities = pm4py.discover_dfg(log)\npm4py.view_dfg(dfg, start_activities, end_activities)\nWhich keyword is necessary for using external libraries in Python?",
    "jawaban": "The correct answer is: import -include -add -load -using -require"
  },
  "414cc356c137ec8c881f4c02926e05396073866fa7af3e7a92398b399bb682e5": {
    "soal": "The Tree algorithm is a simple algorithm that can separate data into nodes based on class purity (category/ .................). The Tree algorithm is the predecessor of the Random Forest algorithm. The Tree widget in Orange is designed in-house and can handle both discrete and continuous datasets.\n",
    "jawaban": "The correct answer is: class"
  },
  "28bcc8dfaf4d93a3459ddb352b9a85d0ef15aed80345d43de903bd06ceafc375": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nWhat is the resolution of each image in the CIFAR-10 dataset?",
    "jawaban": "The correct answer is: 32x32 pixels"
  },
  "5cf119402c1471a8a1b25fa919c723d9c463a843c161a2ca6ee7127008988a6d": {
    "soal": "The distance matrix generated by the Distances Widget can be further fed into the Hierarchical Clustering Widget to uncover groups in the data, into the Distance Map Widget or the Distance Matrix Widget to visualize distances (the Distance Matrix Widget can be very slow for large datasets), into the MDS Widget to map data samples using tensorflow distance, and finally, saved with the Save Distance Matrix Widget. The Distance File can be loaded with the Distance File Widget.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "65576d5f70e967bca6c85e79bff0cc69b64bcf1a73b2fa194fe9cf45cfffe130": {
    "soal": "import numpy as np\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n# Contoh data: nilai aktual dan prediksi\ny_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0]) # Nilai aktual\ny_pred = np.array([1, 0, 1, 0, 0, 1, 0, 1, 1, 0]) # Prediksi model\n# Menghitung confusion matrix\ncm = confusion_matrix(y_true, y_pred)\n# Menampilkan confusion matrix dalam bentuk teks\nprint(\"Confusion Matrix:\n\", cm)\n# Menampilkan confusion matrix dalam bentuk grafik\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.show()\nApa fungsi dari parameter cmap dalam fungsi ConfusionMatrixDisplay?",
    "jawaban": "The correct answer is: Untuk menentukan colormap yang digunakan dalam plot."
  },
  "a4872499005ceb809c92b4c43bddc947e875f154bd9ba2926bdcbdb5d8189807": {
    "soal": "101 (binary) + 10 (binary) + 1000 (binary) = ........ (binary)",
    "jawaban": "The correct answer is: 1111"
  },
  "8d0de5b50cc79d98813d58bcb202ab1045a3b758f1d352580e1b541a14c23cfc": {
    "soal": "Data Science is an interdisciplinary field that uses methods, processes, algorithms, and scientific systems to extract ............................... and insights from data in various forms, both structured and unstructured, similar to data mining.\n",
    "jawaban": "The correct answer is: knowledge"
  },
  "988796f055869411f0c457615cf1938ca2ca92b9d3ebb3da82ca3f1b84fb27d5": {
    "soal": "In ensemble learning, what is the purpose of bagging?",
    "jawaban": "The correct answer is: To reduce variance by combining predictions from multiple models trained on different subsets of data"
  },
  "69ac59a9c499d1edf0af91da6ffec23e02e88d933073ac27c9bd76a32746c94e": {
    "soal": "7 (decimal) = ?? (binary)",
    "jawaban": "The correct answer is: 111"
  },
  "35cc384c007c6c714118fdda5ed8bff7b62dbf251bbb8410b1c16a633dec0672": {
    "soal": "The kNN widget uses the kNN algorithm, which searches for the k nearest training instances in the feature space and uses the average of the nearest features to predict.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "b5749d41851f92ca1dbe3e186e5b8a8b1b297eccf672c7c74abb79fb2ac64b60": {
    "soal": "1 (binary) x 100 (decimal) = ....... (decimal)",
    "jawaban": "The correct answer is: 100"
  },
  "18a6eb02a5921de0428fa19677653fbbc7454e0323641b4e7a9836b1b26e1024": {
    "soal": "model.save('my_model')\nfrom google.colab import files\nfiles.download('my_model')\nnew_model = tf.keras.models.load_model('my_model')\nWhich method is used to load a previously saved TensorFlow model?",
    "jawaban": "The correct answer is: tf.keras.models.load_model('my_model')"
  },
  "a9fe84aaaa7d71ffe9bd2597324d40a8463a7850cbb11375a9bd81787ff79eae": {
    "soal": "import pandas as pd\n# Contoh: Membuat DataFrame dengan missing values\ndata =\ndf = pd.DataFrame(data)\n# Menampilkan jumlah missing values di setiap kolom\nprint(\"Jumlah missing values per kolom:\")\nprint(df.isnull().sum())\n# Menghapus baris yang memiliki missing values\ndf_cleaned = df.dropna()\nprint(df_cleaned)\n# Mengisi missing values pada kolom 'Usia' dengan mean\ndf['Usia'].fillna(df['Usia'].mean(), inplace=True)\n# Mengisi missing values pada kolom 'Usia' dengan median\ndf['Usia'].fillna(df['Usia'].median(), inplace=True)\n# Mengisi missing values pada kolom 'Kota' dengan modus\ndf['Kota'].fillna(df['Kota'].mode()[0], inplace=True)\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n# Inisialisasi imputer untuk mengisi missing values dengan mean\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n# Mengimputasi kolom 'Usia'\ndf[['Usia']] = imputer.fit_transform(df[['Usia']])\nWhich is the correct way to import SimpleImputer from scikit-learn?\n{\n=from sklearn.impute import SimpleImputer\n~from sklearn.preprocessing import SimpleImputer\n~from sklearn.impute import Imputer\n~from sklearn.preprocessing import Imputer\n~import SimpleImputer from sklearn\n~import Imputer from sklearn.impute\n}",
    "jawaban": "The correct answer is: 'Nama': ['Andi', 'Budi', 'Citra', 'Dewi'], 'Usia': [25, 30, None, 22], 'Kota': ['Jakarta', None, 'Bandung', 'Surabaya']"
  },
  "52312e30acf9ac2a8d66e0f955bdde5c215e012e3d7f8e9c390f201967ba7338": {
    "soal": "\n\n\n\n\n\nDalam contoh di atas, kita menggunakan dataset zoo dan mengirimkannya ke CN2 Rule Rotation. Kita bisa me-review dan meng-interpretasi model yang dibuat dengan CN2 Rule Viewer widget.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "14197ae4737588b833442681d40e4cd89e4e661fb661bd8409b9da3fb240aad5": {
    "soal": "What is the primary purpose of applying cross-validation methods in model evaluation?",
    "jawaban": "The correct answer is: To ensure the model generalizes well to unseen data"
  },
  "1c04618f5c46b4da1bd1d0db6c544c57faef8d6611d1f9ab038f2ebe384f2d6c": {
    "soal": "The Import Images widget scans the directory and returns ................ rows per image taken. The columns include image name, directory path to the image, width, height, and image size. The column with the directory path of the image is then used as an attribute for visualization and Image Embedding.\n",
    "jawaban": "The correct answer is: one"
  },
  "89126fc9c69a74eaf2f6ddf004bb1dd795336f0bd4fbce04688df556aaf0dcf4": {
    "soal": "Only two inputs are suitable for the Distance Matrix widget, which are the ..............(1)............... and the Distance Transformation widget. The output from the Distance Matrix widget is ...........(2)............. containing the distance matrix. Users can decide how to label the table and distance matrix (or examples in the distance matrix), which can then be visualized or displayed in a separate data table.\n",
    "jawaban": "The correct answer is: (1) \u2192 Distances, (2) \u2192 data table"
  },
  "9095e622d5f4dd26c274339e73872904b23306f4d75821a19f740bcc0b4ebb5b": {
    "soal": "The Distance Transform widget maps the distances present in the dataset.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "bf9438754503383dbcdc7f6e0a2d487a652c901dff8a47a42d407aa7fdf25dc4": {
    "soal": "import pandas as pd\n# Contoh: Membuat DataFrame dengan missing values\ndata =\ndf = pd.DataFrame(data)\n# Menampilkan jumlah missing values di setiap kolom\nprint(\"Jumlah missing values per kolom:\")\nprint(df.isnull().sum())\n# Menghapus baris yang memiliki missing values\ndf_cleaned = df.dropna()\nprint(df_cleaned)\n# Mengisi missing values pada kolom 'Usia' dengan mean\ndf['Usia'].fillna(df['Usia'].mean(), inplace=True)\n# Mengisi missing values pada kolom 'Usia' dengan median\ndf['Usia'].fillna(df['Usia'].median(), inplace=True)\n# Mengisi missing values pada kolom 'Kota' dengan modus\ndf['Kota'].fillna(df['Kota'].mode()[0], inplace=True)\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n# Inisialisasi imputer untuk mengisi missing values dengan mean\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n# Mengimputasi kolom 'Usia'\ndf[['Usia']] = imputer.fit_transform(df[['Usia']])\nWhich method is appropriate for imputing missing values in a column with a skewed distribution?\n{\n=Using the median of the column.\n~Using the mean of the column.\n~Using the mode of the column.\n~Removing the rows with missing values.\n~Filling with zero.\n~Filling with a constant value.\n}",
    "jawaban": "The correct answer is: 'Nama': ['Andi', 'Budi', 'Citra', 'Dewi'], 'Usia': [25, 30, None, 22], 'Kota': ['Jakarta', None, 'Bandung', 'Surabaya']"
  },
  "d7fba361e00b7f78e2da06ae78a9a17da36e5065c5592f2438adf0fcdfc7c101": {
    "soal": "..........(1).............. (PCA) transforms data into a dataset with uncorrelated variables, also called principal components. The ..........(2)............ widget displays a graph (scree diagram) that shows the degree of variance explained by the best principal components and allows you to interactively adjust the number of components to be included in the output dataset. In this workflow, we can observe the transformation in the Data Table and in ..............(3)..............\n\n\n",
    "jawaban": "The correct answer is: (2) \u2192 PCA, (1) \u2192 Principal Component Analysis, (3) \u2192 Scatter Plot"
  },
  "50530d0a2082c64eaa5ebab1bde931827fedbec5648a2dabc6567693e6009a2b": {
    "soal": "The ................(1)............... widget scans the directory and returns one row per image found. The columns include the image name, the path directory to the image, width, height, and image size. The column with the directory path to the image is then used as an attribute for visualization and ................(2)........... .\n",
    "jawaban": "The correct answer is: (2) \u2192 Image Embedding, (1) \u2192 Import Images"
  },
  "63265522c85158b0c4214263818b50208b9b8e411f21d80e01f26bfe059d0d33": {
    "soal": "What is a potential issue if a model performs well on training data but poorly on test data?",
    "jawaban": "The correct answer is: Overfitting."
  },
  "adbde58974b23487192de152467ac158356e3c452301747101aeb654f8029250": {
    "soal": "Why is direct scraping from 'https://www.google.com' discouraged?",
    "jawaban": "The correct answer is: Google protects its pages with anti-bot measures."
  },
  "64ada9974f6e83eec0b19e53fa999ef75eb3767facbee93e7d13788fa547fefb": {
    "soal": "The Text Preprocessing widget in ORANGE can process text files (ASCII) based on the method we choose.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "ab17211c906df791545a150186a60286d7f25efad240a06fe183411d5faaed31": {
    "soal": "To obtain numerical representations of the images from Image Import, we need to send the images to the Image Embedding widget as shown in the workflow below.\n\nNow we have the numbers we want. There are 2048 of them (columns n0 to n2047). From now on, we can apply all standard machine learning techniques, such as clustering.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "fb0407e23c87a2d6aedd9a738c6b635ce4c5dbb09a79a78889b787174a9205d8": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#-xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\ndfg, start_activities, end_activities = pm4py.discover_dfg(log)\npm4py.view_dfg(dfg, start_activities, end_activities)\nWhat is the likely domain of data processed by this script?",
    "jawaban": "The correct answer is: Process mining -Sentiment analysis -Network security -Image processing -Natural language processing -Stock market analysis"
  },
  "b07454141f52918b592af908492a73add2cb4eec093e94b6b67b7d09aeba02fc": {
    "soal": "010 (binary) x 100 (decimal) = ....... (decimal)",
    "jawaban": "The correct answer is: 200"
  },
  "7a3f846bd49a7f82289764f9802cfad91beae5415e74a18c64c6f37ba55ef2f2": {
    "soal": "What is the primary purpose of importing the `numpy` library in the provided Python code?",
    "jawaban": "The correct answer is: To generate and manipulate arrays and matrices."
  },
  "504dbf86a15cd999eb589252ee733951b277086a4f0d61bea46e3a36d399e867": {
    "soal": "In the PCA Widget in ORANGE2 - the number of transformation components can be selected from the Components Selection input box and must be dragged vertically on the cutoff line on the graph.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "7291b7985a234fe46e0fa6f7808fa9ccb6e22e37729ed2deb1f15ade2008ded0": {
    "soal": "instalasi orange3 yang baik bisa menggunakan perintah\n\npip3 install numpy scipy mkl nose sphinx pydot-ng pandas-datareader\npip3 install parameterized Theano cntk matplotlib sklearn seaborn h5py\npip3 install matplotlib pandas scipy sklearn seaborn features\npip3 install orange3\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "55416ac28841ea9e7c027bcedc980c633483644aa267969006c3dbd48ebf8f33": {
    "soal": "The Line Plot widget is a standard visualization widget that displays data profiles, typically numeric data arranged in order. In this simple example, we will display iris data in a line plot, grouped by iris attributes. The plot shows how petal width separates class values well.\n\n\nIf we observe this in the Scatter Plot widget, we can confirm this is true. Petal length is an interesting attribute for class separation, especially when enhanced with petal width, which is also well separated in the line plot.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "5478eed556e0e55db15f2f5e98f7adefe216fa799b38e0edc8875a42daad6937": {
    "soal": "PM4Py primarily analyzes and visualizes business processes from which type of files?",
    "jawaban": "The correct answer is: XES"
  },
  "ae015b4fc03fbb4c3d61bc1ff781ea115cf2da3ccbfb0b7a4be9f6e13b3ebd52": {
    "soal": "The ................. algorithm is a classification technique designed to efficiently induce simple and comprehensive rules in the form of \u201cif cond then predict class\u201d, even in domains where there may be noise.\n\n\n\n",
    "jawaban": "The correct answer is: CN2"
  },
  "4a56d1a1942d782b8665396102f91c247069f3952efe4ff61e3b511d9095519e": {
    "soal": "In the Orange workflow below, we can predict the classification of a text. The classification model is obtained using a logistic regression learner with training data from the Grimms Corpus. Text predictions from the Andersen Corpus are made by the Scatter Plot widget.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "7fe3144a3d14cf27010022cb4c6e7bdb26ad5e49ea3c7e2a6f0d2126aa7ce479": {
    "soal": "Network Clustering Widget tries to find clusters in the network. Network Clustering works with two algorithms, one from Raghavan et al. (2007), which uses label propagation to find suitable clusters, and one from Leung et al. (2009), which builds on Raghavan's work and adds node attenuation as a parameter for cluster formation.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "5e6146c59cf4dcf3f3325e2a2db7142ccde612c7d84fb587f8950963f5be6299": {
    "soal": "from googlesearch import search\nimport csv\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef google_scrape_bulk(keywords, num_results=10, output_file='results.csv'):\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'Rank', 'URL'])\nfor keyword in keywords:\nprint(f\"\ud83d\udd0d Searching for: \")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nwriter.writerow([keyword, i+1, url])\ntime.sleep(1) # Delay agar tidak dianggap bot\nexcept Exception as e:\nprint(f\"\u274c Error while searching '{keyword}': {e}\")\nprint(f\"\n\u2705 All results saved to '{output_file}'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\ngoogle_scrape_bulk(keywords, num_results=10)\nWhat is the role of the `enumerate` function in the script?\n{\n=To keep track of the rank (position) of each search result.\n~To count the number of keywords processed.\n~To iterate over the list of keywords.\n~To reverse the order of search results.\n~To filter out duplicate URLs from the results.\n~To sort the search results alphabetically.\n}",
    "jawaban": "The correct answer is: keyword"
  },
  "66534c0ff6a730626cf579410c428da076f0192f77d961811e541eda6c6a216c": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#_xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nmap = pm4py.discover_heuristics_net(log)\npm4py.view_heuristics_net(map)\nWhat function is used to read the XES file in the script?",
    "jawaban": "The correct answer is: pm4py.read_xes -pm4py.load_xes -pm4py.open_xes -pm4py.import_xes -pm4py.read_file -pm4py.get_xes"
  },
  "c705d712eaecffffd946191318688b772ab66d3c0f95d3b42ba45c777e6182a2": {
    "soal": "\n\n\nIn this example, we are using the lastfm dataset, which can be loaded into the Network .................... widget under Browse documentation networks. The network nodes represent musicians, who are marked by the genre they play, the number of albums produced, and so on. The edges represent the number of listeners on LastFm.\n\n\nThe entire dataset is visualized in the Network Explorer Widget. In this widget, we remove the coloring and adjust the node size to match the number of albums. Then we select a few nodes from the network. We can observe the selection in the Network Explorer Widget (1).\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: file"
  },
  "0c5da8aa80c087e4a007050fd3c0d2cde1d89f492d65c019271b39d0777614a8": {
    "soal": "Cassandra, or Apache Cassandra, is one of the open-source products for distributed database management by Apache. Cassandra is designed to manage large (big data) structured data spread across many servers. This software is highly scalable, so it is no surprise that many large companies, such as Facebook, Twitter, and Apple, have trusted Cassandra as one of their supporting tools.",
    "jawaban": "The correct answer is 'True'."
  },
  "363a9e411a62bdcfc46355115fe173714d1139d23d55cf588e6b879ab84baeab": {
    "soal": "For a more complex use of the Select Columns widget, we create a workflow to redefine the classification problem in the heart-disease dataset. Initially, the task was to predict whether the patient had coronary artery diameter narrowing. We change the problem to gender classification, based on age, chest pain, and cholesterol levels, while informally keeping diameter narrowing as a meta attribute.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "f1e5d7ac55f5b50bfc67e1008caaed7a88605fa8e2a9aeefa3e6a09fc8362ed1": {
    "soal": "111 (binary) = ...... decimal",
    "jawaban": "The correct answer is: 7"
  },
  "8746640c1781f08a182043eb8060450320cd976a2c0687c8c4ec922c5327c9e1": {
    "soal": "What is the decimal value of the binary number 1101...",
    "jawaban": "The correct answer is: 13"
  },
  "51e913bb9c79d93d583825330658af34db7b66c5d133698f3a34ece4291149a5": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nHow can you explicitly add a softmax activation function to a `Dense` layer?",
    "jawaban": "The correct answers are: By setting the `activation` parameter to 'softmax' in the `Dense` layer, True` when compiling the model"
  },
  "bf028bfb8805da820167079de384f3bcefe3358ed2149119e5250be39efe2892": {
    "soal": "Principal Component Analysis (PCA) transforms data into a dataset with uncorrelated variables, also called principal components. The .............. widget displays a graph (scree diagram) that shows the amount of variance explained by the best principal components and allows you to interactively adjust the number of components to include in the output dataset. In this workflow, we can observe the transformation in the Data Table and in the Scatter Plot.\n\n\n",
    "jawaban": "The correct answer is: PCA"
  },
  "5e5816c6af4911403e9285cfc74aeb8fa7001c7359324a7ae1b10280d7f1da03": {
    "soal": "What is TensorFlow Lite primarily used for?",
    "jawaban": "The correct answer is: Deploying machine learning models on mobile and edge devices"
  },
  "619b5d70fe5c106cc1ceece098d001dc6dcb87e49cd2ef160dce14c19d655d6d": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nWhich digits are represented in the MNIST dataset images?",
    "jawaban": "The correct answer is: 0 to 9"
  },
  "7acb580a3048023968156645b5ff3556a066751abb8f9a1cade094e7e2bcdada": {
    "soal": "How does LLM determine the answer?",
    "jawaban": "The correct answer is: By analyzing context and prompt"
  },
  "c971b6f49340faf6373a4055490fa99a5cbb1a2ae13cc2eef2f95c45d605e10f": {
    "soal": "What enables transformer models to be efficient in language tasks?",
    "jawaban": "The correct answer is: Their self-attention mechanism"
  },
  "3c0d20684f9f07e29db6e620b856149067134f20dda45a5da9f6afcdacfcae9e": {
    "soal": "import pandas as pd\n# Contoh: Membuat DataFrame dengan missing values\ndata =\ndf = pd.DataFrame(data)\n# Menampilkan jumlah missing values di setiap kolom\nprint(\"Jumlah missing values per kolom:\")\nprint(df.isnull().sum())\n# Menghapus baris yang memiliki missing values\ndf_cleaned = df.dropna()\nprint(df_cleaned)\n# Mengisi missing values pada kolom 'Usia' dengan mean\ndf['Usia'].fillna(df['Usia'].mean(), inplace=True)\n# Mengisi missing values pada kolom 'Usia' dengan median\ndf['Usia'].fillna(df['Usia'].median(), inplace=True)\n# Mengisi missing values pada kolom 'Kota' dengan modus\ndf['Kota'].fillna(df['Kota'].mode()[0], inplace=True)\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n# Inisialisasi imputer untuk mengisi missing values dengan mean\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n# Mengimputasi kolom 'Usia'\ndf[['Usia']] = imputer.fit_transform(df[['Usia']])\nWhich of the following strategies are supported by SimpleImputer for imputing missing values?\n{\n=mean\n=median\n=most_frequent\n=constant\n~mode\n~zero\n}",
    "jawaban": "The correct answer is: 'Nama': ['Andi', 'Budi', 'Citra', 'Dewi'], 'Usia': [25, 30, None, 22], 'Kota': ['Jakarta', None, 'Bandung', 'Surabaya']"
  },
  "c08fc335ba6ed695809e64c8efd1e6ccf7a4a518712feed07d4d1b09798ca921": {
    "soal": "111 (binary) + 100 (binary) = ?? (binary)",
    "jawaban": "The correct answer is: 1011"
  },
  "08c01c0a602062b0b3151df05b708e529c80f8bee9e28e219082a68be0246a1f": {
    "soal": "To group based on similarity, we can measure the distance between images. The numeric representation of images obtained from ................(1)..................., we measure the distance using the Distances widget. Usually, cosine distance works best for images, but you can experiment on your own. Then we will send the distance matrix to ................(2).................. to visualize similar pairs in the dendrogram.\n\nAs a result, all animals are correctly grouped. In the dendrogram of the clustering results, we do not see images of animals. We can use the Image Viewer widget to view the images.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 Image Embedding, (2) \u2192 Hierarchical Clustering"
  },
  "0dcede2ad09ec2e997579c713690a7e3c04618e6cc65482a707917b6afad6d10": {
    "soal": "In the workflow below, we use the Zoo dataset. We load the data into the ................. widget, which allows us to select a subset of data instances. Then, we can send the selected data instances to the Save Data widget to save them into a file.\n\n\n",
    "jawaban": "The correct answer is: Scatter Plot"
  },
  "d6c72edce497881cc456554995b657a0e3312434266b602eb998ad086b18b403": {
    "soal": "The Save Data widget can consider the dataset provided in the input channel and save it to a data file with a specified name. The Save Data widget can save the data as a file with data separated by tabs or separated by .......................\n",
    "jawaban": "The correct answer is: comma"
  },
  "12e0632b412947a17d1a8db343a0fad296b701f59308054c247c2677a6589f36": {
    "soal": "What kind of system is an LLM?",
    "jawaban": "The correct answer is: Conversational"
  },
  "6789b9818c75f8796c935d6dbab7f6fc87103b4365d4f8155b950328b9deae31": {
    "soal": "The Distances Transformation widget is used for normalization and inversion of distance matrices. Normalizing the data is necessary to bring all variables into proportion with one another.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "0d062a46643a99cf3dbf623c9b0d75adf078ee7072331802c3130e384b279700": {
    "soal": "from googlesearch import search\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef get_page_content(url):\ntry:\nheaders =\nresponse = requests.get(url, headers=headers, timeout=10)\nsoup = BeautifulSoup(response.content, 'html.parser')\n# Ambil judul halaman\ntitle = soup.title.string if soup.title else 'No Title'\n# Ambil konten paragraf utama\nparagraphs = soup.find_all('p')\ntext_content = ' '.join([p.get_text() for p in paragraphs[:5]]) # Batasi 5 paragraf pertama\nreturn title.strip(), text_content.strip()\nexcept Exception as e:\nreturn 'Error', f\"Failed to fetch content: {e}\"\ndef google_scrape_with_content(keywords, num_results=5, output_file='scraped_results.csv'):\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'Rank', 'Title', 'URL', 'Content'])\nfor keyword in keywords:\nprint(f\"\n\ud83d\udd0d Searching for: {keyword}\")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nprint(f\" \u2192 Fetching: {url}\")\ntitle, content = get_page_content(url)\nwriter.writerow([keyword, i+1, title, url, content])\ntime.sleep(2) # Delay biar aman\nexcept Exception as e:\nprint(f\"\u274c Error while searching '{keyword}': {e}\")\nprint(f\"\n\u2705 All results + content saved to '{output_file}'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\ngoogle_scrape_with_content(keywords, num_results=5)\nWhich file does the script read to obtain the list of search keywords?\n{\n=keywords.txt\n~input.csv\n~search_terms.json\n~queries.xlsx\n~terms_list.txt\n~search_keywords.dat\n}",
    "jawaban": "The correct answer is: \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\""
  },
  "88b3dc3746fe2dcafefd7d719b385b1ebd5b5c99856f208a70dd10dbe1334a14": {
    "soal": "The Distance Map widget can minimize distances between items.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "66535d64b7126ee57cdd469feb66a93eba4b56a8c418c9cf6171df2ac7b7a135": {
    "soal": "What is the binary value of the decimal number 120 ?",
    "jawaban": "The correct answer is: 1111000"
  },
  "cc637a8e1cdd571ca72ac9dd6a5fdc24daa783157392d1e0eb265d74b5eba594": {
    "soal": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n# Contoh: Memuat dataset dari file CSV\n# Gantilah 'data.csv' dengan path ke dataset Anda\ndf = pd.read_csv('data.csv')\n# Memisahkan fitur (X) dan target \n# Asumsikan kolom terakhir adalah target\nX = df.iloc[:, :-1]\ny = df.iloc[:, -1]\n# Membagi dataset menjadi data latih dan data uji (80% : 20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model Decision Tree\nmodel = DecisionTreeClassifier()\n# Melatih model dengan data latih\nmodel.fit(X_train, y_train)\n# Melakukan prediksi pada data uji\ny_pred = model.predict(X_test)\n# Menghitung akurasi\nakurasi = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model: ')\n# Menampilkan laporan klasifikasi yang mencakup presisi, recall, dan F1-score\nprint(classification_report(y_test, y_pred))\nWhat is the ratio of training to testing data in the provided code?\n{\n~70% training, 30% testing.\n~60% training, 40% testing.\n=80% training, 20% testing.\n~50% training, 50% testing.\n~90% training, 10% testing.\n~100% training, 0% testing.\n}",
    "jawaban": "The correct answer is: akurasi:.2f"
  },
  "ab037bf75174c89c7e86f64f53a8b65b8e000ace96d1bddc33708b57f428b13a": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nplt.imshow(x_train[0], cmap='gray')\nplt.title(f\"Label: \")\nplt.show()\nmodel = keras.Sequential([\nkeras.layers.Flatten(input_shape=(28, 28)),\nkeras.layers.Dense(128, activation='relu'),\nkeras.layers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nWhat activation function is used in the first Dense layer?\n{\n= relu\n~ sigmoid\n~ softmax\n~ tanh\n~ linear\n~ elu\n}",
    "jawaban": "The correct answer is: y_train[0]"
  },
  "53d65c012d3d1d91d0ea07a69a8acd95d309437b4092989cc7f85c4a91e7307e": {
    "soal": "The programming languages used by both statisticians and data scientists are,\n\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answers are: python, R"
  },
  "13b797c86861d030a88000ad9abed01c270ef64c2d663fa6f6d895d7ff1ec8d5": {
    "soal": "import pandas as pd\n# Contoh: Membuat DataFrame dengan missing values\ndata =\ndf = pd.DataFrame(data)\n# Menampilkan jumlah missing values di setiap kolom\nprint(\"Jumlah missing values per kolom:\")\nprint(df.isnull().sum())\n# Menghapus baris yang memiliki missing values\ndf_cleaned = df.dropna()\nprint(df_cleaned)\n# Mengisi missing values pada kolom 'Usia' dengan mean\ndf['Usia'].fillna(df['Usia'].mean(), inplace=True)\n# Mengisi missing values pada kolom 'Usia' dengan median\ndf['Usia'].fillna(df['Usia'].median(), inplace=True)\n# Mengisi missing values pada kolom 'Kota' dengan modus\ndf['Kota'].fillna(df['Kota'].mode()[0], inplace=True)\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n# Inisialisasi imputer untuk mengisi missing values dengan mean\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n# Mengimputasi kolom 'Usia'\ndf[['Usia']] = imputer.fit_transform(df[['Usia']])\nWhich method is suitable for imputing missing values in categorical data?\n{\n=Filling with the mode (most frequent value).\n~Filling with the mean.\n~Filling with the median.\n~Removing the rows with missing values.\n~Filling with zero.\n~Filling with a constant value.\n}",
    "jawaban": "The correct answer is: 'Nama': ['Andi', 'Budi', 'Citra', 'Dewi'], 'Usia': [25, 30, None, 22], 'Kota': ['Jakarta', None, 'Bandung', 'Surabaya']"
  },
  "fd35b476fef886ed7203d07ea2dc9c687e3719d5ed29731d3e3e8538cdd0136a": {
    "soal": "What is meant by a BINARY number?",
    "jawaban": "The correct answer is: BNumbers consisting of the digits 0 and 1"
  },
  "939c19915ebc7d590469cfd341faed7839ea20dca61898e4b198c7ec16e1027f": {
    "soal": "Big data is a term referring to large and complex datasets obtained from various sources, including business transactions, social data, census data, medical data, scientific data, and others. The main characteristics of big data are \nAnswer Question 72\n, velocity, and variety.\n- Volume refers to the very large amount of data generated constantly.\n- Velocity (Velocity) refers to the speed at which data is generated and updated, as well as the ability to process data in real-time.\n- Variety (Variety) refers to the various sources of data and different types of data that can be collected.\nBig data typically requires specialized technology and techniques to process, analyze, and interpret it, including machine learning, data mining, and statistical analysis techniques. The goal of big data analysis is to gain valuable insights and information, such as trends and patterns useful for business, scientific, or social decision-making.",
    "jawaban": "The correct answer is: volume"
  },
  "565ac397f6643be8da5310d6a0126ef35540776fe14eebeb9ea1d0724f8956a8": {
    "soal": "The Predictions widget displays ................... predictions on the data.\n",
    "jawaban": "The correct answer is: model"
  },
  "bb0c52e2e03e4c0be5dcec0eea3fbca6f76ca130608117a0e94f58c2805a2129": {
    "soal": "Widget Network Clustering can help us uncover clusters and highly connected groups in a network. First, we will use the Widget Network File to load the lastfm.net dataset. Then we will send the network to the Widget Network Clustering. The Network Clustering widget finds 79 ........................ in the network. To visualize the results, we use the Widget Network Explorer and set the Color attribute to Cluster. This will color the network nodes with the appropriate cluster colors - this is a good way to visualize highly connected groups in a dense network.\n\nKeep in mind that the Widget Network Explorer here will color the 10 largest clusters and color the rest as 'Other'.\n\n\n\n",
    "jawaban": "The correct answer is: cluster"
  },
  "303ae4c5e656beed5a84f7bbebc49bdb246ca29d0ac4fe3e97a14ecf838e7ee9": {
    "soal": "101 (binary) = ........................\u00a0 (decimal)\n",
    "jawaban": "The correct answer is: 5"
  },
  "ea083011d8b48ded9b42f06b51a296a48754b8fc6e7bbec38c647ce19d498611": {
    "soal": "The Distance Matrix widget creates ...........(1)............., which is a ...........(2)............. array containing distances, taken pairwise, between elements of the dataset. The number of elements in the dataset determines the size of the matrix. Data matrices are crucial for the Hierarchical Clustering widget.\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 Distance Matrix, (2) \u2192 two"
  },
  "3f07231c90c7faaacd19cccf916c9dbcbeea5369853b5d55e3d406bb3053b507": {
    "soal": "import pandas as pd\nfile_path = \"/content/drive/MyDrive/your_file.csv\"\ndf = pd.read_csv(file_path)\nprint(df.head())\nWhich function can be used to display the last 5 rows of the DataFrame instead of the first 5?",
    "jawaban": "The correct answer is: df.head()"
  },
  "742877849b5077ce793cbfab1d03ee1f97e1cb85e57cf60a13358e365cc186ec": {
    "soal": "The core of Apache Hadoop consists of the storage part (Hadoop Distributed File \nAnswer Question 78\n (HDFS)) and the processing part (MapReduce). Hadoop splits files into large blocks and distributes them across nodes in the cluster. To process the data, Hadoop MapReduce transfers code to the nodes to process in parallel, based on the data that needs to be processed at each node. This approach takes advantage of data-node locality, manipulating the data they hold in hand to enable faster and more efficient processing than more conventional supercomputer architectures relying on parallel file systems where computation and data are linked through high-speed networks.",
    "jawaban": "The correct answer is: System"
  },
  "a9257fd167314abfb3f8fab61722e517f9c52cef16bcb4dfeee6a75f6742d232": {
    "soal": "The Image Embedding widget in Orange is very easy to use. Technically, Orange sends the image to the server, where the client pushes the image through a pre-trained deep neural network, such as Google Inception v3. Deep networks are often trained with a specific purpose. Inception v3, for example, can classify images into one of 1,000 image classes. We may ignore the suggested classifications and instead consider the penultimate layer of the network with 2048 nodes (numbers) and use it for the image\u2019s vector-based representation.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "0d7de2ff864fa3ed57731cd0f8088b14a6e00781a2fa23d13715cb16305897f5": {
    "soal": "Parent Component Analysis is typically performed on multivariable (multivariate) systems. In simple terms, PCA looks for the most dominant variable from the categories (classes) present in the data. The technique used is to perform a coordinate transformation of the data to obtain new coordinates (new variables), usually fewer than the number of coordinates/variables in the original data, to better represent the categories (classes) in the data.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "f1b64f0bc3836dc0e18ee4bc3b11694c1f1f1aacd3387beb8ee43dd4fc679261": {
    "soal": "Algoritma CN2 adalah teknik klasifikasi yang di rancang agar secara effisien melakukan induksi dari rules yang simple & komprehesif dalam bentuk \u201cif cond then predict ................... \u201d, meskipun dalam domain dimana mungkin ada noise.\n",
    "jawaban": "The correct answer is: class"
  },
  "bf7807e9ffe3db0d6be64ddfeced7e2d870e664861ac731bd89380203ca3cbe3": {
    "soal": "The Calibration Plot widget plots the class probability against the probability predicted by the classifier.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "56572ce8831bedca030fbd2b4f1d737dc76057b2f07dfec57d82b31725fa9340": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#-xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\ndfg, start_activities, end_activities = pm4py.discover_dfg(log)\npm4py.view_dfg(dfg, start_activities, end_activities)\nHow many variables are assigned from the discover_dfg() function?",
    "jawaban": "The correct answer is: 3 -1 -2 -4 -5 -6"
  },
  "eb6a7d4d29abefdf243fc7bb0589f5ee31a0d6487cee03cb97ada2918e7452b3": {
    "soal": "What is the role of HDFS in the Hadoop ecosystem?",
    "jawaban": "The correct answer is: To store large datasets across multiple machines"
  },
  "8ab4a2846a6c0a463a95be3c825c0c43e4d09764b994c75b4bb08c63de52c16a": {
    "soal": "\n\n\nData visualization with many variables (multivariate), same, unordered can use ..............................\n\n\n\n",
    "jawaban": "The correct answers are: heatmap, treemap, stacked bars, pie chart"
  },
  "30d34a3f5c40b63a311428987eb040375cb55813229db1a95315876dd7f9a3ee": {
    "soal": "1111 1111 (binary) = .......... (decimal)",
    "jawaban": "The correct answer is: 255"
  },
  "f408f8181e667b6162e31ec0fc490fecb8cdccc8625a527a1a483cb1118d4199": {
    "soal": "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target biner\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(0, 2, 100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi data latih dan data uji (80% latih, 20% uji)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi dan pelatihan model klasifikasi (misalnya RandomForest)\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n# Prediksi pada data uji\ny_pred = model.predict(X_test)\n# Menghitung Precision, Recall, dan F1-Score\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nprint(f'Precision: ')\nprint(f'Recall: {recall:.2f}')\nprint(f'F1-Score: {f1:.2f}')\n# Alternatif: Menggunakan classification_report untuk ringkasan metrik\nprint('\nClassification Report:')\nprint(classification_report(y_test, y_pred))\nWhich function is used to calculate Recall in scikit-learn?\n{\n=recall_score\n~precision_score\n~f1_score\n~accuracy_score\n~classification_report\n~confusion_matrix\n}",
    "jawaban": "The correct answer is: precision:.2f"
  },
  "3f9efd5b55db59958ba292c9e1236049c2ef74cbbfc1d95cb594e23d89aab1c3": {
    "soal": "Widget Import Images allows us to import images from a directory.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "31d23234b3e059b59f254de30f8124e595bdb43471d3da0d2d4533ecf2e922ea": {
    "soal": "An example of using the kNN Widget for regression task is shown in the workflow below. This workflow demonstrates how to use ..................... output. For this example, we use the housing dataset. We input the kNN prediction model into Predictions and observe the predicted values.\n\n\n\n",
    "jawaban": "The correct answer is: Learner"
  },
  "6fe2b0e6bdaff33ea03f2fc4a5fe45b302af4da817d890cc8e0a523175c539de": {
    "soal": "What does each column in a dataset represent?",
    "jawaban": "The correct answer is: A feature or attribute of the data entries."
  },
  "90f71365e56daa791971e9bb9cff46ac3b50e24f4edbf61992d735ffba53a275": {
    "soal": "Why is the F1-Score used in model evaluation?",
    "jawaban": "The correct answer is: To balance and combine precision and recall into a single metric."
  },
  "42abe05532083420f8b239047c40f81579f5e2426be3aa3f948555b38a11d951": {
    "soal": "Most .................. algorithms assume that we do not have missing values in our data. In this widget, we can select the interpolation method to estimate the missing values. By default, the Interpolate widget will use linear interpolation (quick and reasonable).\n",
    "jawaban": "The correct answer is: Time Series"
  },
  "811b4812896e58e74450b0cee02e2c101259012b90aefa7cccfb9c8c350674be": {
    "soal": "In the Interpolate widget, which is part of the Time Series add-on, the interpolation type - Linear interpolation replaces missing values with linearly-spaced values between ................. nearest and defined data points.\n",
    "jawaban": "The correct answer is: two"
  },
  "a97475a2b08b566d848902c9df474f1505bb9f6cd4a4a6a5661ed70623c1c51c": {
    "soal": "How can you view performance metrics for a running model?",
    "jawaban": "The correct answer is: ollama metrics model"
  },
  "30340241e29a1490ab7245623c3fc69ba40a523a24c3641a3116f59df15e0049": {
    "soal": "What is the container name for the Open WebUI service in the 'docker-compose.yaml' file?",
    "jawaban": "The correct answer is: open-webui"
  },
  "ac8e8ca98991bf0e76ea99e94db40011569cdfc0d62068ebde6d692879bb94be": {
    "soal": "What is the primary purpose of the Dropout layer in a model?",
    "jawaban": "The correct answer is: To reduce overfitting by randomly setting a fraction of input units to 0 during training"
  },
  "f797f33b1fef141392800e360dc698e27d1efc8aa12a86643710be6b2b26b8c6": {
    "soal": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n# Contoh: Memuat dataset dari file CSV\n# Gantilah 'data.csv' dengan path ke dataset Anda\ndf = pd.read_csv('data.csv')\n# Memisahkan fitur (X) dan target \n# Asumsikan kolom terakhir adalah target\nX = df.iloc[:, :-1]\ny = df.iloc[:, -1]\n# Membagi dataset menjadi data latih dan data uji (80% : 20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model Decision Tree\nmodel = DecisionTreeClassifier()\n# Melatih model dengan data latih\nmodel.fit(X_train, y_train)\n# Melakukan prediksi pada data uji\ny_pred = model.predict(X_test)\n# Menghitung akurasi\nakurasi = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model: ')\n# Menampilkan laporan klasifikasi yang mencakup presisi, recall, dan F1-score\nprint(classification_report(y_test, y_pred))\nHow are features (X) and target  separated in the code?\n{\n~By using the `split()` method on the DataFrame.\n~By specifying the column names directly.\n=By selecting all columns except the last for X, and the last column for y.\n~By using the `separate()` function from pandas.\n~By applying the `partition()` method.\n~By utilizing the `divide()` function.\n}",
    "jawaban": "The correct answer is: akurasi:.2f"
  },
  "092ce6beb96a6ff476dd5f35105ad570b8f9c493f34ec7283588134bc489f7e5": {
    "soal": "Output from the Yahoo Finance Widget is a time series table of open, top, low, close (OHLC), volume, and adjusted close prices.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "841140de7fa4fad63e45e19a67d609670d5288ad8cdcbcf76ebebe47ae4712d5": {
    "soal": "Widget Import Images scans a directory and returns one row per image retrieved. The columns contain the image name, directory path to the image, width, height, and image size. The column with the directory path to the image is then used as an attribute for scatter plot and Image Embedding.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "0abcb4f1dd672df5668b0d981a488e75fecaa4857bfbc3f57199b1bdd1f8967d": {
    "soal": "The workflow clusters data items in the iris dataset by first checking the distances between data instances using the Distances widget. The .......... Matrix is passed to the Hierarchical Clustering widget, which creates a dendrogram. We can select different parts of the dendrogram to further analyze the related data.\n\n\n",
    "jawaban": "The correct answer is: Distance"
  },
  "2ea4e25c8945e0a40c6d9bb86276ddee2397043f6768dc871eb873222e7cdd91": {
    "soal": "The Correspondence Analysis Widget is used to perform Correspondence Analysis for categorical multivariate data.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "1576b5397f202bf55432f26f12ef17463ea3e3d35bedf4ede314cb98814341d5": {
    "soal": "If\n\n1 AND 1 = 1\u00a0\u00a0\u00a0\u00a0\u00a0 and\u00a0\u00a0 1 OR 1 = 1\n\n\n1 AND 0 = 0\u00a0\u00a0\u00a0\u00a0\u00a0 and\u00a0\u00a0 1 OR 0 = 1\n\n0 AND 1 = 0\u00a0\u00a0\u00a0\u00a0\u00a0 and\u00a0\u00a0 0 OR 1 = 1\n\n\n0 AND 0 = 0\u00a0\u00a0\u00a0\u00a0\u00a0 and\u00a0\u00a0 0 OR 0 = 0\n\n\u00a0\n\nthen in binary\n\n( 101 AND 010 )\u00a0 OR 111 = ..... (binary)\n",
    "jawaban": "The correct answer is: 111"
  },
  "14f70f767a06d91111c8976ebf30512e25f7359e97c4290189a6b1379eb0dc0e": {
    "soal": "If the dataset has many attributes, it is not feasible to manually scan all pairs to find interesting or useful scatter plots. Orange implements intelligent data visualization with the Find Informative Projections option in the widget.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "a82e5c62959bba0f0e97b61812d9b8baa1c892d6ed06919584baa003b3444239": {
    "soal": "In the following example workflow, we show how to quickly visualize the corpus with the ...........(1)........... widget. We can connect the Word Cloud widget directly to the Corpus widget, but instead, we decide to apply some preprocessing with the Text Preprocess widget. We are working with the book-excerpts.tab dataset. We can convert all text to lowercase, ..............(2)............ (split) text into words only, filter English stopwords, and select the 100 most frequent tokens.\n\n\n\n\n",
    "jawaban": "The correct answer is: (2) \u2192 tokenized, (1) \u2192 Word Cloud"
  },
  "bf5c05dcab4e651488532901504f785ca6909d3f55ce004cad84616e61993d72": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nIn the code `tf.keras.layers.Dense(10)`, what does the number `10` represent?",
    "jawaban": "The correct answer is: The number of neurons in the Dense layer"
  },
  "7c4c5a2f492069a0fe0c6d74caa60deb223b136e604c2ee92055366e610053aa": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nWhat is the primary function of the Flatten layer in a TensorFlow/Keras model?",
    "jawaban": "The correct answer is: Converts a 2D matrix into a 1D vector"
  },
  "2aaebb658229bbe54e938ecfda0df9566682491e0116f6677ee4f9ca7a5d08b4": {
    "soal": "Widget kNN menggunakan algoritma kNN yang akan mencari k instance training terdekat di ruang feature dan menggunakan nilai feature terdekat tersebut untuk mem-prediksi\n",
    "jawaban": "The correct answer is 'False'."
  },
  "c328357386437a01883f7c01c1e72b0041d970211b222cf835eacb3b298a8fd1": {
    "soal": "\n\n\nThe image above shows the sequence of processes in data science, which are:\n\nASK - Ask, interesting questions.\nGET - Acquire data.\nEXPLORE - Explore the data, check if there are any oddities/anomalies/interesting patterns, etc.\n...................... - Build, fit, and validate the model.\nVISUALIZATION - Communicate and visualize data.\n\n",
    "jawaban": "The correct answer is: MODEL"
  },
  "f06621429b2c15f251560ece734546ff5bcfde452d8de5609c533dacdb865a3b": {
    "soal": "Widget Network Analysis calculates statistical summaries from the ...............-level and graph-level for the network. Widget Network Analysis will output the network with the computed statistics and an extended item data table (only node-level index).\n",
    "jawaban": "The correct answer is: node"
  },
  "a03ab828c7a4c38a6ec00a873133a91b5785ef21a1d60f163e5efb94b035cd47": {
    "soal": "......................., a type of plot that displays data as a series of points, connected by straight line segments. Line Plot is used for numeric data. Meanwhile, for categorical data, Line Plot can be used for grouping data points.\n",
    "jawaban": "The correct answer is: Line plot"
  },
  "74c095112735a49a68b5cddaf38a334ad773c4a242787b383147b3473de008ee": {
    "soal": "What is TensorFlow mainly used for?",
    "jawaban": "The correct answer is: Machine learning and deep learning"
  },
  "c27cbff85e3a63f8d1f025815f7aa077e565a3ec051a984f0e94d348cd86c509": {
    "soal": "The Image Viewer widget will display images from the dataset, stored either locally or on the internet. The Image Viewer widget will look for attributes with type=image in the third header row.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "37447b1bc4f0d03b83af807db10ac2b85c912580194f1707064afc0cea097096": {
    "soal": "Widget Network Clustering tries to find clusters in the network. .............(1)............. works with two algorithms, one from Raghavan et al. (2007), which uses label propagation to find ...........(2)............. that are appropriate, and one from Leung et al. (2009), which builds on the work of Raghavan and adds ..............(3)............... as a parameter for cluster formation.\n",
    "jawaban": "The correct answer is: (2) \u2192 cluster, (3) \u2192 hop attenuation, (1) \u2192 Network Clustering"
  },
  "15cf52434cf61a3f25dfde7841bf33150a9a970f4c63a740c88b1d3fa8516cd8": {
    "soal": "The Scatter Plot widget can perform scatter plot visualizations with exploratory analysis and intelligent data visualization enhancements.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "86409751830f84f9132b5b15cc52e63c3d4c67753ae1eafa1aa3a09a913cce5b": {
    "soal": "Widget Network Clustering can help us uncover clusters and highly connected groups in a network. First, we will use the Widget ...................... to load the lastfm.net dataset. Then we will send the network to the Widget Network Clustering. The Network Clustering widget finds 79 clusters in the network. To visualize the results, we use the Widget Network Explorer and set the Color attribute to Cluster. This will color the network nodes with the appropriate cluster colors - this is a good way to visualize highly connected groups in a dense network.\n\nKeep in mind that the Widget Network Explorer here will color the 10 largest clusters and color the rest as 'Other'.\n\n\n\n",
    "jawaban": "The correct answer is: Network File"
  },
  "55112b346f701d82fddf45f94fa19e7c8c8aacefcafeef35a796bf2093ac2dca": {
    "soal": "The Tree Widget uses the .................... algorithm with the ability to perform forward pruning.\n",
    "jawaban": "The correct answer is: Tree"
  },
  "f8b48d333014bb19c248465380713c288fec59d8725355cc9d9032f7e97d2f97": {
    "soal": "In the following workflow example, a very simple use of the Corpus widget will be shown. Place the Corpus widget onto the canvas and connect it to the Corpus Viewer widget. We are using the book-excerpts.tab dataset, which is available in the add-on, and examining it in the ....................... widget.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Corpus Viewer"
  },
  "da7f9d0238ebbb173ad879062c7e1d9fd238d04df7df7d93b003b26db1dd982d": {
    "soal": "The Python function pm4py.read_xes() does what?",
    "jawaban": "The correct answer is: Loads XES files"
  },
  "b4c25db3cbf389b97d6240ccc0561f5cc08890e322902cd247c4170ed044c0e6": {
    "soal": "Heuristic Miner is notable for:",
    "jawaban": "The correct answer is: Noise and variability handling"
  },
  "21556fb3b483500a6c9a10fcdba9ab355afb9097961d6c9165ea1be332c64d77": {
    "soal": "The Import Images widget scans the directory and returns one row per image taken. The columns include image name, directory path to the image, width, height, and image size. The column with the directory path of the image is then used as an attribute for visualization and .......................\n",
    "jawaban": "The correct answer is: Image Embedding"
  },
  "505e0b1501c5bc3425dedb00c7193211a8c099ece60423afeabe6de45c32aac7": {
    "soal": "In which scenario is prioritizing recall over precision more appropriate?",
    "jawaban": "The correct answer is: When missing a positive case (false negative) is more critical than a false positive."
  },
  "bc8716ce7c052932a3ac1686ed39e66074e9f0bdb58874750e55c8191adcb17d": {
    "soal": "Machine learning is a branch of computer science focused on developing algorithms and techniques to create computer systems that can learn from data. In machine learning, computers are trained to recognize patterns and rules in the data, allowing them to make intelligent and accurate predictions or decisions based on that data.\nThe learning process in machine learning can be done through three main methods: supervised learning, unsupervised learning, and reinforcement learning.\n- \nAnswer Question 23\n learning involves using labeled or annotated data to train the algorithm, allowing the computer to recognize patterns or relationships between data features and labels. Examples of supervised learning applications include image classification, stock price prediction, and spam email classification.\n- Unsupervised learning, on the other hand, involves using unlabeled data to identify hidden patterns or structures within the data. Examples of unsupervised learning applications include data clustering and dimensionality reduction.\n- Reinforcement learning involves computer systems learning through experience by taking actions and receiving feedback about whether those actions are correct or incorrect. The goal is to find the optimal decision or action that produces the maximum benefit.\nMachine learning has various applications, including facial recognition, image analysis, speech recognition, forecasting, anomaly detection, and more.",
    "jawaban": "The correct answer is: Supervised"
  },
  "d5ee3f6b18613a8e794ed0ba01b2ccf332f6c0e1785d811c555db69eaaed7d2e": {
    "soal": "The Distance Transformation widget is used for normalization and inversion of the distance matrix. .......................... data is required to bring all variables into proportion with one another.\n",
    "jawaban": "The correct answer is: normalization"
  },
  "2a854e900e95cb14df225e4857b544af7654b95c90eb517c40abc6efda385a07": {
    "soal": "To cluster based on similarity, we can measure the distance between images. The numerical representation of the images obtained from the Image Embedding widget, we measure the distance using the Distances widget. Typically, cosine distance works best for images, but you can experiment yourself. We then send the distance matrix to ...................... to visualize similar pairs in the dendrogram.\n\nThe result is that all the animals are correctly grouped. In the dendrogram, we do not see the images of the animals. We can use the Image Viewer widget to view the images.\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Hierarchical Clustering"
  },
  "38eac1e8bf47de8ed8c9f031659947343d4536045690d1632bedc92e177f471b": {
    "soal": "In the workflow below, we use two ........................ widgets to read the Iris and Glass datasets (provided in the Orange distribution), and send them to the Data Table widget.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: File"
  },
  "be3767a4eb15f5ba72d0bcd51e083e4408e1b1459f987490ee434a927b1a66da": {
    "soal": "In the next example, we will try to predict the category of documents. We use the dataset book-excerpts.tab, which we send through the Preprocess Text widget with default parameters. Then we connect the Preprocess Text widget to the ...............(1)............... widget to obtain term frequency. With term frequency information, we will calculate the model.\n\nConnect the Bag of Words widget to the Test & Score widget for predictive modeling. Connect the SVM widget or another classifier widget to the Test & Score widget (all on the left). The Test & Score widget will calculate the performance score for each learner input. Here, we obtain very good results for the SVM widget.\n\nNext, we need to check where the model is making mistakes. Add the Confusion Matrix widget to the Test & Score widget. The Confusion Matrix widget will display the correctly and incorrectly classified documents. Select \"misclassified\" to output the misclassified documents, which we will examine further using ...............(2)...........\n\n\n",
    "jawaban": "The correct answer is: (2) \u2192 Corpus Viewer, (1) \u2192 Bag of Words"
  },
  "8e6d382b027d9e54c43ef9ce35256a064406f6575b18af764c9e77cba80f4588": {
    "soal": "The Distributions widget displays the distribution of discrete attribute values. If the data contains a class variable, the distribution can be conditioned on the class.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "6a81a938d351614a31bcad0b9adc72102f98b12bdcfe782713d11b26a2678697": {
    "soal": "\n\n\nData visualization with many variables (multivariate), same but with no hierarchy can use ..............................\n\n\n\n",
    "jawaban": "The correct answers are: heatmap, treemap, stacked bars, pie chart, stacked area chart, stacked line chart"
  },
  "f391da18a994f2d4acf0e21101e6c371ba8405182beb8459193778061d83a7ce": {
    "soal": "Which command updates Ollama to the latest version?",
    "jawaban": "The correct answer is: ollama update"
  },
  "2e1aa3122ff9e3fe006c1967c9925ac22a93757f2558755d53d46451f4400f36": {
    "soal": "Simple example with the Calibrated Learner. We use the Titanic dataset because this widget requires a binary class value (in this case, they are 'survived' or 'not survived').\n\n\nWe use .................... as the base learner, which will be calibrated with default settings, i.e., with sigmoid optimization of the distribution of values and optimized with CA.\n\n\nComparing the results of the uncalibrated Logistic Regression model, we will clearly see that the calibrated model is better.\n\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Logistic Regression"
  },
  "cd8338afba4e48932bdaebbb444f610a0bb5cdd9735786b381bd6fdc8693a2ed": {
    "soal": "Which tool is recommended for running Ollama and Open WebUI together?",
    "jawaban": "The correct answer is: Docker Compose"
  },
  "8fd12228d23a090c577efef3136fd8d9f353c079ec594e888e902c50bff5ce5a": {
    "soal": "In the Preprocess Text widget, we can do several things, such as\n\nConvert all letters to lowercase.\nRemove (.........(2)..............), unimportant words like conjunctions such as and, to, from, etc.\nSet the stopword processing for the Indonesian language.\nRemove ..............(3)............... tags.\nRemove URLs.\n\n",
    "jawaban": "The correct answer is: (1) \u2192 Preprocess Text, (2) \u2192 stop word, (3) \u2192 HTML"
  },
  "d23066fb5ed33364f5a94b693bc9aefc703bd9bc49658777b6dada2f009609a4": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\n# xes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\nxes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nprocess_tree = pm4py.discover_bpmn_inductive(log)\nbpmn_model = pm4py.convert_to_bpmn(process_tree)\npm4py.view_bpmn(bpmn_model)\nWhich log file suggests a healthcare context?",
    "jawaban": "The correct answer is: Cross_Hospital.xes"
  },
  "2c5804427b515326baa9b96e72060aa1d64e211f3749313476f96fbecefc6580": {
    "soal": "result 8 (decimal) OR 2 (decimal) is",
    "jawaban": "The correct answer is: 10"
  },
  "77d798ef49d20df81ae738e549e87c5e4c2ecb967adfed63ffe940c4dc433fed": {
    "soal": "Network Explorer Widget allows us to visually explore the data and its properties.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "560fe52bd9472578af620d80142ef20dff6c90266484da5ef2abaaeab0012809": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#_xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nmap = pm4py.discover_heuristics_net(log)\npm4py.view_heuristics_net(map)\nWhat programming language is used in this script?",
    "jawaban": "The correct answer is: Python -Java -C++ -Ruby -PHP -JavaScript"
  },
  "1a28eb4e20ac4cc7b742fdb781d0980ed78cada3c28ab1cd461525ca331d5564": {
    "soal": "Statistical science and techniques are usually used at the beginning of the analysis process. Business Intelligence (BI) is usually performed after the data is collected and organized. Predictive Analytics, whether using traditional methods or machine learning/deep learning, is done after obtaining reports from statistical analysis.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "342f6f0c0fb90707beefdf2d75263816826e94e6e01ed8c727dab23bda967fa4": {
    "soal": "One example of using the Widget kNN for ..................... tasks is shown in the workflow below. This workflow shows how to use the Learner output. For this example, we are using the housing dataset. We input the kNN prediction model into Predictions and observe the predicted values.\n\n\n\n\n\n",
    "jawaban": "The correct answers are: regression, regression"
  },
  "31d9652ffde1691dcf6902cb476e0e691e9f703c4412c5b70775fc723db760c8": {
    "soal": "Which repository offers a comprehensive overview of Large Language Model applications in cybersecurity, including related literature and resources?",
    "jawaban": "The correct answer is: Awesome-LLM4Cybersecurity"
  },
  "38cfd1895a024dc159da87a5beb07beb372801321d0826fbab879c5ba28c3bb2": {
    "soal": "Widget ...................... menggunakan algoritma ..................... yang akan mencari k instance training terdekat di ruang feature dan menggunakan rata-rata feature terdekat tersebut untuk mem-prediksi\n",
    "jawaban": "The correct answer is: kNN"
  },
  "d3e35c28bb6ca4d142a04f0dcfa363df5d8fd774b665aabd62f0bfe0c2087613": {
    "soal": "import pm4py\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\n# Import XES log\nlog = xes_importer.apply(xes_filename)\n# Cek jumlah kasus dan aktivitas\nprint(f\"Jumlah kasus: \")\nactivities = set()\nfor trace in log:\nfor event in trace:\nactivities.add(event['concept:name'])\nprint(f\"Jumlah aktivitas unik: {len(activities)}\")\nfrom pm4py.algo.discovery.dfg import algorithm as dfg_discovery\nfrom pm4py.visualization.dfg import visualizer as dfg_visualization\n# DFG berdasarkan frekuensi\ndfg = dfg_discovery.apply(log,\nvariant=dfg_discovery.Variants.FREQUENCY)\n# Tampilkan visualisasi\ndfg_vis = dfg_visualization.apply(dfg, log=log,\nvariant=dfg_visualization.Variants.FREQUENCY)\ndfg_visualization.view(dfg_vis)\nWhich visualization is NOT suggested for further bottleneck analysis enhancement?\n{\n=Scatter plot\n-Gantt Chart\n-Dotted Chart\n-Performance Spectrum\n-All are recommended\n-None of these are recommended\n}",
    "jawaban": "The correct answer is: len(log)"
  },
  "5ddddd32f2b6f6e83ccdbca794412c32d167cf5a9f0a363ef941e61d412b6eeb": {
    "soal": "In the snapshot below, we can see how transformation affects the distance matrix. We load the Iris dataset and calculate the distance between columns with the help of the Distances widget. To show how Distance Transformation affects the Distance Matrix, we create the workflow below and compare the transformed distance matrix with the \"original\".\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "227d6ae4152149ceb86f1258c21f136ba606ed351771dff15c03d2ac30d76bc8": {
    "soal": "Bahasa pemrogramman yang digunakan untuk membuat Orange3 data mining terutama adalah bahasa\n",
    "jawaban": "The correct answer is: python"
  },
  "61f1bfed6706e0090b69ab08796bbf5b966e4a8f4c1a709d0b47c6a266812f5f": {
    "soal": "\n\n\nThe classification error visualization used in the workflow above is\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: scatter plot"
  },
  "ebf083364bf877176fe5d76320064fef120e51359659013be562ed856063784f": {
    "soal": "One URL location for Orange development/use examples is\n\n\u00a0https://orange.biolab.si/categories/examples/\n",
    "jawaban": "The correct answer is 'True'."
  },
  "c1b0a5558ee3b5a3fb3fcc5a679e0abf9f92a77a1b651c0f175bfce02010269f": {
    "soal": "Proper Orange3 installation on Ubuntu 18.04 can use the command\n\n\u00a0pip3 install orange3\n",
    "jawaban": "The correct answer is 'True'."
  },
  "8fb9c26614feed2424a2bce3d5f0f5843f03323391c6a67000947ed5e51d31e2": {
    "soal": "Widget CN2 Rule Induction akan meng-........................ (menginduksi) rule dari data menggunakan algoritma CN2.\n\n\n\n",
    "jawaban": "The correct answer is: induce"
  },
  "f070e67494141b96f3990f5647c86739cebcd4d86b6efd080f2eb5d43f78200d": {
    "soal": "Using the Yahoo Finance Widget does not allow us to generate time series data from Yahoo Finance stock market data.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "3c57aa1a51c8c2555563105973eaff0e1addbcfc68a1def28cbfd642f7fa8086": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#_xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nmap = pm4py.discover_heuristics_net(log)\npm4py.view_heuristics_net(map)\nThe '__name__ == \"__main__\"' check in Python indicates what?",
    "jawaban": "The correct answer is: Script executed directly -Script imported as a module -Script is a library -Script runs asynchronously -Script is deprecated -Script executed remotely"
  },
  "0b965bc86a16c36ba3d7e984c4198539474cea4f9ac4b759ef58926462cbd504": {
    "soal": "Where is the scraped data typically stored?",
    "jawaban": "The correct answer is: In a central local database or spreadsheet"
  },
  "f2cf3770b7122506ef88208e4252242d4c0680da3cb9306d6be6c7a072e3f685": {
    "soal": "Algoritma .................\u00a0 adalah teknik klasifikasi yang di rancang agar secara effisien melakukan induksi dari rules yang simple & komprehesif dalam bentuk \u201cif cond then predict class\u201d, meskipun dalam domain dimana mungkin ada noise.\n\n\n\n",
    "jawaban": "The correct answer is: CN2"
  },
  "47918fdce8558ede07b261757a5bf09383f8d0a60730bebad1ba80976f8d7512": {
    "soal": "Widget Network Analysis calculates a summary of statistics at the node-level and ..........(1)............-level for a network. The ............(2)........... widget will output the network with the computed statistics and an extended item data table (only ..........(3)..........-level index).\n",
    "jawaban": "The correct answer is: (1) \u2192 graph, (3) \u2192 node"
  },
  "fd25b84c3c1be36fe893ad3c07dc38112f140bf5321f14ae04f552ba325d5192": {
    "soal": "What kind of data is usually gathered in web scraping?",
    "jawaban": "The correct answer is: Specific data from websites"
  },
  "123462c14f4d3d994201218971038409f1581b55ca8a53a8b4ea1a36acad9399": {
    "soal": "According to the official Zoho website, Zoho Analytics is a comprehensive, reliable, and scalable analytics platform. Developers and system integrators (SI) can use this platform to develop and deploy custom analytic applications and integrations.\nAnother advantage of Zoho Analytics is that it is not user-friendly, making it easy for users to upload and control data. Using Zoho Analytics, data practitioners can create multifaceted and custom dashboards. The platform is easy to use and implement.",
    "jawaban": "The correct answer is 'False'."
  },
  "45387306fb30648c4ea0364590ecd53dbab7de9ba6740820709325cd51ff026b": {
    "soal": "The SQL widget accesses data stored in an SQL database. The SQL widget can connect to PostgreSQL (requires the psycopg2 module) or .............................\u00a0 (requires the pymssql module).\n",
    "jawaban": "The correct answer is: SQL Server"
  },
  "d4973795750e49fd620a72cef44481a41d15f2fd65341948c3756028439bd448": {
    "soal": "10101 (binary) + 1010 (binary) = .... (binary)",
    "jawaban": "The correct answer is: 11111"
  },
  "76068597761802b44bae169f0b6fc63fc846989fcc03ed855c55117357585dff": {
    "soal": "In the Interpolate widget, which is part of the Time Series add-on, the interpolation type - Nearest interpolation replaces missing values with undefined values.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "9fb2534128769e4231f6a42778881355a117f766f620e5fd6e9aaae17a56f3cb": {
    "soal": "from googlesearch import search\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef get_page_content(url):\ntry:\nheaders =\nresponse = requests.get(url, headers=headers, timeout=10)\nsoup = BeautifulSoup(response.content, 'html.parser')\n# Ambil judul halaman\ntitle = soup.title.string if soup.title else 'No Title'\n# Ambil konten paragraf utama\nparagraphs = soup.find_all('p')\ntext_content = ' '.join([p.get_text() for p in paragraphs[:5]]) # Batasi 5 paragraf pertama\nreturn title.strip(), text_content.strip()\nexcept Exception as e:\nreturn 'Error', f\"Failed to fetch content: {e}\"\ndef google_scrape_with_content(keywords, num_results=5, output_file='scraped_results.csv'):\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'Rank', 'Title', 'URL', 'Content'])\nfor keyword in keywords:\nprint(f\"\n\ud83d\udd0d Searching for: {keyword}\")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nprint(f\" \u2192 Fetching: {url}\")\ntitle, content = get_page_content(url)\nwriter.writerow([keyword, i+1, title, url, content])\ntime.sleep(2) # Delay biar aman\nexcept Exception as e:\nprint(f\"\u274c Error while searching '{keyword}': {e}\")\nprint(f\"\n\u2705 All results + content saved to '{output_file}'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\ngoogle_scrape_with_content(keywords, num_results=5)\nHow is the output CSV file opened in the script?\n{\n=In write mode with UTF-8 encoding.\n~In append mode with ASCII encoding.\n~In read mode with UTF-16 encoding.\n~In write mode with default system encoding.\n~In binary mode with ISO-8859-1 encoding.\n~In exclusive creation mode with UTF-8 encoding.\n}",
    "jawaban": "The correct answer is: \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\""
  },
  "2eebcd9611aee868e9d0559cf2a86e366fae3434c7adee444b710e8db4f91216": {
    "soal": "Sebagai contoh penggunaan kNN untuk klasifikasi kita mengggunakan dataset iris. Kita bandingkan hasil dari k-Nearest neighbors dengan default model Constant, yang akan memprediksi class majoritas. Tampak kNN lebih buruk.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "7c6eb25318574acf5ab6cdb54e8b0ff6deda762ab04518d98eff110d1a856eba": {
    "soal": "Which command downloads the Anaconda installer script to the '/tmp' directory?",
    "jawaban": "The correct answer is: wget -P /tmp https://repo.anaconda.com/archive/Anaconda3-2023.09-0-Linux-x86_64.sh"
  },
  "c213a792b273e184684c68aa1f959a285f8208270a652878d3365430597412af": {
    "soal": "Advantage of Inductive Miner includes:",
    "jawaban": "The correct answer is: Handling complex and nested structures"
  },
  "f612096eb97a6f9884381d8a9005752336673f5ae441dcdf618d02a81f483384": {
    "soal": "When there is no data in the input, the Corpus widget reads the text corpus from a file and sends the corpus instance to the output channel. The history of the last opened file is stored in the Corpus widget. In the Corpus widget, there is also a directory with .................. corpora pre-installed with the add-on. The widget can read data from Excel (.xlsx), comma-separated (.csv), and tab-delimited (.tab).\n",
    "jawaban": "The correct answer is: sampel"
  },
  "e942d69e644eb876e42ad8e0f0e4c0d9a78f1eff7697c1785c1bc885cda80ebe": {
    "soal": "In the image below, we use the Network .................... Widget to create a Grid graph with a height of 4 and a width of 5 and send it to Network Analysis. We can calculate node degrees and send the data to Network Explorer. Finally, we observe the resulting graph in the visualization and set the node size and color to Degree. Network Explorer is a good tool for observing and explaining network properties.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Generator"
  },
  "07812c4d765e727e1c2ab7488cf4357bca18a5fe2a423e9fdd02f831e8bc5ecc": {
    "soal": "What is the purpose of the 'volumes' section in the 'docker-compose.yaml' file?",
    "jawaban": "The correct answer is: To persist data across container restarts"
  },
  "87a2fa52edfcbe11da772b9118182ba77e2550251799e6b260e2d600e42faa56": {
    "soal": "Which command grants execute permissions to the Docker Compose binary after downloading it?",
    "jawaban": "The correct answer is: chmod +x /usr/local/bin/docker-compose"
  },
  "432270d672c297cb13f3214c3105829550221f77dd855b063cf15b2a7b4f8258": {
    "soal": "This workflow example shows how Network Analysis can enrich the workflow. We have used the lastfm.net data as the input network from the Widget Network File and sent it to the Widget Network Analysis. We can decide to compute the degree, degree centrality, and closeness centrality at the node-level.\n\nWe can then visualize ..................... in the Widget Network Explorer. In the Network Explorer widget, it colors with the best tags, like the default for this dataset. But now we can also adjust the node size to fit the degree centrality results. This is a good way to visualize network properties.\n\n\n\n",
    "jawaban": "The correct answer is: network"
  },
  "ef1134a943c4e3eb6182e14a2f5c87a75bca0b8e9bc5b0471fbe77a753dc21d3": {
    "soal": "Bag of Words model creates a .............(1)............. with word counts for each data instance (document). The count can be absolute, binary (present or not), or sublinear (logarithmic of term frequency). The Bag of Words model is needed in combination with the ...............(2)............ widget and can be used for predictive modeling.\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 corpus, (2) \u2192 Word Enrichment"
  },
  "5f46937b212bec9c5c7da570856a03f9e4059b1f408bc1627da282b9b24bf032": {
    "soal": "Which TensorFlow function is used to stack tensors along a new axis?",
    "jawaban": "The correct answer is: stack()"
  },
  "654413130f061492e8c4ca2ec6b82529e6016ab4b6a875f64de6813f02074266": {
    "soal": "In the workflow below, the Iris data from the File widget is passed into the Select Columns widget, where we choose to display only two attributes (petal width and petal length). Then, we can view the original dataset and the dataset with the selected rows in the Data Table widget.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "1f3a64b38555de71fd2f12e06ca5e6474df5626aae4e5b46c2e0c76e9c902dfc": {
    "soal": "010 (binary) = ...... (decimal)",
    "jawaban": "The correct answer is: 2"
  },
  "d774e4949313145f5dc80d09595118c365afa68a5848b4ca94a40067536a5e0d": {
    "soal": "What is the purpose of creating a virtual environment using 'virtualenv' before installing PM4Py?",
    "jawaban": "The correct answer is: To isolate the PM4Py installation and its dependencies from the system Python environment"
  },
  "b7176c9392680bb6732eafe99a70e17ba57f596a1f1668b5c7fcf5ade2046327": {
    "soal": "What is a validation set in machine learning?",
    "jawaban": "The correct answer is: Data used to evaluate a model's performance during training."
  },
  "c2720d32046d09a81a2eb5ded6fbe369192fc1e06c79ff6c68414926ecf6df94": {
    "soal": "What is the primary purpose of stratified sampling in dataset splitting?",
    "jawaban": "The correct answer is: To maintain the same class distribution across subsets"
  },
  "a64752220856ceb653f1baf80baebd294c71ebfb42f420e17c151f45598e247b": {
    "soal": "The pm4py.view_petri_net function requires which parameters?",
    "jawaban": "The correct answer is: net, initial_marking, final_marking"
  },
  "1e304d17dfb83299a4a3e63c465b3b562f2363a57e8b4328157736bddcfd85d3": {
    "soal": "For more complex use of the ..................... widget, we construct a workflow to redefine the classification problem in the heart-disease dataset. Initially, the task was to predict whether the patient has coronary artery diameter narrowing. We changed the problem to gender classification, based on age, chest pain, and cholesterol levels, and informally kept the narrowing diameter as a meta attribute.\n\n\n",
    "jawaban": "The correct answer is: Select Columns"
  },
  "91499aff9ab2a98b3d855fd1ada023af9138c59624ca4d658a1e88cc50888c43": {
    "soal": "PCA can be used to simplify the visualization of large datasets. Below, we use the Iris dataset to show how we can improve the visualization of the dataset with PCA. The transformed data in the Scatter Plot shows much clearer differences between classes than the default settings.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "6bca5d709dcca8edd967a69c9f7b0ce18fc2fd50166d1299d4154eef991a7430": {
    "soal": "import pandas as pd\ndf = pd.read_excel('path_ke_file.xlsx', sheet_name=0)\ndf_selected = df[['Kolom1', 'Kolom2']]\nprint(df_selected.head())\nHow do you read the first sheet of an Excel file using Pandas?",
    "jawaban": "The correct answers are: pd.read_excel('file.xlsx', sheet_name, 'Sheet2'), pd.read_excel('file.xlsx', sheet, 1), df, pd.read_excel('file.xlsx', sheet_name, 0), pd.read_excel('file.xlsx', sheet, 0), pd.read_excel('file.xlsx', sheet_name, 'Sheet1'), pd.read_excel('file.xlsx')"
  },
  "57d109345885ee6f5d395cde4073b9092e8efd093e4a9902002161a1447d0c91": {
    "soal": "For training Machine Learning models like SVM or Random Forest on TikTok comments, what is the ideal number of comments?",
    "jawaban": "The correct answer is: 3000\u201310,000 comments"
  },
  "96062862103208f84469255e2c3f26a8915942c2637edc1c44e67249ad5988d4": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nx_train = np.random.rand(1000, 10)\ny_train = np.random.randint(2, size=(1000, 1))\nmodel = keras.Sequential([\nkeras.layers.Dense(64, activation='relu', input_shape=(10,)),\nkeras.layers.Dense(32, activation='relu'),\nkeras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer='adam',\nloss='binary_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=10, batch_size=32)\nWhat kind of problem is modeled in the example?",
    "jawaban": "The correct answer is: Binary classification"
  },
  "81872e109f1ef9da230a5f25f40acb58fb5449ed1499a9dff243d7ec2c87b697": {
    "soal": "101 (binary) x 100 (decimal) = ....... (decimal)",
    "jawaban": "The correct answer is: 500"
  },
  "ae79fa773609296ea20c1af1d74bf36ce7dfb0838a9e37c3d23eba91cdabc43d": {
    "soal": "The Calibrated Learner widget generates a model that calibrates the distribution of class probabilities and optimizes the decision threshold.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "7e8c99ffff54e0930016293f5748cfa982dcdc6b8baf013453ed0275495c2e6e": {
    "soal": "When running Open-WebUI without Docker Compose, which port is mapped to the host?",
    "jawaban": "The correct answer is: 3000"
  },
  "ad0ea45e82540204a087b792428f9f74f56d308cb903bf645c8264c972a20b78": {
    "soal": "Contoh penggunaan Widget kNN untuk task regresi di perlihatkan pada workflow di bawah ini. Workflow ini menunjukkan cara menggunakan ..................... output. Untuk tujuan contoh ini, kita menggunakan dataset housing. Kita memasukkan model prediksi kNN ke dalam Predictions dan mengamati nilai yang diprediksi.\n\n\n\n",
    "jawaban": "The correct answer is: Learner"
  },
  "1062b9c0019ceac01216f68978d44ea543ed0a308a8145459947612eb55a031a": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nIn the layer definition `tf.keras.layers.Dropout(0.2)`, what does the value `0.2` represent?",
    "jawaban": "The correct answer is: 20% of the neurons will be randomly deactivated during training"
  },
  "4cf83ecb9c5253db60d0f59f6942e867ee18de7b61bf99eb5e5ded0d4575205f": {
    "soal": "The Calibrated Learner widget wraps/continues the work of another learner with probability calibration and decision threshold maximization.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "28331a55843eaa063a5381ad4b73d2800f30650ce72ecd6521ce49248d5612fe": {
    "soal": "The Select Columns widget is used to manually arrange the data domain. The user can decide which attributes to use and how to do it. Orange distinguishes between regular attributes, class attributes (optional), and meta attributes. For example, to build a classification model, the domain will consist of a set of attributes and class attributes that are discrete. Meta attributes are not used in modeling, but some widgets can use them to ............ label instances.\n",
    "jawaban": "The correct answer is: label"
  },
  "a7968a91904c5b029c94d4914b2181a781af2005f07e36c9db99f593c58ddc53": {
    "soal": "The Calibration Plot widget can show .......................... between predicted probability from the classifier and the actual class probability.\n",
    "jawaban": "The correct answer is: match"
  },
  "8ccda248d7c7c6a335f16c118d9b27a4c84601340f501fa8f9dd39d7bbc35f2e": {
    "soal": "To perform machine learning, we need numbers. To obtain numerical representations of the images, we send the images to the Image Embedding widget.\n\nThe output data from the Image Embedding widget can be inspected using the Data Table widget. Now we have the numerical representation of the images. Each image has 2048 numerical representations (columns n0 to n2047). With this numerical feature representation, we can apply all standard machine learning techniques, such as clustering.\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "46f7784ad38ce5764c2b379af901921ed246dc043eb4061fe7f4ae9285111833": {
    "soal": "Attribute names in the column header can be preceded by a label followed by a hash. Use c for class, m for meta attribute, i for ignoring the column, w for weight (importance) of the column, and C, D, T, X for continuous, discrete, time, and string attribute types. Example: C#mph, mS#name, i#dummy.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "d64b985f11eeda49382a9bc11329276159190cf370f49134851505d7f53f1afa": {
    "soal": "The Predictions widget receives a dataset and one or more predictors (predictive models, not algorithm ......................). The Predictions widget generates data and predictions.\n",
    "jawaban": "The correct answer is: learner"
  },
  "1c4dbc4bbab6b52fdec3cf3cedbed0d1f34c36cd315588b4242b5d6c186100ae": {
    "soal": "Widget Constant akan memprediksi most frequent ............\u00a0 atau mean value dari sebuah training set.\n",
    "jawaban": "The correct answer is: class"
  },
  "2aaae19c21dc1c00849d32ee7f23560a8f2290252389a9a5f2bf82f6bed74ede": {
    "soal": "The SQL widget accesses data stored in an SQL database. The SQL widget can connect to ..............(1)............... (requires the pymssql module) or .............(2)................ (requires the psycopg2 module).\n",
    "jawaban": "The correct answer is: (1) \u2192 SQL Server, (2) \u2192 PostgreSQL"
  },
  "2d55e028c679acde546b77eb82b056d9f5bf780e4df75ead540aa6ee8edcc1da": {
    "soal": "What ensures relevance in LLM responses?",
    "jawaban": "The correct answer is: Context awareness"
  },
  "180ebac985d3db22307f51ed3b6490073b63bd0ffc81b66182b8209e030e7c55": {
    "soal": "The Line Plot widget is a standard visualization widget that displays data profiles, typically numeric data arranged in order. In this simple example, we will display iris data in a line plot, grouped by iris attributes. The plot shows how petal length separates class values well.\n\n\nIf we observe this in the Scatter Line widget, we can confirm this is true. Petal length is an interesting attribute for class separation, especially when enhanced with petal width, which is also well separated in the line plot.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "2e724c6266bdeb370628151b1fad3abd0a544a33aad885eec972f54b25685270": {
    "soal": "There are three main challenges faced by businesses with Big Data:\n- Protecting sensitive and personal information\n- Data rights and ownership of assets\n- Lack of talent (such as data scientists) to analyze data",
    "jawaban": "The correct answer is 'False'."
  },
  "85573b7ef7f7e8d9355fbf295ce945f0c35af4ba17b72816e277ee1e154f2b01": {
    "soal": "What is the ideal class distribution in a dataset to prevent model bias?",
    "jawaban": "The correct answer is: Equal distribution among classes"
  },
  "4cb488d30d3e24152831838bdfa8949f2c2f671e5953547807c895cf08eb6436": {
    "soal": "Network Analysis Widget calculates statistical summaries from node-level and graph-level for network. The Network Analysis Widget will output the model with its statistical computation results and an extended item data table (only node-level index).\n",
    "jawaban": "The correct answer is 'False'."
  },
  "ed27233107993f141624cf23a3672b0f80fb9a1088725b251befd340ddb8bd00": {
    "soal": "Widget Network Clustering tries to find ...........(1).......... in the network. Network Clustering works with two algorithms, one from Raghavan et al. (2007), which uses ..............(2)........... to find the appropriate clusters, and one from ..........(3)......... et al. (2009), which builds on the work of Raghavan and adds hop attenuation as a parameter for cluster formation.\n",
    "jawaban": "The correct answer is: (2) \u2192 label propagation, (1) \u2192 cluster, (3) \u2192 Leung"
  },
  "fe5fc089d59b376867f6ace8831d998220180a7150ffbbca9afc0ff7309c9756": {
    "soal": "What is the primary purpose of an activation function in a neural network?",
    "jawaban": "The correct answer is: It introduces non-linearity to the network."
  },
  "aa43699ab6db2e8ee628484635814f8abf1ba1faf1f5af216af47aae8033ae42": {
    "soal": "What is a key benefit of using stratification when splitting datasets?",
    "jawaban": "The correct answer is: It maintains class distribution across subsets, aiding model learning."
  },
  "3a553e8d0b0a9a9d62a47a2f372d0bce72cf54274069dc07e89a21360a6f08ae": {
    "soal": "Which of the following best describes process enhancement in process mining?",
    "jawaban": "The correct answer is: Improving an existing process model using information from event logs"
  },
  "153f03732e8eac3c67d279eeb3ea6fb4b33b70694fbe39168ccb836f62ffbd50": {
    "soal": "What is the primary purpose of the LSTM (Long Short-Term Memory) layer?",
    "jawaban": "The correct answer is: Handling long-term dependencies in sequential data"
  },
  "f1942bf49c0c061c545b03c582b7bd13769d74d0ca9d55e3554c2896b77bb56d": {
    "soal": "Which type of model requires the smallest dataset size for effective training?",
    "jawaban": "The correct answer is: Simple statistical models"
  },
  "9e4b4d6907088462edb5d48cd628eebcad5dd26bdff11e681201ab583f2ba180": {
    "soal": "import pandas as pd\ndf = pd.read_excel('path_ke_file.xlsx', sheet_name=0)\ndf_selected = df[['Kolom1', 'Kolom2']]\nprint(df_selected.head())\nWhich function is used to read an Excel file into a Pandas DataFrame?",
    "jawaban": "The correct answer is: pd.read_excel()"
  },
  "0b989fd2092c81a74c69e55fc8f8ab08cc870af397aad914d21bcb6fc30b8ead": {
    "soal": "The Image Viewer widget can be used to compare images, for example when looking at similarities or differences between selected data instances (e.g., bacterial growth or Twitter comments).\n",
    "jawaban": "The correct answer is 'False'."
  },
  "3442199a62881900dd362710977c5ccb4b405ef6c174730204f124d0f1d0cd4c": {
    "soal": "What is a major drawback of Directly-Follows Graph visualization?",
    "jawaban": "The correct answer is: Can become complex and difficult to read with many activities"
  },
  "e8d9bcb57543c2900075bc35e4d3241eee5958f6cabeaa102e36c139ea4ffb2a": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nWhat is the total number of images in the MNIST dataset?",
    "jawaban": "The correct answer is: 70,000"
  },
  "7879267262a3d425eb979bb833ea37505f349ee8d0202f573469b4e506acfa62": {
    "soal": "import pandas as pd\ndf = pd.read_excel('path_ke_file.xlsx', sheet_name=0)\ndf_selected = df[['Kolom1', 'Kolom2']]\nprint(df_selected.head())\nHow can you read columns 'A' through 'C' and 'E' through 'F' from an Excel file?",
    "jawaban": "The correct answers are: pd.read_excel('file.xlsx', usecols, 'A:C,E:F'), pd.read_excel('file.xlsx', usecols, 'A-C,E-F'), df, pd.read_excel('file.xlsx', usecols, 'A:F'), pd.read_excel('file.xlsx', usecols, 'A to C and E to F'), pd.read_excel('file.xlsx', usecols, ['A', 'B', 'C', 'E', 'F']), pd.read_excel('file.xlsx', usecols, 'A,B,C,E,F')"
  },
  "fbf81b793019ebe517f07fc9f771596c2679ff4a2d7e6cfc9b479310ee611d74": {
    "soal": "In the Interpolate widget, which is part of the Time Series add-on, the interpolation type - .............(1)................. interpolation replaces missing values with series values.\n",
    "jawaban": "The correct answer is: (1) \u2192 Mean, (2) \u2192 average"
  },
  "57f2802ffd5630a473d0326e78dffa0174b5eab1beac5d90304b17b3dc813d7b": {
    "soal": "A simple example with Calibrated Learner. We use the Titanic dataset because this widget requires binary class values (in this case, they are 'survived' or 'not survived').\n\nWe use Logistic Regression as the base learner, which will be calibrated with the default settings, with sigmoid optimization from the value distribution and optimized with CA.\n\nBy comparing the results of the uncalibrated Logistic Regression model, we will clearly see that the Logistic Regression model is better.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "29527eba352c4097a97474499527dc80af3cf3d6b12683fec1f7fbe42c29cbc1": {
    "soal": "Manual web scraping can be done by:",
    "jawaban": "The correct answer is: The software user"
  },
  "a76743d0898f3668d49e1e1760ed2807aed5d7be70a22acff13038d3d0f6a2a6": {
    "soal": "What does the precision metric indicate in model evaluation?",
    "jawaban": "The correct answer is: The proportion of true positive predictions among all positive predictions."
  },
  "26910d36fac8c1dd446549efba73214fc2afea3de0ade0a9a63f1c3a773bc762": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\n# xes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\nxes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nprocess_tree = pm4py.discover_bpmn_inductive(log)\nbpmn_model = pm4py.convert_to_bpmn(process_tree)\npm4py.view_bpmn(bpmn_model)\nThe script provided primarily relates to what topic?",
    "jawaban": "The correct answer is: Process Mining"
  },
  "d1d730eb18ed358cd78f700983cd923afcbd4e055bb830cffa4296ae1d285f96": {
    "soal": "Network Clustering Widget tries to find clusters in the network. Network Clustering works with two algorithms, one from Raghavan et al. (2007), which uses label propagation to find suitable clusters, and one from Leung et al. (2009), which builds on Raghavan's work and adds hop attenuation as a parameter for cluster formation.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "168a6315567e2aec5f6a8339247569035de04457547f23695694eefa189a399e": {
    "soal": "The CSV File Import widget reads comma-separated files and sends the dataset to the output channel. The separator can be comma, semicolon, .......................... , tab, or manually-defined delimiters. The history of the recently opened files is maintained in the widget.\n",
    "jawaban": "The correct answer is: space"
  },
  "d7ae4694d05529c4a9b0cdc85d557663620d02eb911ba37847fbba8ced6f65e1": {
    "soal": "The basic framework of Apache Hadoop consists of the following modules:\n- Hadoop Common - contains libraries and utilities required by other Hadoop modules;\n- Hadoop Distributed File System (HDFS) - a distributed file system that stores data on commodity machines, providing small aggregate bandwidth in the cluster;\n- Hadoop YARN - a resource management platform responsible for managing computational resources in the cluster and using them for scheduling user applications; and\n- Hadoop MapReduce - a programming model for large-scale data processing.\nThe term \"Hadoop\" has come to refer not only to the basic modules above but also to the \"ecosystem,\" or collection of additional software packages that can be installed above or alongside Hadoop, such as Apache Pig, Apache Hive, Apache HBase, Apache Phoenix, Apache Spark, Apache Zookeeper, Impala, Apache Flume, Apache Sqoop, Apache Oozie, Apache Storm, and others.",
    "jawaban": "The correct answer is 'False'."
  },
  "d623b5023750269357726b0927a37d8bb7adde340741a7f091128ad2b62ce696": {
    "soal": "Which repository offers a collection of datasets commonly used in Machine Learning research?",
    "jawaban": "The correct answer is: UCI Machine Learning Repository"
  },
  "37e8631019ff39866191f5279fb207234ce8605875f0a8ac4701392c5aacd9c5": {
    "soal": "In the .................... widget, the periodogram for non-equispaced series is calculated using the Lomb-Scargle method.\n",
    "jawaban": "The correct answer is: Periodogram"
  },
  "cf58d47a462cd7d8523327afc327a589af6ae5ff28380f6e8bbd13884cf3cb02": {
    "soal": "The Box Plot widget shows the distribution of attribute values. It is good practice to change any new data with this widget to quickly find anomalies such as duplicate values (e.g., gray or grey), outliers, and the like.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "afadb5530f37d4eba562d56fe79eb16c999d4dffbbea73884a7e82d9a483ee0d": {
    "soal": "The distance matrix generated by the Distances Widget can be further fed into the Hierarchical Clustering Widget to uncover groups in the data, into the Distance Map Widget or the Distance Matrix Widget to visualize distances (the Distance Matrix Widget can be very slow for large datasets), into the MDS Widget to map data samples using the distance matrix, and finally, saved with the Save Distance Matrix Widget. The Distance File can be loaded with the Distance File Widget.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "1cbe35edac53c454d5da74caee7d25373ac57ef0c871819e62d4fd9fdb6cac4c": {
    "soal": "Widget ......(1)............ receives .....(2)..... and one or more .....(3)..... (predictive models, not learner algorithms). Widget .......(1)............ generates data and predictions.\n",
    "jawaban": "The correct answer is: (1) \u2192 Predictions, (2) \u2192 dataset, (3) \u2192 predictor"
  },
  "681855be657635f0f5a5f46fd4a7c7cd40daaf1bfb363b27b501f0cc8fc507bd": {
    "soal": "The Import Documents widget takes text files from a folder and creates a ...................... corpus. The Import Documents widget can read .txt, .docx, .odt, .pdf, and .xml. If there are subfolders in the folder, they can be used to label the class.\n",
    "jawaban": "The correct answer is: corpus"
  },
  "435cc961612c139274e5835220e2e99a5ae50dc3d747415b8a7ac5dc8aca78cf": {
    "soal": "The .................. widget can import a data table from a CSV formatted file.\n",
    "jawaban": "The correct answer is: CSV File Import"
  },
  "3f0408c8bc765c0e2440767bad71ae491151787be46084e3aa100700222ebac3": {
    "soal": "The Moving Transform widget can apply the ...................... window function to the time series. Use the Moving Transform widget to get the average value of the series.\n",
    "jawaban": "The correct answer is: rolling"
  },
  "8cbca65d5a1256f57e47003045e1573df2e55e36ed4e36e526ff9421a29f25ff": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nWhich variables contain the training images and labels after loading the MNIST dataset?",
    "jawaban": "The correct answer is: x_train and y_train"
  },
  "2d5a5ea27744c7d1c3fa4a5c1122f714c6aaa87123e40538ae5d91063387ce97": {
    "soal": "When there is no data in the input, the Corpus widget reads the text corpus from a file and sends the corpus instance to the output channel. The history of the last opened file is stored in the Corpus widget. In the Corpus widget, there is also a directory with sample corpora pre-installed with the add-on. The widget can read data from Excel (.xlsx), comma-separated (.csv), and tab-delimited ( ..................).\n\n",
    "jawaban": "The correct answer is: .tab"
  },
  "42786c80e9ae1a7ee415cc26c5de414d496a52a5dfd600992a5d3dd5df51b392": {
    "soal": "The following workflow is an example where we can compare three (3) classifiers (i.e., Naive Bayes, Tree, and Constant) and input them into the Predictions widget. We are using the Titanic dataset. The Test & Score widget will display the evaluation results for each classifier. Then, we can plot through the Calibration Plot widget and the ROC Analysis widget from the output of the Test & Score widget for further analysis of the performance of each classifier. The Calibration Plot widget allows us to see the accuracy predictions of the class probability in the form of a plot/image.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "75b2d3c5cadcb9ee3e1d0cacc47b258154d19ff949b4a7f07259fd23ece2ed1e": {
    "soal": "Which command is used to update the package lists on a Debian-based system before installing Docker?",
    "jawaban": "The correct answer is: sudo apt update"
  },
  "faad1a7a809e544775addaeabe657ebe1e0e191cfa9d812b9623b97e594fd134": {
    "soal": "We use the zoo dataset in combination with Hierarchical Clustering to find animal groups. Now we have the clusters we want to identify and what is significant for each cluster! Provide the cluster to the .................... widget and use \u2018Order by relevance\u2019 to find out what defines the cluster. It appears they are separated by their type, even though the clustering was done without class labels! This is an example of unsupervised learning.\n\n\n\n",
    "jawaban": "The correct answer is: Box Plot"
  },
  "782c2bf52f763f2a5d0669d32f19a2696786af1f4dbf27b1d6438a794abd2b55": {
    "soal": "Widget ..........(1).............. tries to find clusters in the network. Network Clustering works with two algorithms, one from ............(2)......... et al. (2007), which uses label propagation to find the appropriate clusters, and one from Leung et al. (2009), which builds on the work of Raghavan and adds ..............(3)............ as a parameter for cluster formation.\n",
    "jawaban": "The correct answer is: (3) \u2192 hop attenuation, (2) \u2192 Raghavan, (1) \u2192 Network Clustering"
  },
  "ebbd7008668d73280afe3cb0550f7ce82f623fb35e4320786e8222bc7ace4ec8": {
    "soal": "Widget .............(1)............. can help us uncover clusters and highly connected groups in a ............(2)............ First, we will use the Widget Network File to load the lastfm.net dataset. Then we will send this network to the Widget Network Clustering. The Network Clustering widget finds 79 clusters in the network. To visualize the results, we use the Network Explorer Widget and set the Color attribute to Cluster. This will color the network nodes according to the cluster colors - this is a good way to visualize highly connected groups in a dense network.\n\nKeep in mind that the Network Explorer Widget here will color the 10 largest clusters and color the rest as '..............(3)..................'.\n\n\n",
    "jawaban": "The correct answer is: (3) \u2192 Other, (2) \u2192 network, (1) \u2192 Network Clustering"
  },
  "1606638ca6f2b063ad9e6f63696bc479e0b4e980dc1d80455bbd817fe3c9de8f": {
    "soal": "For more complex use of the Select Columns widget, we construct a workflow to redefine the classification problem in the heart-disease dataset. Initially, the task was to predict whether the patient has coronary artery diameter narrowing. We changed the problem to gender classification, based on age, chest pain, and cholesterol levels, and informally kept the narrowing diameter as a ....................\n\n\n",
    "jawaban": "The correct answer is: meta"
  },
  "5864935dd5c3b7e68838ce22bfdc7619ea24d5db1b7bc0f6b4b5f01f7a22df1c": {
    "soal": "In the Import Documents Widget, to retrieve data, select (click) the folder icon on the right side of the widget. Select (choose) the folder that we want to make .................. After loading is complete, we will see how many documents were successfully retrieved by the Import Documents Widget. To observe the obtained corpus, connect the Import Documents widget to the Corpus Viewer Widget. In this workflow, we are using a set of speeches by President Kennedy in plain text format.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: corpus"
  },
  "7853268b088071b0c47edbea5f9b2c2756f44117a812283ef803e65fec839723": {
    "soal": "The Distances widget minimizes the distance between rows/columns in the dataset.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "94a6afe7a545a6b6ab0632eb9aab3651fa3c49b3341eee7b477ab2bb262ab65d": {
    "soal": "How is a dataset typically structured?",
    "jawaban": "The correct answer is: In a tabular form with rows and columns."
  },
  "2c6ed3b811fd905a69220a9c50042cf5e329623811e84a08af73df41b4fb5b8e": {
    "soal": "import pm4py\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\n# Import XES log\nlog = xes_importer.apply(xes_filename)\n# Cek jumlah kasus dan aktivitas\nprint(f\"Jumlah kasus: \")\nactivities = set()\nfor trace in log:\nfor event in trace:\nactivities.add(event['concept:name'])\nprint(f\"Jumlah aktivitas unik: {len(activities)}\")\nfrom pm4py.algo.discovery.dfg import algorithm as dfg_discovery\nfrom pm4py.visualization.dfg import visualizer as dfg_visualization\n# DFG berdasarkan frekuensi\ndfg = dfg_discovery.apply(log,\nvariant=dfg_discovery.Variants.FREQUENCY)\n# Tampilkan visualisasi\ndfg_vis = dfg_visualization.apply(dfg, log=log,\nvariant=dfg_visualization.Variants.FREQUENCY)\ndfg_visualization.view(dfg_vis)\nWhat does the PM4Py library specifically target?\n{\n=Process Mining\n-Machine Learning\n-Image Processing\n-Natural Language Processing\n-Web Development\n-Database Management\n}",
    "jawaban": "The correct answer is: len(log)"
  },
  "42c9bf4ab944c48f1a883b4a78a37c09928479e5520842aa5c92164dbb3c03c7": {
    "soal": "In this example, we will just check what a bag of words model looks like. Load book-excerpts.tab using the Corpus widget and connect it to the Bag of Words widget. Here, we deliberately use the default parameters - the simplest count is term frequency. Check the output of the Bag of Words widget using the Data Table widget. The last column represents the term frequency of each document.\n\n\n",
    "jawaban": "The correct answer is: term"
  },
  "d0dfe25fef20f6078b37c15ed537ca569b85cba4c93cd6e10f7b350be0feb6aa": {
    "soal": "The miner that recursively identifies event patterns:",
    "jawaban": "The correct answer is: Inductive Miner"
  },
  "14d2f693e48a762be3a1d118e0970ea8a438b81eb990b9806632c5b37a2f72a6": {
    "soal": "Which line is used to check the TensorFlow version installed?",
    "jawaban": "The correct answer is: print(\"TensorFlow version:\", tf.__version__)"
  },
  "1a8672ab957a2f5eb5af477e8bdd17784beb9a0e1b4c7413780a5c566da3534d": {
    "soal": "How good is the supervised data mining method for classifying our dataset? Below is a workflow that evaluates various classification techniques on a dataset (the example here is iris). The main widget used here is the Test & Score widget, which receives the data and a learner set, performs cross-validation, calculates prediction accuracy, and generates scores for further inspection.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "8700a0637ae3f61c8a6cb99311ee63a62761a7c27e020ae2ff8d7f64302cec93": {
    "soal": "import pm4py\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\n# Import XES log\nlog = xes_importer.apply(xes_filename)\n# Cek jumlah kasus dan aktivitas\nprint(f\"Jumlah kasus: \")\nactivities = set()\nfor trace in log:\nfor event in trace:\nactivities.add(event['concept:name'])\nprint(f\"Jumlah aktivitas unik: {len(activities)}\")\nfrom pm4py.algo.discovery.dfg import algorithm as dfg_discovery\nfrom pm4py.visualization.dfg import visualizer as dfg_visualization\n# DFG berdasarkan frekuensi\ndfg = dfg_discovery.apply(log,\nvariant=dfg_discovery.Variants.FREQUENCY)\n# Tampilkan visualisasi\ndfg_vis = dfg_visualization.apply(dfg, log=log,\nvariant=dfg_visualization.Variants.FREQUENCY)\ndfg_visualization.view(dfg_vis)\nWhat analysis is primarily conducted by PM4Py using XES files?\n{\n=Performance and bottleneck analysis\n-Regression analysis\n-Clustering analysis\n-Classification analysis\n-Time series forecasting\n-Text sentiment analysis\n}",
    "jawaban": "The correct answer is: len(log)"
  },
  "20599e6d5c5d2c0196de4b462a7619abf6efbacd6a84435971d40cf72e2fae08": {
    "soal": "\n\n\nThe image above shows the sequence of processes in data science, which are:\n\nASK - Ask, interesting questions.\nGET - Acquire data.\nEXPLORE - Explore the data, check if there are any oddities/anomalies/interesting patterns, etc.\nMODEL - Build, fit, and validate the model.\nVISUALIZATION - Communicate and visualize data.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "c9cf8bffae4b76aa6e6de834c32e8660bbe8e195b44950538c5b437a9decdbc3": {
    "soal": "The ................. algorithm is a simple algorithm that can separate data into nodes based on class purity (category/class purity). The Tree algorithm is the predecessor of the Random Forest algorithm. The Tree widget in Orange is designed in-house and can handle both discrete and continuous datasets.\n",
    "jawaban": "The correct answer is: Tree"
  },
  "1a46f34945b7d39ab311a7b41fa1363923cc8aa3c90a9f809d0eb57b3ee2324e": {
    "soal": "In the workflow below, the usage of the Moving Transform widget is shown to obtain a 5-day ...................................., we can use a rolling window with mean aggregation.\n\n\n",
    "jawaban": "The correct answer is: moving average"
  },
  "c346472bc734bd99d3a3238957a666c8b3597c8265acd00b2a8a5a9a9f6df81c": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nWhat is the role of the Flatten layer in the model?",
    "jawaban": "The correct answer is: Transforming the input into a 1D array"
  },
  "18b1f1230c82c7ed1881eb712d393b74a7026cc7570149f9569bf9ec3db5801a": {
    "soal": "Most visualizations in ............(1)................ are interactive. In the workflow below, the .............(2)............. widget for example. Double click its icon to open it and click-and-drag to select multiple data points from the plot. The selected data will automatically go into the Data Table widget. Double click to inspect which data is selected. Change the selection and observe changes in the ..........(3)............. widget.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 Orange, (3) \u2192 Data Table, (2) \u2192 Scatter Plot"
  },
  "d3a83a8eedb775f6bfb985245e24b5981630787ea2796737d3b1ef6e0e98f84f": {
    "soal": "Widget Import Images is likely the first widget we will use in image analysis. Widget Import Images loads images and creates class values from the folder. In this example, we are using Widget Import Images to load 26 paintings by Monet or Manet.\n\n\nWe can observe the results in the Data Table widget. It is clearly visible how Orange adds an additional class attribute with values Monet and Manet.\n\n\nNext, we can proceed to the standard machine learning method. We send the image to the Image Embedding widget, where we will use the Painters embedder to receive the image vector.\n\n\nThen we will use the Prediction widget and the Logistic Regression widget to create a model to predict the painter of a painting (in this case, Monet or Manet). We get a very good score. How is that possible? It turns out these images are images that have already been trained with the Painters embedder, so the accuracy obtained will be high.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "543f1137891792778f634d1bc4f9ae95595fb5730d2d535a5c200a1960423b24": {
    "soal": "The ............(1)............. widget can visualize cycles (seasonality), ............(2)..........., ...............(3).............. (periodicity), and important periods in time series.\n\n",
    "jawaban": "The correct answer is: (2) \u2192 seasonality, (3) \u2192 periodicity, (1) \u2192 Periodogram"
  },
  "fa6f4aaf4de506d3a015eba124fdd6fc18dfda5c214c8d224148d36fe81363a4": {
    "soal": "The Line Chart widget can visualize the sequence of time series and its movement in the most basic time series visualization that is quite easy to imagine.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "b1bde22aba94088b8a3907c957f4d9527385d8420de3f4ec600e5d9d8a0196e9": {
    "soal": "Now, each core Orange widget can function / interact with ................. smoothly, so you can mix and match widgets as you like. Previously, one could not forward output from Select Columns (data table) to Preprocess Text (corpus), but this is no longer an issue.\n\n\n",
    "jawaban": "The correct answer is: Text"
  },
  "a6db28adb395c743d1b54e2a201de087ac8dca48ecfb73a84c6e5a357a42b67e": {
    "soal": "Widget Network Analysis calculates statistical summaries from the node-level and graph-level for the network. Widget Network Analysis will output the network with the result of ........................ statistics and an extended item data table (only node-level index).\n",
    "jawaban": "The correct answer is: computation"
  },
  "def1f970c2243d308fc3cb60e93b3ecaac3b2cf4cddb1d9aa4cb89bc31272ce5": {
    "soal": "The Scatter Plot widget can perform visualizations using tree images with exploratory analysis and intelligent data visualization enhancements.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "7ede4ff89377e300a2d47695b1dc89b74f3dbf4b8c36a6b32bb0998925ad3bf0": {
    "soal": "Widget Data Table can display attribute-value data in multiple spreadsheets.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "ac63a8535b6ed6559a03bf0384a7762ad9b17ceebbe4552e5b4a1cd257c355e9": {
    "soal": "When the user provides data to the input, the Corpus widget will convert the data into a corpus. The user can choose which feature to use as the text feature.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "29502b4b13f93a26779cea60ed313421d9ab5c752175875ed083463fbd6496ab": {
    "soal": "import pandas as pd\nfrom pm4py.objects.conversion.log import converter as log_converter\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom pm4py.visualization.petrinet import visualizer as pn_visualizer\nfrom pm4py.objects.log.util import dataframe_utils\n# csv_filename='Insurance_claims_event_log.csv'\n# csv_filename='Business Travel Permits.csv'\n# csv_filename='Parent-Child Union Output.csv'\n# csv_filename='PM4PY_Dummy_Event_Log_100_Cases_pm4py.csv'\n# csv_filename='PM4Py_Dummy_Event_Log-congested-business-process.csv'\n# 1. Load CSV event log\ndf = pd.read_csv(csv_filename)\n# df = pd.read_csv(csv_filename,sep=';')\n# print(df.head())\n# 2. Format timestamps properly\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n# 3. Ensure column names match PM4Py expectations\ndf = dataframe_utils.convert_timestamp_columns_in_df(df)\n# 4. Convert to event log object\nevent_log = log_converter.apply(df, parameters=)\n# 5. Discover process model using Alpha Miner\nnet, initial_marking, final_marking = alpha_miner.apply(event_log)\n# 6. Visualize Petri Net\ngviz = pn_visualizer.apply(net, initial_marking, final_marking)\npn_visualizer.view(gviz)\nWhat's the default separator when using 'pd.read_csv()' without specifying?\n{\n=Comma (,)\n~Semicolon (;)\n~Tab (\\t)\n~Space\n~Pipe (|)\n~Colon (\n}",
    "jawaban": "The correct answer is: log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case_id'"
  },
  "3d81bf0b43a1cc238dc0135dfc4c17cf13e5428d467f2dd8d1a62bd14fb2faf4": {
    "soal": "Distances are most commonly calculated between instances (\"rows\" in the Distances widget) or attributes (\"columns\" in the Distances widget). Many inputs are compatible for the Distance Map, one of which is the Distances widget. In the output, the user can select the map region, and the Distance Map widget will display the corresponding instances or attributes.",
    "jawaban": "The correct answer is 'False'."
  },
  "38b5efb2f0f37facfe5c84c2f9cbdb3a6403eaf4950289f15f1138efe7a4353a": {
    "soal": "In TensorFlow, which function computes the mean of elements across dimensions of a tensor?",
    "jawaban": "The correct answer is: reduce_mean()"
  },
  "3d3bc01db75945377a5d4304ad0d099ddcfc6f7b169d356df7137534447c0932": {
    "soal": "Some visualization widgets, such as Scatter Plot and several data projection widgets, can expose data instances within a data subset. In this workflow, the Scatter Plot visualizes data from the input data file, but also marks the data points selected in the Matrix Table (the selected rows).\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "f8abbd046c8d9bb14426265f2593ce1c7140ec61ed55e5f92e87663d8016c05b": {
    "soal": "The Distances widget ignores discrete values and calculates distances only for discrete data, so the Distance Map widget can only display distance maps for discrete data.",
    "jawaban": "The correct answer is 'False'."
  },
  "f3240a419c10a5709bb613bba489a252e2161ec68b7e504a91dd00e5e6e16c92": {
    "soal": "After creating a virtual environment named 'venv', which command activates it?",
    "jawaban": "The correct answer is: source venv/bin/activate"
  },
  "c419ac4c7121c624830bfe37155180e8c470d3a8063e97fb1f5ff9b8301a8cc3": {
    "soal": "\n\n\nExamples of professions in machine learning include,\n\n\n\n\n\n\n",
    "jawaban": "The correct answers are: Data Scientist, Machine Learning Engineer"
  },
  "6f8344eb1e71c903a05083548416be19e418755c6ddbe8016f548e5f9cb60161": {
    "soal": "The Distances widget also works well with other Orange add-ons. The Distance Matrix widget can be fed into the Network from Distances Widget (Network add-on) to convert the matrix into a graph and into the Duplicate Detection Widget (..................... add-on) to find duplicate documents in the corpus.\n",
    "jawaban": "The correct answer is: Text"
  },
  "17903bf52de3c8c71b4e80e530f1f86fb2b2fed29e75f3248043fed2fa3533b1": {
    "soal": "Data Mining is critical to supporting the success of modern data-driven organizations. An IDG survey of 70 IT and business leaders found that 92% of respondents want to apply advanced analytics more broadly across their organizations. The same survey found that the benefits of data mining are deep and wide.\nIn fact, respondents identified no fewer than 30 different ways in which data mining positively impacts their business. Here are the top 10:\n- Improving decision-making processes\n- Improving security risk posture\n- Improving Planning and Forecasting\n- Competitive advantage\n- Cost reduction/savings\n- Customer acquisition\n- New revenue streams\n- Acquisition/retention of long-term employees\n- Strengthening customer relationships\n- New product development",
    "jawaban": "The correct answer is 'False'."
  },
  "c80d7dd515e98999db1327b97fef1370c6e341bd4b2e5dbd7b1f905b6692d9de": {
    "soal": "Widget ............(1).............. can be used for .............(2)............. analysis of network ............(3)................\n",
    "jawaban": "The correct answer is: (3) \u2192 data, (1) \u2192 Network Analysis, (2) \u2192 statistical"
  },
  "46d85da52fb4a5d326faa93b2207314e59936deb2fb379480f17a9d1ea925daa": {
    "soal": "Parent Component Analysis (PCA) computes the PCA linear transformation of input data. It outputs a transformed dataset with the weight of individual instances or the weight of principal components.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "8da873b1fbc63e1e7bc9c5b0b897029efd36c18cb0413d7df3ecc7aeb89c90e4": {
    "soal": "How do you check the status of a specific model?",
    "jawaban": "The correct answer is: ollama show model"
  },
  "ef0cfe91211c6d65477c4e1302660364a72afce40911f2654afe955ac8c9a381": {
    "soal": "This is a binary number",
    "jawaban": "The correct answer is: 100110"
  },
  "42ee113d2fedbb232a24c5bdd734fb8f8a78814d8f3392f7e4ad55aa9ecc0abf": {
    "soal": "For supervised problems, where data instances are explained with .................. labels, we want to know which feature is the most informative. The Rank Widget provides a table of features and their informativeness scores, and supports manual feature selection. In this workflow, we use it to find the two best features (from the 79 initial ones from the selected dataset) and display their scatter plot.\n\n\n",
    "jawaban": "The correct answer is: class"
  },
  "2515065b50f8bba487681d7dec43003136c20e55a38af2f7e7691f6bc365e580": {
    "soal": "Word Cloud data can be built from the text file (ASCII) we have. First, the data from the text file must be segmented into words. Then, the output segmented word data needs to be converted from segmented data into a corpus to be processed by the text mining toolbox. Before displaying it as a word cloud, no preprocessing is needed to remove unnecessary words, such as conjunctions, etc.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "bd4947f6021b492f92d4fbd5ecd294631eb83188bd531926a8f8ad3437742982": {
    "soal": "In the snapshot below, we can see how transformation affects the distance matrix. We load the Iris dataset and calculate the distance between rows with the help of the Distance Map widget. To show how Distance Transformation affects the Distance Matrix, we create the workflow below and compare the transformed distance matrix with the \"original\".\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "aeed476e33d8aa3a4b4ecdc59ac71c235c827eabcd128e8a24f6d1c201578e73": {
    "soal": "The easiest way to use the Venn Diagram Widget is to select a data subset and find matching examples in the visualization. We use the breast-cancer dataset to select five subsets with the Select Rows widget \u2013 the first subset is breast cancer patients aged between 40 and 49 years, and the second subset is patients with tumor sizes between 20 and 29. The Venn Diagram helps us find examples that meet both criteria, which can be found at the intersection of the two circles.\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "48a5e469839573c220028131a7c2bc33c0bb2a8ab4f4defa752c747775f85662": {
    "soal": "The basic framework of Apache Hadoop consists of the following modules:\n- Hadoop Common - contains libraries and utilities required by other Hadoop modules;\n- Hadoop Distributed File System (HDDS) - a distributed file system that stores data on commodity machines, providing very high aggregate bandwidth in the cluster;\n- Hadoop YARN - a resource management platform responsible for managing computational resources in the cluster and using them for scheduling user applications; and\n- Hadoop MapReduce - a programming model for large-scale data processing.",
    "jawaban": "The correct answer is 'False'."
  },
  "0a9ca7b9cb1ddbd4d5ddadc7fd2dc1fe9241381940b9ea9960a0a91995ac765b": {
    "soal": "Pada Widget Constant, Learner akan menghasilkan model yang selalu memprediksi mayoritas untuk classification atau nilai rata-rata untuk ........................\n",
    "jawaban": "The correct answer is: regresi"
  },
  "651280ff7481afa111be57c4a844c40af4f4334fe8381f3f2ad3ff22d37c93dc": {
    "soal": "In the image below, we use the Network Generator Widget to create a Grid graph with a height of 4 and a width of 5 and send it to Network Analysis. We can calculate node degrees and send the data to Network Explorer. Finally, we observe the resulting graph in the visualization and set the node size and color to Degree. Network Explorer is a good tool for observing and explaining network properties.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "862d08d55a306cea8f80a3c273a8f661e9d3b4505ea4e5a97853ab874bebb12d": {
    "soal": "Widget Save Data saves data every time a new signal is received in the input because it would continuously (and mostly unintentionally) overwrite the file. Instead, data is saved only after a new file name is set or the user presses the Save button.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "1910df5af0a15a1302b24ad98a3a126645d93961418273a6d345039730320399": {
    "soal": "Word Cloud data can be built from the text file (ASCII) we have. First, the data from the text file must be segmented into words. Then, the output segmented word data does not need to be converted from segmented data into a corpus to be processed by the text mining toolbox. Before displaying it as a word cloud, it is advisable to perform preprocessing first to remove unnecessary words, such as conjunctions, etc.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "1236f4d27afc7e5ae2ce1304192455dca1f2aa08ede259b31b35f00707c05512": {
    "soal": "import pandas as pd\nfrom pm4py.objects.conversion.log import converter as log_converter\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom pm4py.visualization.petrinet import visualizer as pn_visualizer\nfrom pm4py.objects.log.util import dataframe_utils\n# csv_filename='Insurance_claims_event_log.csv'\n# csv_filename='Business Travel Permits.csv'\n# csv_filename='Parent-Child Union Output.csv'\n# csv_filename='PM4PY_Dummy_Event_Log_100_Cases_pm4py.csv'\n# csv_filename='PM4Py_Dummy_Event_Log-congested-business-process.csv'\n# 1. Load CSV event log\ndf = pd.read_csv(csv_filename)\n# df = pd.read_csv(csv_filename,sep=';')\n# print(df.head())\n# 2. Format timestamps properly\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n# 3. Ensure column names match PM4Py expectations\ndf = dataframe_utils.convert_timestamp_columns_in_df(df)\n# 4. Convert to event log object\nevent_log = log_converter.apply(df, parameters=)\n# 5. Discover process model using Alpha Miner\nnet, initial_marking, final_marking = alpha_miner.apply(event_log)\n# 6. Visualize Petri Net\ngviz = pn_visualizer.apply(net, initial_marking, final_marking)\npn_visualizer.view(gviz)\nWhat key parameter is required when converting DataFrames to event logs?\n{\n=CASE_ID_KEY\n~ACTIVITY_KEY\n~TIMESTAMP_KEY\n~EVENT_KEY\n~ID_KEY\n~LOG_KEY\n}",
    "jawaban": "The correct answer is: log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case_id'"
  },
  "169c314fef400ba0390b8b70f6375f4ebaa96e47d37ba8666ffb121cb964569f": {
    "soal": "The selected data in the first Data Table widget is passed to the second widget .............................. Observe that we can choose which dataset to view (iris or glass). Switching from one dataset to another changes the data instance selection that is communicated if Commit on any change is selected.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Data Table"
  },
  "c7406c7dbf2ee0f74ef8eb6d7d324bf81ba3d50492ff2c05d7bdd5152777b526": {
    "soal": "The Periodogram widget can visualize cycle, seasonality, periodicity, sinusity, and important periods in time series.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "58edf653dcfa66c18d438bcad0db5f9d20a16f82762a994741de0393ae38ed4a": {
    "soal": "Widget SQL Table accesses data stored in an SQL database. The SQL widget can connect to PostgreSQL (requires the psycopg2 module) or MySQL Server (requires the pymssql module).\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "2fa34902faaae965b8a71df6555ba8a5763e908b0765cc34f36a033d36ec45ee": {
    "soal": "The visualization in the Network Explorer Widget functions similarly to the one used in the .................... Plot widget. To select a subset of nodes, draw a rectangle around the subset. Shift will add a new group. Ctrl-Shift (Cmd-Shift) will add to the existing group. Alt (Option) will remove from the group. Clicking outside the network will deselect the selection.\n\n\n\n",
    "jawaban": "The correct answer is: Scatter"
  },
  "3695dda1345bfd9a1bd9b3712cd2f2a78ac691ad1b4c1d14ee3e163e4ccd2e29": {
    "soal": "What is the purpose of batch normalization?",
    "jawaban": "The correct answer is: To normalize the inputs of each layer, stabilizing the learning process"
  },
  "db6ce490636f64e45abf45b1ca1bf715ef6b7bb5e44386309cb188b618f69092": {
    "soal": "Orange can import comma- or tab-delimited data files, or native Excel files or Google Sheets documents. Use the File widget to load data and, if necessary, specify class and meta attributes.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "5e6a79ea6f43c5ed4ea2b4b5e4bf081296fbe81a90236287b484a5ebfddf675f": {
    "soal": "Widget Network Clustering tries to find clusters in a network. Network Clustering works with two algorithms, one from Raghavan et al. (2007), which uses label propagation to find the appropriate clusters, and one from Leung et al. (2009), which builds on the work of Raghavan and adds ....................... as a parameter for cluster formation.\n",
    "jawaban": "The correct answer is: hop attenuation"
  },
  "a5721c2a49004ed665cc1a6b6de6d1a9c9affde242406facdcd5f6d32cc65d58": {
    "soal": "A heat map is a graphical method for visualizing attribute values based on class in a two-way matrix. Heat maps can only be used on datasets that contain continuous variables. The values are represented by colors: the higher the value, the darker the color. By combining class and attributes on the x and y axes, we see where which attribute values are strongest and weakest, allowing us to find typical features (continuous) or value ranges (discrete) for each class.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "b9fc6123575f08b3f2089614d45a3f2f0afe85a2c748ceade89459b5eaa19e02": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nIn the CIFAR-10 dataset, what do the `x_train` and `x_test` arrays represent?",
    "jawaban": "The correct answer is: Image data"
  },
  "bfe4c3fb655820b4e3a28c2d12a270ac774d163eb31efce051265bc3bc1b149e": {
    "soal": "In the Moving Transform widget, to integrate the time series\u2019 difference from the Difference widget, use Cumulative ...................... on a wide enough window to capture the entire series.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: sum aggregation"
  },
  "b27f8cc09670b94b97bc0e369c48523eb5b33cefa9e2acff7ac03ae6ccd30311": {
    "soal": "If the Import Documents widget fails to read a specific file for any reason, that file will not be skipped. Files that are successfully read will be sent to the output.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "04ead315226efe2ca552c4b5aa17797a87fcd4892414a472077c4996ffc2df5e": {
    "soal": "Why is testing a model on new, unseen data crucial?",
    "jawaban": "The correct answer is: To ensure the model generalizes well to real-world data."
  },
  "9bf47e9300fbc0c4c1fd11402dbaaf041c6ed7958c5d8122eea352d82702472f": {
    "soal": "Widget Save Data does not save data every time a new signal is received in the input because it would continuously (and mostly unintentionally) overwrite the file. Instead, data is saved only before a new file name is set or the user presses the Save button.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "a7172b440ce738ba2fa10eae69a3a780c0dd6f870410ed13f39d05afe12dc835": {
    "soal": "perform the AND operation on the following IP address\n\n10.111.123.245 AND 255.255.255.0\n\nthe result is\n\n\n",
    "jawaban": "The correct answer is: 10.111.123.0"
  },
  "276ad3cd52bd433742579c36484d1ed6ff8f283c782d051256e7632b514c9f6a": {
    "soal": "If parameters are poorly set, Inductive Miner might:",
    "jawaban": "The correct answer is: Overgeneralize the model"
  },
  "1ab6d543676fdcb4f8df8d6b1b7ab862c89700550c16fa4f28c7f65f916ecde6": {
    "soal": "Widget Calibrated Learner menghasilkan sebuah model yang mengkalibrasi distribusi dari class probabilities dan meng-optimasi decision threshold. Widget ini hanya bekerja untuk ......................... task saja.\n",
    "jawaban": "The correct answer is: binary classification"
  },
  "47a837291d59951fb6818d326793ec95df925b3735d97602d5ce8d22f1e12313": {
    "soal": "In the workflow below, the widget ..............(1)............... can load subfolders. We store President Kennedy's speeches in two (2) subfolders, pre-1962 and post-1962. If we load the parent folder, both subfolders will be used as class labels. Check the output of the Import Documents widget in the ...........(2)........... widget.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 Import Documents, (2) \u2192 Data Table"
  },
  "d206ada964f2cc47c106158789e22f1eb59d8f69e672b7e3ff97a8927ca2fc2f": {
    "soal": "In the workflow below, the Tree widget will train a model and evaluate it against Logistic Regression. The accuracy comparison between the ................. widget and Logistic Regression widget is done through the Test & Score widget. The results from the Test & Score widget can be viewed through the Confusion Matrix widget to compare the classification errors that occurred.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Tree"
  },
  "a61ed5bebd81f3232e9fc8d48476690a866e1edeef04cc7744d6b445c00684fd": {
    "soal": "In the ..................... widget, to integrate the time series\u2019 difference from the Difference widget, use Cumulative sum aggregation on a wide enough window to capture the entire series.\u00a0\n\n\n",
    "jawaban": "The correct answer is: Moving Transform"
  },
  "cf3dc7875953cb4c72f68db66dbce60394bfe299b25043e0937c55b14857cf13": {
    "soal": "The ................. widget receives a dataset and one or more predictors (predictive models, not algorithm learners). The ................. widget generates data and predictions.\n",
    "jawaban": "The correct answer is: Predictions"
  },
  "bc094f54bb7fae0b9e83aab5d7cc3b1179a9f5926108300e8190203949114000": {
    "soal": "One example of using the Widget...................... for regression tasks is shown in the workflow below. This workflow shows how to use the Learner output. For this example, we are using the housing dataset. We input the prediction model ................. into Predictions and observe the predicted values.\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: kNN"
  },
  "4b483410280e36f51ed6bd938ac079331f5bd5b55e3d4b0cd88ab1c07d2c48ce": {
    "soal": ".................. widget can also be used to explore different prediction models. In the following example, we analyze 3 prediction methods, namely Naive Bayes, SVM, and Random Forest, based on their classification errors on data instances.\n\nBy selecting misclassification in the three Confusion Matrix widgets and sending them to the Venn Diagram widget, we can see all misclassification examples visualized by the method used. We then open the Venn Diagram widget and select, for example, misclassified instances identified by all three methods. This is represented as the intersection of the three circles. Click on the intersection to view these two examples highlighted in the Scatter Plot widget. Try selecting different parts of the diagram to see how the scatter plot visualization changes.\n\n\n",
    "jawaban": "The correct answer is: Venn Diagram"
  },
  "ce7cc70f24a7a16ff39c299f8e322ba02417f22d4f15186bddc3e1a876478246": {
    "soal": "What is a testing set in machine learning?",
    "jawaban": "The correct answer is: Data used to evaluate the final model's performance."
  },
  "d5b8629600535a9ee7a9e331530adfe984caedca1c1f60b169eec0e5c1b0a930": {
    "soal": "Widget Import Images is likely the first widget we will use in image analysis. Widget Import Images loads images and creates class values from the folder. In this example, we are using Widget Import Images to load 26 paintings by Monet or Manet.\n\n\nWe can observe the results in the Data Table widget. It is clearly visible how Orange adds an additional class attribute with values Monet and Manet.\n\n\nNext, we can proceed to the standard machine learning method. We send the image to the Image Embedding widget, where we will use the Painters embedder to receive the image vector.\n\n\nThen we will use the Test & Score widget and the Logistic Regression widget to create a model to predict the painter of a painting (in this case, Monet or Manet). We get a very good score. How is that possible? It turns out these images are images that have already been trained with the Painters embedder, so the accuracy obtained will be high.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "0491108a210954743b5b454b981bd264b91d065f207382c7639d99286518401e": {
    "soal": "Which API in TensorFlow allows you to define and train machine learning models?",
    "jawaban": "The correct answer is: Keras"
  },
  "76e52c64a047e7e71d1295f8779b810859c733577a5d8afb8013e680edd4f1d6": {
    "soal": "Which platform provides access to various datasets, workflows, and experiments for the Machine Learning community?",
    "jawaban": "The correct answer is: OpenML"
  },
  "353f3bd38e685d5ee566a18f85d307da1feae0ce1fefd0fb88e0dd289f9208fa": {
    "soal": "\n\n\nVisualisasi kesalahan klasifikasi yang digunakan di workflow di atas adalah\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: scatter plot"
  },
  "788bc8a878047c95bbe12377dea0f7b8ef02b8b4902af8a9d067c0013913d7b7": {
    "soal": "The ...................... widget is probably the first widget we will use in image analysis. The Import Images widget loads images and creates a class value from the folder. In this example, we use the Import Images widget to load 26 paintings by Monet or Manet.\n\nWe can observe the results in the Data Table widget. It is clear how Orange adds an additional class attribute with values Monet and Manet.\n\nNext, we can continue with standard machine learning methods. We send the images to the Image Embedding widget, where we will use the Painters embedder to get the image vector.\n\nThen we will use the Test & Score widget and the Logistic Regression widget to create a model to predict the artist of a painting (in this case Monet or Manet). We get a very good score. How could that be? It turns out these images were already trained on the Painters embedder, so the accuracy achieved will be high.\n\n\n",
    "jawaban": "The correct answer is: Import Images"
  },
  "7ae8b38351b40b344a7740613defc33af2ae5fd6220ce526da16261518946c92": {
    "soal": "Widget Network Clustering tries to find clusters in a network. Network Clustering works with two algorithms, one from Raghavan et al. (2007), which uses label propagation to find the appropriate clusters, and one from ....................... et al. (2009), which builds on the work of Raghavan and adds hop attenuation as a parameter for cluster formation.\n",
    "jawaban": "The correct answer is: Leung"
  },
  "3e4b63fd92448bea7c542eafc3f713651adb91de6d1ed1fec31750a0eaaace73": {
    "soal": "import os\nimport requests\nfrom googlesearch import search\nfrom bs4 import BeautifulSoup\nimport re\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef clean_filename(text):\n# Hilangkan karakter ilegal untuk nama file\nreturn re.sub(r'[\\/*?:\"<>|]', , text).strip().replace(' ', '_')[:50]<%1\u001f%\u001f0%> def get_page_content(url):\ntry:\nheaders =\nresponse = requests.get(url, headers=headers, timeout=10)\nsoup = BeautifulSoup(response.content, 'html.parser')\ntitle = soup.title.string if soup.title else 'No Title'\nparagraphs = soup.find_all('p')\ncontent = '\n\n'.join([p.get_text() for p in paragraphs[:5]])\nreturn title.strip(), content.strip()\nexcept Exception as e:\nreturn 'Error', f\"Gagal mengambil konten: {e}\"\ndef save_to_txt(keyword, rank, title, url, content, folder='outputs'):\nos.makedirs(folder, exist_ok=True)\nfilename = f\"{clean_filename(keyword)}_{rank}_{clean_filename(title)}.txt\"\nfilepath = os.path.join(folder, filename)\nwith open(filepath, 'w', encoding='utf-8') as f:\nf.write(f\"Keyword : {keyword}\n\")\nf.write(f\"Peringkat : {rank}\n\")\nf.write(f\"Judul : {title}\n\")\nf.write(f\"URL : {url}\n\n\")\nf.write(content)\ndef scrape_and_save_txt(keywords, num_results=5):\nfor keyword in keywords:\nprint(f\"\n\ud83d\udd0d Searching: {keyword}\")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nprint(f\" \u2192 ({i+1}) Fetching: {url}\")\ntitle, content = get_page_content(url)\nsave_to_txt(keyword, i+1, title, url, content)\ntime.sleep(2)\nexcept Exception as e:\nprint(f\"\u274c Error saat mencari '{keyword}': {e}\")\nprint(\"\n\u2705 Semua konten telah disimpan di folder 'outputs/'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\nscrape_and_save_txt(keywords, num_results=5)\nWhat is the purpose of the 'load_keywords' function in the script?\n{\n=To read search keywords from the 'keywords.txt' file and return them as a list.\n~To perform Google searches for each keyword.\n~To write the extracted content to text files.\n~To clean and preprocess the extracted content.\n~To validate the URLs obtained from the search results.\n~To remove duplicate keywords from the list.\n}",
    "jawaban": "The correct answer is: \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\""
  },
  "408787e5cd1a85c54b101ff30982c590951003b9433c46bc19bd949c5c4ffc21": {
    "soal": "The Tree widget can work for regression tasks. In the workflow below, we use the housing dataset and provide the dataset to the Tree widget. The selected Tree node in the Tree Viewer will be displayed in ................... and we can see that the selected example has the same feature.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Scatter Plot"
  },
  "b49ca438417b69cb42c09712900724228f77cef9c9b111b56be74c32e959fe3e": {
    "soal": "......................... (distance) is most commonly calculated between instances (\"rows\" in the Distances widget) or attributes (\"columns\" in the Distances widget). The only suitable input for the Distance Map widget is ............... widget. In the output, users can select regions on the map, and the widget will display the corresponding instances or attributes.\n",
    "jawaban": "The correct answer is: Distances"
  },
  "80cc6db7777be8bd23fda2f36fa510adaf799033cf5ebb8cbef109004d011254": {
    "soal": "The Moving Transform widget uses the Aggregation function to aggregate values in time series windows ...........(1)........... Available options include: mean (average), sum, max, min, median, mode, standard deviation, ................(2)................., product, linearly-weighted moving average, exponential moving average, harmonic mean, geometric mean, non-zero count, cumulative sum, and cumulative product\n",
    "jawaban": "The correct answer is: (2) \u2192 cumulative sum, (1) \u2192 time series, (2) \u2192 variance, (1) \u2192 Aggregation function"
  },
  "e7ca6104648c74c8f809aefe60db08ba0e189af28d54fc6da4c4ca7882c28c77": {
    "soal": "Sebagai contoh penggunaan kNN untuk .......................... kita mengggunakan dataset iris. Kita bandingkan hasil dari k-Nearest Neighbors dengan default model Constant, yang akan memprediksi class majoritas. Tampak kNN lebih baik.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: klasifikasi"
  },
  "348571517bfc1b77e1b82bd4ff5858a73b7f00fb089bdd05a48ec321a7c1ad40": {
    "soal": "The Box Plot widget shows the distribution of attribute values. It is good practice to examine any new data with this widget to quickly spot anomalies, such as .................... values (e.g., gray or grey), outliers, and the like.\n",
    "jawaban": "The correct answer is: duplicate"
  },
  "69c3130a41a724b3161287aba3088773814419fd591873a19b1a808182470058": {
    "soal": "import pm4py\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\n# Import XES log\nlog = xes_importer.apply(xes_filename)\n# Cek jumlah kasus dan aktivitas\nprint(f\"Jumlah kasus: \")\nactivities = set()\nfor trace in log:\nfor event in trace:\nactivities.add(event['concept:name'])\nprint(f\"Jumlah aktivitas unik: {len(activities)}\")\nfrom pm4py.algo.discovery.dfg import algorithm as dfg_discovery\nfrom pm4py.visualization.dfg import visualizer as dfg_visualization\n# DFG berdasarkan frekuensi\ndfg = dfg_discovery.apply(log,\nvariant=dfg_discovery.Variants.FREQUENCY)\n# Tampilkan visualisasi\ndfg_vis = dfg_visualization.apply(dfg, log=log,\nvariant=dfg_visualization.Variants.FREQUENCY)\ndfg_visualization.view(dfg_vis)\nTo install PM4Py correctly, which command should be executed?\n{\n=pip install pm4py graphviz pandas\n-pip install pm4py numpy scipy\n-pip install pm4py matplotlib sklearn\n-conda install pm4py graphviz pandas\n-pip install pm4py opencv scipy\n-conda install pm4py numpy pandas\n}",
    "jawaban": "The correct answer is: len(log)"
  },
  "e81df14376e5f28eba64c237ed89d63ca354a0794c036f5c4f7fb2aa7cb5f9cd": {
    "soal": "Widget Network Clustering tries to find clusters in a network. Network Clustering works with ................ algorithms, one from Raghavan et al. (2007), which uses label propagation to find the appropriate clusters, and one from Leung et al. (2009), which builds on the work of Raghavan and adds hop attenuation as a parameter for cluster formation.\n",
    "jawaban": "The correct answer is: two"
  },
  "3205a9a1c67ac9276744d9cf2c7a0f950f639a54a9b86612a3b97cac71bcd4af": {
    "soal": "Which PM4Py visualization method explicitly represents direct connections between activities?",
    "jawaban": "The correct answer is: Directly-Follows Graph (DFG)"
  },
  "3a7ad22dfc6837c86c861625b2f2105d29f66d356e9eb6ff760430791433f3ed": {
    "soal": "Below, we will use the Attrition - Train data from the Datasets widget. This data is about employee attrition. In other words, we want to know whether a certain employee will leave their job or not. We will create a prediction model with the Tree widget and observe the probabilities in Predictions.\n\nFor prediction, we need training data, which we have loaded in the first Datasets widget, and data to predict, which we will load in another Datasets widget. We will now use the Attrition - Predict data. Connect the second dataset to Scatter Plot. Now we can see the predictions for three data instances from the second dataset.\n\nThe Tree model predicts that no employees will leave the company. We can try another model and see if the predictions change. Or, test the predictive score first in the Test & Score widget.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "dcb73cff946e01077d59a9b965dea3a9a2abcb42aefeff1d4a2b8ed8c10b7339": {
    "soal": "Data Mining is not important for supporting the success of modern organizations driven by data. An IDG survey of 70 IT and business leaders found that 92% of respondents want to implement advanced analytics more widely across their organizations. The same survey found that the benefits of data mining are profound and vast.",
    "jawaban": "The correct answer is 'False'."
  },
  "197911ea12e039c2a77917aa1bc27345335e265f5bf4352786f1a6e9dfc076d8": {
    "soal": "The PCA widget in ORANGE3 provides two outputs: transformed data and principal component. Transformed Data represents the weights for individual instances in a new coordinate system, while component is the descriptor of the system (weights for the principal components). When inserted into the Data Table, we can see all three outputs as numbers. We use two data tables to provide a cleaner workflow visualization, but we can also choose to edit the links so that we display the data in just one data table. We only need to create two links and connect the input data and the transformed components to the output data.\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "bed3d4e1d9f1d7faef8822a0e923b22220c64939a41b769034fe5325b33d4b4b": {
    "soal": "What is a disadvantage of Heuristic Miner?",
    "jawaban": "The correct answer is: Complex and less understandable models"
  },
  "66d85f3c1d250586677511fad10f1f5b33202ac2d585c9f44165865c92084a72": {
    "soal": "import pandas as pd\nfrom pm4py.objects.conversion.log import converter as log_converter\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom pm4py.visualization.petrinet import visualizer as pn_visualizer\nfrom pm4py.objects.log.util import dataframe_utils\n# csv_filename='Insurance_claims_event_log.csv'\n# csv_filename='Business Travel Permits.csv'\n# csv_filename='Parent-Child Union Output.csv'\n# csv_filename='PM4PY_Dummy_Event_Log_100_Cases_pm4py.csv'\n# csv_filename='PM4Py_Dummy_Event_Log-congested-business-process.csv'\n# 1. Load CSV event log\ndf = pd.read_csv(csv_filename)\n# df = pd.read_csv(csv_filename,sep=';')\n# print(df.head())\n# 2. Format timestamps properly\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n# 3. Ensure column names match PM4Py expectations\ndf = dataframe_utils.convert_timestamp_columns_in_df(df)\n# 4. Convert to event log object\nevent_log = log_converter.apply(df, parameters=)\n# 5. Discover process model using Alpha Miner\nnet, initial_marking, final_marking = alpha_miner.apply(event_log)\n# 6. Visualize Petri Net\ngviz = pn_visualizer.apply(net, initial_marking, final_marking)\npn_visualizer.view(gviz)\nWhich column is NOT mandatory in the CSV input file for PM4Py?\n{\n~case_id\n~activity\n~timestamp\n=duration\n~None (all listed are mandatory)\n~priority\n}",
    "jawaban": "The correct answer is: log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case_id'"
  },
  "e3f3193f8aea18facbec84666437cdfc491029a2aa676c039e1f7e7adecebe27": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\n# xes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\nxes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nprocess_tree = pm4py.discover_bpmn_inductive(log)\nbpmn_model = pm4py.convert_to_bpmn(process_tree)\npm4py.view_bpmn(bpmn_model)\nWhat would you expect the function 'view_bpmn' to do?",
    "jawaban": "The correct answer is: Visualize the BPMN diagram"
  },
  "53d6d552f2eb91db7447dc6719af2dff130c91b2fdaae939b10011135d850d01": {
    "soal": "When there is no data in the input, the Corpus widget reads the text corpus from a file and sends the corpus instance to the output channel. The history of the last opened file is stored in the Corpus widget. In the Corpus widget, there is also a directory with sample corpora pre-installed with the add-on. The widget can read data from Excel (......................), comma-separated (.csv), and tab-delimited (.tab).\n",
    "jawaban": "The correct answer is: .xlsx"
  },
  "940e6c91bcd78f6ea15ace1e0870347cbc24176522e0726783a555f28fa8ac60": {
    "soal": "The visualization technique that can be used for both ordered and unordered data is ...................\n",
    "jawaban": "The correct answer is: box plot"
  },
  "5a5c8fb4a701c38a0cf6ab2c32576cd9b0d2393b37a84632932b3eed0a3f2662": {
    "soal": "The following ticker code used in the Yahoo Finance Stock Data Widget (ORANGE)\n\nGOOG\n\n\nBelongs to the company,\n\n\n",
    "jawaban": "The correct answer is: Alphabet Inc."
  },
  "181cd6ecbac54b05b57b47f48072862a5babbe9998c0772a6838b27787f0e0c5": {
    "soal": "model.save('my_model')\nfrom google.colab import files\nfiles.download('my_model')\nnew_model = tf.keras.models.load_model('my_model')\nWhat is the name of the saved model file in the example?",
    "jawaban": "The correct answer is: my_model"
  },
  "effb445f740aed8fa76652af9cd30a49d7a87edbc17225a9dc1554526dd83a33": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nWhich variables contain the testing images and labels after loading the MNIST dataset?",
    "jawaban": "The correct answer is: x_test and y_test"
  },
  "ae8bffd460cd8eadfbf07406111d273f0292a2b829a1fec7e779410fd8db4ddb": {
    "soal": "\n\n\nKesimpulan kesalahan klasifikasi (jumlah kesalahan) di workflow di atas bisa di baca di adalah\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: confusion matrix"
  },
  "0584dd0fa7b11a19c5c5227dbbaaf5a591f1230d274dc4afa8237a6e1b3559d9": {
    "soal": "Only two inputs are suitable for the Distance Matrix widget, which are the Distances widget and the Distance Transformation widget. The output from the Distance Matrix widget is a data table containing the distance matrix. Users can decide how to label the table and ................... (or examples in the distance matrix), which can then be visualized or displayed in a separate data table.\n",
    "jawaban": "The correct answer is: distance matrix"
  },
  "94cd3de2ba12e6b4d9a3dd10a7ebddbd7f080cfa322ed87e4285576bd69fc573": {
    "soal": "The Distances widget maximizes the distance between rows/columns in the dataset.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "78e77c2f819c2a2295a521f755e8fcb289032b2d32ba5bb04993eca7f2f9e3b8": {
    "soal": "Which method is used to minimize the loss function during training in TensorFlow?",
    "jawaban": "The correct answer is: Optimizer.minimize()"
  },
  "04cf5ca2daa3f0d55bec9d3d3443c39bfa5db4caeb25dd3b81c5396e502b87bf": {
    "soal": "What is the purpose of `!nvidia-smi` in Google Colab?",
    "jawaban": "The correct answer is: To show GPU details"
  },
  "ffd68b82b36f2fbd5eb9d32090866b0696ca12efbc8458db7333303369950df5": {
    "soal": "import pandas as pd\nfrom pm4py.objects.conversion.log import converter as log_converter\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom pm4py.visualization.petrinet import visualizer as pn_visualizer\nfrom pm4py.objects.log.util import dataframe_utils\n# csv_filename='Insurance_claims_event_log.csv'\n# csv_filename='Business Travel Permits.csv'\n# csv_filename='Parent-Child Union Output.csv'\n# csv_filename='PM4PY_Dummy_Event_Log_100_Cases_pm4py.csv'\n# csv_filename='PM4Py_Dummy_Event_Log-congested-business-process.csv'\n# 1. Load CSV event log\ndf = pd.read_csv(csv_filename)\n# df = pd.read_csv(csv_filename,sep=';')\n# print(df.head())\n# 2. Format timestamps properly\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n# 3. Ensure column names match PM4Py expectations\ndf = dataframe_utils.convert_timestamp_columns_in_df(df)\n# 4. Convert to event log object\nevent_log = log_converter.apply(df, parameters=)\n# 5. Discover process model using Alpha Miner\nnet, initial_marking, final_marking = alpha_miner.apply(event_log)\n# 6. Visualize Petri Net\ngviz = pn_visualizer.apply(net, initial_marking, final_marking)\npn_visualizer.view(gviz)\nWhich library is specifically used for process mining in Python?\n{\n=PM4Py\n~Pandas\n~Graphviz\n~Matplotlib\n~Seaborn\n~Numpy\n}",
    "jawaban": "The correct answer is: log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case_id'"
  },
  "76d470ded5d7fcda3a0559d320515720114cdd4f26adb75e9337b141f0189759": {
    "soal": "import pandas as pd\ndf = pd.read_excel('path_ke_file.xlsx', sheet_name=0)\ndf_selected = df[['Kolom1', 'Kolom2']]\nprint(df_selected.head())\nHow do you import the Pandas library in Python?",
    "jawaban": "The correct answer is: import pandas as pd"
  },
  "e43d151c07e0d3ca72ac161950173c828415958bb003033002c2104f6e18762c": {
    "soal": "The Save Data widget does not save data every time it receives a new signal in the input because this would continuously (and mostly, accidentally) overwrite the file. Instead, the data is saved only after the new file name is set or the user presses the ........................\n",
    "jawaban": "The correct answer is: Save"
  },
  "3f993b2321103efbac7bda62a70d9c395717d6937940212f33ca4c82f058f433": {
    "soal": "Some formats supported by ORANGE3 (based on its add-ons):\n\n.............................. : Distance File\n",
    "jawaban": "The correct answer is: distance matrix"
  },
  "349f72a071bed5efb70a022ef88a38b69d1de5f70342a98ddc07d22fafc9ec51": {
    "soal": "What is the purpose of the 'map_fn()' function in TensorFlow?",
    "jawaban": "The correct answer is: To apply a function to each element of a tensor"
  },
  "6f01aedc724d80b1743e78cd6e0e9259bc480637546563fc2a957f3ef29e8ff3": {
    "soal": "The Scatter Plot widget provides 2D scatter plot visualizations for continuous and discrete attributes. Data is displayed as a set of points, each having an attribute value on the x-axis that determines its position on the vertical axis and an attribute value on the y-axis that determines its position on the vertical axis. Various graphical properties, such as color, size, and shape of the points, axis titles, maximum point size, and jittering, can be adjusted on the left side of the widget.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "65eb9dd05704d5724de54ce1b81508372f8b6f0ca01efdce6efdadcc93bfe02f": {
    "soal": "import os\nimport string\nfrom collections import Counter\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\n# Buat daftar stopwords Bahasa Indonesia\nstop_words = set(stopwords.words('indonesian'))\ndef bersihkan_teks(teks):\n# Lowercase + hilangkan tanda baca\nteks = teks.lower()\nteks = teks.translate(str.maketrans('', '', string.punctuation))\nreturn teks\ndef analisis_folder(folder='outputs', top_n=20):\nsemua_konten = ''\nfor filename in os.listdir(folder):\nif filename.endswith('.txt'):\nfilepath = os.path.join(folder, filename)\nwith open(filepath, 'r', encoding='utf-8') as f:\nisi = f.read()\n# Ambil hanya bagian isi, setelah header\nif '\n\n' in isi:\nisi = isi.split('\n\n', 1)[1]\nsemua_konten += isi + ' '\n# Bersihkan teks dan tokenisasi\nteks_bersih = bersihkan_teks(semua_konten)\nkata_kata = teks_bersih.split()\n# Hapus stopwords\nkata_kata = [kata for kata in kata_kata if kata not in stop_words and kata.isalpha()]\n# Hitung frekuensi\ncounter = Counter(kata_kata)\ntop_kata = counter.most_common(top_n)\nprint(f\"\n\ud83d\udd0d Top Kata Paling Sering Muncul:\")\nfor kata, jumlah in top_kata:\nprint(f\"{kata}: {jumlah}\")\n# Buat Word Cloud\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(counter)\n# Tampilkan\nplt.figure(figsize=(12, 6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud dari Semua Konten')\nplt.tight_layout()\nplt.savefig('wordcloud.png')\nplt.show()\nprint(\"\n\u2705 Word Cloud disimpan sebagai 'wordcloud.png'\")\n# Jalankan analisis\nif __name__ == '__main__':\nanalisis_folder('outputs', top_n=30)\nWhat is the purpose of the 'WordCloud' class from the 'wordcloud' library in the script?\n{\n=To generate a visual representation of word frequency in the form of a word cloud.\n~To perform sentiment analysis on the text.\n~To extract keywords from the text.\n~To translate text from Indonesian to English.\n~To count the number of words in the text.\n~To remove stopwords from the text.\n}",
    "jawaban": "The correct answer is: top_n"
  },
  "41adc8f0e009258516ed99f3bc3a0f4f5e30815eda309e0027712827de703ef3": {
    "soal": "Big data is a term referring to large and complex datasets obtained from various sources, including business transactions, social data, census data, medical data, scientific data, and others. The main characteristics of big data are volume, velocity, and variety.\n- Volume refers to the very large amount of data generated constantly.\n- Velocity (Velocity) refers to the speed at which data is generated and updated, as well as the ability to process data in real-time.\n- Variety (\nAnswer Question 44\n in English) refers to the various sources of data and different types of data that can be collected.\nBig data typically requires specialized technology and techniques to process, analyze, and interpret it, including machine learning, data mining, and statistical analysis techniques. The goal of big data analysis is to gain valuable insights and information, such as trends and patterns useful for business, scientific, or social decision-making.",
    "jawaban": "The correct answer is: Variety"
  },
  "fd4989d82bd286e8a4429f6d00e11e5d0b9cc6fb14540b8c657b9545dbca8136": {
    "soal": "For machine learning, we need numbers. To obtain numerical representations of images, we send images to the Image Embedding widget.\n\nThe output data from the Image Embedding widget can be inspected using the ..............(1)................ widget. Now we have a numeric/numerical presentation of the images. For each image, there are 2048 numerical representations (columns n0 to n2047). With these feature numeric representations, we can apply all standard machine learning techniques, such as, ..............(2)..................\n\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 Data Table, (2) \u2192 clustering"
  },
  "6be49f00645fa08cf01a4673d3ad20b45acb259e5450c7e22abd07abe573a61e": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#_xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nmap = pm4py.discover_heuristics_net(log)\npm4py.view_heuristics_net(map)\nWhat does commenting out a line in Python script achieve?",
    "jawaban": "The correct answer is: It disables that line of code -It runs the code asynchronously -It improves performance -It deletes the code permanently -It executes code conditionally -It compiles the code faster"
  },
  "415a23e020a9e2319f35e076c45ad8f2408d79732080661a44afea1597a77137": {
    "soal": "1111 1111 (binary) + 1000 0000 (binary) =",
    "jawaban": "The correct answer is: 1 0111 1111"
  },
  "3717314f8443f151fef563dc85ec06b64d39083d0f87c4714052bb6361ae429d": {
    "soal": "The ...................... widget takes text files from a folder and creates a corpus. The Import Documents widget can read .txt, .docx, .odt, .pdf, and .xml. If there are subfolders in the folder, they can be used to label the class.\n",
    "jawaban": "The correct answer is: Import Documents"
  },
  "7598621639b32ee99f1e46317ec757a3d6c458d53b8f63b96a458a349b0dd2cb": {
    "soal": "Some formats supported by ORANGE3 (based on its add-ons):\n\n.............................. : Load Model\n",
    "jawaban": "The correct answer is: predictive model"
  },
  "6d2e56f6e66d4e2e4c7e2429a0a3e8890130dd3a8850d1b8a71db77feecfb08e": {
    "soal": "The Moving Transform widget uses the Aggregation function to aggregate values in time series windows. The available options are: mean, sum, max, min, median, mode, standard deviation, variance, product, linearly-weighted moving average, exponential moving average, harmonic mean, geometric mean, non-zero count, cumulative sum, and ............................................\n",
    "jawaban": "The correct answer is: cumulative product"
  },
  "1963ac8743c31a8b04d6241dfa6834ee24c95fe33335f86b7543218caaf0b041": {
    "soal": "\n\n\nExamples of traditional statistics applications include,\n\n\n\n",
    "jawaban": "The correct answers are: Basic Customer Data, Historical Stock Price Data"
  },
  "9005dc3c4db2455936e2ef8231ffcfb798bbecec78ee96262d21d6c14045abcc": {
    "soal": "The widget ............(1)............. uses the Tree algorithm with the ability to perform ...........(2).......... pruning (forward pruning).\n",
    "jawaban": "The correct answer is: (1) \u2192 Tree, (2) \u2192 forward"
  },
  "d0c4f5d08dc404e7e7e4d8e75a1b356bdd999e98e72d0074c310037d3dffd31a": {
    "soal": "import pandas as pd\nfrom pm4py.objects.conversion.log import converter as log_converter\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom pm4py.visualization.petrinet import visualizer as pn_visualizer\nfrom pm4py.objects.log.util import dataframe_utils\n# csv_filename='Insurance_claims_event_log.csv'\n# csv_filename='Business Travel Permits.csv'\n# csv_filename='Parent-Child Union Output.csv'\n# csv_filename='PM4PY_Dummy_Event_Log_100_Cases_pm4py.csv'\n# csv_filename='PM4Py_Dummy_Event_Log-congested-business-process.csv'\n# 1. Load CSV event log\ndf = pd.read_csv(csv_filename)\n# df = pd.read_csv(csv_filename,sep=';')\n# print(df.head())\n# 2. Format timestamps properly\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n# 3. Ensure column names match PM4Py expectations\ndf = dataframe_utils.convert_timestamp_columns_in_df(df)\n# 4. Convert to event log object\nevent_log = log_converter.apply(df, parameters=)\n# 5. Discover process model using Alpha Miner\nnet, initial_marking, final_marking = alpha_miner.apply(event_log)\n# 6. Visualize Petri Net\ngviz = pn_visualizer.apply(net, initial_marking, final_marking)\npn_visualizer.view(gviz)\nWhich parameter in PM4Py ensures the proper column is identified as the process identifier?\n{\n~EVENT_KEY\n~ACTIVITY_KEY\n=CASE_ID_KEY\n~TIME_KEY\n~PROCESS_KEY\n~LOG_ID\n}",
    "jawaban": "The correct answer is: log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case_id'"
  },
  "0cea9dd6b28331ddf5af461950bc67677824ab993e3c0112b54b8eadb3c5487e": {
    "soal": "If a dataset is used for a small research project or initial experiment, what is the recommended dataset size?",
    "jawaban": "The correct answer is: 500\u20133000 data points"
  },
  "2d80e9d67ca4f305033dd6fb08b24e03792322a9813da749d18dbb15983b7b55": {
    "soal": "from googlesearch import search\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef get_page_content(url):\ntry:\nheaders =\nresponse = requests.get(url, headers=headers, timeout=10)\nsoup = BeautifulSoup(response.content, 'html.parser')\n# Ambil judul halaman\ntitle = soup.title.string if soup.title else 'No Title'\n# Ambil konten paragraf utama\nparagraphs = soup.find_all('p')\ntext_content = ' '.join([p.get_text() for p in paragraphs[:5]]) # Batasi 5 paragraf pertama\nreturn title.strip(), text_content.strip()\nexcept Exception as e:\nreturn 'Error', f\"Failed to fetch content: {e}\"\ndef google_scrape_with_content(keywords, num_results=5, output_file='scraped_results.csv'):\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'Rank', 'Title', 'URL', 'Content'])\nfor keyword in keywords:\nprint(f\"\n\ud83d\udd0d Searching for: {keyword}\")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nprint(f\" \u2192 Fetching: {url}\")\ntitle, content = get_page_content(url)\nwriter.writerow([keyword, i+1, title, url, content])\ntime.sleep(2) # Delay biar aman\nexcept Exception as e:\nprint(f\"\u274c Error while searching '{keyword}': {e}\")\nprint(f\"\n\u2705 All results + content saved to '{output_file}'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\ngoogle_scrape_with_content(keywords, num_results=5)\nWhich exception handling construct is used in the script to manage errors during the search process?\n{\n=try-except\n~if-else\n~while-try\n~do-catch\n~begin-rescue\n~attempt-handle\n}",
    "jawaban": "The correct answer is: \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\""
  },
  "e816f557820a86665c2160f49930badd436e0699b02baca633fe38399d0991ca": {
    "soal": "In the snapshot below, we can see how transformation affects the distance matrix. We load the Iris dataset and calculate the distance between rows with the help of the Distances widget. To show how Distance Transformation affects the Distance Matrix, we create the workflow below and compare the transformed ..................... with the \"original\".\n\n\n\n\n",
    "jawaban": "The correct answer is: distance matrix"
  },
  "ae638f82ce7510a5a9fa84c90f48c97dd16b7e26847c820b0df564641fc3bf26": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#-xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\ndfg, start_activities, end_activities = pm4py.discover_dfg(log)\npm4py.view_dfg(dfg, start_activities, end_activities)\nThe '#' character in Python indicates:",
    "jawaban": "The correct answer is: A comment -An error -A directive -A function -A loop -A condition"
  },
  "18cb4afeaf96cabed76607ee3bbc4e0d718f89e03bf534f8c7b8c0b197b1fb0a": {
    "soal": "A heat map is a graphical method for visualizing attribute values based on class in a two-way matrix. Heat maps can only be used on datasets that contain discrete variables. The values are represented by colors: the higher the value, the darker the color. By combining class and attributes on the x and y axes, we see where which attribute values are strongest and weakest, allowing us to find typical features (discrete) or value ranges (continuous) for each class.\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "69f97094ffa1f26d37f09cc038c217ba700c22a16d2c14aca51aa757c690d5ec": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nHow many classes are there in the CIFAR-10 dataset?",
    "jawaban": "The correct answer is: 10"
  },
  "05f1ddc2a6f0a091615361deb87bb6a5396164ba80db8f97ede2c7804d3fdcaa": {
    "soal": "Which function in TensorFlow is used to initialize global variables?",
    "jawaban": "The correct answer is: global_variables_initializer()"
  },
  "265d7bc6d4380c1811af30f34f7c38890f0a39971dd63a1b71d94cccc6255b04": {
    "soal": "In the image below, we use the Network Generator Widget to create a Grid graph with a height of 4 and a width of 5 and send it to Network Analysis. We can calculate node degrees and send the data to Network Explorer. Finally, we observe the resulting graph in the box and set the node size and color to Degree. Network Explorer is a good tool for observing and explaining network properties.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "20b5c7ca77237a7012326559720e1aa63bca268621e6d89e4a3e729eb28e9cab": {
    "soal": "In the Scatter Plot Widget - Selection can be used to define a manually defined subgroup in the data. Use the Shift modifier when selecting data instances to place them into a new group. Shift + Ctrl (or Shift + Cmd in macOS) adds instances to the last group.\n\nSignal data outputs a data table with an additional column containing the group index.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "aac31cbeeef7ffef352c4f06fe2643a739d5b62dd9d435da1e5623ee48bac963": {
    "soal": "Instalasi Orange3 yang\u00a0 baik di Ubuntu 18.04 dapat menggunakan perintah\n",
    "jawaban": "The correct answer is: pip3 install orange3"
  },
  "a49d76f5f727535558bc596d9be546da531d88558b7daa1982b59482c77e31e6": {
    "soal": "What will happen if TPU is not found in Google Colab?",
    "jawaban": "The correct answer is: It prints \"No TPU found, using CPU/GPU.\""
  },
  "598dd9c3c61b533eeed81957bd20a44d6d85f853b3b7889bc5f1e705a56e0734": {
    "soal": "The Distance Matrix widget creates a distance matrix, which is a two-dimensional array containing distances, taken pairwise, between elements of the dataset. The number of elements in the dataset determines the size of the matrix. Data matrix is essential for the ........................................ Widget.\n\n\n",
    "jawaban": "The correct answer is: Hierarchical Clustering"
  },
  "6600fde97e81988c725036469c036fa70da1469e45eec67d78223193ba1aabf4": {
    "soal": "In the next example, we will try to predict the category of a document. We will use the dataset book-excerpts.tab, which we will send through the Preprocess Text widget with default parameters. Then we will connect the Preprocess Text widget to the Bag of Words widget to get term frequency. With the term frequency information, we will calculate the model.\n\nConnect the Bag of Words widget to the Test & Score widget for predictive modeling. Connect the SVM widget or another classifier widget to the Test & Score widget (all on the left side). The Test & Score widget will calculate the performance score for each learner in the input. Here we get excellent results with the SVM widget.\n\nNext, we need to check where the model made mistakes. Add the Confusion Matrix widget to the Test & Score widget. The Confusion Matrix widget will display documents that were correctly classified and misclassified. Select misclassified to output misclassified documents, which we will review further using the Corpus Viewer.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "40b245852f1d92351f288b63e05151fcb44277c50ee5c9d674af0147cafcdb4d": {
    "soal": "When the user provides data to the input, the Corpus widget will convert the data into a corpus. The user can select which feature to use as the ......................... feature.\n",
    "jawaban": "The correct answer is: text"
  },
  "390f58cd862140ddcddc2f6b378343d6a4fd7e3e6bdaf9f27fd8adbd7046ae8a": {
    "soal": "Which image is used for the Ollama service in the 'docker-compose.yaml' file?",
    "jawaban": "The correct answer is: ollama/ollama"
  },
  "ca9eaf4521c7017a528e3335a99fd6c07e0fe069600025ef668270ced7a43eff": {
    "soal": "What is the first step in process mining?",
    "jawaban": "The correct answer is: Collecting and preparing event data"
  },
  "ba7fed921d0c857362a97d6c44a547121d0ab65ed33f7787ded7152b82d8375f": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nx_train = np.random.rand(1000, 10)\ny_train = np.random.randint(2, size=(1000, 1))\nmodel = keras.Sequential([\nkeras.layers.Dense(64, activation='relu', input_shape=(10,)),\nkeras.layers.Dense(32, activation='relu'),\nkeras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer='adam',\nloss='binary_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=10, batch_size=32)\nWhich activation function is used in the last layer?",
    "jawaban": "The correct answer is: sigmoid"
  },
  "619e75e98e05e18c123b60a6a8aa1d5a1239c8dcb99d4572d8287eec821a11db": {
    "soal": "Which of the following is a technique for reducing overfitting?",
    "jawaban": "The correct answer is: Regularization"
  },
  "2937940966421194a4857b138c269b759745db21978578a78fd1c22f953f13ed": {
    "soal": "Some even call data science sexy, as stated by Hans Rosling, featured in a 2011 BBC documentary with the quote, \"Statistics is now the sexiest subject around.\" Nate Silver called data science a sexed-up version of statistics. In many ways, previous approaches have simply been rebranded as \"data science\" to make it more attractive, ultimately leading to the term becoming \"diluted beyond usefulness.\"",
    "jawaban": "The correct answer is 'True'."
  },
  "006271d146136825cb65a2de9529b1b53e22a768cb47032ec13ecdea4ce4cf5a": {
    "soal": "Most Time Series algorithms assume that we do not have missing values in our data. In this widget, we can select an interpolation method to estimate the missing values. By default, the Interpolate widget will use polynomial interpolation (fast and reasonable).\n",
    "jawaban": "The correct answer is 'False'."
  },
  "be0178162987aa405b47df528bd6068cec24ebce287962cb8584e2299b3150a5": {
    "soal": "\n\n\nIn preprocess text, we can do several things, such as\n\n\n\nConvert all letters to lowercase.\nRemove (stop words), words that are not useful, such as conjunctions like and, in, to, from, etc.\nRemove HTML tags\nRemove URLs\netc.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "2eeeca51f611e57659897aed3041a6ee31ac0374cce684784399fda7a1bc0dde": {
    "soal": "Which command allows you to remove multiple models at once?",
    "jawaban": "The correct answer is: ollama rm model1 model2"
  },
  "96fb15464d26079e5969135074fdd78a0940a487efc338f4c0ba338573d9685a": {
    "soal": "In this example, we will just check what a bag of words model looks like. Load book-excerpts.tab using the Corpus widget and connect it to the Bag of Words widget. Here we intentionally use the default parameters - the simplest count is term frequency. Check what the Bag of Words widget outputs using the Data Save widget. The last column represents the term frequency for each document.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "96425a80368462aebcb49981b5c62c5e1bdf4799e0064c611f4e4b7a63243133": {
    "soal": "PM4Py\u2019s Inductive Miner generates what type of model?",
    "jawaban": "The correct answer is: Petri net"
  },
  "5afb371b88008ad9d383a0532358ef0c9fbfd069a5ef5fe97732fdebb127a857": {
    "soal": "What does Recall measure in a classification model?",
    "jawaban": "The correct answer is: The proportion of actual positives correctly identified."
  },
  "c413f9619ce7c6c391814444883e01e41a5fa5da4758cc8584131607a8162cf1": {
    "soal": "Widget Constant akan memprediksi most frequent class\u00a0 atau mean .............. dari sebuah training set.\n",
    "jawaban": "The correct answer is: value"
  },
  "1698ad2a3b43987870dc2e15e220d99c2c1929235a29d3d15a6aee8cc9f36fb9": {
    "soal": "Which protocol is commonly used for web scraping?",
    "jawaban": "The correct answer is: HTTP (Hypertext Transfer Protocol)"
  },
  "adb9f6f763cf3b3ce66b573d4fb6ee27b8a862724f1695418705964ef43e1db1": {
    "soal": "Apa tujuan utama dari confusion matrix dalam evaluasi model klasifikasi?",
    "jawaban": "The correct answer is: Untuk membandingkan prediksi model dengan nilai aktual."
  },
  "6a54bab0c079420b08181024f98b3d1c0f9c8c57e7cac0e35319d504aa7c160e": {
    "soal": "In the workflow below, we compare the statistics of two Data Info widgets - one with information from the entire dataset and the other with information from a manually selected subset from the Scatter Plot widget. Here, we are using the Iris dataset.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "5d1e18e1fa9325ccc67ae46c1d707bc6f04ea7977ae87f2efd365d1f711931c9": {
    "soal": "Match the Ticker code used in the Yahoo Finance Widget (ORANGE) with the actual company name.\n",
    "jawaban": "The correct answer is: FB \u2192 Facebook, Inc., GOOG \u2192 Alphabet Inc., AAPL \u2192 Apple, AMZN \u2192 Amazon.com, ^TNX \u2192 Tanzanian Royalty Exploration Corporation"
  },
  "0d8f7d36d08570a000f0b8d81dcda44bacc1b5af82366c14e09e2f5fcffcc4e8": {
    "soal": "RapidMiner, formerly known as YALE (Yet Another Learning Environment), is open-source software. It serves as a solution for performing data mining, text mining, and predictive analysis.\nRapidMiner uses various descriptive and predictive techniques to provide insights to users, enabling them to make the best decisions. RapidMiner is written in Java, allowing it to work across all operating systems.",
    "jawaban": "The correct answer is 'True'."
  },
  "2b90c8b8be2b70fc627d24e0ff2c18385344263141467d84443ee0ba9bc04adb": {
    "soal": "The Distances Widget also works well with other Orange add-ons. The Distance Matrix Widget can be fed into the Network from Distances Widget (Network add-on) to turn the matrix into a graphics interface and into the Duplicate Detection Widget (Text add-on) to find document duplicates in the corpus.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "cbca6c354873d55f52a60a9005c908ae67d6aa8949cd43e4375c61cebf1f7fb7": {
    "soal": "What does 'Scalability' refer to in the context of Big Data technologies?",
    "jawaban": "The correct answer is: The ability to handle increasing amounts of work or data growth"
  },
  "fbc34cf216058f006a455d04d6f4a8d17d4c3370f537f00a2a1268c2f13e3124": {
    "soal": "Machine learning is a branch of computer science that focuses on developing algorithms and techniques to create computer systems that can learn from data. In machine learning, computers are trained to recognize patterns and rules in data so that they can make intelligent and accurate predictions or decisions based on that data.\nThe learning process in machine learning can be done with three main methods: supervised learning, unsupervised learning, and reinforcement learning.\n- Supervised learning involves using unlabeled or unannotated data to train the algorithm so that the computer can recognize patterns or relationships between data features and labels. Examples of supervised learning applications are image classification, stock price prediction, and spam email classification.\n- Unsupervised learning, on the other hand, involves using unlabeled data to identify hidden patterns or structures in the data. Examples of unsupervised learning applications are data clustering and dimensionality reduction.\n- Reinforcement learning involves computer systems learning through experience by taking actions and receiving feedback on whether the actions were right or wrong. The goal is to find the optimal decision or action that results in maximum benefit.\nMachine learning has various applications, including in facial recognition, image analysis, speech recognition, forecasting, anomaly detection, and more.",
    "jawaban": "The correct answer is 'False'."
  },
  "28c0f29b31519fb663d5db11e946311b780a07b354dd7114d740fe3674806b2e": {
    "soal": "\n\n\nData visualization with many variables (multivariate), same and with hierarchy can use ..............................\n\n\n\n",
    "jawaban": "The correct answers are: sankey diagram, venn diagram"
  },
  "1cc1325faeb4ce49ac6259aeb5f3b95bb32436af95de12284949357f786eaaa6": {
    "soal": "In the Correlogram widget, we will visualize the nocorrelation coefficient for the time series we select.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "1909295311a6fcb26e742ad95352da5cf1975e075ec6571ada48c1ba2da74b47": {
    "soal": "Which of the following is a solution to the vanishing gradient problem?",
    "jawaban": "The correct answer is: Using activation functions like ReLU."
  },
  "f3c5c9bb36d2947be489182a118dd987d28e0dd1f03ec0774f340e4dad8af9ca": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\n# xes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\nxes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nprocess_tree = pm4py.discover_bpmn_inductive(log)\nbpmn_model = pm4py.convert_to_bpmn(process_tree)\npm4py.view_bpmn(bpmn_model)\nWhat does BPMN stand for?",
    "jawaban": "The correct answer is: Business Process Model and Notation"
  },
  "8c03d87dc0e4960cfebb34611bc1c59c5fd1616b4579daea46f472c260e3542c": {
    "soal": "Perform the AND operation on the following IP address\n\n192.168.123.245 AND 255.255.0.0\n\nThe result is",
    "jawaban": "The correct answer is: 192.168.0.0"
  },
  "139bb5cddc0b12515ae5df0235747610143dd342eefc6ca500dbd945b57eb0d1": {
    "soal": "Which activation function is commonly used in hidden layers of neural networks?",
    "jawaban": "The correct answer is: ReLU (Rectified Linear Unit)"
  },
  "096a2a30994604c4f656bc27ac3b521d5258fdb5032dac5a975c27a154e1ad80": {
    "soal": "\n\n\nIn the process of building a machine learning model, model performance evaluation using regression is usually measured by R3 error, RMSE, and MSE.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "67c7955a42c59bd0a93417971a76e3a77c6ff39429fa43909b49b59ff70b364b": {
    "soal": "In the Correlogram widget, we will visualize the ..................... autocorrelation coefficient for the time series we selected.\n",
    "jawaban": "The correct answer is: coefficient"
  },
  "bcd412fde328dddcd82df8f88271ed1da41bbdc381642d5502050dc8b34b9ac8": {
    "soal": "In PM4Py, what do you visualize after process discovery?",
    "jawaban": "The correct answer is: Petri Net"
  },
  "25affb919f3d16678072f39a91f23da91d18e0eca67ddf3577101cea2f843c35": {
    "soal": "Which extension provides custom nodes for ComfyUI to process datasets automatically and sequentially, useful for iterative training or batch image/video creation workflows?",
    "jawaban": "The correct answer is: ComfyUI Dataset Helper & Batch Node"
  },
  "06648186a2b720a78178d1a8387128fa9f68ecdbe3eeabde437aa0804c457108": {
    "soal": "Major drawback of Petri Net visualization:",
    "jawaban": "The correct answer is: Complex and difficult for beginners"
  },
  "d23791cee6bc4e6bd5bf0d55f950fe49b27180fc63c60fdcde6af32de6124493": {
    "soal": "In the following workflow example, using the zoo dataset and creating a clustering workflow with Distances and k-Means Clustering. Now set the threshold for cluster selection (click on the ruler above). Connect the Box Plot to k-Means Clustering, check Order by relevance, and select Cluster as a subgroup. This will sort the attributes by how well they define the selected subgroup, in our case, a cluster.\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "107bb7b6ef20947e8ef34df428a9d934c4e759b2e5a2b2b240031f982d48f534": {
    "soal": "Cassandra, or Apache Cassandra, is one of the closed-source products for distributed database management by Apache. Cassandra is designed to manage large (big data) structured data spread across many servers. This software is highly scalable, so it is no surprise that many large companies have trusted Cassandra as one of their support tools, such as Facebook, Twitter, and Apple.",
    "jawaban": "The correct answer is 'False'."
  },
  "dbd23d2e0a5a1d22236647309ed163109c6f29e34c23f003cb04a7bf7c74dccc": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(2, size=100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi training (80%) dan testing (20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model, misalnya RandomForestClassifier\nmodel = RandomForestClassifier(random_state=42)\n# Melatih model pada data training\nmodel.fit(X_train, y_train)\n# Memprediksi data testing\ny_pred = model.predict(X_test)\n# Mengevaluasi akurasi model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model pada data testing: ')\nWhy is the `random_state=42` parameter used in the `train_test_split` function?\n{\n~To increase the randomness of data splitting.\n~To ensure the data is split differently each time.\n=To ensure reproducibility of the data split.\n~To limit the size of the dataset.\n~To set the seed for the RandomForestClassifier.\n~To specify the number of splits.\n}",
    "jawaban": "The correct answer is: accuracy:.2f"
  },
  "28f1d24b91a80371df6edaf1906a9777fef642d2381a1b9f128189cc67bc867d": {
    "soal": "The Distances widget calculates the distance between rows/columns in the dataset.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "493d990e159f6fc38a55625f936b85c91b9fffd8b644f5e7ed0b1b757cfe09fc": {
    "soal": "\n\n\n\nIn this example, we are using the lastfm dataset, which can be loaded into the Network File widget under Browse documentation networks. The network node represents musicians, marked by the genre they play, the number of albums produced, and so on. Edge is the number of listeners on LastFm.\n\nThe entire dataset is visualized in the Network Explorer Widget. In this widget, we remove the coloring and adjust the node size to match the number of albums. Then, we select several nodes from the network. We can observe the selection in the Network ................... Widget (1).\n",
    "jawaban": "The correct answer is: explorer"
  },
  "f2b27d2447e536846ef29f7f54a15b3fd33f39f161eacbe931c7f004792addeb": {
    "soal": "To avoid confusion, Orange supports dates and/or times formatted in one of the ISO 27001 formats. For example, the following values are all valid:\n\n2016\n2016-12-27\n2016-12-27 14:20:51\n16:20\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "39bc75bbbdd839dba0f5e6ba55dbe8dc0943d74b2ce7c7c58a028f93e4508471": {
    "soal": "\n\n\n\n\n\nDalam contoh di atas, kita menggunakan dataset zoo dan mengirimkannya ke CN2 Rule Induction. Kita bisa me-review dan meng-interpretasi model yang dibuat dengan CN2 Rule Viewer widget.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "82bbe7ad4d2d0c004ab4579f0bd0ea25ed97d6909a8d1f5deab4cd2c0037b25f": {
    "soal": "Contoh sederhana dengan Calibrated Learner. Kita menggunakan dataset titanic karena widget ini membutuhkan nilai binary class (dalam hal ini mereka adalah 'survived\u2019 atau \u2018not survived\u2019).\n\nKita menggunakan Logistic Regression sebagai base learner yang akan dikalibrasi dengan nilai setting default, yaitu dengan sigmoid optimization dari distribusi nilai dan di optimasi dengan CA.\n\nMembandingkan hasil dari uncalibrated Logistic Regression model kita akan melihat dengan jelas bahwa calibrated model lebih baik.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "646d6a1eb6238ab950ad083a1e6341c01d73a9912f0c3499f29026d3b2a8a11f": {
    "soal": "In the Text Preprocessing Widget in ORANGE, N-grams Range will create n-grams from tokens. The number specifies the range of n-grams. The default is one-gram and two-gram.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "107a500d554cd3ea5773df5dc0f1cbaf3f603df8c42b61c5cf8a664693a2a196": {
    "soal": "Which of the following is NOT considered one of the 3Vs of Big Data?",
    "jawaban": "The correct answer is: Validity"
  },
  "599e2916613d5f4f0d7d40a7928658c25a3bb0d223d43a362f90e1ac292ccfb9": {
    "soal": "Why might the output layer not have an activation function specified in a model?",
    "jawaban": "The correct answer is: The loss function used (e.g., SparseCategoricalCrossentropy) applies the softmax activation internally"
  },
  "e946033449c334ad3550d22aeb2fcb55b54ce77f389a239c3e25a90a998eb799": {
    "soal": "What is the main goal of data augmentation?",
    "jawaban": "The correct answer is: To increase data variability and help the model learn better."
  },
  "fc2d8d29c8977e9fbc102f0f5d06b751f3e8c79b8bbf0d49708d7f60efc1e61f": {
    "soal": "Choosing a process discovery method in PM4Py depends on:",
    "jawaban": "The correct answer is: Event log characteristics and analysis goals"
  },
  "d40b39b8e6753b57990b20ad126cab4e0b812417744e402a5c25e51f18e5a23b": {
    "soal": "The Save Data widget does not save data every time it receives a new signal in the .................... because this would continuously (and mostly, accidentally) overwrite the file. Instead, the data is saved only after the new file name is set or the user presses the Save button.\n",
    "jawaban": "The correct answer is: input"
  },
  "333232bc482cd1002cc01b4b789ec20b3cea348a85eddfe4fffa4aebdda72abd": {
    "soal": "In the workflow below, Iris data from the File widget is passed to the Select Columns widget, where we choose to display only two attributes (i.e., petal width and petal length). Then, we can view the original dataset and the dataset with the selected columns in the ...................\n\n\n",
    "jawaban": "The correct answer is: Data Table"
  },
  "f0b4d9eb478033516a6d68e9610440381ca2d8519ff323765d41a772e9b0361f": {
    "soal": "The ORANGE Constant Widget is typically used as a baseline for other model learners.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "8e69f0e884c79d43cf7e030396e0a5e07bcca2dea3300da8dd63fd8917ac657d": {
    "soal": "The Image Embedding widget in Orange is very easy to use. Technically, Orange will send the image to the server, where the server will push the image through a pre-trained deep neural network, such as Google Inception v3. Deep networks are often trained for a specific purpose. Inception v3, for example, can classify images into one of 1,000 image classes. We can ignore the suggested classification and instead consider the penultimate layer of the network with 2048 nodes (numbers) and use them for the image\u2019s vector-based representation.\n",
    "jawaban": "The correct answer is: deep neural network"
  },
  "60457450c9071640810f834b8bb947b2a14770bdde98750115dbcbf78d50e76f": {
    "soal": "One example of using the Tree widget is shown in the workflow below. Using the Tree widget, we can induce a model and check it using a view similar to the .................\n\n\n\n",
    "jawaban": "The correct answer is: Tree Viewer"
  },
  "b511ed7c04f30969ff5f36a84d4b1658b457ed8b1f2e188bcdecbe362db547ee": {
    "soal": "Column attribute names can be preceded by a label followed by a hash. Use c for class and m for meta attributes, i for ignoring columns, w for ............. , and C, D, T, S for continuous, discrete, time, and string attribute types. Example: C#mph, mS#name, i#dummy.\n\nWhere ........ is,\n",
    "jawaban": "The correct answer is: weight column"
  },
  "ddff1f2af1deccc924c561f2c7b50bfa510741981a3279e63d4a2e8c905180c3": {
    "soal": "Handling Sparse data in the Distances Widget can only be used with ...................., Manhattan, and Cosine metrics.\n",
    "jawaban": "The correct answer is: Euclidean"
  },
  "35ccbb9566b6aa2bde08d61e5451448db4489f335efb3654ef7f63c62f65f4ee": {
    "soal": "Widget Import Images scans a directory and returns one row per image retrieved. The columns contain the user name, directory path to the image, width, height, and image size. The column with the directory path to the image is then used as an attribute for visualization and Image Embedding.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "46a9d29f2ebb738f9756547035c3d74f7eafd38ae8dafa02e8f722b67787691e": {
    "soal": "If\n\n1 AND 1 = 1 and 1 OR 1 = 1\n\n1 AND 0 = 0 and 1 OR 0 = 1\n\n0 AND 1 = 0 and 0 OR 1 = 1\n\n0 AND 0 = 0 and 0 OR 0 = 0\n\nthen in binary\n\n( 001 AND 100 ) OR 111 = ..... (binary)",
    "jawaban": "The correct answer is: 111"
  },
  "58abc68ef3c550b6b59b5af882a204cd106d8fb71fe61dc227d06ae7d8c0f2a4": {
    "soal": "Attribute names in the column header can be preceded by a label followed by a hash. Use c for class, m for meta attribute, i for ignoring the column, w for weight (importance) of the column, and C, H, T, S for continuous, discrete, time, and string attribute types. Example: C#mph, mS#name, i#dummy.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "e0bc321e94c0b8d87c27e86fce0c241886d9c742f2bf1be3596a286d4fe40de5": {
    "soal": "\n\n\nThe image above shows the sequence of processes in data ......................, which are:\n\nASK - Ask, interesting questions.\nGET - Obtain data.\nEXPLORE - Explore data, whether there are any anomalies / interesting patterns, etc.\nMODEL - Build, fit, and validate the model.\nVISUALIZATION - Communicate and visualize the data.\n\n",
    "jawaban": "The correct answer is: science"
  },
  "528b34df3fb63f0ad3206c452e558e4a9300be304ba3a28077f2e290b75380e0": {
    "soal": "Principal Component Analysis (PCA) is typically performed on multivariable (multivariate) systems. In simple terms, PCA looks for the most dominant variable from the categories (classes) present in the data. The technique used is to perform a coordinate transformation of the data to obtain new coordinates (new variables), usually fewer than the number of coordinates/variables in the original data, to better represent the categories (classes) in the data.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "751c3d0360823cc9de0f13da5af759b393d6d8936a3908b08bb9fd893c20970a": {
    "soal": "In the next example, we will try to predict the category of documents. We use the dataset book-excerpts.tab, which we send through the ..........(1).............. widget with default parameters. Then we connect the Preprocess Text widget to the Bag of Words widget to obtain term frequency. With term frequency information, we will calculate the model.\n\nConnect the Bag of Words widget to the ...............(2).......... widget for predictive modeling. Connect the SVM widget or another classifier widget to the Test & Score widget (all on the left). The Test & Score widget will calculate the performance score for each learner input. Here, we obtain very good results for the SVM widget.\n\nNext, we need to check where the model is making mistakes. Add the Confusion Matrix widget to the Test & Score widget. The Confusion Matrix widget will display the correctly and incorrectly classified documents. Select \"misclassified\" to output the misclassified documents, which we will examine further using the Corpus Viewer widget.\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 Preprocess Text, (2) \u2192 Test & Score"
  },
  "bc9595e22df0e56800bfc6db883bd83a34cf75bacb1a216a1fffff58194d3b42": {
    "soal": "Which type of data is used to train LLMs?",
    "jawaban": "The correct answer is: Large text datasets"
  },
  "f3a2127b2179f66041da2e1a13cb724515fa3a6570302940ca9c7e4964fb06b6": {
    "soal": "How often does the LLM update the context window?",
    "jawaban": "The correct answer is: With every new prompt and output"
  },
  "277c595b25583df0c5f1db77c8de11015c1d09917451fcdff750f079970892ae": {
    "soal": "Word Cloud data can be built from text files (........(1)............) we have, as shown in the workflow below. First, data from the Text Files Widget must be segmented into words using the ...........(2).............. widget. Then, the output of the segmented data needs to be converted from segmented data to corpus so that it can be processed by the text mining toolbox using the ..........(3)............. widget. Before displaying it as a ..............(3)............... it is advisable to preprocess the data first, to reduce unnecessary words like conjunctions using the Preprocess Text widget.\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 ASCII, (2) \u2192 Segment, (3) \u2192 word cloud"
  },
  "48a866985ede7fc12f56c3c8145612803a2f418ca87e7804c47d200062f756c1": {
    "soal": "\n\n\n\n\n\nIn the example above, we use the zoo dataset and send it to CN2 Rule Induction. We can review and interpret the model created with the CN2 Rule Viewer widget.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "80b4809496b1a7275a5eab1db9b7a3ce6dd6d5ce9c8dd8ac2217c557a1f54924": {
    "soal": "\n\n\nWorkflow di atas melakukan test / evaluasi CN2 Rule Induction dan Tree di Widget Test & Score. Terlihat Tree lebih buruk.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "4d30ac90ceeae32b11e8ca4cfd2d53056954b9c74a7d82214cf543c2bb491d67": {
    "soal": "When aiming for high accuracy in a journal publication, what is the minimum dataset size you should consider?",
    "jawaban": "The correct answer is: More than 10,000 data points"
  },
  "15b86f4c989af406399c24f82fec77fdbadba22989092d77af3b5152226553c8": {
    "soal": "The Distances widget calculates the distance between rows or columns in the dataset. By default, the data is maximized to ensure equal treatment of individual features. Maximization is always done column-wise (using columns as the reference).\n",
    "jawaban": "The correct answer is 'False'."
  },
  "7eb75a58b4f0891029bdea8fe164927e9ed96203ba29902fcf2f2e5bf5ebce6c": {
    "soal": "The basic framework of Apache Hadoop consists of the following modules:\n- Hadoop Common - contains libraries and utilities required by other Hadoop modules;\n- Hadoop Distributed File System (HDFS) - a distributed file system that stores data on commodity machines, providing very high aggregate bandwidth in the cluster;\n- Hadoop YARN - a resource management platform responsible for managing computational resources in the cluster and using them for scheduling user applications; and\n- Hadoop MapReduce - a programming model for large-scale data processing.\nThe term \"Hadoop\" has come to refer not only to the basic modules above but also to the \"ecosystem,\" or collection of additional software packages that can be installed above or alongside Hadoop, such as Apache Pig, Apache Hive, Apache HBase, Apache Phoenix, Apache Spark, Apache Zookeeper, Impala, Apache Flume, Apache Sqoop, Apache Oozie, Apache Storm, and others.",
    "jawaban": "The correct answer is 'True'."
  },
  "1da1c97fe4740b9b9ab81adae0198d0691ee2b6ab2471703752e43fd85820ef7": {
    "soal": "import pandas as pd\nfrom pm4py.objects.conversion.log import converter as log_converter\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom pm4py.visualization.petrinet import visualizer as pn_visualizer\nfrom pm4py.objects.log.util import dataframe_utils\n# csv_filename='Insurance_claims_event_log.csv'\n# csv_filename='Business Travel Permits.csv'\n# csv_filename='Parent-Child Union Output.csv'\n# csv_filename='PM4PY_Dummy_Event_Log_100_Cases_pm4py.csv'\n# csv_filename='PM4Py_Dummy_Event_Log-congested-business-process.csv'\n# 1. Load CSV event log\ndf = pd.read_csv(csv_filename)\n# df = pd.read_csv(csv_filename,sep=';')\n# print(df.head())\n# 2. Format timestamps properly\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n# 3. Ensure column names match PM4Py expectations\ndf = dataframe_utils.convert_timestamp_columns_in_df(df)\n# 4. Convert to event log object\nevent_log = log_converter.apply(df, parameters=)\n# 5. Discover process model using Alpha Miner\nnet, initial_marking, final_marking = alpha_miner.apply(event_log)\n# 6. Visualize Petri Net\ngviz = pn_visualizer.apply(net, initial_marking, final_marking)\npn_visualizer.view(gviz)\nWhat process mining algorithm is used in the minimal example provided?\n{\n~Heuristic Miner\n~Inductive Miner\n=Alpha Miner\n~Genetic Miner\n~Fuzzy Miner\n~Social Miner\n}",
    "jawaban": "The correct answer is: log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case_id'"
  },
  "853d1658a4d72292281454b44ee1461a6e8687488c7974fe361428aa0942db37": {
    "soal": "In the Distributions widget, for continuous attributes, attribute values are displayed as a function graph. Class probabilities for continuous attributes are obtained using Linear kernel density estimation, while the curve display is adjusted with the Precision bar (smooth or precise). As an example, below we use the Iris dataset.\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "a38a10128a9c16abdf87a6fb76db9326c59cec44df545ad2492ef0141e1168ed": {
    "soal": "To cluster based on similarity, we can measure the ................... between images. The numerical representation of the images obtained from the Image Embedding widget can be measured using the Distances widget. Typically, cosine distance works best for images, but you can experiment yourself. We then send the distance matrix to Hierarchical Clustering to visualize similar pairs in the dendrogram.\n\nThe result is that all the animals are correctly grouped. In the dendrogram, we don't see the images of the animals. We can use the Image Viewer widget to view the images.\n\n\n\n",
    "jawaban": "The correct answer is: distance"
  },
  "57e4e146be29cd288b9afb5d67dfd3683fd08d058383f1136a50e0f5052b8dcd": {
    "soal": "The CSV File Import widget reads comma-separated files and sends the dataset to the output channel. The separator can be comma, semicolon, space, .......................... , or manually-defined delimiters. The history of the recently opened files is maintained in the widget.\n",
    "jawaban": "The correct answer is: tab"
  },
  "db680c8ffbd284ed576302ba2a1b9ee6d27aeecacbc2cb475ba6b9b9993a97d8": {
    "soal": "Data does not always come in the form of neat tables. Data can also be in the form of text, audio recordings, video content, or even images. However, computers can only work with numbers, so for any data mining, we need to transform that unstructured data into a ........... embedder. Orange has just started incorporating various embedders in Orange in the Image Embedding widget, and currently, they are available for text and images.\n",
    "jawaban": "The correct answer is: deep network"
  },
  "dbe32d11781b4c58b6e7980709e0206c8f14597164e0135f9e26ebe4d8f667b6": {
    "soal": "Which command sets an environment variable for the Ollama server on macOS?",
    "jawaban": "The correct answers are: launchctl setenv VAR_NAME value, value"
  },
  "c207eee57cf0bc3da3cf08967cc2be1d44ccd68fd9d265f349e40a1ff87881d5": {
    "soal": "Which dataset provides a collection of data for detecting, diagnosing, and addressing cyber threats using network traffic data, textual content, and more?",
    "jawaban": "The correct answer is: Cyber Threat Dataset on Kaggle"
  },
  "605eddaf698536cacce58e96a3b7d5a9383dcb1d447d27d242f8a7b104eeb586": {
    "soal": "111 (binary) OR 101 (binary) = ................. (binary)",
    "jawaban": "The correct answer is: 111"
  },
  "cba479d850799b9448dfc4fed9969d9ddd73392d9a1534a7860bdca587d4db9d": {
    "soal": "Widget Import Images is likely the first widget we will use in image analysis. Widget Import Images loads images and creates class values from the folder. In this example, we are using Widget Import Images to load 26 paintings by Monet or Manet.\n\n\nWe can observe the results in the Scattering Plot widget. It is clearly visible how Orange adds an additional class attribute with values Monet and Manet.\n\n\nNext, we can proceed to the standard machine learning method. We send the image to the Image Embedding widget, where we will use the Painters embedder to receive the image vector.\n\n\nThen we will use the Test & Score widget and the Logistic Regression widget to create a model to predict the painter of a painting (in this case, Monet or Manet). We get a very good score. How is that possible? It turns out these images are images that have already been trained with the Painters embedder, so the accuracy obtained will be high.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "96f47ce5a271f49de3d641fcafaad6787203d21bede4baca838fadce194cd45c": {
    "soal": "Just like R-programming, Apache Hadoop is open source. It is a tool framework created by Google and Apache. The Hadoop framework enables more data processing, stores heterogeneous data, and accelerates its processing.\nAccording to AWS, Hadoop is an open-source framework that is highly effective for storing very large datasets. In addition to storing, it is also capable of processing data ranging from gigabytes to petabytes efficiently.",
    "jawaban": "The correct answer is 'True'."
  },
  "2e27ad7999051d7dc0edf504365c0e20946c4f00d2f1f01cb5aaa1ccd70a0caa": {
    "soal": "In the Preprocess Text widget, we can do several things, such as:\n\nConvert all letters to lowercase.\nRemove stop words, words that are not useful, such as connecting words like \"and,\" \"in,\" \"to,\" \"from,\" etc.\nSet stopword processing in the Indonesian language.\nRemove audio embed.\n\nRemove URLs.\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "a000451de7e5f8ea34665286b1743426c0ef54d59c40f14802eb6ebf5f7ebea7": {
    "soal": "The Moving Transform widget can apply the rolling window function to the time series. Use the .................... widget to get the average value of the series.\n",
    "jawaban": "The correct answer is: Moving Transform"
  },
  "56f91eb528e785767bd52b223674e9a17832dbe32f8a97d5b28bf904aaf3c0ca": {
    "soal": "import os\nimport requests\nfrom googlesearch import search\nfrom bs4 import BeautifulSoup\nimport re\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef clean_filename(text):\n# Hilangkan karakter ilegal untuk nama file\nreturn re.sub(r'[\\/*?:\"<>|]', , text).strip().replace(' ', '_')[:50]<%1\u001f%\u001f0%> def get_page_content(url):\ntry:\nheaders =\nresponse = requests.get(url, headers=headers, timeout=10)\nsoup = BeautifulSoup(response.content, 'html.parser')\ntitle = soup.title.string if soup.title else 'No Title'\nparagraphs = soup.find_all('p')\ncontent = '\n\n'.join([p.get_text() for p in paragraphs[:5]])\nreturn title.strip(), content.strip()\nexcept Exception as e:\nreturn 'Error', f\"Gagal mengambil konten: {e}\"\ndef save_to_txt(keyword, rank, title, url, content, folder='outputs'):\nos.makedirs(folder, exist_ok=True)\nfilename = f\"{clean_filename(keyword)}_{rank}_{clean_filename(title)}.txt\"\nfilepath = os.path.join(folder, filename)\nwith open(filepath, 'w', encoding='utf-8') as f:\nf.write(f\"Keyword : {keyword}\n\")\nf.write(f\"Peringkat : {rank}\n\")\nf.write(f\"Judul : {title}\n\")\nf.write(f\"URL : {url}\n\n\")\nf.write(content)\ndef scrape_and_save_txt(keywords, num_results=5):\nfor keyword in keywords:\nprint(f\"\n\ud83d\udd0d Searching: {keyword}\")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nprint(f\" \u2192 ({i+1}) Fetching: {url}\")\ntitle, content = get_page_content(url)\nsave_to_txt(keyword, i+1, title, url, content)\ntime.sleep(2)\nexcept Exception as e:\nprint(f\"\u274c Error saat mencari '{keyword}': {e}\")\nprint(\"\n\u2705 Semua konten telah disimpan di folder 'outputs/'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\nscrape_and_save_txt(keywords, num_results=5)\nIn which directory does the script save the extracted content files?\n{\n=outputs/\n~results/\n~data/\n~saved_pages/\n~extracted/\n~content/\n}",
    "jawaban": "The correct answer is: \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\""
  },
  "1c72964fbd730fdea34c64c07e21886737c1b80c2794e434c91da6b9d9ffc2fb": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nWhat is the shape of the `x_train` array after loading the Fashion-MNIST dataset?",
    "jawaban": "The correct answer is: (60000, 28, 28)"
  },
  "505d75a8875c85d01b41ac4f2232d4597ed2058758f90e53c19a2d31fb23e996": {
    "soal": "\n\n\nThe most popular and widely used programming language to support data science is .......................\n\n\n\n",
    "jawaban": "The correct answer is: python"
  },
  "491ed8fdadc483c73d5d6e9d79b1abc70ae34634dc26d28ed9d08b5df58fc157": {
    "soal": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n# Contoh: Memuat dataset dari file CSV\n# Gantilah 'data.csv' dengan path ke dataset Anda\ndf = pd.read_csv('data.csv')\n# Memisahkan fitur (X) dan target \n# Asumsikan kolom terakhir adalah target\nX = df.iloc[:, :-1]\ny = df.iloc[:, -1]\n# Membagi dataset menjadi data latih dan data uji (80% : 20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model Decision Tree\nmodel = DecisionTreeClassifier()\n# Melatih model dengan data latih\nmodel.fit(X_train, y_train)\n# Melakukan prediksi pada data uji\ny_pred = model.predict(X_test)\n# Menghitung akurasi\nakurasi = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model: ')\n# Menampilkan laporan klasifikasi yang mencakup presisi, recall, dan F1-score\nprint(classification_report(y_test, y_pred))\nHow is the `DecisionTreeClassifier` model initialized in the code?\n{\n~By calling `DecisionTreeClassifier.fit()`.\n~By using `DecisionTreeClassifier.train()`.\n=By creating an instance with `DecisionTreeClassifier()`.\n~By importing it from `sklearn.metrics`.\n~By defining a new class for it.\n~By loading a pre-trained model.\n}",
    "jawaban": "The correct answer is: akurasi:.2f"
  },
  "fa99dce9626a58dbba33d2612353c6c0d7ed255766e937ef8da8ddee6aca3fea": {
    "soal": "Which environment variable sets the database name in the PostgreSQL service configuration?",
    "jawaban": "The correct answer is: POSTGRES_DB"
  },
  "239688e4230450a9102e747f70a4e8e52c918a9a37cf2f8c8c61791d43eb1acd": {
    "soal": "Which of the following is an example of manual data input?",
    "jawaban": "The correct answer is: Entering survey responses into a spreadsheet."
  },
  "6515d0f6d309e51a988ec89a7850c36fd259e0499e8233da4d9f146b1bd40d9d": {
    "soal": "011 (binary) x 100 (decimal) = ....... (decimal)",
    "jawaban": "The correct answer is: 300"
  },
  "ba4e719a24a3c95d75ecf69d4edcd776a92ba063228d4d4da853631e80ac0620": {
    "soal": "Why is data cleaning a critical step before training a model?",
    "jawaban": "The correct answer is: To ensure the data is accurate, consistent, and free of errors, leading to better model performance."
  },
  "edf407653c253e66d2efeffdc81194bf4fb898f717688de8ae2a2327a782f10c": {
    "soal": "Column attribute names can be preceded by a label followed by a hash. Use c for class and m for meta attributes, i for ignoring columns, w for weight (of a column), and C, D, T, S for continuous, discrete, time, and string attribute types. Example: C#mph, mS#name, i#dummy.\n\nWhere ........ is,\n",
    "jawaban": "The correct answer is: meta"
  },
  "e449e2828b97c716ab4a7a14ee507c44543591bdb8953afa47cdffe27008b979": {
    "soal": "What is the primary purpose of the provided Python script?",
    "jawaban": "The correct answer is: To scrape Google search results for multiple keywords and save them to a CSV file."
  },
  "1618c4ea83e758973f92642533480ee05f8863100a107f0ca4ff958636af2141": {
    "soal": "perform the AND operation on the following IP address\n\n202.11.13.245 AND 255.0.0.0\n\nthe result is\n\n\n",
    "jawaban": "The correct answer is: 202.0.0.0"
  },
  "48993ad15692cd0ac5dc915fa7bc0ad935ed583a3f4f341d4fbb91598b919472": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nHow many images are designated for training in the MNIST dataset?",
    "jawaban": "The correct answer is: 60,000"
  },
  "101c4703bc1004d226a90dd8b537bffd89dac615462697acb5d1eeaecdef2710": {
    "soal": "The SQL Table widget accesses data stored in an SQL database. The SQL widget can connect to .................. (requires the psycopg2 module) or SQL Server (requires the pymssql module).\n",
    "jawaban": "The correct answer is: PostgreSQL"
  },
  "ae21c79383128b70c85678e1e7d6d1086ff163b0440b0f1c66bbb54d88b55813": {
    "soal": "In TensorFlow, which function is used to compute the absolute value of a tensor?",
    "jawaban": "The correct answer is: abs()"
  },
  "eb43a53101a777e1b64eb3de41ce5488aa3a7f5b58e70dbd4c9144783ae9e1fd": {
    "soal": "What makes LLMs different from traditional rule-based systems?",
    "jawaban": "The correct answer is: They learn patterns from data"
  },
  "e419a98b754ad93eaad192184be74ebbb09efd4651b9dca0b4894fc3ee917ab0": {
    "soal": "\n\n\nData visualization with a single variable can use ..............................\n\n\n\n",
    "jawaban": "The correct answers are: histogram, density plot, box plot, line chart, area chart"
  },
  "c15763e26b1cb27e170ad3b7fb7360d4595965c2be70118aafc839647bfd496f": {
    "soal": "Widget Network Analysis calculates statistical summaries from the node-level and graph-level for the network. Widget Network Analysis will output the network with the computed statistics and an extended item data table (only ..................-level index).\n",
    "jawaban": "The correct answer is: node"
  },
  "81285aa984f2026688a426677b5bca1ede2cbdcbf1ace148c864c87c60c973b9": {
    "soal": "We can measure the distance between the embedded images and see which ones are most similar. We can use the Distances widget to measure the distance. Typically, cosine distance works best for images. We send the distance matrix to Hierarchical Clustering to visualize non-similar pairs in the dendrogram.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "03a2d2877829bfe4fdc3c6a1bb8cb620d0ccc03b7b6968003a40816db6d77af5": {
    "soal": "\n\n\n\n\n\nSupervised Learning uses unlabeled data.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "890a556c7d204ff399fff01d6090c648f59989c8f1337faff30f1e2dbed7ee73": {
    "soal": "\n\n\n\n\n\n\n\n\nThe image above shows the techniques/science/mindmap of machine learning in broad terms.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "7aca82cd22b4580a31ff93e15665006d6dcd2ef04e620fe481cce39be289b9b7": {
    "soal": "The Scatter Plot widget provides 3D scatter plot visualizations for continuous and discrete attributes. Data is displayed as a set of points, each having an attribute value on the x-axis that determines its position on the horizontal axis and an attribute value on the y-axis that determines its position on the vertical axis. Various graphical properties, such as color, size, and shape of the points, axis titles, maximum point size, and jittering, can be adjusted on the left side of the widget.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "49a84b4c348e2d621726215404cd4c772b39d8af993c6a5c6b9440b19321d3a1": {
    "soal": "In the next example, we will try to predict the category of documents. We use the book-excerpts.tab dataset, which we send through the Preprocess Text widget with default parameters. Then we connect the Preprocess Text widget to the Bag of Words widget to obtain term frequency. With the term frequency information, we will calculate the model.\n\nConnect the Bag of Words widget to the Test & Score widget for predictive modeling. Connect the SVM widget or another classifier widget to the Test & Score widget (all on the left). The Test & Score widget will compute performance scores for each learner input. Here, we get very good results with the SVM widget.\n\nNext, we need to check where the model made errors. Add the Confusion Matrix widget to the Test & Score widget. The Confusion Matrix widget will display the documents that were correctly classified and misclassified. Select \"misclassified\" to output the misclassified documents, which we will further examine using the Corpus Viewer.\n\n\n",
    "jawaban": "The correct answer is: Confusion Matrix"
  },
  "a6712039147846d6e0b5960902093d63f1639e53a82e74db56201dcad4664ac9": {
    "soal": "The ...........(1)............ algorithm is a simple algorithm that can split data into nodes based on ................(2)............ (class purity). The Tree algorithm is the predecessor of the Random Forest Algorithm. The Tree widget in Orange was designed in-house and can handle both discrete and continuous datasets.\n",
    "jawaban": "The correct answer is: (1) \u2192 Tree, (2) \u2192 class purity"
  },
  "e5073f2f411be68dbd414936e1c3ea32a01f7d740e3fcb11eacff09791b9ce40": {
    "soal": "In the Text Preprocessing Widget in ORANGE, N-grams Range will create x-grams from tokens. The number specifies the range of n-grams. The default is one-gram and two-gram.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "473c9dc7de822592523de58a854cfa83a02290540c33ea7d1ab0b715c445a9af": {
    "soal": "import pandas as pd\nfrom pm4py.objects.conversion.log import converter as log_converter\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom pm4py.visualization.petrinet import visualizer as pn_visualizer\nfrom pm4py.objects.log.util import dataframe_utils\n# csv_filename='Insurance_claims_event_log.csv'\n# csv_filename='Business Travel Permits.csv'\n# csv_filename='Parent-Child Union Output.csv'\n# csv_filename='PM4PY_Dummy_Event_Log_100_Cases_pm4py.csv'\n# csv_filename='PM4Py_Dummy_Event_Log-congested-business-process.csv'\n# 1. Load CSV event log\ndf = pd.read_csv(csv_filename)\n# df = pd.read_csv(csv_filename,sep=';')\n# print(df.head())\n# 2. Format timestamps properly\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n# 3. Ensure column names match PM4Py expectations\ndf = dataframe_utils.convert_timestamp_columns_in_df(df)\n# 4. Convert to event log object\nevent_log = log_converter.apply(df, parameters=)\n# 5. Discover process model using Alpha Miner\nnet, initial_marking, final_marking = alpha_miner.apply(event_log)\n# 6. Visualize Petri Net\ngviz = pn_visualizer.apply(net, initial_marking, final_marking)\npn_visualizer.view(gviz)\nHow are columns formatted properly in PM4Py?\n{\n=dataframe_utils.convert_timestamp_columns_in_df()\n~pm4py.format_columns()\n~df.set_format()\n~pd.convert_columns()\n~pm4py.adjust_df()\n~df.format_columns()\n}",
    "jawaban": "The correct answer is: log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case_id'"
  },
  "431c224aeb65535fd9ef5adf2cbd7d444b6be10ebd6e82ce87018ceed21bc650": {
    "soal": "In the workflow below, the use of the Moving Transform widget to obtain a 5-day difference is shown, we can use a rolling window with mean aggregation.\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "f30dab0b614dc8551dfa295b365262990301c395e3dff5a224dfff5cba3d3098": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#-xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\ndfg, start_activities, end_activities = pm4py.discover_dfg(log)\npm4py.view_dfg(dfg, start_activities, end_activities)\nWhich function from pm4py visualizes the discovered Directly-Follows Graph?",
    "jawaban": "The correct answer is: view_dfg() -discover_dfg() -read_xes() -import_xes() -open_xes() -create_dfg()"
  },
  "b4c6c0bd78ce7991cf6f9bacd38f9de6c3e12e121c42d4b2d2f03b28a43f7e95": {
    "soal": "In the Interpolate widget, which is part of the Time Series add-on, the interpolation type - Mean interpolation replaces missing values with the average values of the series.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "46b31d7139b1af374e9d93d5f107dd21f79dcbc9eb97b22bf382f1488f943771": {
    "soal": "CN2 Rule Induction can only be used for regression tasks.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "4590e49eeaf95534723abb9941e94529e3cd33d47b9f2806ab94cadd1d5649d7": {
    "soal": "Most Time Series algorithms assume that we do not have missing values in our data. In this widget, we can select the interpolation method to estimate the missing values. By default, the Interpolate widget will use .............. interpolation (quick and reasonable).\n",
    "jawaban": "The correct answer is: linear"
  },
  "af8e58f7a3c11b58072760d055ed0625426d825a5a7b9cb4c217558b4e21f2f9": {
    "soal": "How does cross-validation help in preventing overfitting?",
    "jawaban": "The correct answer is: By providing a reliable estimate of model performance on unseen data"
  },
  "1a267f6bdad4f87dc36f6d537d821eb9e3ae02649c572f50392ac48e63e7afe4": {
    "soal": "Widget Constant akan memprediksi most frequent class atau max value dari sebuah training set.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "36696ff8482e78697f56d9c161e758c13bffd94082d164638b0c46c9c515ffdd": {
    "soal": "Some formats supported by ORANGE3 (based on its add-ons):\n\n.............................. : Load Data from Single Cell add-on\n",
    "jawaban": "The correct answer is: single cell data"
  },
  "f886a9f1dbf3eca902e35369448d5747b0308e96eac1ee854f9b4054b6f7f0c7": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nx_train = np.random.rand(1000, 10)\ny_train = np.random.randint(2, size=(1000, 1))\nmodel = keras.Sequential([\nkeras.layers.Dense(64, activation='relu', input_shape=(10,)),\nkeras.layers.Dense(32, activation='relu'),\nkeras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer='adam',\nloss='binary_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=10, batch_size=32)\nHow many training examples are generated in the dataset?",
    "jawaban": "The correct answer is: 1000"
  },
  "1e72b0b535740d0d29ca82bf31b1922870fba003d057ba631b54e518f63e42ac": {
    "soal": "The Distance Transform widget transforms the distances present in the dataset.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "4d534324ccd984c8c2e03a1eb4e8ea44a926d84a4ec488b8a54dc361795d8c65": {
    "soal": "1111 1110 (binary) = ...... (decimal)",
    "jawaban": "The correct answer is: 254"
  },
  "4e5b7d9eb847a6c69cc1d2495d9545fa56627130334ae902fdaab6a8b6188d57": {
    "soal": "Widget Tree menggunakan algoritma Tree dengan kemampuan untuk melakukan .................... pruning (pemangkasan ke depan).\n",
    "jawaban": "The correct answer is: forward"
  },
  "a0eac2f2169d269c8b19810a736e784d2e03299ea71970220df85b01718f2b2e": {
    "soal": "In the Correspondence Analysis Widget - Correspondence Analysis (CA) computes the linear transformation of .............. data, not continuous data.\n",
    "jawaban": "The correct answer is: linear"
  },
  "537dedff954cc590077c4a995131d2a1ed28ffad436b2b61652a26064da745a8": {
    "soal": "What is a potential risk of using 'requests' and 'BeautifulSoup' to scrape Google's HTML pages directly?",
    "jawaban": "The correct answer is: Being blocked or detected as a bot by Google."
  },
  "badccb951183848caf1ac4e13fb441dfd2fcfe98f80019efda9009725b422711": {
    "soal": "The Pythagorean Tree widget plots the class probability against the probability predicted by the classifier.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "f43b47362a2aa01e98c923d3de8d328cb574121a1877ebc6bf86ef7943581a11": {
    "soal": "To perform machine learning, we need numbers. To obtain numerical representations of the images, we send the images to the Image Embedding widget.\n\nThe output data from the Image Embedding widget can be inspected using the Data Table widget. Now we have the numerical representation of the images. Each image has 2047 numerical representations (columns n0 to n2047). With this numerical feature representation, we can apply all standard machine learning techniques, such as clustering.\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "176df153f918fb723101896d20d19da3bc71a38902763a34e5fda8eb28a2a8ab": {
    "soal": "One use of the Tree widget is shown in the workflow below. By using the Tree widget, we can induce a model and check it using a view similar to the Confusion Matrix widget.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "781d25e3ef0c2bdc129745b69c3903924c269923b34c032accf7a7927b0d83fa": {
    "soal": "Widget\u00a0 ....................... akan meng-induce (menginduksi) rule dari data menggunakan algoritma CN2.\n\n\n\n",
    "jawaban": "The correct answer is: CN2 Rule Induction"
  },
  "db41bb6e821f47d5f48732f4d4294968ddbbc2133abc9d8f317702e717d0043c": {
    "soal": "Algoritma CN2 adalah teknik klasifikasi yang di rancang agar secara effisien melakukan .............. dari rules yang simple & komprehesif dalam bentuk \u201cif cond then predict class\u201d, meskipun dalam domain dimana mungkin ada noise.\n",
    "jawaban": "The correct answer is: induksi"
  },
  "88e6d2648c2b37d42f0240be5c472b27b8307b370f4973a7d20f81b15e4cb16a": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=10000)\nIn the IMDB dataset, how is a positive sentiment labeled?",
    "jawaban": "The correct answer is: 1"
  },
  "9a1ef1485ca185ea76135030a925e1afae09fbe01bcbd6410285617cc319c8a7": {
    "soal": "In the workflow below, the Iris data from the File widget is passed into the Select Columns widget, where we choose to display only two attributes (petal width and petal length). Then, we can view the original dataset and the dataset with the selected columns in the Data Table widget.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "11ac54f4a0f2ed355e093937c7ad7575f8de80c9073c26e2b593111d5e70baaa": {
    "soal": "What is the purpose of the 'reshape()' function in TensorFlow?",
    "jawaban": "The correct answer is: To change the shape of a tensor without altering its data"
  },
  "9094872ebf7caac72ae496811fe2f842209a0546613ca4bab8c717f4aeb8c642": {
    "soal": "Column attribute names can be preceded by a label followed by a hash. Use c for class and ...... for meta attributes, i for ignoring columns, w for weight (of a column), and C, D, T, S for continuous, discrete, time, and string attribute types. Example: C#mph, mS#name, i#dummy.\n\nWhere ........ is,\n",
    "jawaban": "The correct answer is: m"
  },
  "e33a91620cc2411b0deb3f0225a7c7db9894d9b3e02d1fd967ee86c64362c774": {
    "soal": "Another alternative to the hash notation format is Orange's native data format with three (3) header rows: the first with attribute names, the second specifying the type (continuous, discrete, time, image, or string), and the third row providing information about the attribute roles (class, meta, weight, or ignore).\n",
    "jawaban": "The correct answer is 'False'."
  },
  "03b795cd0a4595620e9122d72d77f73a685946cbbcfbe8e7e072b95b16fb8074": {
    "soal": "\n\n\nWorkflow di atas melakukan test / evaluasi CN2 Rule Inductin dan Tree di Widget Test & Score.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "fa7b7c7860be740895dd49a2dc63654cca14867fdd3029c93e342c451bc3ecf6": {
    "soal": "100 (binary) = ...... decimal",
    "jawaban": "The correct answer is: 4"
  },
  "66d67fd08d505eac49c01e74a06379ffe68d1792d768cb33a5ad38a965fa7cca": {
    "soal": "Widget Data Table can display attribute-value data in a spreadsheet.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "65c5acfccce274d0da0a6d941f2dd90153e522481715381ddaf2442952fe29fa": {
    "soal": "Widget Data Info is a simple widget that presents information about dataset size, feature, target, meta attributes, and comment.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "b879ba85cac25bfe86c2e591166d714e66a85a24a50cc642c28ef4fc44b041c8": {
    "soal": "The Preprocess Text widget in ORANGE applies preprocessing steps in sequence. This means it will first transform the text, then apply tokenization, POS tagging, normalization, filtering, and finally build n-grams from the given tokens. This is crucial for the WordNet Lemmatizer because it requires POS tags for proper normalization.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "51fdc7540eeb32f54f1c3e5c59a4de32ec706ba7545d83c5e06d49c4fa55e7e0": {
    "soal": "Intalasi Orange3 untuk data mining di Ubuntu 18.03 bisa menggunakan perintah\n",
    "jawaban": "The correct answer is: pip3 install orange3"
  },
  "298c5ec9c5937c8ba7be1649e22564f25159fc8cc66d15b15165bdacc138daf3": {
    "soal": "import pm4py\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\n# Import XES log\nlog = xes_importer.apply(xes_filename)\n# Cek jumlah kasus dan aktivitas\nprint(f\"Jumlah kasus: \")\nactivities = set()\nfor trace in log:\nfor event in trace:\nactivities.add(event['concept:name'])\nprint(f\"Jumlah aktivitas unik: {len(activities)}\")\nfrom pm4py.algo.discovery.dfg import algorithm as dfg_discovery\nfrom pm4py.visualization.dfg import visualizer as dfg_visualization\n# DFG berdasarkan frekuensi\ndfg = dfg_discovery.apply(log,\nvariant=dfg_discovery.Variants.FREQUENCY)\n# Tampilkan visualisasi\ndfg_vis = dfg_visualization.apply(dfg, log=log,\nvariant=dfg_visualization.Variants.FREQUENCY)\ndfg_visualization.view(dfg_vis)\nWhat does a Directly-Follows Graph (DFG) display?\n{\n=Relationships and frequency between activities\n-Only event log metadata\n-List of participants\n-Individual user actions\n-Error rates in logs\n-Duration statistics only\n}",
    "jawaban": "The correct answer is: len(log)"
  },
  "6f5f1096f877ef50dca820e46abaa623a37ad6110efd8539a0168592243db6be": {
    "soal": "Network Clustering Widget can detect clusters in a network.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "6e9a7ddc23b180d13fbf74c5ee34cca33ffd978f49652968a2217a8f93ba4984": {
    "soal": "The Distances widget calculates the distance between rows or columns in the dataset. By default, the data will be ................... to ensure equal treatment of individual features. ......................... is always done column-wise (using columns as reference).\n",
    "jawaban": "The correct answer is: normalization"
  },
  "57b9673a6a3d4299e9dcc8b03c52c226706d6e34bb63023002db829fe0e59024": {
    "soal": "import pm4py\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\n# Import XES log\nlog = xes_importer.apply(xes_filename)\n# Cek jumlah kasus dan aktivitas\nprint(f\"Jumlah kasus: \")\nactivities = set()\nfor trace in log:\nfor event in trace:\nactivities.add(event['concept:name'])\nprint(f\"Jumlah aktivitas unik: {len(activities)}\")\nfrom pm4py.algo.discovery.dfg import algorithm as dfg_discovery\nfrom pm4py.visualization.dfg import visualizer as dfg_visualization\n# DFG berdasarkan frekuensi\ndfg = dfg_discovery.apply(log,\nvariant=dfg_discovery.Variants.FREQUENCY)\n# Tampilkan visualisasi\ndfg_vis = dfg_visualization.apply(dfg, log=log,\nvariant=dfg_visualization.Variants.FREQUENCY)\ndfg_visualization.view(dfg_vis)\nWhat Python package must be installed alongside PM4Py to visualize graphs?\n{\n=graphviz\n-matplotlib\n-numpy\n-pandas\n-sklearn\n-scipy\n}",
    "jawaban": "The correct answer is: len(log)"
  },
  "4b2ec53341c984933bd98e9de7f0046ac02088ba7abc29695fb52d82a34a6a00": {
    "soal": "What is the role of neural networks in LLMs?",
    "jawaban": "The correct answer is: They process and model language patterns"
  },
  "7e2985e1a76f4a187c227dc041e98e724ca6f3d219cc5cc1f53186d5cee457d3": {
    "soal": "R-programming is a programming language used in big data processing. The nature of this programming language is \nAnswer Question 10\n source, meaning it can be used for free and modified by anyone. Its open-source nature allows many active users to contribute to the development of R-programming.\nSome advantages of R-programming include:\n- R programming can integrate with other programming languages, such as SQL.\n- It is used for data cleansing and manipulation, spatial analysis, data analysis and model building, data visualization, and even text analysis with natural language processing.\n- It has many functions and packages that make it easier for data practitioners.",
    "jawaban": "The correct answer is: open"
  },
  "3afc7d4b2c295d3e36e6286602430e4f9253fbe50c2e4bc60d87ee5f2736c432": {
    "soal": "\n\n\n\n\n\nIn this example, we are using the lastfm dataset, which can be loaded into the Network File widget under Browse documentation networks. The network node represents musicians, marked by the genre they play, the number of albums produced, and so on. Edge is the number of listeners on LastFm.\n\nThe entire dataset is visualized in Network Explorer. In this widget, we add coloring/data and adjust the node size to match the number of albums. Then, we select several nodes from the network. We can observe the selection in the Network Explorer Widget (1).\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "73dce2130a5b2bc303a39f4d175f0583eff5eac6f6d6e1e61e52a0acd930077d": {
    "soal": "DFG is primarily used for quickly understanding:",
    "jawaban": "The correct answer is: Simple process flows"
  },
  "4803d305e3faf9c42ad3199a4a83154bdd96fca5c56e39ade668e2f19d2d2f63": {
    "soal": "In the following example, we will see how to properly use Preprocess with the Predictions widget or the Test & Score widget.\n\nThis time we are using the heart disease.tab data from the File widget. We can access the data through the dropdown menu. This dataset consists of 303 patients who visited a doctor with chest pain. After the test was conducted, some patients were found to have narrowed arteries while others did not (this is our class variable).\n\nThe heart disease data has some missing values, and we want to handle that. First, we will split the dataset into train and test data using the Data Sampler.\n\nNext, we will send the Data Sample to Preprocess. We will use the Impute widget to handle the missing values, but we can try combinations of preprocessors on our data. We will send the processed data to the Logistic Regression widget and the model built to the Predictions widget.\n\nFinally, the Predictions widget also needs data to predict. We will use the output from the Data Sampler widget for prediction, but this time not the Data Sample, but the Remaining Data (remaining data), which is the data that was not used to train the model.\n\nNotice how we send the remaining data directly to the Predictions widget without applying any preprocessing. This is because Orange handles preprocessing on new data internally to avoid errors in model construction. The same preprocessor used on the training data will be used for prediction. The same process applies to the Test & Score widget.\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "9bafaa143c42894129a1555a4d96a8d36af76adec0ad36945d50eaa2c796aac9": {
    "soal": "from googlesearch import search\nimport csv\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef google_scrape_bulk(keywords, num_results=10, output_file='results.csv'):\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'Rank', 'URL'])\nfor keyword in keywords:\nprint(f\"\ud83d\udd0d Searching for: \")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nwriter.writerow([keyword, i+1, url])\ntime.sleep(1) # Delay agar tidak dianggap bot\nexcept Exception as e:\nprint(f\"\u274c Error while searching '{keyword}': {e}\")\nprint(f\"\n\u2705 All results saved to '{output_file}'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\ngoogle_scrape_bulk(keywords, num_results=10)\nWhich file does the script read to obtain the list of search keywords?\n{\n=keywords.txt\n~input.csv\n~search_terms.json\n~queries.xlsx\n~terms_list.txt\n~search_keywords.dat\n}",
    "jawaban": "The correct answer is: keyword"
  },
  "88c2a474ccaa4c77360cb966be086195531f5eb536d0fdc54e1d95657147a745": {
    "soal": "Which command installs Docker Compose on a Linux system?",
    "jawaban": "The correct answer is: curl -L \"https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose"
  },
  "2df60c3704f7d93ec1977acb06dcb5d3183570f91b5e41f6b6a03c98aa981b60": {
    "soal": "Contoh penggunaan Widget kNN untuk task ............................ di perlihatkan pada workflow di bawah ini. Workflow ini menunjukkan cara menggunakan Learner output. Untuk tujuan contoh ini, kita menggunakan dataset housing. Kita memasukkan model prediksi kNN ke dalam Predictions dan mengamati nilai yang diprediksi.\n\n\n\n",
    "jawaban": "The correct answer is: regresi"
  },
  "7f491b2ffa19fc5bc258bbd129a8529ec5b3535ca2480f5232beab7dee539535": {
    "soal": "In the Interpolate widget, which is part of the Time Series add-on, the interpolation type - Spline interpolation fits with a ................. polynomial to the values around the missing value. Therefore, this technique will be very slow but will give the best results.\n",
    "jawaban": "The correct answer is: cubic"
  },
  "90982cadf94998cbdee4016500c149b8369bac4d2f2b6e6002c66e3864a4f33d": {
    "soal": ".............(1)............. data can be built from text files (ASCII) we have, as shown in the workflow below. First, the data from the ..............(2)............ widget must be segmented into words using the Segment widget. Then, the output of the segmented data needs to be converted from segmented data to a corpus so that it can be processed by the text mining toolbox using the ..........(3)............. widget. Before displaying it as a word cloud, it is best to preprocess the data first, to reduce unnecessary words such as conjunctions using the Preprocess Text widget.\n\n\n",
    "jawaban": "The correct answer is: (3) \u2192 Interchange, (2) \u2192 Text Files, (1) \u2192 Word Cloud"
  },
  "216e21d1432c158db3232fc14679cc3803d19eb77e23313d4bd109aba858c69c": {
    "soal": "In the Constant Widget, the Learner will produce a model that always predicts the majority for classification or the .......... value for regression.\n",
    "jawaban": "The correct answer is: mean"
  },
  "efa55438af8d380b5085b22b29760b8f0284678e6fe6ff32c4d84b5b5c3d0e39": {
    "soal": "How many free requests per month does SerpApi offer?",
    "jawaban": "The correct answer is: 100"
  },
  "e10d59f7fd4d77d9b6af5b7b67be68ae8f2da75761ce32032b8f67daf5de296b": {
    "soal": "Thoughts & concepts supporting data science began to develop since the year .....................\n\n\n\n",
    "jawaban": "The correct answer is: 1943"
  },
  "bbe3012edc4b6e5f2679454eb2885518c7ca02bb5d8b4f3e395e5ab1d0049a6c": {
    "soal": "\n\n\nIn the example above, we use the zoo dataset and send it to the CN2 .................. . We can review and interpret the model created with the CN2 Rule Viewer widget.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Rule Induction"
  },
  "0917440cc2b26724019fce22788472cabaa380867fbb34a6c0383448306c93a3": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.rand(100) # 100 nilai target\n# Inisialisasi model regresi linier\nmodel = LinearRegression()\n# Tentukan jumlah fold (misalnya, 5-fold cross-validation)\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n# Evaluasi model menggunakan cross-validation\nscores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n# Menampilkan skor R^2 untuk setiap fold dan rata-ratanya\nprint(\"Skor R^2 untuk setiap fold:\", scores)\nprint(\"Rata-rata skor R^2:\", np.mean(scores))\nWhich module from `sklearn` contains the `LinearRegression` class?",
    "jawaban": "The correct answer is: sklearn.linear_model"
  },
  "0fc21a36536fe04e7d742667ca6157f3de1a8a48907062d4a2c1b77b47352ca1": {
    "soal": "The Predictions widget accepts a dataset and one or more predictors (predictive models, not learner algorithms). The Predictions widget generates data and predictions.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "3f3b1cb8b6670f85b6514b93b571dac36644c211a14f14d51e572d3c79c22b36": {
    "soal": "What is the command to install Docker on Ubuntu 24.04?",
    "jawaban": "The correct answer is: sudo apt install -y docker.io"
  },
  "4abf4ad3d5bf49b9d9219e95a26550054bed849c7c3478fe150b784b6c1f09c2": {
    "soal": "The CN2 algorithm is a classification technique designed to efficiently induce simple and comprehensive rules in the form of \u201cif cond then predict class,\u201d even in domains where there may be data.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "3fdb3ea8785b7ddf59b03313cf6bc9fe7a9692e9c6b1da842647188aa8a96d62": {
    "soal": "The ................... widget does not save data every time it receives a new signal in the input because this would continuously (and mostly, accidentally) overwrite the file. Instead, the data is saved only after the new ................... is set or the user presses the Save button.\n",
    "jawaban": "The correct answer is: Save Data"
  },
  "b54b6e7b1daa18bb5fbae47572ad172894c19cde1d5304d142d3fec98f92c21b": {
    "soal": "The ................................... widget does not save data every time it receives a new signal in the input, as this would continuously (and, mostly, accidentally) overwrite the file. Instead, data is saved only after the file name is set or the user presses the Save button.\n",
    "jawaban": "The correct answer is: Save Data"
  },
  "20a5d3ae16a692edd40a7813c70f9e9d2f380044b7c5f714b6dbc40b2e930025": {
    "soal": "Only two inputs are suitable for the Distance Matrix Widget: the Distances widget and the Distance Transformation widget. The output from the Distance Matrix widget is a data table containing the distance matrix. The user can decide how to label the table and the distance matrix (or examples in the distance matrix) and then visualize or display them in a built-in data table.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "a41bd901455a514add8e26900588c70122971ea65ae5750f9acd3ff874d21b6b": {
    "soal": "Contoh sederhana dengan Calibrated Learner. Kita menggunakan dataset titanic karena widget ini membutuhkan nilai binary class (dalam hal ini mereka adalah 'survived\u2019 atau \u2018not survived\u2019).\n\nKita menggunakan Logistic Regression sebagai base learner yang akan dikalibrasi dengan nilai setting default, yaitu dengan sigmoid optimization dari distribusi nilai dan di optimasi dengan CA.\n\nMembandingkan hasil dari uncalibrated Logistic Regression model kita akan melihat dengan jelas bahwa calibrated model lebih ..................\u00a0\n\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: baik"
  },
  "393b806496780c4f36c55c2104d6b4d4c033596c13a18fc2a6f1242da73361b2": {
    "soal": "Why is it important to split the dataset into Training, Validation, and Testing sets?",
    "jawaban": "The correct answer is: To ensure objective and unbiased model evaluation"
  },
  "c0efa3564a33f66f274b5ea9d63495400a187000471c2b39e041d4fa8aec2fe2": {
    "soal": "\n\n\nIn the example above, we use the zoo dataset and send it to the CN2 Rule Induction. We can review and interpret the model created with the CN2 ..................... widget.\n\n\n\n",
    "jawaban": "The correct answer is: Rule Viewer"
  },
  "985a4e05ae9bcdb9d4e83f9162bc9b328037d2822e5ea9afab552f82c2cbe33c": {
    "soal": "Widget Network Clustering can help us uncover clusters and ....................... in a network. First, we will use the Widget Network File to load the lastfm.net dataset. Then we will send the network to the Widget Network Clustering. The Network Clustering widget finds 79 clusters in the network. To visualize the results, we use the Widget Network Explorer and set the Color attribute to Cluster. This will color the network nodes with the appropriate cluster colors - this is a good way to visualize highly connected groups in a dense network.\n\nKeep in mind that the Widget Network Explorer here will color the 10 largest clusters and color the rest as 'Other'.\n\n\n\n",
    "jawaban": "The correct answer is: highly connected group"
  },
  "f7700860b5977c28895a1a30270b0dc0495923b63246af9c59cba92321b30e0d": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nx_train = np.random.rand(1000, 10)\ny_train = np.random.randint(2, size=(1000, 1))\nmodel = keras.Sequential([\nkeras.layers.Dense(64, activation='relu', input_shape=(10,)),\nkeras.layers.Dense(32, activation='relu'),\nkeras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer='adam',\nloss='binary_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=10, batch_size=32)\nWhat function is used to train the model?",
    "jawaban": "The correct answer is: model.fit()"
  },
  "42246d00c74862743689655358b3dbe533d2f3d1cc46cc282032dbf28bdc2b1e": {
    "soal": "The .................. widget can plot a Venn diagram for two or more data subsets.\n",
    "jawaban": "The correct answer is: Venn Diagram"
  },
  "81076433e8e3c035b1e8943d0de82705b52c2795e14a0cfe3c877a3aa916d29f": {
    "soal": "Network Generator Widget can create an example power plant network.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "20bd1d2994f2e90d18b8eb9d8908d09efb41c6581bc60a4016c659b242a6a271": {
    "soal": "1111 1111 (binary) + 0000 0001 (binary) =",
    "jawaban": "The correct answer is: 1 0000 0000"
  },
  "d30e8a6502ce7dc116e6e9000b7613ea5da4506cfef4cc09eecb2d8ef3949bcc": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\n# xes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\nxes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nprocess_tree = pm4py.discover_bpmn_inductive(log)\nbpmn_model = pm4py.convert_to_bpmn(process_tree)\npm4py.view_bpmn(bpmn_model)\nWhat type of process model is initially discovered from the event log?",
    "jawaban": "The correct answer is: BPMN (inductive)"
  },
  "a49bbce7e11773759f356ad0da1b868155b62415aefba8728fb9dee7dca61536": {
    "soal": "import tensorflow as tf\nfrom tensorflow.keras import layers, models\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nmodel = models.Sequential([\nlayers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(64, (3,3), activation='relu'),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(128, (3,3), activation='relu'),\nlayers.Flatten(),\nlayers.Dense(128, activation='relu'),\nlayers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5, batch_size=64)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nWhich activation function is used in convolutional layers?",
    "jawaban": "The correct answer is: ReLU"
  },
  "2e16cf3eac7393f1b0c7e7f53b6167c3903b9ede386371b2bad256fb0bcf8c9e": {
    "soal": "Word Cloud data can be created from text files (ASCII) that we have, as seen in the workflow below. First, the data from the Text Files widget must be segmented into words using the Segment widget. Then, the segmented data output needs to be converted from segmented data into a corpus so it can be processed by the text mining toolbox using the Interchange widget. Before displaying it as a word cloud, it is better to preprocess it first, to reduce unnecessary words like connecting words, etc., using the Preprocess Text widget.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "4108cae2a3f47d7f20447195027981905d73e06517900329535450dee516cd7a": {
    "soal": "import tensorflow as tf\nfrom tensorflow.keras import layers, models\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nmodel = models.Sequential([\nlayers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(64, (3,3), activation='relu'),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(128, (3,3), activation='relu'),\nlayers.Flatten(),\nlayers.Dense(128, activation='relu'),\nlayers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5, batch_size=64)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nWhat does CNN stand for?",
    "jawaban": "The correct answer is: Convolutional Neural Network"
  },
  "f7654a616dc873515d106f3ac4484acac9776d15f641cb3cb392be8c1b0521c5": {
    "soal": "Widget Network Clustering tries to find .................. in a network. Network Clustering works with two algorithms, one from Raghavan et al. (2007), which uses label propagation to find the appropriate clusters, and one from Leung et al. (2009), which builds on the work of Raghavan and adds hop attenuation as a parameter for cluster formation.\n",
    "jawaban": "The correct answer is: cluster"
  },
  "744537872556137594eb860e2210d74bd899c8634f2eb05680968a4dd91f07bc": {
    "soal": "If\n\n1 AND 1 = 1\n\n1 AND 0 = 0\n\n0 AND 1 = 0\n\n0 AND 0 = 0\n\nThen 000 AND 110 = ......... (binary)",
    "jawaban": "The correct answer is: 000"
  },
  "99e1438a03717a506a315a5e6eb675cde47179dce9a59e2338edfc0e1a835820": {
    "soal": "Which command runs the PostgreSQL container without using Docker Compose?",
    "jawaban": "The correct answers are: docker run --name openwebui-postgres -e POSTGRES_USER, openwebui -e POSTGRES_PASSWORD, your_password -e POSTGRES_DB, openwebui_db -p 5432:5432 -d postgres:latest"
  },
  "d341de25168b82ec667f8f729c065fd9d13e7aadab9d9257374da25f303c8fbf": {
    "soal": "The .................... widget can be used to visualize data profiles (e.g., Time series).\n",
    "jawaban": "The correct answer is: Line Plot"
  },
  "f8876b1f48737d19e6cd72ea91475a26ab1fd6cc22c2e76f461ea75bdbb33d24": {
    "soal": "What is the primary data structure used in TensorFlow to represent data?",
    "jawaban": "The correct answer is: Tensor"
  },
  "1fb099e30c29ecc98a1f9518bbb9d548f15763c706faf6ebf6edb7a394088eb4": {
    "soal": "To extract numbers from unstructured data (such as text and images), Orange cannot use a deep network embedder.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "52cf07abd22dfe14e5d4004c3913ecb6ede2dc6dcd89ec46749a354c5f2fc019": {
    "soal": "ProM's official website is used for:",
    "jawaban": "The correct answer is: Downloading and installation"
  },
  "5414a04ae48749bdcf016e2ebe21adbfd6f16ede690b1f1f9f9f09f49f983f3d": {
    "soal": "According to the official Apache website, Apache Spark is a framework used to analyze big data. Data processing through the Apache Spark framework is considered faster than other frameworks like MapReduce because the data is processed with right-memory. The growth of data at the terabyte level produced daily requires solutions that provide real-time analysis with high speed, one of which is using Apache Spark.\nThe advantages of Apache Spark include:\n- Faster performance compared to traditional data processing frameworks.\n- Easy to use, data processing applications built with Spark can be written in programming languages like Python, R, Java, and Scala.\n- Equipped with SQL Library, Streaming, and Graph Analysis, which makes it easier for data processing and analysis.",
    "jawaban": "The correct answer is 'False'."
  },
  "236b3fd6e772c324cb268314c453a9693b92785ef2b587f6b91635b4f1ed4f94": {
    "soal": "Which of the following is a key advantage of LLMs?",
    "jawaban": "The correct answer is: Flexibility across multiple applications"
  },
  "160b99ccc2b3f6a4bc49cc3b3933b4762c39ab6973770c5581ac2940f1e45837": {
    "soal": "In the Interpolate widget, which is part of the Time Series add-on, the interpolation type - Nearest interpolation ................. missing values with previously defined values.\n",
    "jawaban": "The correct answer is: replaces"
  },
  "de1ffa1d00023a6d7bcae2e75eaa295e701ebe41b7e97064ad465ce11824438f": {
    "soal": "The Confusion Matrix widget provides the count/proportion of instances between predicted and actual class. Selecting elements within the matrix will provide the corresponding instances to the output. In this way, we can observe which specific instances were misclassified and how.\n",
    "jawaban": "The correct answer is: classification"
  },
  "975af09341b7637ebe659b85063a0d185afd5895fa0912b18fec0b77f5b02afc": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\n# xes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\nxes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nprocess_tree = pm4py.discover_bpmn_inductive(log)\nbpmn_model = pm4py.convert_to_bpmn(process_tree)\npm4py.view_bpmn(bpmn_model)\nWhich comment line represents a file from the BPI Challenge?",
    "jawaban": "The correct answer is: BPI_Challenge_2019.xes"
  },
  "e54d80d9a785e7d94349f899ed2bb208b1b1cd58857bf6856d3402cba44fb3d7": {
    "soal": "model.save('my_model')\nfrom google.colab import files\nfiles.download('my_model')\nnew_model = tf.keras.models.load_model('my_model')\nWhich file would you use with tf.keras.models.load_model()?",
    "jawaban": "The correct answer is: my_model"
  },
  "c3d9978e4b730cc2371bfd83cefea9277b654b4882b10d11265bb870132f4cae": {
    "soal": "Widget CN2 Rule Induction akan meng-induce (menginduksi) rule dari data menggunakan algoritma ................... .\n\n\n\n",
    "jawaban": "The correct answer is: CN2"
  },
  "1b1292a51b27a87cc5b5bc2f22538b04f7aac333ef793da92b4185dbd2c60d7d": {
    "soal": "001 (binary) = ...... decimal",
    "jawaban": "The correct answer is: 1"
  },
  "59163cd1fb9478206ec66027267af3bb905610cb510e75ee3f0d93b3d080d56d": {
    "soal": "Currently, the only widget that provides the correct signal needed by the Calibration Plot widget is the Test & Score widget. Therefore, the Calibration Plot widget always follows the Test & Score widget, and since it has no output, no other widget follows it.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "d83a73bea2119125a9332c1f7750b8ab0c1a427a6b1ff1f2d0c7382f0e0e377a": {
    "soal": "The Tree algorithm is a simple algorithm that can separate data into nodes based on class purity (class/category purity). The Tree algorithm is the predecessor of the Logistic Regression algorithm. The Tree widget in Orange is designed in-house and can handle discrete and continuous datasets.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "2a459817c7d00ac5999c1b07c11d38c6e361ce9475fd9d0bb53a977845c12503": {
    "soal": "import pandas as pd\nfile_path = \"/content/drive/My Drive/NamaFolder/NamaFile.xlsx\"\ndf = pd.read_excel(file_path)\nprint(df.head())\nWhat does the 'print(df.head())' statement do?",
    "jawaban": "The correct answer is: Prints the first 5 rows of the DataFrame."
  },
  "0774851de388708d81b171493a91bdee623f4f99d9ff2133ad69f7e6752387c5": {
    "soal": "Why is handling missing values important in data analysis?",
    "jawaban": "The correct answer is: To ensure the quality and accuracy of results."
  },
  "0e2c24bd20fdec52737f1c0dbdf1b7bf8ed7cc1e0f0ad5bc77e19edee7bc9a18": {
    "soal": "from google.colab import drive\ndrive.mount('/content/drive')\nWhat should you do if your Colab session times out after mounting Google Drive?",
    "jawaban": "The correct answer is: Only remounting is necessary."
  },
  "378978a3aa380361299afb2e3a2a5fbfbde2455a703121599450b7a1f8d1214a": {
    "soal": "Network Explorer Widget is the main widget for visualizing the network graphically. It displays the graph using Fruchterman-Reingold layout optimization and allows for the adjustment of node color, size, and labels. One can also highlight nodes based on certain properties and output them.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "3e89e23386b9ff5220cdce657384b68734fabb9d1e4dbab612e9bbb517057776": {
    "soal": "In the Interpolate widget, which is part of the Time Series add-on, the interpolation type - Spline interpolation fits with a cubic .............................. to the values around the missing value. Therefore, this technique will be very slow but will give the best results.\n",
    "jawaban": "The correct answer is: polynomial"
  },
  "ad6df8731e1f2f6db0805753f9ac8e7758726b038e094d746c965e4f0ac88b1a": {
    "soal": "Which visualization method uses mathematical modeling to represent workflow processes, including conditions and transitions?",
    "jawaban": "The correct answer is: Petri Net"
  },
  "fe7ac821c5911e5e14711832845a5fe5abf262c0d42821ff736c490dcf42e470": {
    "soal": "A Pivot Map can help us collect and transform data. This workflow takes Kickstarter projects and aggregates them by month. We can check the frequency of projects published per month and observe the differences between funded and non-funded projects.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "828a73c6564beeadd077ed6c71b6013906849c8fdb01495fd1f30865dfb102ba": {
    "soal": "The ......................... widget can import text documents from a folder to become a corpus.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Import Documents"
  },
  "85dbd2411ffad63f5f9b723b37034fed71982b1e61ebd0d83643d9e6a8e79a62": {
    "soal": "The result of 8 (decimal) AND 5 (decimal) is",
    "jawaban": "The correct answer is: 0"
  },
  "7449b7dc341fb056f28383506175d7b7051c9e885a742f7690bb179bb45dff5a": {
    "soal": "111 (binary) + 110 (binary) = ?? (binary)",
    "jawaban": "The correct answer is: 1101"
  },
  "e6ccd8c559cdfd02fcf8d915796d9739de734dfb411d0233ff746e18e97f964d": {
    "soal": "The Moving Transform widget uses the Aggregation function to aggregate values in time series windows. The available options are: mean, sum, max, min, median, mode, standard deviation, variance, product, linearly-weighted moving average, exponential moving average, harmonic mean, geometric mean, non-zero count, cumulative sum, and cumulative product.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "81a3103fa715a914bd4bbc89e14be7edc94cd5a82dc667e1b84f2c741db1e7c4": {
    "soal": "Data Science is an interdisciplinary field that uses methods, processes, algorithms, and scientific systems to extract knowledge and insights from ............................ in various forms, both structured and unstructured, similar to data mining.\n",
    "jawaban": "The correct answer is: data"
  },
  "b7af4ae484277d42246dcabbb9b71d900811e655584723dd5716062c7fda49e8": {
    "soal": "The Moving Transform widget uses the Aggregation function to aggregate values in time series windows. The available options are: mean, sum, max, min, median, mode, standard deviation, variance, product, linearly-weighted moving average, exponential .................... average, harmonic mean, geometric mean, non-zero count, cumulative sum, and cumulative product\n",
    "jawaban": "The correct answer is: moving"
  },
  "b4d6fef13fd08eb69406e4ee76c4bafa31f874f636e7826ff691e2690d95528a": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.rand(100) # 100 nilai target\n# Inisialisasi model regresi linier\nmodel = LinearRegression()\n# Tentukan jumlah fold (misalnya, 5-fold cross-validation)\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n# Evaluasi model menggunakan cross-validation\nscores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n# Menampilkan skor R^2 untuk setiap fold dan rata-ratanya\nprint(\"Skor R^2 untuk setiap fold:\", scores)\nprint(\"Rata-rata skor R^2:\", np.mean(scores))\nWhat does the `n_splits=5` parameter specify in the `KFold` function?",
    "jawaban": "The correct answer is: That the dataset is divided into 5 folds for cross-validation."
  },
  "25e2d64daf4a76b063bdd6fe19f2620d0486f20c9c8a8b3f5b52d6d6bfe7266d": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#-xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\ndfg, start_activities, end_activities = pm4py.discover_dfg(log)\npm4py.view_dfg(dfg, start_activities, end_activities)\nWhat type of visualization does the 'view_dfg' function generate?",
    "jawaban": "The correct answer is: Process flow visualization -Bar chart -Histogram -Pie chart -Scatter plot -Line graph"
  },
  "29dc8883b20794798633d40e1a588cc9f2d9a0bb6f58c626d3b3018d9c7abff9": {
    "soal": "The ............(1)......... widget is a standard visualization widget, displaying the data profile, usually in the form of ordered numeric data. In this simple example, we will display the iris data in a line plot, grouped by ..............(2).......... iris. The plot shows how petal length separates the class values well.\n\nIf we observe this in the Scatter Plot Widget, we can confirm this is indeed true. Petal length is an interesting attribute for class separation, especially when enhanced with petal width, which is also well separated in the line plot.\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 Line Plot, (2) \u2192 attribute"
  },
  "c44a81caf59c54322355130dfe3976dd233864a5eb1bc25416d1201213af0f5e": {
    "soal": "For an initial experiment analyzing TikTok comments, what is the maximum recommended number of comments?",
    "jawaban": "The correct answer is: 3,000 comments"
  },
  "d947265f82105b49fd3b77c16356ba668631163c9b395bd3996e822acd353ae8": {
    "soal": "110 (binary) x 100 (decimal) = ....... (decimal)",
    "jawaban": "The correct answer is: 600"
  },
  "7fb90647568bf0e0378e2ceb0f40bac872e3939e4ec0ac83f3b87dd1ce6a43d9": {
    "soal": "The Tree widget can work for ................. tasks. In the workflow below, we use the housing dataset and give the dataset to the Tree widget. The selected Tree node in the Tree Viewer will be displayed in the Scatter Plot, and we can see that the selected example has the same feature.\n\n\n",
    "jawaban": "The correct answer is: Tree"
  },
  "cd9a918f5cf46a7f45685b7c4e6cdf8aa0b49e3e8307bab02b79b943c02625ff": {
    "soal": "Example data selected in the first Data Table widget is deleted to the second Data Table widget. Note that we can choose which dataset to view (iris or glass). Changing from one dataset to another changes the selection of data instances communicated if Commit on any change is selected.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "49374ea70bd095c56cdec7771c9b2a739777eb6405f21d736d71b73a438596bd": {
    "soal": "Where does TensorFlow come from?",
    "jawaban": "The correct answer is: It is an open-source library"
  },
  "c154e65a70169c1a0ebbe016c298cfcbc58335bf59ce1d4c5af298c15072fb4f": {
    "soal": "Line plot, a type of plot that displays data as a series of points, connected by line segments .............(1)...........\n\n\n.............(2)..............is used for numeric data. Meanwhile, for categorical data, Line Plot can be used for grouping data points ..............(3).................\n",
    "jawaban": "The correct answer is: (2) \u2192 Line Plot, (3) \u2192 data, (1) \u2192 straight line"
  },
  "bb17b7d4763da3631fc25b4a9984b1c0f0d139d9f71d7471579b80266540e76c": {
    "soal": "Which of the following is a core component of TensorFlow's architecture?",
    "jawaban": "The correct answer is: Data Flow Graph"
  },
  "6e3ccad942026c55f287b931d79f5aafe6935dfc1e4eabf06680fa3bc01689b4": {
    "soal": "Why is it important to have a structured dataset?",
    "jawaban": "The correct answer is: To facilitate efficient analysis and processing."
  },
  "60d8879d2447c5c849db23339aa40ecf03f827513f77a1538e5cadf7d54129d6": {
    "soal": "What is the main difference between batch gradient descent and stochastic gradient descent?",
    "jawaban": "The correct answer is: Batch gradient descent uses the entire dataset to compute the gradient, while stochastic gradient descent uses a single data point"
  },
  "c7343bca707f09a10561af01182255686b3e4ae3508efdcd0ca65161e45b4c1c": {
    "soal": "The kNN widget uses the kNN algorithm to find the k nearest training instances in the feature space and uses the average of those nearest features to predict.\n",
    "jawaban": "The correct answer is: kNN"
  },
  "c2b4ad0e2d373873f5d9dcd32ffe4669cb54fe1f7d4d7dcfd31e5242e8f59d05": {
    "soal": "The Calibrated Learner widget generates a model that calibrates the distribution of class probabilities and maximizes the decision threshold.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "3a5ae41883b56bb0a52dcdf48c9272bef8b6fba1078478fb82adb0f8bd37bd90": {
    "soal": "!pip install googlesearch-python nltk matplotlib wordcloud\nfrom googlesearch import search\nimport csv\ndef google_scrape(keyword, num_results=40,\noutput_file='results.csv'):\nprint(f\"Searching Google for: \")\n# Lakukan pencarian dan ubah hasil ke dalam list\nresults = list(search(keyword, num_results=num_results))\n# Simpan ke file CSV\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'URL'])\nfor url in results:\nwriter.writerow([keyword, url])\nprint(f\"Saved {len(results)} results to '{output_file}'\")\n# Contoh penggunaan\nkeyword = \"berita terbaru teknologi Indonesia\"\ngoogle_scrape(keyword, num_results=40)\nBesides CSV, which other file format is mentioned for saving the search results?\n{\n=TXT\n~JSON\n~XML\n~HTML\n~XLSX\n~SQL\n}",
    "jawaban": "The correct answer is: keyword"
  },
  "141854ecfec34509aa760d4568a98952752042fb9c2a1b7db444be566c7c1711": {
    "soal": "The core of Apache Hadoop consists of the storage part (Hadoop \nAnswer Question 92\n File System (HDFS)) and the processing part (MapReduce). Hadoop splits files into large blocks and distributes them across nodes in the cluster. To process the data, Hadoop MapReduce transfers code to the nodes to process in parallel, based on the data that needs to be processed at each node. This approach takes advantage of data-node locality, manipulating the data they hold in hand to enable faster and more efficient processing than more conventional supercomputer architectures relying on parallel file systems where computation and data are linked through high-speed networks.",
    "jawaban": "The correct answer is: Distributed"
  },
  "331e65e87b95bf56095ed5a5cef9ca45c15c6c45f15fd51ba8fe8f96d66824e7": {
    "soal": "The Tree widget can work for regression tasks. In the workflow below, we use the housing dataset and provide the dataset to the Tree widget. The selected Tree node in the Tree Viewer will be displayed in the Scatter Plot, and we can see that the selected example has the .......... feature in common.\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: feature"
  },
  "7825663272242d4e4dc3a951c886554f4fb610e7a397e3ee3a7fcf5891107215": {
    "soal": "The Distance Map widget can visualize distances (.....................) between items.\n",
    "jawaban": "The correct answer is: distance"
  },
  "c21229fc5f6316f62d977de05ed3cc45912a25f417026793c1ab52248e229da5": {
    "soal": "PM4Py can perform which of the following tasks?",
    "jawaban": "The correct answer is: Process discovery"
  },
  "681fe611b21639f3d2bdbca4d1457ccf7aa1069c3bbf2dee915d5b2eed3fdaf3": {
    "soal": "We can measure the distance between the embedded images and see which ones are least similar. We can use the Distances widget to measure the distance. Typically, cosine distance works best for images. We send the distance matrix to Hierarchical Clustering to visualize the similar pair in the dendrogram.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "698d66c33cbdaebd9b0cd2fb03b180f88fe339df6b8b3c58afef984cca641dd8": {
    "soal": "In the context of support vector machines, what is a kernel function used for?",
    "jawaban": "The correct answer is: To transform the input data into a higher-dimensional space to make it easier to separate with a hyperplane"
  },
  "421c4d0d24b110d027ddd0bfd1f6b465cb4104d5f7b2f092a804b90444a9ce8d": {
    "soal": "If the dataset has many attributes, it is not feasible to manually scan all pairs to find interesting or useful scatter plots. Orange implements intelligent market visualization with the Find Informative Projections option in the widget.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "36a9b6f4e749a3d88efb55c5ae40e1e880a0af2065864765f36d16f27e9b7306": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nplt.imshow(x_train[0], cmap='gray')\nplt.title(f\"Label: \")\nplt.show()\nmodel = keras.Sequential([\nkeras.layers.Flatten(input_shape=(28, 28)),\nkeras.layers.Dense(128, activation='relu'),\nkeras.layers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nWhat is the purpose of softmax in the output layer?\n{\n= To generate probability distribution over 10 classes\n~ To normalize input features\n~ To prevent overfitting\n~ To reduce model complexity\n~ To convert image to grayscale\n~ To create dropout\n}",
    "jawaban": "The correct answer is: y_train[0]"
  },
  "16cd317259b40d8267542f3c96772e6aabb079d828a5f1acaf6179cab9da0e11": {
    "soal": "What do LLMs generate based on their input?",
    "jawaban": "The correct answer is: Relevant text outputs"
  },
  "5d154c6578ddac905822e01fda6df80669f23261fe1cd58bd8655be4810f80de": {
    "soal": "Petri Nets use which of the following concepts explicitly?",
    "jawaban": "The correct answer is: Transitions and conditions"
  },
  "06e52a2c4bd9430c7094fb631f7ba903d3a3378fe5526fa65ec55496d528e339": {
    "soal": "Most Time Series algorithms assume that we do not have missing values in our data. In this widget, we can select an interpolation method to estimate the missing values. By default, the Interpolate widget will use linear interpolation (fast and reasonable).\n",
    "jawaban": "The correct answer is 'True'."
  },
  "a6d7893310225291561470d2fa5163619ec36555b7ac9526f5fab2b264038eaf": {
    "soal": "Widget Save Data does not save data every time a new signal is received in the input because it would continuously (and mostly unintentionally) overwrite the file. Instead, data is saved only after a new file name is set or the user presses the Save button.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "a29b430493dc3f21fc3aa44eb831d3c94cbdd0d7ae2ebe1b7769ec6d5f771d39": {
    "soal": "The Heat Map widget can plot a heat map for .................... attributes.\n",
    "jawaban": "The correct answer is: pairwise"
  },
  "8a74c3cbf0dd96ae9d994463502b8b702856cd0382213d4a4da4a7d733bddf63": {
    "soal": "101 (binary) = ..... (decimal)",
    "jawaban": "The correct answer is: 5"
  },
  "e4cc05f617805fa541a1c875cfaa3b4ac26b80378edbb9cdcc681f96cc6c2b69": {
    "soal": "How do you create a new model from an existing one for customization or training?",
    "jawaban": "The correct answer is: ollama create new_model"
  },
  "8781bdff564b4ddeee6a28d37774824edff582ea4613b30e9f5318a0817e3e4b": {
    "soal": "The Scatter Plot widget provides 2D scatter plot visualizations for continuous and discrete attributes. Data is displayed as a set of points, each having an attribute value on the x-axis that determines its position on the horizontal axis and an attribute value on the y-axis that determines its position on the vertical axis. Various graphical properties, such as color, size, and shape of the points, axis titles, maximum point size, and jittering, can be adjusted on the left side of the widget.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "c066729e706a5c7a82d4ae57717a1219719298e4606104884249ca7370315996": {
    "soal": "Big data is a term that refers to the very large and complex amount of data obtained from various sources, including business transactions, social data, census data, medical data, scientific data, and others. The main characteristics of big data are volume, velocity, and variety.\n- Volume refers to the enormous amount of data created constantly.\n- Velocity (Velocity) refers to the speed at which the data is created and updated, as well as the ability to process data in real-time.\n- Variety (Variety) refers to the different sources and types of data that can be collected.\nBig data usually does not require special technologies and techniques to process, analyze, and interpret the data, including machine learning techniques, data mining, and statistical analysis. The goal of big data analysis is to gain valuable insights and information, such as trends and patterns useful for business, scientific, or social decision-making.",
    "jawaban": "The correct answer is 'False'."
  },
  "854d8ba44aeb81d2f77cf2ff2df04b6a1e6cf06ebfaef23768f5061480089a47": {
    "soal": "The ......................... widget can visualize the sequence of the time series and its movement in the most basic time series visualization that is easy to imagine.\n",
    "jawaban": "The correct answer is: Line Chart"
  },
  "d5b7d485549d8399d196d5f6d8d02bbe22f6be1879680d105bf24ef35d2b4d2f": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#-xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\ndfg, start_activities, end_activities = pm4py.discover_dfg(log)\npm4py.view_dfg(dfg, start_activities, end_activities)\nWhich file name suggests a running example provided by PM4Py?",
    "jawaban": "The correct answer is: PM4PY-running-example.xes -excercise.xes -BPIC_2012_W.xes -BPIC_2012_O.xes -training_log_1.xes -Production.xes"
  },
  "ee726a0453e22e14e6c6f1115d8ce35e048e5193a810e6b6dc245123117294ae": {
    "soal": "Widget Network Analysis calculates statistical summaries from the node-level and graph-level for ........................... Widget Network Analysis will output the network with the computed statistics and an extended item data table (only node-level index).\n",
    "jawaban": "The correct answer is: network"
  },
  "71d94e1354723f7c1de9140b56231299ccd19a59ffd0204ba67e453a5e524ea5": {
    "soal": "The Scatter Plot widget provides 2D scatter plot visualizations for continuous and discrete attributes. Data is displayed as a set of points, each having an attribute value on the x-axis that determines its position on the horizontal axis and an attribute value on the y-axis that determines its position on the horizontal axis. Various graphical properties, such as color, size, and shape of the points, axis titles, maximum point size, and jittering, can be adjusted on the left side of the widget.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "99bee59d952aa59f03f5082ffa310fcc8f50b1ed057f34b8a456477bf8f5d49d": {
    "soal": "The input for the Scatter Plot widget is:\n\nData: input dataset\nData Subset: subset of instances\nFeature: list of attributes\nCategorical: list of categories\n",
    "jawaban": "The correct answer is 'False'."
  },
  "5a626e7d8a8799320253674fa778261c04f77c46fef4eee98fc5d414ea7ae051": {
    "soal": "import pandas as pd\nfrom pm4py.objects.conversion.log import converter as log_converter\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom pm4py.visualization.petrinet import visualizer as pn_visualizer\nfrom pm4py.objects.log.util import dataframe_utils\n# csv_filename='Insurance_claims_event_log.csv'\n# csv_filename='Business Travel Permits.csv'\n# csv_filename='Parent-Child Union Output.csv'\n# csv_filename='PM4PY_Dummy_Event_Log_100_Cases_pm4py.csv'\n# csv_filename='PM4Py_Dummy_Event_Log-congested-business-process.csv'\n# 1. Load CSV event log\ndf = pd.read_csv(csv_filename)\n# df = pd.read_csv(csv_filename,sep=';')\n# print(df.head())\n# 2. Format timestamps properly\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n# 3. Ensure column names match PM4Py expectations\ndf = dataframe_utils.convert_timestamp_columns_in_df(df)\n# 4. Convert to event log object\nevent_log = log_converter.apply(df, parameters=)\n# 5. Discover process model using Alpha Miner\nnet, initial_marking, final_marking = alpha_miner.apply(event_log)\n# 6. Visualize Petri Net\ngviz = pn_visualizer.apply(net, initial_marking, final_marking)\npn_visualizer.view(gviz)\nWhich library is needed alongside PM4Py for proper visualization?\n{\n~Matplotlib\n~Seaborn\n=Graphviz\n~PyGraph\n~NetworkX\n~Plotly\n}",
    "jawaban": "The correct answer is: log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case_id'"
  },
  "1404d27dac4dc6b298a96b7d31efffdbaf7e3fb6f84fe062a726c833be4aa86f": {
    "soal": "What is the main advantage of using Long Short-Term Memory (LSTM) networks over standard RNNs?",
    "jawaban": "The correct answer is: They can capture long-term dependencies without suffering from the vanishing gradient problem."
  },
  "6a3876fb5ba71f6983e7fe40b3701a8c6b00bf98cd7c98e3b42d35b1c6b3b4ac": {
    "soal": "Sebagai contoh penggunaan kNN untuk klasifikasi kita mengggunakan dataset iris. Kita bandingkan hasil dari k-Nearest neighbors dengan default model Constant, yang akan memprediksi class majoritas. Tampak kNN lebih baik.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "d567db53672ae38faabb4a106b8e348cdf30ac5a16daefe7c799a4f3569e68af": {
    "soal": "A heat map is a graphical method for visualizing attribute values based on class in a two-way matrix. Heat maps can only be used on datasets that contain continuous variables. The values are represented by colors: the higher the value, the ............ darker the color. By combining class and attributes on the x and y axes, we see where which attribute values are strongest and weakest, allowing us to find typical features (discrete) or value ranges (continuous) for each class.\n\n\n\n\n",
    "jawaban": "The correct answer is: darker"
  },
  "b882942f6d1cab5b02d0d066084e055e5c7f99e06ebe64e4258430bb70f8904c": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nGiven an input image of size 28x28 pixels, what will be the output size after applying the Flatten layer?",
    "jawaban": "The correct answer is: 784"
  },
  "0dff64ac046de995366390aeb11fb832f111a867e799726adfa41d0cb3f711ad": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nHow many images are used for testing in the CIFAR-100 dataset?",
    "jawaban": "The correct answer is: 10,000"
  },
  "9f2f7be02aeffeb5e49b4422eb2c5d451082fe1d30b82e48287fa092f5159aa9": {
    "soal": "The Import Documents widget can import .......................... documents from a folder to become a corpus.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: text"
  },
  "4e615ebeb86539ec15b992e7fb34e3c8e3aff60ec0cb78060df413f85f5594b9": {
    "soal": "What happens after the prompt enters the context window?",
    "jawaban": "The correct answer is: LLM analyzes it based on previous context"
  },
  "eaccc2cace42d618f708493837575d57f39c1816676eb4f8a794175e79678934": {
    "soal": "The Confusion Matrix widget shows the number/proportion of instances between predicted and actual class. Selecting elements within the matrix gives the corresponding instances in the output. This way, we can observe which specific instances were misclassified and why.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "c5559c63f2aa400294dba12a6509db4c75704c6f96e9e7a90803005d9099f2fb": {
    "soal": "To perform machine learning, we need numbers. To obtain numerical representations of the images, we send the images to the Image Embedding widget.\n\nThe output data from the Image Embedding widget can be examined using the Data Table widget. Now we have numerical/pixel data for the images. Each image has 2048 numerical representations (columns n0 to n2047). With this numeric feature representation, we can apply all standard machine learning techniques, such as ............................., including clustering.\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: machine learning"
  },
  "64236ac2a2ff8b4e0cbf76521ba9592cca9eda54a0252f4bb805d4c4ac419414": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nWhat is the main purpose of the Dropout layer in the model?",
    "jawaban": "The correct answer is: Reduces overfitting by randomly setting a fraction of input units to 0 during training"
  },
  "9b56a1d2d3d273dac4be70f64cac4adcbc48802de52172a3dfa851a821754745": {
    "soal": "The Bag of Words model creates a corpus with word counts for each data instance (document). The count can be absolute, binary (present or not) or ..................... (logarithmic of term frequency). The Bag of Words model is required in combination with the Word Enrichment widget and can be used for predictive modelling.\n\n\n",
    "jawaban": "The correct answer is: sublinear"
  },
  "c2b696fce7d4a87cd919e9a6d8ff4b74bdd767c5a1876be6444083743b0cdb5b": {
    "soal": "In TensorFlow, which object is used to represent data flow graphs?",
    "jawaban": "The correct answer is: Tensor"
  },
  "53d74c56255714a175cd4cd21c9221e351b40ad3198c29e0578b3ad8856eaea2": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nHow does incorporating a Dropout layer affect the complexity of a neural network model?",
    "jawaban": "The correct answer is: It effectively reduces complexity during training by deactivating neurons"
  },
  "c05addbc3d5ba9d27d957b6d3a4202121ae723f462e9f1ffe12943dfab521f3c": {
    "soal": "The Periodogram widget can visualize cycle, seasonality, periodicity, and important periods in time series.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "f008b18f8260eb999a710127e0e0563966111a8b37dc426a3d1745893ba923d0": {
    "soal": "import pandas as pd\nfrom pm4py.objects.conversion.log import converter as log_converter\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom pm4py.visualization.petrinet import visualizer as pn_visualizer\nfrom pm4py.objects.log.util import dataframe_utils\n# csv_filename='Insurance_claims_event_log.csv'\n# csv_filename='Business Travel Permits.csv'\n# csv_filename='Parent-Child Union Output.csv'\n# csv_filename='PM4PY_Dummy_Event_Log_100_Cases_pm4py.csv'\n# csv_filename='PM4Py_Dummy_Event_Log-congested-business-process.csv'\n# 1. Load CSV event log\ndf = pd.read_csv(csv_filename)\n# df = pd.read_csv(csv_filename,sep=';')\n# print(df.head())\n# 2. Format timestamps properly\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n# 3. Ensure column names match PM4Py expectations\ndf = dataframe_utils.convert_timestamp_columns_in_df(df)\n# 4. Convert to event log object\nevent_log = log_converter.apply(df, parameters=)\n# 5. Discover process model using Alpha Miner\nnet, initial_marking, final_marking = alpha_miner.apply(event_log)\n# 6. Visualize Petri Net\ngviz = pn_visualizer.apply(net, initial_marking, final_marking)\npn_visualizer.view(gviz)\nWhich function is specifically used for visualizing process models as Petri Nets in PM4Py?\n{\n~log_converter.apply\n~alpha_miner.apply\n~dataframe_utils.convert\n=pn_visualizer.apply\n~pm4py.view\n~visualizer.graph\n}",
    "jawaban": "The correct answer is: log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case_id'"
  },
  "096715691ff307808e93b6dca4621a05b680b1e9e55554288b3ddf4434adf31a": {
    "soal": "In the Distribution Widget, for continuous attributes, the attribute values are displayed as a function plot. ...................... class for continuous attributes is obtained using Gaussian kernel density estimation, while the curve display is set with the Precision bar (smooth or precise). For example, below we use the Iris dataset.\n\n\n",
    "jawaban": "The correct answers are: Probability, Probability"
  },
  "949419429d974e94fbcda7d60583390f96cc139eab40c7cdee9bfcb1cc594f96": {
    "soal": "In the Constant Widget, the Learner will produce a model that always predicts the majority for classification or the mean value for ........................\n",
    "jawaban": "The correct answer is: regression"
  },
  "19d2f156a18f1fabdf538d97d445ae79fc29500066fe0faf24bde20ace0eb8a9": {
    "soal": "The Distances widget calculates the average distance between rows/columns in the dataset.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "fc1daa295d3d09bc0a530d414ed1865234960bdeeabc9dd62287296e287b82e5": {
    "soal": "What kind of text can a prompt be?",
    "jawaban": "The correct answer is: Sentence, paragraph, or complex conversation"
  },
  "455615edf05770671cf2405fed686e324e00fc32ced9e6483c7ad6887d84da45": {
    "soal": "To perform machine learning, we need ....................... To obtain numerical representations of the images, we send the images to the Image Embedding widget.\n\nThe output data from the Image Embedding widget can be examined using the Data Table widget. Now we have numerical/pixel data for the images. Each image has 2048 numerical representations (columns n0 to n2047). With this numeric feature representation, we can apply all standard machine learning techniques, such as clustering.\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: numbers"
  },
  "8ddca6ea147443e3095089bfcfe0657a6fd99e41ef5352ba7e153c803d5f9e87": {
    "soal": "11 (decimal) = .... (binary)",
    "jawaban": "The correct answer is: 1011"
  },
  "c5ba7916c30247e868d1220d83a67c9fc1062b4e06711cc02b730b28bf709aa6": {
    "soal": "One URL location for Orange development/use examples is\n\n\u00a0https://orange.biolab.si/workflows/\n",
    "jawaban": "The correct answer is 'True'."
  },
  "1793feb75f54c62057fb5c7a6461095ff71395b1863140f9c82136e5f0ce72e5": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\n# xes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\nxes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nprocess_tree = pm4py.discover_bpmn_inductive(log)\nbpmn_model = pm4py.convert_to_bpmn(process_tree)\npm4py.view_bpmn(bpmn_model)\nWhat does the function 'discover_bpmn_inductive' output?",
    "jawaban": "The correct answer is: Process tree"
  },
  "f5c18d8b78651b15c8871edf5ef3ce7b4d5eb74c71f4301c2f81a8e909e543cc": {
    "soal": "The Moving Transform widget can apply the rolling window function to images. Use the Moving Transform widget to get the average value from the series.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "df42ac249fccea06ce88927e3c2c800530bf85ee3d50b5b03c51fc0ac19e6a25": {
    "soal": "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target biner\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(0, 2, 100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi data latih dan data uji (80% latih, 20% uji)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi dan pelatihan model klasifikasi (misalnya RandomForest)\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n# Prediksi pada data uji\ny_pred = model.predict(X_test)\n# Menghitung Precision, Recall, dan F1-Score\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nprint(f'Precision: ')\nprint(f'Recall: {recall:.2f}')\nprint(f'F1-Score: {f1:.2f}')\n# Alternatif: Menggunakan classification_report untuk ringkasan metrik\nprint('\nClassification Report:')\nprint(classification_report(y_test, y_pred))\nWhich function is used to set the random seed in NumPy?\n{\n~np.random.random\n~np.random.rand\n~np.random.randint\n=np.random.seed\n~np.random.shuffle\n~np.random.permutation\n}",
    "jawaban": "The correct answer is: precision:.2f"
  },
  "4d7331466516b8676a8e8cf1e978fd3b107f82418df59435b4f7f0cf758067b3": {
    "soal": "The Distance Matrix widget can visualize the measurement results of distances in a distance matrix.\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "c124605ef381085c0778206143198657b5ba910b305c33bfc130b4a8bf529472": {
    "soal": "The Save Data widget can consider the dataset provided in the input channel and save it to a data file with a specified name. It can save the data as a file with data separated by tabs or separated by .......................\n",
    "jawaban": "The correct answer is: comma"
  },
  "2e326705e8880b4164d1daf3912be005aba29ac7da7762961e150b523f2a4faa": {
    "soal": "What could be a consequence of not using stratification when splitting an imbalanced dataset?",
    "jawaban": "The correct answer is: The training and test sets may have different class distributions, leading to biased model evaluation."
  },
  "875e4a542cff6ba54ada2d22a013a73975ee7ee4a14a2b5cfbecd8b77e673b32": {
    "soal": "The Venn Diagram widget can also be used to explore different prediction models. In the following example, we analyze 3 prediction methods, namely Naive Bayes, SVM, and ............................., based on their classification errors on data instances.\n\nBy selecting misclassification in the three Confusion Matrix widgets and sending them to the Venn Diagram widget, we can see all misclassification examples visualized by the method used. We then open the Venn Diagram widget and select, for example, misclassified instances identified by all three methods. This is represented as the intersection of the three circles. Click on the intersection to view these two examples highlighted in the Scatter Plot widget. Try selecting different parts of the diagram to see how the scatter plot visualization changes.\n\n\n\n",
    "jawaban": "The correct answer is: Random Forest"
  },
  "cb8e73758e58d1d40245c8a84102b871615b20c7856aef466af6f5e744168012": {
    "soal": "Widget ...........(1)............. calculates ..........(2)............. statistics at the node-level and graph-level for a network. The Widget Network Analysis will output the network with the computed statistics and an extended item data table (only node-level .............(3)..............).\n",
    "jawaban": "The correct answer is: (1) \u2192 Network Analysis, (2) \u2192 summary, (3) \u2192 index"
  },
  "c2e97e0f8187e48ebc1a1af612fa2f6fa43d399759c49446425d994f47b79de4": {
    "soal": "Data Mining is critical to supporting the success of modern data-driven organizations. An IDG survey of 70 IT and business leaders found that 92% of respondents want to apply advanced analytics more broadly across their organizations. The same survey found that the benefits of data mining are deep and wide.\nIn fact, respondents identified no fewer than 30 different ways in which data mining positively impacts their business. Here are the top 10:\n- Improving decision-making processes\n- Improving security risk posture\n- Improving Planning and Forecasting\n- Competitive advantage\n- Cost reduction/savings\n- Customer acquisition\n- New revenue streams\n- Acquisition/retention of new customers\n- Severing customer relationships\n- New product development",
    "jawaban": "The correct answer is 'False'."
  },
  "897c45734f67e99389f248eaa98e40e7edc8c013742866f14e0e61c64c0b4e19": {
    "soal": "Convert the binary number 10011 to decimal...",
    "jawaban": "The correct answer is: 19"
  },
  "78d24144688ad21a5997df848ae9fadcaa2a31ebea4dcd61b8d65ec33f550edf": {
    "soal": "Widget Select Columns can manually select data attributes and composition of image files.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "a687c8d82f4c387260882e51850572d4984428706031890ae094e418c160adb0": {
    "soal": "Answers can be more than one.\n\n101 (binary) + 10 (binary) = .............................. ?",
    "jawaban": "The correct answers are: 111 (binary), 7 (decimal)"
  },
  "c381e773017a10cd8ff0e72f269332f2f3fbc6d1cf57177b5aa350c3a1dfb24e": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nHow many trainable parameters does the Flatten layer have?",
    "jawaban": "The correct answer is: 0"
  },
  "6313e6c3e5fd61085208bf3130dee3f4d58b6427ed8b81c7069476807ef63b58": {
    "soal": "Match the regular expression functions useful for the following filtering\n",
    "jawaban": "The correct answer is:\nmatches words that are longer than 4 characters\n\u2192 \\w{4,}, matches only words, no punctuation \u2192 \\w+, \u2192 \\b(B|b)\\w+\\b, matches exact word \u2192 \\bword\\b"
  },
  "9655c7d6a33b2a45eeee3737190eeac730251fdb18e846875e697e9c3c1af2ee": {
    "soal": "When training Machine Learning models like SVM or Random Forest on TikTok comments, what is the minimum recommended number of comments?",
    "jawaban": "The correct answer is: 3,000 comments"
  },
  "24e220b84cf2e431a210cc5f09ff4f814bb00a41177ce5233e12f08cd6adaf00": {
    "soal": "Cassandra, or Apache Cassandra, is one of the open-source products for distributed database management by Apache. Cassandra is designed to manage large (big data) structured data spread across one server. This software is highly scalable, so it is no surprise that many large companies have trusted Cassandra as one of their support tools, such as Facebook, Twitter, and Apple.",
    "jawaban": "The correct answer is 'False'."
  },
  "ab73bcd353a27b8bee7c584c5678e2d53233cb83de8b062d3539bcf3865cfa0e": {
    "soal": "For ..........(1)............... problems, where data instances are described by ...........(2)........... labels, we want to know which feature is the most informative. The .....(3)............ widget provides a table of features and their informativeness scores, and supports manual feature selection. In the workflow, we use it to find the top two features (out of 79 initially selected features) and display the scatter plot.\n\n\n",
    "jawaban": "The correct answer is: (3) \u2192 Rank, (1) \u2192 supervised, (2) \u2192 class"
  },
  "a4431003d7af45ec5c4cba8b3510d326a58e9f2147d918699dc58e26448f6d2c": {
    "soal": "import pandas as pd\n# Contoh: Membuat DataFrame dengan missing values\ndata =\ndf = pd.DataFrame(data)\n# Menampilkan jumlah missing values di setiap kolom\nprint(\"Jumlah missing values per kolom:\")\nprint(df.isnull().sum())\n# Menghapus baris yang memiliki missing values\ndf_cleaned = df.dropna()\nprint(df_cleaned)\n# Mengisi missing values pada kolom 'Usia' dengan mean\ndf['Usia'].fillna(df['Usia'].mean(), inplace=True)\n# Mengisi missing values pada kolom 'Usia' dengan median\ndf['Usia'].fillna(df['Usia'].median(), inplace=True)\n# Mengisi missing values pada kolom 'Kota' dengan modus\ndf['Kota'].fillna(df['Kota'].mode()[0], inplace=True)\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n# Inisialisasi imputer untuk mengisi missing values dengan mean\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n# Mengimputasi kolom 'Usia'\ndf[['Usia']] = imputer.fit_transform(df[['Usia']])\nHow do you initialize a SimpleImputer to replace missing values with the mean?\n{\n=imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n~imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n~imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n~imputer = SimpleImputer(missing_values=0, strategy='mean')\n~imputer = SimpleImputer(strategy='mean')\n~imputer = SimpleImputer()\n}",
    "jawaban": "The correct answer is: 'Nama': ['Andi', 'Budi', 'Citra', 'Dewi'], 'Usia': [25, 30, None, 22], 'Kota': ['Jakarta', None, 'Bandung', 'Surabaya']"
  },
  "67584ba4fd4f58097a45497ccff67f12d59410e06503e9da3f655fdaeab329f2": {
    "soal": "import pandas as pd\nfrom pm4py.objects.conversion.log import converter as log_converter\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom pm4py.visualization.petrinet import visualizer as pn_visualizer\nfrom pm4py.objects.log.util import dataframe_utils\n# csv_filename='Insurance_claims_event_log.csv'\n# csv_filename='Business Travel Permits.csv'\n# csv_filename='Parent-Child Union Output.csv'\n# csv_filename='PM4PY_Dummy_Event_Log_100_Cases_pm4py.csv'\n# csv_filename='PM4Py_Dummy_Event_Log-congested-business-process.csv'\n# 1. Load CSV event log\ndf = pd.read_csv(csv_filename)\n# df = pd.read_csv(csv_filename,sep=';')\n# print(df.head())\n# 2. Format timestamps properly\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n# 3. Ensure column names match PM4Py expectations\ndf = dataframe_utils.convert_timestamp_columns_in_df(df)\n# 4. Convert to event log object\nevent_log = log_converter.apply(df, parameters=)\n# 5. Discover process model using Alpha Miner\nnet, initial_marking, final_marking = alpha_miner.apply(event_log)\n# 6. Visualize Petri Net\ngviz = pn_visualizer.apply(net, initial_marking, final_marking)\npn_visualizer.view(gviz)\nWhich method does PM4Py use to process raw CSV data?\n{\n~pd.load_csv\n=pd.read_csv\n~pm4py.read_csv\n~dataframe.read\n~csv_to_log\n~import_csv\n}",
    "jawaban": "The correct answer is: log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case_id'"
  },
  "cdd36f792ea9e77e2550c706d397aad92ef6b68bef110676d7456354d7ca9f68": {
    "soal": "Which of the following is NOT a function of LLMs?",
    "jawaban": "The correct answer is: Playing video games"
  },
  "0bcbd4d191f8c13706c185599ba7dbe9a7ea7bf719c8fe9e89684880d2a80848": {
    "soal": "Widget Save Data can save data to a file.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "b49c2ab5cd0d9c26e0b4297917c2c0a33b02880e3f88907a6497fc3596b8e3f5": {
    "soal": "import pm4py\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\n# Import XES log\nlog = xes_importer.apply(xes_filename)\n# Cek jumlah kasus dan aktivitas\nprint(f\"Jumlah kasus: \")\nactivities = set()\nfor trace in log:\nfor event in trace:\nactivities.add(event['concept:name'])\nprint(f\"Jumlah aktivitas unik: {len(activities)}\")\nfrom pm4py.algo.discovery.dfg import algorithm as dfg_discovery\nfrom pm4py.visualization.dfg import visualizer as dfg_visualization\n# DFG berdasarkan frekuensi\ndfg = dfg_discovery.apply(log,\nvariant=dfg_discovery.Variants.FREQUENCY)\n# Tampilkan visualisasi\ndfg_vis = dfg_visualization.apply(dfg, log=log,\nvariant=dfg_visualization.Variants.FREQUENCY)\ndfg_visualization.view(dfg_vis)\nWhat command is used to visualize a DFG using PM4Py?\n{\n=dfg_visualization.view()\n-pm4py.show_dfg()\n-visualize_dfg()\n-dfg.view()\n-graph.show()\n-dfg.plot()\n}",
    "jawaban": "The correct answer is: len(log)"
  },
  "762221b5d4278c0dfe5b664804448860b54cd06173466f12cb7836741516ad86": {
    "soal": "Widget Select Columns is used to manually organize data domains. Users can decide which attributes to use and how. Orange differentiates between regular attributes, class attributes (optional), and meta attributes. For example, to build a regression model, the domain will consist of a set of attributes and discrete class attributes. Meta attributes are not used in modeling, but some widgets can use them to label instances.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "943bfcfb847ff2e1c2a933b93d5392d82da31de6dfe0ca30e323137c5023445c": {
    "soal": "To extract numbers from unstructured data, Orange can use deep ...............\u00a0 embedder.\n",
    "jawaban": "The correct answer is: network"
  },
  "6cdda31a60a765114c4834cb4e6ab89972dbfb2bd7a20107a15baaf37c3cf1da": {
    "soal": "In a typical classification example, we will use the Constant widget to compare the scores of other learning algorithms (such as kNN) with the default score. Use the iris dataset and connect to Test & Score. Then, connect Constant and kNN to Test & Score and observe how well kNN performs against the baseline Constant.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "a009e8e8fae65071219e956b97498b7d38f622c325da9384b8ee95ba6b522fcf": {
    "soal": "Before visualizing a Petri net, what does PM4Py need to perform?",
    "jawaban": "The correct answer is: Process discovery"
  },
  "0128f53469fbe90a90fc91e33d064eb06889780c15375db97495cf387c78f877": {
    "soal": "The Tree algorithm is a simple algorithm that can separate data into nodes based on class purity (category/class). The Tree algorithm is the predecessor of the Random Forest algorithm. The Tree widget in ............................ is designed in-house and can handle both discrete and continuous datasets.\n",
    "jawaban": "The correct answer is: Orange"
  },
  "3b7eedc6b4f264a978bf0f0b7a733e59999332e075795003e2435ce6fbb984f6": {
    "soal": "\n\n\n\nIn this example, we are using the lastfm dataset, which can be loaded into the Network File widget under Browse documentation networks. The network node represents musicians, marked by the genre they play, the number of albums produced, and so on. Edge is the number of listeners on LastFm.\n\nThe entire dataset is visualized in the .................. Explorer Widget. In this widget, we remove the coloring and adjust the node size to match the number of albums. Then, we select several nodes from the network. We can observe the selection in the Network Explorer Widget (1).\n",
    "jawaban": "The correct answer is: network"
  },
  "baf5eddff1560fee0998eb06cf59baefa11fba244e02aaffea0accda462f7e36": {
    "soal": "In the context of neural networks, what is backpropagation used for?",
    "jawaban": "The correct answer is: To update the weights of the network based on the error gradient"
  },
  "22836ecd84ed0fdb2dbd6d0c3f967162459bc8918342fd1fbf6a6d133b121ff6": {
    "soal": "Widget File can read data from Excel (.pptx), tab-delimited (.txt) simple, comma-separated file (.csv), or URL.\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "ffe991bd6760bc10fd98e1d99ec146b0a2b5f25883c960bb3166736c54539d99": {
    "soal": "The Tree widget can work for regression tasks. In the workflow below, we use the housing dataset and give the dataset to the .............. Tree node selected in the Tree Viewer will be displayed in the Scatter Plot, and we can see that the selected example has the same feature.\n\n\n\n\n",
    "jawaban": "The correct answer is: Tree"
  },
  "2f9029632578c9ab9e537549da4164e391f707dd570127edf02abb8a47b28d3b": {
    "soal": "What is transfer learning in TensorFlow?",
    "jawaban": "The correct answer is: Reusing a pre-trained model for a different but related task"
  },
  "1a04217ff572c5497db5c673ee1febb04a7d47fb9bad9fd33b653dc3f647c804": {
    "soal": "The Image Embedding widget in Orange is very easy to use. Technically, Orange sends the image to the server, where the server pushes the image through a pre-trained deep neural network, such as Google Inception v3. Deep networks are often trained with a specific purpose. Inception v3, for example, can classify images into one of 1,000,000 image classes. We may ignore the suggested classifications and instead consider the penultimate layer of the network with 2048 nodes (numbers) and use it for the image\u2019s vector-based representation.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "8d5aec88970adb6dde7d4b1acb169d02430eb5d956fc60e224dabe9eb1808856": {
    "soal": "The Import Images widget is probably the first widget we will use in image analysis. The Import Images widget loads images and creates a class value from the folder. In this example, we use the Import Images widget to load 26 paintings by Monet or Manet.\n\nWe can observe the results in the Data Table widget. It is clear how Orange adds an additional class attribute with values Monet and Manet.\n\nNext, we can continue with standard machine learning methods. We send the images to the Image Embedding widget, where we will use the Painters embedder to get the image vector.\n\nThen we will use the Test & Score widget and the Logistic Regression widget to create a model to predict the artist of a painting (in this case Monet or Manet). We get a very good score. How could that be? It turns out these images were already trained on the Painters embedder, so the accuracy achieved will be high.\n\n\n\u00a0\n\n\n\n",
    "jawaban": "The correct answer is: Logistic Regression"
  },
  "ca870cd69c408f8263077c3b3b7d11334dce28f18892b728236ac0a33bc8f3cb": {
    "soal": "The following workflow is an example where we compare three (3) classifiers (Naive Bayes, Tree, and Constant) and input them into the Test & Score widget. We are using the Titanic dataset. The Test & Score widget will display the evaluation results for each classifier. Then, we can plot through the Calibration Plot widget and the ROC Analysis widget from the Test & Score output for further analysis of the performance of each classifier. The Calibration Plot widget allows us to see the predicted accuracy of class probabilities in a plot/image form.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Test & Score"
  },
  "579e53a99d7913445cf3009e92594f89fa911c4af9cb6033603549181983fd7b": {
    "soal": "The ....................\u00a0 widget displays the distribution of discrete or continuous attribute values. If the data contains a class variable, the distribution can be conditioned on the class.\n",
    "jawaban": "The correct answer is: Distributions"
  },
  "f957fa28319f938dd454196859a6a78bdedfc28edded29e67c927c1ec0b9937c": {
    "soal": "What impact does the Dropout layer have during training?",
    "jawaban": "The correct answer is: It randomly deactivates 20% of the neurons in the previous layer to prevent overfitting"
  },
  "d33b2d4d7b5f96c30a01262047617b594613f0f03e0697475f28c4c439d6db3c": {
    "soal": "Only two inputs are suitable for the Distance Matrix widget, which are the Distances widget and the Distance Transformation widget. The output from the Distance Matrix widget is ....................... that contains the distance matrix. Users can decide how to label the table and distance matrix (or examples in the distance matrix), which can then be visualized or displayed in a separate data table.\n",
    "jawaban": "The correct answer is: data table"
  },
  "4a8333e90ff851a09638fdcc8ccb8c22f246758c8a71e5ec0cdc1592991b59e1": {
    "soal": "The Line Plot widget is a standard visualization widget, which displays the ............ profile, usually in the form of ordered numeric data. In this simple example, we will display the iris data in a line plot, grouped by iris attributes. The plot shows how petal length separates the class values well.\n\nIf we observe this in the ......................... Widget, we can confirm this is indeed true. Petal length is an interesting attribute for class separation, especially when enhanced with petal width, which is also well separated in the line plot.\n\n\n\n\n",
    "jawaban": "The correct answer is: Scatter Plot"
  },
  "0064245be4216c6e3c2d02c589ea8da7224d1f1e9d56c5950e37e37f4d6c6931": {
    "soal": "Which of the following is a popular framework for processing large datasets in a distributed computing environment?",
    "jawaban": "The correct answer is: Apache Spark"
  },
  "c2963fb485b1497f0f2de428701e811f81550027edfc0ab642f4c015ccbbe7e9": {
    "soal": "Word Cloud can be created from the Documents we have. In the text mining toolbox, there is an Import Documents widget to read document files. Before displaying them as a word cloud, it is advisable to pass them through the Preprocess Text widget first, to reduce unnecessary words like conjunctions, etc. Then we can pass it through the ......................... widget first, or directly to the Word Cloud widget to display.\n\n\n\n\n",
    "jawaban": "The correct answer is: Bag of Words"
  },
  "82d803175c4153657b516d48a69b1b8f326bb54368e360042b612218627b8da4": {
    "soal": "Data does not always come in the form of neat tables. Data can also be in the form of text, audio recordings, video content, or even images. However, computers can only work with ..........(1) ......., so for any data mining, we need to transform that unstructured data into a vector representation.\n\nTo extract numbers from unstructured data, Orange can use a deep network embedder. Orange has just started incorporating various embedders in Orange in the widget ..................(2)...................., and currently, they are available for text and images.\n",
    "jawaban": "The correct answer is: (2) \u2192 Image Embedding, (1) \u2192 numbers"
  },
  "50cea8caf739000b7f76de7dc5525888b86fc2f5030df6774388a6b3d59dc849": {
    "soal": "Which step is NOT required for process animation in ProM?",
    "jawaban": "The correct answer is: Writing Python code"
  },
  "c37c485c5636a18e32eba30a38f8f6d22269ca6e76b6b3688c910e5da5da117f": {
    "soal": "R-programming is a programming language used in big data processing. The nature of this programming language is open-source, meaning it can be used for free and modified by anyone. Its open-source nature allows many active users to contribute to the development of R-programming.\nSome advantages of R-programming include:\n- R programming can integrate with other programming languages, such as SQL.\n- It is used for data cleansing and manipulation, spatial analysis, data analysis and model building, data visualization, and even text analysis with natural \nAnswer Question 81\n processing.\n- It has many functions and packages that make it easier for data practitioners.",
    "jawaban": "The correct answer is: language"
  },
  "bb830a24d99638a475a5e1d539dfb9f0046ebb49d1340c8df4e3102fc12f6b25": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(2, size=100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi training (80%) dan testing (20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model, misalnya RandomForestClassifier\nmodel = RandomForestClassifier(random_state=42)\n# Melatih model pada data training\nmodel.fit(X_train, y_train)\n# Memprediksi data testing\ny_pred = model.predict(X_test)\n# Mengevaluasi akurasi model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model pada data testing: ')\nWhich machine learning model is initialized in the code?\n{\n~DecisionTreeClassifier\n~SVMClassifier\n=RandomForestClassifier\n~KNeighborsClassifier\n~GradientBoostingClassifier\n~LogisticRegression\n}",
    "jawaban": "The correct answer is: accuracy:.2f"
  },
  "4d2dde44ae3c0f05386d2e7512ef2f9c73650f19b8468557ca37682838cb0639": {
    "soal": "Widget Tree menggunakan algoritma Tree dengan kemampuan untuk melakukan forward pruning (pemangkasan ke belakang).\n",
    "jawaban": "The correct answer is 'False'."
  },
  "e1bce0c1aaf2a13e2c88eb2ce7865cc7c64bbde7ddecd54d943884c6accd1c42": {
    "soal": "Which environment variable in the Open-WebUI service specifies the connection URL to the PostgreSQL database?",
    "jawaban": "The correct answer is: DATABASE_URL"
  },
  "ac0db1de7b7049b9a4d5b9f6111240bbd370dd87c90ba0d157f1560029cd2dc3": {
    "soal": "The Line Chart widget can visualize the sequence of time series and its movement in the most basic time series visualization that is quite difficult to imagine.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "add924b91490faaf52ce0b6b22a1956141640ade8e7026566bc3f08c80a51e83": {
    "soal": "The Predictions widget shows the learner's predictions against the data.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "e808164fc82f4b9997cef2be92d84d20b77d579b5451b4c27dec1137dd253b83": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nWhich Python library provides a function to load the CIFAR-100 dataset?",
    "jawaban": "The correct answer is: TensorFlow"
  },
  "187222d88aa29989f07a64bc19aa54201ef838707b1a0e05c34d08ecfd656479": {
    "soal": "PM4Py visualizes processes primarily using:",
    "jawaban": "The correct answer is: Petri nets"
  },
  "987e7ad511f92771b397f31efcdb4ebeda84a579718ff81a0f7b751b597710b5": {
    "soal": "import pandas as pd\nfile_path = \"/content/drive/MyDrive/your_file.csv\"\ndf = pd.read_csv(file_path)\nprint(df.head())\nWhat is the conventional alias used when importing the 'pandas' library in Python?",
    "jawaban": "The correct answer is: pd"
  },
  "9b4415bcb22b65f4a068022477517cec0f5af9870a8e097bea0d8f7a290944a8": {
    "soal": "In the Orange workflow below, we can predict the classification of a text. The classification model is obtained using a logistic regression learner with training data from the Grimms Corpus. Text predictions from the Andersen Corpus are made by the Test & Score widget.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "07c4417fd31a04d1b5b16993642b4be7a6b448fd591ae4b4c93780c29fe2be75": {
    "soal": "In Big Data analytics, what is 'data cleansing'?",
    "jawaban": "The correct answer is: The process of detecting and correcting inaccurate or corrupt data"
  },
  "7fe0c6d170e1aefa44c51822d4f772982737f7979844db40e7fbde0faad00e0c": {
    "soal": "The Bag of Words widget can create a bag of words from the input .........................\n",
    "jawaban": "The correct answer is: corpus"
  },
  "74490430a5ec1f8c1560131bb864e268aa0dd9d2d2634b8541e7072129cd1e4a": {
    "soal": "import pm4py\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\n# Import XES log\nlog = xes_importer.apply(xes_filename)\n# Cek jumlah kasus dan aktivitas\nprint(f\"Jumlah kasus: \")\nactivities = set()\nfor trace in log:\nfor event in trace:\nactivities.add(event['concept:name'])\nprint(f\"Jumlah aktivitas unik: {len(activities)}\")\nfrom pm4py.algo.discovery.dfg import algorithm as dfg_discovery\nfrom pm4py.visualization.dfg import visualizer as dfg_visualization\n# DFG berdasarkan frekuensi\ndfg = dfg_discovery.apply(log,\nvariant=dfg_discovery.Variants.FREQUENCY)\n# Tampilkan visualisasi\ndfg_vis = dfg_visualization.apply(dfg, log=log,\nvariant=dfg_visualization.Variants.FREQUENCY)\ndfg_visualization.view(dfg_vis)\nThe analysis of durations between activities to find bottlenecks is performed by which variant in PM4Py?\n{\n=performance_dfg.variants.performance()\n-frequency_dfg.variants.frequency()\n-performance_spectrum.apply()\n-xes_importer.apply()\n-activity_analysis.performance()\n-performance.analyze()\n}",
    "jawaban": "The correct answer is: len(log)"
  },
  "f9cafe5975f5f81c557900b4c1894f3549dbbacc971185da529b09da986dd0ed": {
    "soal": "The Import Images widget scans ..............(1)........... and returns one row per image found. The columns include the image name, the path directory to the image, width, height, and image size. ..........(2)............. with the directory path to the image is then used as an attribute for visualization and Image Embedding.\n",
    "jawaban": "The correct answer is: (1) \u2192 directory, (2) \u2192 Columns"
  },
  "857fe7db15f26b17b5ea0f4e0ab054c309ef48c65fe95531fc2207ffb7b54dff": {
    "soal": "\n\n\nThe image above shows the sequence of processes in data science, which are:\n\n................... - Ask, interesting questions.\nGET - Obtain data.\nEXPLORE - Explore data, whether there are any anomalies / interesting patterns, etc.\nMODEL - Build, fit, and validate the model.\nVISUALIZATION - Communicate and visualize the data.\n\n",
    "jawaban": "The correct answer is: ASK"
  },
  "bfab5089775031dad52d9a2cd6682a4fec85d648d60dc185c41f6ce2e7334019": {
    "soal": "import pm4py\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\n# Import XES log\nlog = xes_importer.apply(xes_filename)\n# Cek jumlah kasus dan aktivitas\nprint(f\"Jumlah kasus: \")\nactivities = set()\nfor trace in log:\nfor event in trace:\nactivities.add(event['concept:name'])\nprint(f\"Jumlah aktivitas unik: {len(activities)}\")\nfrom pm4py.algo.discovery.dfg import algorithm as dfg_discovery\nfrom pm4py.visualization.dfg import visualizer as dfg_visualization\n# DFG berdasarkan frekuensi\ndfg = dfg_discovery.apply(log,\nvariant=dfg_discovery.Variants.FREQUENCY)\n# Tampilkan visualisasi\ndfg_vis = dfg_visualization.apply(dfg, log=log,\nvariant=dfg_visualization.Variants.FREQUENCY)\ndfg_visualization.view(dfg_vis)\nBefore visualizing a DFG, you must first:\n{\n=Discover the DFG from the log\n-Export the log to CSV\n-Create an XML file\n-Install TensorFlow\n-Run machine learning predictions\n-Clean log data manually\n}",
    "jawaban": "The correct answer is: len(log)"
  },
  "0b7b39327814c2f276941f4c3fcc1f3bc5ca63e644b76b567da8c40fa22c3f2a": {
    "soal": "import numpy as np\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n# Contoh data: nilai aktual dan prediksi\ny_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0]) # Nilai aktual\ny_pred = np.array([1, 0, 1, 0, 0, 1, 0, 1, 1, 0]) # Prediksi model\n# Menghitung confusion matrix\ncm = confusion_matrix(y_true, y_pred)\n# Menampilkan confusion matrix dalam bentuk teks\nprint(\"Confusion Matrix:\n\", cm)\n# Menampilkan confusion matrix dalam bentuk grafik\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.show()\nFungsi mana yang digunakan untuk menghitung confusion matrix dalam pustaka scikit-learn?",
    "jawaban": "The correct answer is: confusion_matrix"
  },
  "a934f5e1fe0044676ca0e42703c7cb7edb2b3be01f39078d9ea0f0186804ed03": {
    "soal": "The Predictions widget shows the probabilities and the final decision of the prediction model. The output of the Predictions widget is another dataset, where the predictions are added as a new meta attribute. We can choose which features we want to remove (original data, predictions, probabilities). The results can be observed in the Data Table. If the predicted data includes the ............... value, the prediction results can also be observed in the Confusion Matrix Widget.\n",
    "jawaban": "The correct answer is: class"
  },
  "df3b237d6e8ab6a4c888bacf541b1dadd83c48dfc8722a1f06d3df80c8d47199": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=10000)\nWhen loading the IMDB dataset using TensorFlow's Keras API with the parameter num_words=10000, what does the number 10,000 signify?",
    "jawaban": "The correct answer is: The number of most frequent words to consider in the dataset"
  },
  "3d13e524eb01945d7a7bdd5e322e579fcc5747de3f36e04caf26d0c094d55de6": {
    "soal": "The Scatter Plot widget cannot show the match between predicted probability by the classifier and the actual class probability.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "cae4ef7c3f1f64333798e0da921bf81b66efbc3994d579c293d95ab95dd13b3c": {
    "soal": "In machine learning, what is the primary purpose of a dataset?",
    "jawaban": "The correct answer is: To train models to learn patterns from data."
  },
  "f7475765de086c8566895e02bab6bd4bcc431f86503373e2d597708bbf5a56ed": {
    "soal": "The Apache Hadoop framework consists of the following modules:\n- Hadoop Common - contains libraries and utilities needed by other Hadoop modules;\n- Hadoop Distributed \nAnswer Question 2\n System (HDFS) - a distributed file system that stores data on commodity machines, providing very high aggregate bandwidth in the cluster;\n- Hadoop YARN - a resource management platform responsible for managing computational resources in the cluster and using them for scheduling user applications;\n- Hadoop MapReduce - a programming model for processing large-scale data.",
    "jawaban": "The correct answer is: File"
  },
  "4364967bf1ccf5bff8e2f8c84e62930aa98edb3111ac752bda153d91b83a0efd": {
    "soal": "In the workflow below, the usage of the Moving Transform widget is shown to obtain a 5-day moving average, we can use ......................... with mean aggregation.\n\n\n\n",
    "jawaban": "The correct answer is: rolling window"
  },
  "4ce91093d67eea89a0f9ebd1b5bfd4da5aa4e39c561dbd5a930c7d88363e5a0f": {
    "soal": "Only two inputs are suitable for the ...........(1)......... widget, which are the Distances widget and ...............(2).............. widget. The output from the Distance Matrix widget is ...........(2)............. containing the distance matrix. Users can decide how to label the table and distance matrix (or examples in the distance matrix), which can then be visualized or displayed in a separate data table.\n",
    "jawaban": "The correct answer is: (2) \u2192 Distance Transformation, (1) \u2192 Distance Matrix"
  },
  "af509d37307aa6e6a8ee5e8ec88f1fe53ff0c22f1cd57d828343c01c3a88446b": {
    "soal": "Handling Sparse data in the ......................... widget can only be used with Euclidean, Manhattan, and Cosine metrics.\n",
    "jawaban": "The correct answer is: Distances"
  },
  "a6c8d1859cf8febfe9aa083ae8294b5d793bab8957b602c3283ff1e4a8e81761": {
    "soal": "The Sentiment Analysis widget enables basic sentiment analysis of corpora. It works for English and uses two techniques supported by NLTK - Liu Hu and Vader. Both techniques are lexicon-based. Liu Hu calculates a single normal score for sentiment in text (negative score for negative sentiment, positive for positive, 0 for neutral), while .................... produces scores for each category (positive, negative, neutral) and adds a total sentiment score called compound.\n\n\n\n\nLiu Hu score\nVader score\n\n\nAnswer: \nQuestion 6",
    "jawaban": "The correct answer is: Vader"
  },
  "066b282dc9925c0ecd97e6aa0b07734d394662c1c5d335a42525cbc7b6d808ae": {
    "soal": "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target biner\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(0, 2, 100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi data latih dan data uji (80% latih, 20% uji)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi dan pelatihan model klasifikasi (misalnya RandomForest)\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n# Prediksi pada data uji\ny_pred = model.predict(X_test)\n# Menghitung Precision, Recall, dan F1-Score\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nprint(f'Precision: ')\nprint(f'Recall: {recall:.2f}')\nprint(f'F1-Score: {f1:.2f}')\n# Alternatif: Menggunakan classification_report untuk ringkasan metrik\nprint('\nClassification Report:')\nprint(classification_report(y_test, y_pred))\nWhich function is used to calculate the F1-Score in scikit-learn?\n{\n~recall_score\n~precision_score\n=f1_score\n~accuracy_score\n~classification_report\n~confusion_matrix\n}",
    "jawaban": "The correct answer is: precision:.2f"
  },
  "22f1edf8356f1961ac8c2fdec943f93f26261c2242bef4d8dc8eaf126b579c18": {
    "soal": "The Periodogram widget can visualize cycle, seasonality, ................... (periodicity), and important periods in time series.\n",
    "jawaban": "The correct answer is: periodicity"
  },
  "96293dad626a57f340d9c0f200025361547b51667ca975c51bf6716726e106d0": {
    "soal": "If\n\n1 AND 1 = 1\n\n1 AND 0 = 0\n\n0 AND 1 = 0\n\n0 AND 0 = 0\n\nThen 111 AND 001 = ......... (binary)",
    "jawaban": "The correct answer is: 001"
  },
  "c19d053b968336cc4ed4aa1fbc4d3d7e2d5596c2a650fd2828b4740cedfcf918": {
    "soal": "To avoid confusion, Orange supports dates and/or times formatted in one of the ISO 9001 formats. For example, the following values are all valid:\n\n2016\n2016-12-27\n2016-12-27 14:20:51\n16:20\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "14cf347fdc0025bbd68656485652abbe6daaa1e2ae6044a1984fb62e307a69c5": {
    "soal": "The Moving Transform widget uses the .......................... function to aggregate values in time series windows. The available options are: mean, sum, max, min, median, mode, standard deviation, variance, product, linearly-weighted moving average, exponential moving average, harmonic mean, geometric mean, non-zero count, cumulative sum, and cumulative product\n",
    "jawaban": "The correct answer is: Aggregation"
  },
  "68e32a6ad443511210f44f25d88bce741f2ef3c78374a95f913f5f0e17dee961": {
    "soal": "111 (binary) = ...... (decimal)",
    "jawaban": "The correct answer is: 7"
  },
  "4eb1898fce77640b80455651572ecdac64b5551ae27d35b93594c470310fc300": {
    "soal": "In the Cross Validation workflow above, the learner not used is,\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answers are: Naive Bayes, AdaBoost"
  },
  "2afccefd6d9fb3616deeebae7269a6ea061ba68d9d752311cfbceb301aff4fa9": {
    "soal": "from google.colab import drive\ndrive.mount('/content/drive')\nAfter mounting, where can you access your Google Drive files in Colab?",
    "jawaban": "The correct answer is: '/content/drive/MyDrive/'"
  },
  "d10e61bb021d17001f5d60225fd621410f22f3898983e7c33dc9ddbe3217b2c6": {
    "soal": "The result of 8 (decimal) AND 2 (decimal) is",
    "jawaban": "The correct answer is: 0"
  },
  "310668e047749805bedeab86a1cfe3b5b1f03e6626613ecf192b30071d5fa636": {
    "soal": "Network Explorer Widget is the main widget for visualizing the network graphically. It displays the graph using Fruchterman-Reingold layout optimization and allows for the adjustment of node color, size, and labels. It is impossible to highlight nodes based on certain properties and output them.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "9db04c159f598b96fffb392785f95c9a599ccdfdf176f16e3f59e8a9563d5e0d": {
    "soal": "What is 'The Pile'?",
    "jawaban": "The correct answer is: A diverse, open-source English text dataset designed for training large language models"
  },
  "9e75aa2e3704c2e81e2be49c4fe06b03fce6997044db3cf55e49208b83e5dbf3": {
    "soal": "model.save('my_model')\nfrom google.colab import files\nfiles.download('my_model')\nnew_model = tf.keras.models.load_model('my_model')\nWhat is the purpose of the files.download('my_model') command?",
    "jawaban": "The correct answer is: To download the saved model to local storage"
  },
  "218b24d2ab122f1335452d9bce9e0591e7bd7fc253943dfe904ee1538b387272": {
    "soal": "In the following example workflow, we show how to quickly visualize the corpus with the Word Cloud widget. We can connect the Word Cloud widget directly to the Corpus widget, but instead, we decide to apply some preprocessing with the Preprocess Text widget. We are working with the book-excerpts.............(1)............. dataset. We can convert all text to lowercase, tokenize (split) text into words only, filter ...............(2)............... English stopwords and select the 100 most frequent tokens.\n\n\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 tab, (2) \u2192 stopword"
  },
  "eb37e9338c4fd796e539b8da6d8acffe5c9a7c26d9735a036d261df7897e2bde": {
    "soal": "Which tool can be used to search for datasets across the internet?",
    "jawaban": "The correct answer is: Google Dataset Search."
  },
  "ed2716067c1cdaef72c81a1e834252e8eb6b95841b26afa3706cd21f521fa208": {
    "soal": "Data attributes in Orange have types discrete, continuous or character ............................ The attribute type is marked by a symbol that appears before the attribute name (D, C, S).\n\n\n\nAnswer: \nQuestion 21",
    "jawaban": "The correct answer is: string"
  },
  "2a3641fb5c19ec2469392ac4f732314869cfa2081cbc6baec6dd6ddda26ed06e": {
    "soal": "Complexity in model output is a challenge for:",
    "jawaban": "The correct answer is: Heuristic Miner"
  },
  "56698346c3dce65ebd34a465e2cf87c96321a219561653a4ad8452ad7b7a8e38": {
    "soal": "In the Correspondence Analysis Widget - Correspondence Analysis (CA) will compute the CA linear transformation of the input data. Although similar to PCA, CA computes the linear transformation on continuous data, not on discrete data.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "c1c286a476a15361b6b02db3d3aec9c8b643b2440f234c8e4d40a6a61e285c62": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=10000)\nIn the IMDB dataset, how is a negative sentiment labeled?",
    "jawaban": "The correct answer is: 0"
  },
  "09ff23df0c0f640e194a3096b3eef191059b098c5c12509301e11124d319aacc": {
    "soal": "In this example, we will just check what a ...........(1)............. model looks like. Load book-excerpts.tab using the Corpus widget and connect it to the Bag of Words widget. Here, we intentionally use the default parameter - the simplest count is term frequency. Check what is output by the Bag of Words widget using the ...............(2)............... widget. The last column represents the term frequency of each document.\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 bag of words, (2) \u2192 Data Table"
  },
  "6b99939b7dc74227979ca375e98f36a3364ce4dddf8ce623dd09f7f2f6e6418f": {
    "soal": "The result of 8 (decimal) OR 0 (decimal) is",
    "jawaban": "The correct answer is: 8"
  },
  "a99ee020538134b784b5da006ee9352d9ddfa2ed019e4c0848387ee51e646c26": {
    "soal": "import pandas as pd\nfile_path = \"/content/drive/My Drive/NamaFolder/NamaFile.xlsx\"\ndf = pd.read_excel(file_path)\nprint(df.head())\nWhich parameter allows specifying the sheet name when reading an Excel file using pandas?",
    "jawaban": "The correct answer is: sheet_name"
  },
  "9571c841a290bb00ec35fab0d480f4557abcd03241149b18f8c34ac11ef2cc22": {
    "soal": "The example workflow below shows the very standard use of the Distance Matrix widget. We calculate the distance between columns in a sample from the Iris dataset and output it into the Distance Matrix. It is not surprising that Iris Virginica and Iris Setosa are the farthest apart.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "8c20642602a2f31cb67ee00cf97073bce6896b831fa6df8d18a66e6f22f4bbde": {
    "soal": "Handling sparse data in the Distances Widget can only be used with Bell, Manhattan, and Cosine metrics.\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "e5e24edd0c3ee71061ec57e185c3afafb09f97c3cceda24e80cd416cb9299c17": {
    "soal": "The Line Plot widget can be used to visualize data profiles (e.g., Time Series).\n",
    "jawaban": "The correct answer is 'True'."
  },
  "ec08bfaecd1b12eb0657087ac5846d1c2fffbf3137c2874037136e0672281f80": {
    "soal": "import numpy as np\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n# Contoh data: nilai aktual dan prediksi\ny_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0]) # Nilai aktual\ny_pred = np.array([1, 0, 1, 0, 0, 1, 0, 1, 1, 0]) # Prediksi model\n# Menghitung confusion matrix\ncm = confusion_matrix(y_true, y_pred)\n# Menampilkan confusion matrix dalam bentuk teks\nprint(\"Confusion Matrix:\n\", cm)\n# Menampilkan confusion matrix dalam bentuk grafik\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.show()\nPerintah mana yang benar untuk mengimpor fungsi yang diperlukan untuk menghitung dan menampilkan confusion matrix dalam scikit-learn?",
    "jawaban": "The correct answer is: from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
  },
  "e80e5a347abab0b385fa606cfa3f5f8c1ad79abc3d037ddfa023021bfbd193aa": {
    "soal": "To group based on similarity, we can measure the distance between images. The numerical representation of the images obtained from the Image Embedding widget can be used to measure the distance using the Distances widget. Typically, cosine distance works best for images, but you can experiment on your own. We then send the distance matrix to Hierarchical Clustering to visualize the similar pair in the dendrogram.\n\nThe result is that all animals are correctly grouped together. In the dendrogram of the clustering results, we do not see the animal images. We can use the Image Viewer widget to view the images.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "41f93f32ba96e947abe0f3bf8fa9701af7d17ede090b51b7dc7adf40ea02ac24": {
    "soal": "Apache Hadoop is an open-source software framework written in \nAnswer Question 33\n for distributed storage and processing of massive data sets on clusters of commodity hardware. All modules in Hadoop are designed under the assumption that hardware failures (individual machines or machine racks) are common and must be automatically handled by the software in the framework.",
    "jawaban": "The correct answer is: Java"
  },
  "6cfaf212f68bffab381f9907798d9305bb38a55e09703067056ddd45afa8956c": {
    "soal": "The following workflow is an example where we compare three (3) classifiers (Naive Bayes, Tree, and Constant) and input them into the Test & Score widget. We are using the Titanic dataset. The Test & Score widget will show the evaluation results for each classifier. Then, we can plot through the Calibration Plot widget and ROC Analysis widget from the output of the ..................... widget for further analysis of the performance of each classifier. The Calibration Plot widget allows us to visualize the predicted accuracy of class probabilities in a plot/image form.\n\n\n",
    "jawaban": "The correct answer is: Test & Score"
  },
  "27060adc34c7dd35f2326902b13aec9cf6d672cd9dbb1384055846e72c24aae8": {
    "soal": "Data Mining is essential for supporting the success of modern organizations driven by data. An IDG survey of 70 IT and business leaders found that 92% of respondents want to implement advanced analytics more widely across their organizations. The same survey found that there are no benefits from data mining.",
    "jawaban": "The correct answer is 'False'."
  },
  "b73b7a958fbbae32cfbd6e15c79a8abce846f76ccadb25f7a806c90719daf8b1": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\n# xes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\nxes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nprocess_tree = pm4py.discover_bpmn_inductive(log)\nbpmn_model = pm4py.convert_to_bpmn(process_tree)\npm4py.view_bpmn(bpmn_model)\nWhich function is used to read the XES file in the provided script?",
    "jawaban": "The correct answer is: read_xes"
  },
  "85337c31eaafb81c932aa511f616730c724bcd88533d249346adc0a2e5b46824": {
    "soal": "A heat map is a graphical method for visualizing attribute values based on class in a two-way matrix. Heat maps can only be used on datasets that contain continuous variables. The values are represented by colors: the higher the value, the darker the color. By combining class and attributes on the x and y axes, we see where which attribute values are strongest and weakest, allowing us to find typical features (discrete) or value ranges (continuous) for each class.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "9b334279098e91ace868abce57d2db0e0c21cfa29fdc0a016859dbf049707d1b": {
    "soal": "To avoid confusion, Orange supports dates and/or times formatted in one of the ISO 31001 formats. For example, the following values are all valid:\n\n2016\n2016-12-27\n2016-12-27 14:20:51\n16:20\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "5b7704afa05663a069cc721127c5df6e9caf4790b9c36c8b63dc05c90d6fb03d": {
    "soal": "Which of the following is a common source of datasets?",
    "jawaban": "The correct answer is: Public repositories like Kaggle."
  },
  "a25c2fb9a231ebe842e0ce810c0ed5020589939ab2a6e663309f75ccf82a85fb": {
    "soal": "When the user provides data to the input, the Corpus widget will convert the data into a corpus. The user can choose which feature to use as the image feature.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "e186431f3fa35e34e713de06495a1e505bcf359a405a0e53d2fc3d657e42c6db": {
    "soal": "The Distances widget calculates the distance between rows or columns in the dataset. By default, the data will be....................... to ensure equal treatment of individual features. ........................... is always done column-wise (using columns as a reference).\n",
    "jawaban": "The correct answer is: normalization"
  },
  "25534f4d822355bf9eac0386e64a5f703be0af8fbf4c8596960273e70accabae": {
    "soal": "Word Cloud data can be created from text files (DECBIN) that we have, as seen in the workflow below. First, the data from the Text Files widget must be segmented into words using the Segment widget. Then, the segmented data output needs to be converted from segmented data into a corpus so it can be processed by the text mining toolbox using the Interchange widget. Before displaying it as a word cloud, it is better to preprocess it first, to reduce unnecessary words like connecting words, etc., using the Preprocess Text widget.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "678499fd3dc5fb6427cbceef5849ea327a5d502283eb363a0141a53cd358867f": {
    "soal": "The programming language used to create Orange3 data mining is primarily\n",
    "jawaban": "The correct answer is: python"
  },
  "7bf6c36ca05e76b3b5d9a53005e389615d217528ab99ec077064560580854045": {
    "soal": "import pandas as pd\nfrom pm4py.objects.conversion.log import converter as log_converter\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom pm4py.visualization.petrinet import visualizer as pn_visualizer\nfrom pm4py.objects.log.util import dataframe_utils\n# csv_filename='Insurance_claims_event_log.csv'\n# csv_filename='Business Travel Permits.csv'\n# csv_filename='Parent-Child Union Output.csv'\n# csv_filename='PM4PY_Dummy_Event_Log_100_Cases_pm4py.csv'\n# csv_filename='PM4Py_Dummy_Event_Log-congested-business-process.csv'\n# 1. Load CSV event log\ndf = pd.read_csv(csv_filename)\n# df = pd.read_csv(csv_filename,sep=';')\n# print(df.head())\n# 2. Format timestamps properly\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n# 3. Ensure column names match PM4Py expectations\ndf = dataframe_utils.convert_timestamp_columns_in_df(df)\n# 4. Convert to event log object\nevent_log = log_converter.apply(df, parameters=)\n# 5. Discover process model using Alpha Miner\nnet, initial_marking, final_marking = alpha_miner.apply(event_log)\n# 6. Visualize Petri Net\ngviz = pn_visualizer.apply(net, initial_marking, final_marking)\npn_visualizer.view(gviz)\nWhich command is used to preview a Petri net in PM4Py?\n{\n~pn_visualizer.draw()\n~pn_visualizer.show()\n=pn_visualizer.view()\n~pn_visualizer.open()\n~pn_visualizer.render()\n~pn_visualizer.display()\n}",
    "jawaban": "The correct answer is: log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case_id'"
  },
  "2856470b9abf2ca22c7c528413d72390eb2889b0403d2d2216ba35964a343c16": {
    "soal": "The following workflow demonstrates the use of the Confusion Matrix widget\n\n\nTest & Score obtains data from File and two learning algorithms from Naive Bayes and Tree. Test & ................. performs cross-validation or other train-and-test procedures to obtain class predictions by both algorithms for all (or some) data instances. The test results are sent to the Confusion Matrix, where we can observe how many instances were misclassified and why.\n\nIn the output, we use the Data Table to show the examples we selected in the confusion matrix. If we, for example, click on Misclassified, the table will contain all the instances that were misclassified by the selected method.\n\nScatter Plot receives two sets of data. From the File widget, it gets the complete data, while the confusion matrix only sends selected data, for instance, misclassified data. The Scatter Plot will display all data, with bold symbols representing the selected data.\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: score"
  },
  "8f9e5603d738193476cc3739346fbc94079efa337e1f22f874e45cd5f96a5e28": {
    "soal": "There are three main challenges faced by businesses with Big Data:\n- Not protecting sensitive and personal information\n- Data rights and ownership\n- Lack of talent (such as data scientists) to analyze data",
    "jawaban": "The correct answer is 'False'."
  },
  "043918830bc7c9913f9f44273574030752f24354a4ec24be51dea36e4c07277d": {
    "soal": "In the `docker-compose.yml` file, what is the purpose of the `restart: unless-stopped` directive?",
    "jawaban": "The correct answer is: To ensure the container restarts automatically unless explicitly stopped"
  },
  "3b75e5d2e6b69dbaca1a987d6e630a9ca62d82aaff4621c0f03667ba4dabd373": {
    "soal": "import os\nimport requests\nfrom googlesearch import search\nfrom bs4 import BeautifulSoup\nimport re\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef clean_filename(text):\n# Hilangkan karakter ilegal untuk nama file\nreturn re.sub(r'[\\/*?:\"<>|]', , text).strip().replace(' ', '_')[:50]<%1\u001f%\u001f0%> def get_page_content(url):\ntry:\nheaders =\nresponse = requests.get(url, headers=headers, timeout=10)\nsoup = BeautifulSoup(response.content, 'html.parser')\ntitle = soup.title.string if soup.title else 'No Title'\nparagraphs = soup.find_all('p')\ncontent = '\n\n'.join([p.get_text() for p in paragraphs[:5]])\nreturn title.strip(), content.strip()\nexcept Exception as e:\nreturn 'Error', f\"Gagal mengambil konten: {e}\"\ndef save_to_txt(keyword, rank, title, url, content, folder='outputs'):\nos.makedirs(folder, exist_ok=True)\nfilename = f\"{clean_filename(keyword)}_{rank}_{clean_filename(title)}.txt\"\nfilepath = os.path.join(folder, filename)\nwith open(filepath, 'w', encoding='utf-8') as f:\nf.write(f\"Keyword : {keyword}\n\")\nf.write(f\"Peringkat : {rank}\n\")\nf.write(f\"Judul : {title}\n\")\nf.write(f\"URL : {url}\n\n\")\nf.write(content)\ndef scrape_and_save_txt(keywords, num_results=5):\nfor keyword in keywords:\nprint(f\"\n\ud83d\udd0d Searching: {keyword}\")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nprint(f\" \u2192 ({i+1}) Fetching: {url}\")\ntitle, content = get_page_content(url)\nsave_to_txt(keyword, i+1, title, url, content)\ntime.sleep(2)\nexcept Exception as e:\nprint(f\"\u274c Error saat mencari '{keyword}': {e}\")\nprint(\"\n\u2705 Semua konten telah disimpan di folder 'outputs/'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\nscrape_and_save_txt(keywords, num_results=5)\nWhat content does the script extract from each visited URL?\n{\n=The title and the first 5 paragraphs of the article.\n~Only the meta description of the page.\n~The entire HTML source code of the page.\n~Only the images present on the page.\n~The last 5 paragraphs of the article.\n~Only the hyperlinks present on the page.\n}",
    "jawaban": "The correct answer is: \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\""
  },
  "68d02bb9e08366eb5a134f7207c241ef02b28a9641364a987fc7f402dc6725fb": {
    "soal": "\n\n\nWorkflow di atas melakukan test / evaluasi CN2 Rule Induction dan Random Tree di Widget Test & Score.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "26cf54faa4afcf725e4a5dbfc7b8e86fff9b7d2d54469db453647f85a98353b1": {
    "soal": "In the following workflow example, a very simple use of the Corpus widget will be shown. Place the Corpus widget onto the canvas and connect it to the ..................... widget. We are using the book-excerpts.tab dataset, which is available in the add-on, and examining it in the Corpus Viewer widget.\n\n\n\n",
    "jawaban": "The correct answer is: Corpus Viewer"
  },
  "7420091dca8afea885855fee039b7720c6fa919728b649b48e09118b2437c908": {
    "soal": "The Periodogram widget can visualize ..................... (cycle), seasonality, periodicity, and important periods in time series.\n",
    "jawaban": "The correct answer is: cycle"
  },
  "16886ad1484eae56748ec27be92a0067d5e001c6c400b48550a6465259128917": {
    "soal": "import pm4py\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\n# Import XES log\nlog = xes_importer.apply(xes_filename)\n# Cek jumlah kasus dan aktivitas\nprint(f\"Jumlah kasus: \")\nactivities = set()\nfor trace in log:\nfor event in trace:\nactivities.add(event['concept:name'])\nprint(f\"Jumlah aktivitas unik: {len(activities)}\")\nfrom pm4py.algo.discovery.dfg import algorithm as dfg_discovery\nfrom pm4py.visualization.dfg import visualizer as dfg_visualization\n# DFG berdasarkan frekuensi\ndfg = dfg_discovery.apply(log,\nvariant=dfg_discovery.Variants.FREQUENCY)\n# Tampilkan visualisasi\ndfg_vis = dfg_visualization.apply(dfg, log=log,\nvariant=dfg_visualization.Variants.FREQUENCY)\ndfg_visualization.view(dfg_vis)\nWhat visualization is used in PM4Py to identify process bottlenecks visually?\n{\n=Performance Spectrum\n-Histogram\n-Scatter Plot\n-Pie Chart\n-Bar Chart\n-Heatmap\n}",
    "jawaban": "The correct answer is: len(log)"
  },
  "eae447144cbe9bb4f85a136d459c18eda78da26863e959a2ce75644cc8590a20": {
    "soal": "Which of the following commands checks if Google Colab is using a GPU?",
    "jawaban": "The correct answers are: if tf.config.list_physical_devices('GPU'):, 'GPU':"
  },
  "3f8825a5e1e1d95585b0f83949e182c3e7654a5ac5d93f39e6df702817ba99f5": {
    "soal": "Data Mining has the power to transform businesses; however, implementing processes that meet the needs of all stakeholders often hinders successful data mining investments \u2014 78% of respondents say they struggle to find the right data mining strategies or solutions.\nDespite these obstacles, businesses that are able to effectively mine widespread data have several key similarities. Successful companies:\n- Know the core needs of their business, both tactical and strategic, that can be met by data mining;\n- Identify and evaluate data sources to be used by data mining tools for accuracy and relevance;\n- Define applications, including Business Intelligence (BI) systems, where data mining tools should be interoperable;\n- Identify available data mining solutions that meet the entire scope of the organization's requirements, from budget to end-user technical capabilities;\n- Use one standard data mining tool that meets the needs of IT, data scientists, and analysts, while also meeting the consumption and visualization needs of business users.",
    "jawaban": "The correct answer is 'True'."
  },
  "b0bf1712f65f33134745b9abb20b03deeb3cf3be4d1e4b723793ef65bac2f526": {
    "soal": "Why is the output added back to the context window?",
    "jawaban": "The correct answer is: To support ongoing interaction"
  },
  "23a41bfd2189991b258c38c12f39f9882221983cf4175331f9ee1455df06ca48": {
    "soal": "10000 (binary) + 1001 (binary) + 100 (binary) + 10 (binary) = ...... (binary)",
    "jawaban": "The correct answer is: 11111"
  },
  "cbf2dc138b61f27d4f0b1c27bcfec946a621b43e3d27d82a87cd0d254a96224d": {
    "soal": "In using the Constant Widget for regression, we use the Constant Widget to create a predictor in Prediction. We use the housing dataset. In Prediction, we can see that the Mean Learner returns one (average) value for all instances.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "a49be6b09a49f71ce64a161a00b7611b8bb857d344809e6176c1c28686c2952b": {
    "soal": "\n\n\nKesalahan klasifikasi (misclassification) di tampilkan dalam bentuk scatter plot dengan membandingkan dua (2) input sekaligus, yaitu total data dan data yang kita pilih di confusion matrix.\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "71d1ca819775c375265599f9becdf6164564dde6dcf492370c177825b114ab3e": {
    "soal": "A heat map is a graphical method for visualizing attribute values based on class in a two-way matrix. Heat maps can only be used on datasets that contain ..................... variables. The values are represented by colors: the higher the value, the darker the color. By combining class and attributes on the x and y axes, we see where which attribute values are strongest and weakest, allowing us to find typical features (discrete) or value ranges (continuous) for each class.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: continuous"
  },
  "cf63983f2e2c0f993d5197e8e95646312ec8d1284dbc066a9ae1d5b2ef0df827": {
    "soal": "Inductive Miner constructs models by:",
    "jawaban": "The correct answer is: Recursively splitting event logs"
  },
  "01c4888301ad999145c5ba4bc5eaa55a9e7e49a91eb3e8915c2deb4f2a23684c": {
    "soal": "What is web scraping in the context of data collection?",
    "jawaban": "The correct answer is: The process of extracting data from websites."
  },
  "35e55464d1ba5b717d58cc20ffa2b5a6f88e016fe8758667132656006e2fb4ca": {
    "soal": "from sklearn.model_selection import train_test_split\nimport pandas as pd\n# Contoh: Membuat DataFrame dengan fitur 'X' dan target 'y'\ndata =\ndf = pd.DataFrame(data)\n# Memisahkan fitur dan target\nX = df[['X1', 'X2']]\ny = df['y']\n# Membagi dataset menjadi data pelatihan dan pengujian dengan stratifikasi\nX_train, X_test, y_train, y_test = train_test_split(\nX, y, test_size=0.3, stratify=y, random_state=42\n)\n# Menampilkan proporsi kelas di setiap subset\nprint(\"Proporsi kelas di y_train:\n\", y_train.value_counts(normalize=True))\nprint(\"\nProporsi kelas di y_test:\n\", y_test.value_counts(normalize=True))\nWhat does the `test_size=0.3` parameter specify in the `train_test_split` function?\n{\n~That 30% of the features are used for testing.\n~That the test set will contain 30 samples.\n=That 30% of the dataset is allocated for the test set.\n~That the training set will contain 30 samples.\n~That the test set will have a fixed size of 30 units.\n~That 70% of the dataset is allocated for the test set.\n}",
    "jawaban": "The correct answer is: 'X1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'X2': [11, 12, 13, 14, 15, 16, 17, 18, 19, 20], 'y': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]"
  },
  "ae88f923f93f55dc2148d97c889cf590e723af2dafada28182081605ae041527": {
    "soal": "In the workflow below, the Tree widget will train a model and evaluate it against .................... The accuracy comparison between the Tree widget and Logistic Regression widget is done through the Test & Score widget. The results from the Test & Score widget can be viewed through the Confusion Matrix widget to compare the classification errors that occurred.\n\n\n\n",
    "jawaban": "The correct answer is: Logistic Regression"
  },
  "dbcfc4b6fd91d4ddf6b9d31439ed3014a31b84992a727d2b3c71e90ecd30ba44": {
    "soal": "The Line Plot widget can be used to regress data profiles (e.g., Time Series).\n",
    "jawaban": "The correct answer is 'False'."
  },
  "eb5ca383c6c95bfc3e42258827dbf34028cfd3353d78f00130c5800cdfe6b83b": {
    "soal": "Data does not always come in the form of neat tables. Data can also be in the form of text, audio recordings, video content, or even images. However, computers can only work with numbers, so for any data mining, we need to transform that unstructured data into a vector representation.\n\nTo extract numbers from unstructured data, Orange can use a deep network embedder. Orange has just started incorporating various embedders in Orange in the widget ............................., and currently, they are available for text and images.\n",
    "jawaban": "The correct answer is: Image Embedding"
  },
  "60b444c3f7e18201ab58753f8c888b1988d7f33cb39167ff05f9cb84fa39891b": {
    "soal": "Dalam contoh klasifikasi tipikal, kita akan menggunakan widget Contant untuk membandingkan skor algoritma pembelajaran lainnya (seperti kNN) dengan skor default. Gunakan dataset irisan dan connect ke Test & Score. Kemudian hubungkan Constant dan kNN ke Test & Score dan amati seberapa baik kinerja kNN terhadap baseline Constant.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "183074807d85721e76b6cdc8fcc894626ebda4f947461a16faec9d859044543e": {
    "soal": "Word Cloud data can be built from the text file (ASCII) we have. First, the data from the text file must be segmented into words. Then, the output segmented word data needs to be converted from segmented data into a corpus so it can be processed by the text mining toolbox. Before displaying it as a word cloud, it is advisable to perform preprocessing first to remove unnecessary words, such as conjunctions, etc.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "c03aaa3525cce35e1e67752d0bb6baee47c2276c2d32cc8250d8a383644e50ee": {
    "soal": "What is the role of backpropagation in training a neural network?",
    "jawaban": "The correct answer is: To compute gradients for updating the network's weights."
  },
  "190608c306868219aa0aeeefff98d254b601f37532e63a0e3d97fe0cc4817cd7": {
    "soal": "import os\nimport requests\nfrom googlesearch import search\nfrom bs4 import BeautifulSoup\nimport re\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef clean_filename(text):\n# Hilangkan karakter ilegal untuk nama file\nreturn re.sub(r'[\\/*?:\"<>|]', , text).strip().replace(' ', '_')[:50]<%1\u001f%\u001f0%> def get_page_content(url):\ntry:\nheaders =\nresponse = requests.get(url, headers=headers, timeout=10)\nsoup = BeautifulSoup(response.content, 'html.parser')\ntitle = soup.title.string if soup.title else 'No Title'\nparagraphs = soup.find_all('p')\ncontent = '\n\n'.join([p.get_text() for p in paragraphs[:5]])\nreturn title.strip(), content.strip()\nexcept Exception as e:\nreturn 'Error', f\"Gagal mengambil konten: {e}\"\ndef save_to_txt(keyword, rank, title, url, content, folder='outputs'):\nos.makedirs(folder, exist_ok=True)\nfilename = f\"{clean_filename(keyword)}_{rank}_{clean_filename(title)}.txt\"\nfilepath = os.path.join(folder, filename)\nwith open(filepath, 'w', encoding='utf-8') as f:\nf.write(f\"Keyword : {keyword}\n\")\nf.write(f\"Peringkat : {rank}\n\")\nf.write(f\"Judul : {title}\n\")\nf.write(f\"URL : {url}\n\n\")\nf.write(content)\ndef scrape_and_save_txt(keywords, num_results=5):\nfor keyword in keywords:\nprint(f\"\n\ud83d\udd0d Searching: {keyword}\")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nprint(f\" \u2192 ({i+1}) Fetching: {url}\")\ntitle, content = get_page_content(url)\nsave_to_txt(keyword, i+1, title, url, content)\ntime.sleep(2)\nexcept Exception as e:\nprint(f\"\u274c Error saat mencari '{keyword}': {e}\")\nprint(\"\n\u2705 Semua konten telah disimpan di folder 'outputs/'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\nscrape_and_save_txt(keywords, num_results=5)\nWhich file does the script read to obtain the list of search keywords?\n{\n=keywords.txt\n~input.csv\n~search_terms.json\n~queries.xlsx\n~terms_list.txt\n~search_keywords.dat\n}",
    "jawaban": "The correct answer is: \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\""
  },
  "2a391f1bf1d421daaf53cc21218ca9c3a263816569fb0ab0a261cf1656d4ce4a": {
    "soal": "Which library is used to import the MNIST dataset?",
    "jawaban": "The correct answer is: tensorflow.keras"
  },
  "759fa2a8577694e1f56ecdaec30807dc97585ad4eb913c9cb99973327e966207": {
    "soal": "In this example, we will just check what a bag of words model looks like. Load book-excerpts.tab using the Corpus widget and connect it to the Bag of Words widget. Here we intentionally use the default parameters - the simplest count is term frequency. Check what the Bag of Words widget outputs using the Data Table widget. The last column represents the term bandwidth for each document.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "67ee5f8a02ad2aa04dc7de9e380a99caf8842544435dbf7a36f1c41aa6479a56": {
    "soal": "The Import Images widget allows us to import image (images) from ............................\n",
    "jawaban": "The correct answer is: directory"
  },
  "792737f8c29033ae4ea122dced10caa43be6c4ee594a7a37c779ab84ef68520e": {
    "soal": "The Distance Transformation widget is used for normalization and inversion of ........................... Normalizing the data is required to bring all variables into proportion with one another.\n",
    "jawaban": "The correct answer is: distance matrix"
  },
  "f87309fc4fd1f1569cdd0c3eebd48ab32537850ddd6ad2060def7dbbe30f2121": {
    "soal": "In the following workflow example, we will show the very simple use of the Corpus widget. Place the Corpus widget on the canvas and connect it to the Corpus Viewer widget. We use the dataset book-excerpts.tab, which is available in the add-on, and examine it in the Corpus Clustering widget.\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "490fc0c683f738f165db5274065f38dc079ebbaadc7671de9a0a13b40e2a856b": {
    "soal": "In the ............................... widget, we can do several things, such as\n\nConvert all letters to lowercase.\nRemove (stop words), words that are less useful like conjunctions such as and, in, to, from, etc.\nSet preprocessing for stopwords in Indonesian.\nRemove HTML tags\nRemove URLs\n\n",
    "jawaban": "The correct answer is: Preprocess Text"
  },
  "07e6d08dbd05001e035e0a8ad01c97dc92c968f02ba888b15003d09d945cca43": {
    "soal": "What does conformance checking aim to achieve in process mining?",
    "jawaban": "The correct answer is: Compare event logs with existing process models to identify deviations"
  },
  "bfdf09cb8bb156d88b871a3ae4d6d3fc1954edde34756b23fdb61cc16047bf00": {
    "soal": "Which of the following is a characteristic of a well-structured dataset?",
    "jawaban": "The correct answer is: Consistent formatting across all entries."
  },
  "aed1e148404b740c7d908cf4d941eddba723146957c6b830a1aa0d302ce37994": {
    "soal": "If the Import Documents widget fails to read a particular file for one reason or another, that file will be skipped. The successfully read files will be sent to the output.\n",
    "jawaban": "The correct answer is: tidak berhasil"
  },
  "e8819083a39ab361576ff0c51d0e71c2a853de9bb99b20720e74c1450a5f1066": {
    "soal": "The Venn Diagram widget can plot a Venn diagram for one or more data subsets.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "855ed5d7d21936a9263864c4d5c5642e00a0826cdfdf7625473f84b8bbbfeab8": {
    "soal": "In the following workflow example, a very simple use of the ................... widget will be shown. Place the Corpus widget onto the canvas and connect it to the Corpus Viewer widget. We are using the book-excerpts.tab dataset, which is available in the add-on, and examining it in the Corpus Viewer widget.\n\n\n\n",
    "jawaban": "The correct answer is: Corpus"
  },
  "1565edcfb6f226897428e169b35970d61990f6f83297b215339a20cbb23b0a1d": {
    "soal": "In the ORANGE example below, we can predict ..................... a text. The classification model is obtained using the logistic regression learner with training data from the Corpus Grimms. The text prediction from Corpus Andersen is performed by the Predictions widget.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: classification"
  },
  "da7841cb08d4b8715d8e0875973c76664d89ea4bad8d113dea2a40daf5445cc0": {
    "soal": "Distances are most commonly calculated between instances (\"rows\" in the Distances widget) or attributes (\"columns\" in the Distances widget). The only compatible input for the Distance Map is the Distances widget. In the output, the user can select the map region, and the Distance Map widget will display the corresponding instances or attributes.",
    "jawaban": "The correct answer is 'True'."
  },
  "2e0e3cb1f827aa7e6dc1bdfc3d22ef56e165ebfce892149ee80cf33a9bdf8659": {
    "soal": "In the workflow below, the Iris data from the File widget is entered into the Select Columns widget, where we choose to display only two attributes (i.e. flower petal width and flower petal length). Then, we can see the original dataset and the dataset with ..............(1)............. that were selected in the .................(2)................ widget.\n\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 column, (2) \u2192 Data Table"
  },
  "509b6dce8a67ce953e32ae4e671f61a72afd464fdf5c31924045ae119affd6f4": {
    "soal": "..................... was introduced/found by Yahoo! in 2006\n",
    "jawaban": "The correct answer is: hadoop"
  },
  "7a70c213476cc55d734dd97ad2b6435f31e53c8641af6bd3f4e5b4efce33d06a": {
    "soal": "Which of the following is used to check for TPU availability in Colab?",
    "jawaban": "The correct answer is: tf.distribute.cluster_resolver.TPUClusterResolver()"
  },
  "57f4c576fad098cddbca68071130d52c11a368228362bd69bbbb30c759574865": {
    "soal": "Widget Data Info is a simple widget that presents information about dataset size, feature, target, meta attributes, and location.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "1fe4d3177f063f09e885a8a868df6c851cdf4ffb6fa04f056a4e8e855551d417": {
    "soal": "The Venn Diagram widget can also be used to explore different prediction models. In the following example, we analyze 3 prediction methods, namely Naive Bayes, SVM, and Random Forest, according to their classification errors on data instances.\n\nBy selecting misclassification in three Confusion Matrix widgets and sending them to the Venn diagram widget, we can see all misclassification examples visualized per method used. Then we open the Venn Diagram widget and select, for example, misclassified instances identified by all three methods. This is represented as the intersection of the three circles. Click on the intersection to see these two examples marked in the Scatter Plot widget. Try selecting different parts of the diagram to see how the scatter plot visualization changes.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "92bf9d3bdb8d41d36404af7526ca48a52eed5af113bdd5a664408b0d34cfab80": {
    "soal": "The easiest way to use the Venn Diagram Widget is to select a data subset and find matching examples in the visualization. We use the breast-cancer dataset to select two subsets with the Select Rows widget \u2013 the first subset is breast cancer patients aged between 40 and 49 years, and the second subset is patients with tumor sizes between 20 and 29. The Venn Diagram helps us find examples that meet both criteria, which can be found at the intersection of the two circles.\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "6eb4c41f64a0351b74a457042baeceba0ce4a1fac87260aaf7c75ead960a6680": {
    "soal": "Widget Calibrated Learner menghasilkan sebuah model yang mengkalibrasi distribusi dari class statistics dan meng-optimasi decision threshold.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "22b45273c950cc6abb85f715c12a42a7342bb64d4a633f6dae9979682c3799a6": {
    "soal": "In which scenario is unsupervised learning typically used?",
    "jawaban": "The correct answer is: Grouping customers based on purchasing behavior"
  },
  "77c4d40231e80aa6688092e07ac9d5af0d68840ee6e2485e2f62d4ef2ba41a20": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nplt.imshow(x_train[0], cmap='gray')\nplt.title(f\"Label: \")\nplt.show()\nmodel = keras.Sequential([\nkeras.layers.Flatten(input_shape=(28, 28)),\nkeras.layers.Dense(128, activation='relu'),\nkeras.layers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nWhat color map is used when displaying the image?\n{\n= gray\n~ red\n~ blue\n~ hsv\n~ rainbow\n~ none\n}",
    "jawaban": "The correct answer is: y_train[0]"
  },
  "b7feddb8c9d9e6f7a1d9261f6eea5eee75a1ac040700d97898b31d9f3b03b99e": {
    "soal": "High variability in event data is best managed by:",
    "jawaban": "The correct answer is: Heuristic Miner"
  },
  "1273e639cc79b59571197b45a67666bb32ba3aebd8417ad667028dbf1894cc1f": {
    "soal": "Widget Select Columns can automatically select data attributes and composition of the data domain.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "ada701ba75c2975405bccc5ac76556487de2831d56dc00056775ecf6d9e19103": {
    "soal": "The Confusion Matrix widget provides the count/proportion of instances between predicted and ................. class. Selecting elements within the matrix will provide the corresponding instances to the output. In this way, we can observe which specific instances were misclassified and how.\n",
    "jawaban": "The correct answer is: actual"
  },
  "e0563e97574aa0f4733f0bf7456ed343b9ac5a343d635303a23e5b380e2a1522": {
    "soal": "\n\n\nThe most popular programming language widely used to support data science is Python.",
    "jawaban": "The correct answer is 'True'."
  },
  "85df42c2a55b2d120f7668ed9725c90ec9e3b2a59675edffaf27305fd611d120": {
    "soal": "The Data Info widget is a simple widget that presents information about ................. dataset, features, target, meta attributes, and location.\n",
    "jawaban": "The correct answer is: size"
  },
  "e0499ddb57d9db23ec735500a9d29b3adfd9c1043beb7ae021d619b3eeefa928": {
    "soal": "Which function in scikit-learn is used to split datasets into training and testing sets?",
    "jawaban": "The correct answer is: train_test_split"
  },
  "b26b40e021972e86cd0b7d0abf7b28f58c19a656027262453adbfd80e057410f": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#_xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nmap = pm4py.discover_heuristics_net(log)\npm4py.view_heuristics_net(map)\nHow does the script indicate multiple filenames?",
    "jawaban": "The correct answer is: Using comments -Using parentheses -Using brackets -Using quotes -Using slashes -Using hyphens"
  },
  "8743d5f84caed836fac821e05ef2dfb858b5bebd4a3d2aadef3392996b606551": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nIn a neural network model for classifying digits 0-9, how should the output layer be configured?",
    "jawaban": "The correct answer is: With 10 neurons corresponding to each digit"
  },
  "52cccf04425088075bdea0d4e8bff78ddcb0f1c114682bd5f83a07001e4ca2e7": {
    "soal": "Orange can import comma- or tab-delimited data files, or original Excel or Google Sheets documents. Use the File widget to load data and, if necessary, specify the ...... attribute.\n\n\nSelect ..... below,\n",
    "jawaban": "The correct answers are: class, meta"
  },
  "7fe6582adadef144517e29720d3b9ef55f6432015b07425e344cad71d7563742": {
    "soal": "Data attributes in Orange have types discrete, continuous or character string. The attribute type is marked by a symbol that appears before the attribute name (..................., C, S).\n\n\n\nAnswer: \nQuestion 4",
    "jawaban": "The correct answer is: D"
  },
  "c48898aa155d7674fc0a9957871c20c56346156f1a49fc26e772b01b126381e3": {
    "soal": "In the workflow below, we use the Zoo dataset. We load data into the Scatter Plot widget, which allows us to select a subset of data instances. Then, we can send the selected data instances to the Save Data widget to save them into a file.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "a58d3f5a5164f49c6933c37ab7c87482756c114f480a28f4198ed32ad7de4716": {
    "soal": "Principal Component Analysis (PCA) transforms data into a dataset with uncorrelated variables, also called principal components. The ................. widget displays a graph (scree diagram) that shows the amount of variance explained by the best principal components and allows you to interactively adjust the number of components to include in the output dataset. In this workflow, we can observe the transformation in the Data Table and in the Scatter Plot.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: PCA"
  },
  "09fe503d2650b656ee143e7d918ff55a6163fa0cf337fa6a0c5cb29d6f5c06bc": {
    "soal": "The Data Info widget is a simple widget that presents information about the dataset size, features, target, meta attributes, and .............................\n\n\n",
    "jawaban": "The correct answer is: location"
  },
  "59f46d4d7eae8293685da0b7d60cc303615d622d4646a253e1dfdc809f14af1a": {
    "soal": "111 (binary) = ..... (decimal)",
    "jawaban": "The correct answer is: 7"
  },
  "0e640c6cd1ed3e25bf555a3d7e0ddc11c1d7d7dbd1ddeea014d5ba257f1d54c8": {
    "soal": "Principal Component Analysis (PCA) transforms data into a dataset with uncorrelated variables, also called principal components. The PCA widget displays a graph (scree diagram) that shows the amount of variance explained by the best principal components and allows you to interactively adjust the number of components to include in the output dataset. In this workflow, we can observe the transformation in the Data Table and in the ................ Plot.\n\n\n\n\n",
    "jawaban": "The correct answer is: Scatter"
  },
  "1abe68043700319ed52be92f93f8bdc3f277e25d8b4f0bae3070c001490c96db": {
    "soal": "Most visualizations in ........................ are interactive. In the workflow below, the Scatter Plot Widget for example. Double-click the icon to open it and click-and-drag to select several data points from the plot. The selected data will automatically enter the Data Table Widget. Double-click to check which data was selected. Change the selection and observe the changes in the Data Table Widget.\n\n\n\n",
    "jawaban": "The correct answer is: Orange"
  },
  "956eb92b8f9d598704f98657f2566059f03271c57d9da945a1e72a4c0dc86115": {
    "soal": "101 (binary) = ...... (decimal)",
    "jawaban": "The correct answer is: 5"
  },
  "a3fdf03c29a70aab11bc8b682cadf3f23764040d08501b1e111d2279fa8014c7": {
    "soal": "Which miner has the simplest underlying principle?",
    "jawaban": "The correct answer is: Alpha Miner"
  },
  "b5a0256b7e4bcbcee45d839101da00b28b1c1951c766d4ddf23dcd8c444f5ce9": {
    "soal": "What should you do if prompted when accessing Open-WebUI for the first time?",
    "jawaban": "The correct answer is: Create an admin account"
  },
  "b3bdcacd1ac4a4d95e416d8d515085006ac4d5fa4d2e9e1133263fc08fb21e46": {
    "soal": "Which model is mentioned as part of the LLM family?",
    "jawaban": "The correct answer is: Gemini"
  },
  "8448acc5f79d4dc07c46e689fc185cd593d88418dc32a31fd13e5d4db1cfae72": {
    "soal": "Widget Calibrated Learner menghasilkan sebuah model yang mengkalibrasi distribusi dari class probabilities dan meng-optimasi ..................\u00a0 threshold. Widget ini hanya bekerja untuk binary classification task saja.\n",
    "jawaban": "The correct answer is: decision"
  },
  "a2b42a9982398f720f4ac43768818db847f3a878c34734e68eabb93bb24bab3e": {
    "soal": "What is the purpose of monitoring a model for drift after deployment?",
    "jawaban": "The correct answer is: To identify changes in model performance due to evolving data patterns."
  },
  "359d64e7b06888a27e055c83fc396dc0820d24c4214ff6ddf4be215d02210702": {
    "soal": "In the Periodogram widget, the periodogram for equispaced series is calculated using the Lomb-Scargle method.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "0355b60b4fb367350ea6c810d5d254b43b8196e724ff89f266d1285ccb8d619d": {
    "soal": "When there is no data in the input, the Corpus widget reads .................. corpus from a file and sends the corpus instance to the output channel. The history of the last opened file is stored in the Corpus widget. In the Corpus widget, there is also a directory with sample corpora pre-installed with the add-on. The widget can read data from Excel (.xlsx), comma-separated (.csv), and tab-delimited (.tab).\n\n",
    "jawaban": "The correct answer is: text"
  },
  "77d2b2e1fa8bd285b54ad71e7516f5c4603c7fe1ed623c45cbb18521880ffc1d": {
    "soal": "There are three main challenges faced by businesses with Big Data:\n- Protecting sensitive and personal information\n- Data rights and ownership\n- Lack of talent (such as data scientists) to analyze data",
    "jawaban": "The correct answer is 'True'."
  },
  "9d1336d5f2f0895264e7ca1c9cf218e6233d6ff7688284f82ffea6bd8a1b742e": {
    "soal": "In the following workflow example, we will show the very simple use of the Corpus widget. Place the Corpus widget on the canvas and connect it to the Corpus Viewer widget. We use the dataset book-excerpts.png, which is available in the add-on, and examine it in the Corpus Viewer widget.\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "7f6f87e01c3ea1129609bc16c73b7cbdb0fed3d0231a02fc6c5423795d05461b": {
    "soal": "We use the zoo dataset in combination with Hierarchical Clustering to find visitor groups. Now we have the clusters that we want to find, and what is significant for each cluster! Give the cluster to the Box Plot and use \u2018Order by relevance\u2019 to find what defines the cluster. It seems they are separated by type, even though the grouping is done without any class labels! This is an example of unsupervised learning.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "7981f98de346fd7b309f0c343cad0e247b698a0bd332ecb458e570fefce830fe": {
    "soal": "Another alternative to the hash notation format is Orange's native data format with three (3) header rows: the first with attribute names, the second specifying the type (continuous, discrete, time, binary, or string), and the third row providing information about the attribute roles (class, meta, weight, or ignore).\n",
    "jawaban": "The correct answer is 'False'."
  },
  "c94be9111dccc78d011aea126a22101d946b12b4f6edb4b54adcee8270260e8d": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(2, size=100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi training (80%) dan testing (20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model, misalnya RandomForestClassifier\nmodel = RandomForestClassifier(random_state=42)\n# Melatih model pada data training\nmodel.fit(X_train, y_train)\n# Memprediksi data testing\ny_pred = model.predict(X_test)\n# Mengevaluasi akurasi model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model pada data testing: ')\nAfter training, which function is used to make predictions on the test data?\n{\n~model.evaluate\n~model.transform\n~model.fit_predict\n=model.predict\n~model.assess\n~model.analyze\n}",
    "jawaban": "The correct answer is: accuracy:.2f"
  },
  "d91d1fe07cb512d9aedfb3ebb1849f01fa088317c66026f4b3e2745a6f4ff191": {
    "soal": "from google.colab import drive\ndrive.mount('/content/drive')\nWhat is the primary purpose of the drive.mount() function in Google Colab?",
    "jawaban": "The correct answer is: To access and interact with files stored in your Google Drive."
  },
  "d3a7bc3e4e0ca252d804c6775be8d8a2cc2c2a4bc356b224ea2ddacad42cc365": {
    "soal": "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target biner\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(0, 2, 100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi data latih dan data uji (80% latih, 20% uji)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi dan pelatihan model klasifikasi (misalnya RandomForest)\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n# Prediksi pada data uji\ny_pred = model.predict(X_test)\n# Menghitung Precision, Recall, dan F1-Score\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nprint(f'Precision: ')\nprint(f'Recall: {recall:.2f}')\nprint(f'F1-Score: {f1:.2f}')\n# Alternatif: Menggunakan classification_report untuk ringkasan metrik\nprint('\nClassification Report:')\nprint(classification_report(y_test, y_pred))\nIf `y = np.random.randint(0, 2, 100)`, what is the shape of `y`?\n{\n~(100, 2)\n~(100, 1)\n=(100,)\n~(1, 100)\n~(2, 100)\n~(100, 100)\n}",
    "jawaban": "The correct answer is: precision:.2f"
  },
  "c1fa62a82e8b45b4420f78535992ac4bb2232115c14ab6a9c668d2295fb0306a": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nplt.imshow(x_train[0], cmap='gray')\nplt.title(f\"Label: \")\nplt.show()\nmodel = keras.Sequential([\nkeras.layers.Flatten(input_shape=(28, 28)),\nkeras.layers.Dense(128, activation='relu'),\nkeras.layers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nWhich metric is monitored during training?\n{\n= accuracy\n~ precision\n~ recall\n~ f1-score\n~ AUC\n~ loss_only\n}",
    "jawaban": "The correct answer is: y_train[0]"
  },
  "604ef99f5e914e08461f8c435d29dbf2a953a5536b5ecbfa0c30f97f909090f2": {
    "soal": "Which of the following is an example of supervised learning?",
    "jawaban": "The correct answer is: Regression"
  },
  "7df9e2a85f050d2c2c524bd270e4e6ac86b58ca7b5b906a94dd5264881f72037": {
    "soal": "The ...................... widget uses the ..................... algorithm that will find the k nearest training instances in the feature space and use the average of those nearest features to predict.\n",
    "jawaban": "The correct answer is: kNN"
  },
  "b5101777361c96b25a594dd7fd25675ae4278abf1d4dd16af66c5f964d81f374": {
    "soal": "What is the purpose of the learning rate in gradient descent?",
    "jawaban": "The correct answer is: To control the step size at each iteration while moving toward a minimum of the loss function"
  },
  "ed5908ce4845512b26635c9f6743717accff8fa0fe8bd2116e1f326ec6ad60e5": {
    "soal": "The Predictions widget shows the model's predictions against the data.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "3d43fb681c03182f0040898936816deee3ef8d4098a2befdab32c5faabd84941": {
    "soal": "The Test & Score widget will test the learning algorithm. Different sampling schemes are available, including using separate test results. The widget does two things. First, it will display a table with performance metrics for different classifiers, such as classification accuracy and area under the curve. Second, the evaluation output can be used by other widgets to analyze classifier performance, such as ROC Analysis or Confusion Matrix.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "74e18ba663cc550a4e6d9988a420f97ce177862fbf8f3049ef32d67bde80a5c8": {
    "soal": "101 (binary) + 10 (binary) = ..............................\u00a0 (binary)\n",
    "jawaban": "The correct answer is: 111"
  },
  "bb6a639514e587a6771842fb76b5621ff99b6e77868c04b28516524e975d6ee2": {
    "soal": "Which command installs 'python3-pip' on Ubuntu 24.04?",
    "jawaban": "The correct answer is: sudo apt install python3-pip"
  },
  "60f87f89c54b730fb41fad9fd9a44545845e4b4c3bd72e9123acb34340f5950b": {
    "soal": "Which component of the Hadoop ecosystem is responsible for resource management?",
    "jawaban": "The correct answer is: YARN"
  },
  "0fd5a88a623e51659d9013f102276bafe08a3dd70e76baedd41d3df64df3c00a": {
    "soal": "Which GitHub repository provides a collection of public datasets to address various cybersecurity issues using Machine Learning or other methods?",
    "jawaban": "The correct answer is: Real-CyberSecurity-Datasets"
  },
  "2871e656a874c2e55d1a9ed2520cc3366dbbd1c11642f8ea0f8af9fc597c8155": {
    "soal": "Widget Save Data can save data to a stream.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "c392a9f7fe671874c2aa42902132c6a8e1bb6d7ede74428fcc6add90c9f5d4dc": {
    "soal": "Algoritma CN2 adalah teknik regresi yang di rancang agar secara effisien melakukan induksi dari rules yang simple & komprehesif dalam bentuk \u201cif cond then predict class\u201d, meskipun dalam domain dimana mungkin ada noise.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "8d19d07a6ed64bfe0dd033b4d5794f741c25848478f192e050c05369cb0624fa": {
    "soal": "Orange3 installation for data mining on Ubuntu 18.03 can use the command\n",
    "jawaban": "The correct answer is: pip3 install orange3"
  },
  "4e1f8b24f9568bb5fa1b07a510a625edad41bb2436602dc697bfd23673edc216": {
    "soal": "Sebagai contoh penggunaan kNN untuk klasifikasi kita mengggunakan dataset iris. Kita bandingkan hasil dari k-Nearest Neighbors dengan default model Constant, yang akan memprediksi class ............. Tampak kNN lebih baik.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: mayoritas"
  },
  "9a9e091ce51a2f96a2e6db3e792fa9ef8c980721572b1b6250ba1f9ce36cc08c": {
    "soal": "The Calibration Plot widget can show the match between predicted probability by the classifier and the calculated class probability.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "00f91b87263c5cd6e37c841b182721092ef031f344944ed3374c030e7dca6636": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#-xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\ndfg, start_activities, end_activities = pm4py.discover_dfg(log)\npm4py.view_dfg(dfg, start_activities, end_activities)\nWhich Python conditional structure checks if the script runs directly?",
    "jawaban": "The correct answer is: if __name__"
  },
  "23c2e441c891a51244ff0152c44880f9d43cfcd067bdb528df5dc224db1502be": {
    "soal": "How good is the supervised data mining method for classifying our dataset? Below is a workflow that evaluates various classification techniques on a dataset (the example here is iris). The main widget used here is the Predictions widget, which receives the data and a learner set, performs cross-validation, calculates prediction accuracy, and generates scores for further inspection.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "f256d26d4f7bc2a56733ba2a4adf2015806bdd51f6424644c6fb1b6551462eba": {
    "soal": "\n\n\nThe workflow above can be used to perform item clustering based on purchases.\n\n\nThe distance information between items is obtained due to the date information. Items purchased together on the same date will result in the distance between those items becoming closer. This is often referred to as association in the machine learning world.",
    "jawaban": "The correct answer is 'True'."
  },
  "1d5f9e33460ca67227c5943e2c6f7f3db41c5109261f2a13c9d4f5690fca9572": {
    "soal": "Data does not always come in the form of neat tables. Data can also be in the form of text, audio recordings, video content, or even images. However, computers can only work with numbers, so for any data mining, we need to transform that unstructured data into a vector representation.\n\nTo extract numbers from unstructured data, Orange can use a deep network embedder. Orange has just started incorporating various embedders in Orange in the Image Embedding widget, and currently, they are available for text and images.\n",
    "jawaban": "The correct answer is: embedder"
  },
  "430475f15d8e087cb617ef7df191ffa4b336a5755c087f243fbd0652f29d6abc": {
    "soal": "Widget Calibrated Learner membungkus / melanjutkan kerja dari learner lain dengan ................... calibration dan decision threshold optimization.\n",
    "jawaban": "The correct answer is: probability"
  },
  "7b2322fd77f6b9b22f7eef45d560c9d66dbbe28179e9ca0a2261dbdec2ca88e7": {
    "soal": "The Heat Map widget can plot a heat map for a pair of attributes.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "07d052af3a708b64deccaa18415a464ac8c0fb0518e0d3a969c2d79122e54783": {
    "soal": "What enables LLMs to generate coherent responses?",
    "jawaban": "The correct answer is: Understanding context and structure"
  },
  "85054d68e9b1d2c4dc62af9df66f1e2248624203cee1cc29a5c1f4676463ada2": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\n# xes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\nxes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nprocess_tree = pm4py.discover_bpmn_inductive(log)\nbpmn_model = pm4py.convert_to_bpmn(process_tree)\npm4py.view_bpmn(bpmn_model)\nThe function 'convert_to_bpmn' takes what as its input?",
    "jawaban": "The correct answer is: process_tree"
  },
  "80d63f8b627320c653e583ef378fb3101957e30d9e0e34fc575a750dc6953528": {
    "soal": "10.150.10.1 AND 255.0.0.0 = ..... ?",
    "jawaban": "The correct answer is: 10.0.0.0"
  },
  "871996fb094865fb81b5b30ea8eb3df485e257adc608df631bddc73bba3bbbee": {
    "soal": "import tensorflow as tf\nfrom tensorflow.keras import layers, models\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nmodel = models.Sequential([\nlayers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(64, (3,3), activation='relu'),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(128, (3,3), activation='relu'),\nlayers.Flatten(),\nlayers.Dense(128, activation='relu'),\nlayers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5, batch_size=64)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nWhat is the purpose of dividing the dataset by 255.0?",
    "jawaban": "The correct answer is: Normalization of data"
  },
  "73b748e8511571b169f10971bc7f183c9dc9456c5485bf75ce46c3bf5dc489e8": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nWhat is the default activation function applied in a `Dense` layer if not specified?",
    "jawaban": "The correct answer is: Linear (no activation function)"
  },
  "9a7927e161d4f89dc0e9aa8e42bfef3dcf6ddd5cf8b1580f83a2807ea4d3fbe3": {
    "soal": "The Import Images widget is probably the first widget we will use in image analysis. The Import Images widget loads images and creates a class value from the folder. In this example, we use the Import Images widget to load 26 paintings by Monet or Manet.\n\nWe can observe the results in the Data Table widget. It is clear how Orange adds an additional class attribute with values Monet and Manet.\n\nNext, we can continue with standard machine learning methods. We send the images to the Image Embedding widget, where we will use the Painters embedder to get the image vector.\n\nThen we will use the Test & Score widget and the Logistic Regression widget to create a model to predict the artist of a painting (in this case Monet or Manet). We get a very good score. How could that be? It turns out these images were already trained on the Painters embedder, so the accuracy achieved will be high.\n\n\n\n\u00a0\n\n\n\n",
    "jawaban": "The correct answer is: Test & Score"
  },
  "ec2047ab3a3829d5a95cfb5099293404a0b2257ca329f5b330fca67195339237": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#_xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nmap = pm4py.discover_heuristics_net(log)\npm4py.view_heuristics_net(map)\nWhat does the command 'pm4py.discover_heuristics_net(log)' produce?",
    "jawaban": "The correct answer is: A heuristics net based on the input log -An XML file -A CSV file -An interactive web page -A database query -A machine learning model"
  },
  "3f6656128f4a1b7f146f4794e34d530ba76c5f0944879f09472582e34e93a23f": {
    "soal": "To extract numbers from unstructured data (such as text and images), Orange can use a deep network embedder.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "e1a46a72e73cd7b9ba2e4a2c2ab7afd0bd6c2dfb970eb798a2cd7fbf79e2bd30": {
    "soal": "The Predictions widget displays model predictions on ......................\n",
    "jawaban": "The correct answer is: data"
  },
  "112d42ec7e96790bf2da271a0cd3da8dd60e02a1c47cfac23d1226dcbb9d743e": {
    "soal": "import pandas as pd\nfrom pm4py.objects.conversion.log import converter as log_converter\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom pm4py.visualization.petrinet import visualizer as pn_visualizer\nfrom pm4py.objects.log.util import dataframe_utils\n# csv_filename='Insurance_claims_event_log.csv'\n# csv_filename='Business Travel Permits.csv'\n# csv_filename='Parent-Child Union Output.csv'\n# csv_filename='PM4PY_Dummy_Event_Log_100_Cases_pm4py.csv'\n# csv_filename='PM4Py_Dummy_Event_Log-congested-business-process.csv'\n# 1. Load CSV event log\ndf = pd.read_csv(csv_filename)\n# df = pd.read_csv(csv_filename,sep=';')\n# print(df.head())\n# 2. Format timestamps properly\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n# 3. Ensure column names match PM4Py expectations\ndf = dataframe_utils.convert_timestamp_columns_in_df(df)\n# 4. Convert to event log object\nevent_log = log_converter.apply(df, parameters=)\n# 5. Discover process model using Alpha Miner\nnet, initial_marking, final_marking = alpha_miner.apply(event_log)\n# 6. Visualize Petri Net\ngviz = pn_visualizer.apply(net, initial_marking, final_marking)\npn_visualizer.view(gviz)\nHow can you ensure that the CSV timestamps match PM4Py requirements?\n{\n=dataframe_utils.convert_timestamp_columns_in_df(df)\n~df.timestamp_format()\n~df.fix_timestamps()\n~pm4py.validate_timestamps(df)\n~df.set_timestamp()\n~df.standardize_time()\n}",
    "jawaban": "The correct answer is: log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case_id'"
  },
  "e293a6f02926c89320b8ef1b7bf841cb556456226c017bcbffcda3f6851d073f": {
    "soal": "\n\n\nExamples of machine learning applications include,\n\n\n\n",
    "jawaban": "The correct answers are: Fraud Detection, Client Retention"
  },
  "3ba2e24fdd58ce65392fb58bf487c2c7b8ebe5b5a4a2d8ba59687dea1874c0e8": {
    "soal": "import pandas as pd\nfrom pm4py.objects.conversion.log import converter as log_converter\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom pm4py.visualization.petrinet import visualizer as pn_visualizer\nfrom pm4py.objects.log.util import dataframe_utils\n# csv_filename='Insurance_claims_event_log.csv'\n# csv_filename='Business Travel Permits.csv'\n# csv_filename='Parent-Child Union Output.csv'\n# csv_filename='PM4PY_Dummy_Event_Log_100_Cases_pm4py.csv'\n# csv_filename='PM4Py_Dummy_Event_Log-congested-business-process.csv'\n# 1. Load CSV event log\ndf = pd.read_csv(csv_filename)\n# df = pd.read_csv(csv_filename,sep=';')\n# print(df.head())\n# 2. Format timestamps properly\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n# 3. Ensure column names match PM4Py expectations\ndf = dataframe_utils.convert_timestamp_columns_in_df(df)\n# 4. Convert to event log object\nevent_log = log_converter.apply(df, parameters=)\n# 5. Discover process model using Alpha Miner\nnet, initial_marking, final_marking = alpha_miner.apply(event_log)\n# 6. Visualize Petri Net\ngviz = pn_visualizer.apply(net, initial_marking, final_marking)\npn_visualizer.view(gviz)\nWhat does the function 'alpha_miner.apply()' return?\n{\n=Petri net, initial marking, final marking\n~DataFrame, timestamp, event log\n~Event log, CSV, timestamp\n~Graph, nodes, edges\n~Visualization, Petri net, data\n~Alpha model, CSV file, log\n}",
    "jawaban": "The correct answer is: log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case_id'"
  },
  "76ea1a53a5a2012a052476e551a09d9795bdbe3ef667998958aec5fb04a0ee0d": {
    "soal": "The ..............(1).............. widget creates a distance matrix, which is a two-dimensional array containing distances, taken pairwise, between elements of the dataset. The number of elements in the dataset determines the size of the matrix. Data matrices are crucial for the ............(2)..................\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 Distance Matrix, (2) \u2192 Hierarchical Clustering"
  },
  "778ead549c54fad6ab51cc147f46c639903dc177172b0c9b02ca31bc2b8a72b7": {
    "soal": "Widget Import Documents takes text files from a folder and creates a corpus. The Import Documents widget can read .txt, .docx, .odt, ............(1)................., and .xml. If there are subfolders in the folder, they can be used to label .................(2)....................\n",
    "jawaban": "The correct answer is: (1) \u2192 odt, (2) \u2192 class, (1) \u2192 pdf, (2) \u2192 subfolder"
  },
  "a51ebf072bf346fa212c932bb3a31e9200a4aa0b486eb590713e48df332ebf67": {
    "soal": "The Distances Transformation widget is used for normalization and inversion of distance matrices. Normalizing the data is necessary to bring all variables into integration with one another.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "6d69122ed288ff13fa3fea29fb81b38d5426446ffc0c1c3656e8a7e534750676": {
    "soal": "import pandas as pd\ndf = pd.read_excel('path_ke_file.xlsx', sheet_name=0)\ndf_selected = df[['Kolom1', 'Kolom2']]\nprint(df_selected.head())\nHow can you read only columns 'A' and 'C' from an Excel file?",
    "jawaban": "The correct answers are: pd.read_excel('file.xlsx', usecols, [1, 3]), pd.read_excel('file.xlsx', columns, ['A', 'C']), df, pd.read_excel('file.xlsx', usecols, 'A,C'), pd.read_excel('file.xlsx', usecols, ['A', 'C']), pd.read_excel('file.xlsx', usecols, 'A;C'), pd.read_excel('file.xlsx', usecols, 'A-C')"
  },
  "a1bb3d9f4dd43eee70b8de45245c97d8ad2c033e8967c8e2a1c235a921eb24bb": {
    "soal": "The Silhouette Plot widget plots the class probability against the probability predicted by the classifier.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "1c7be8ef571af9aff6382165936d09838eeb48916ee9ab0236560c9da5973667": {
    "soal": "Currently, the only widget that provides the correct signal needed by the Calibration Plot widget is the Predictions widget. Therefore, the Calibration Plot widget always follows the Predictions widget, and since it has no output, no other widget follows it.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "f4cf632e284182aeda0d6a7472f0fb4603331d8ff5e72545fc222642d5edc796": {
    "soal": "In 2012, when Harvard Business Review called it \"The Sexiest Job of the 21st Century\", the term \"data science\" became a buzzword. Data Science is often interchanged with previous concepts such as business analytics, business intelligence, predictive modeling, and statistics.",
    "jawaban": "The correct answer is 'True'."
  },
  "c3b129b6485d9eab5425f6cfea8ec1bf6039d8f3a483d49b62519027579fc93f": {
    "soal": "The Distance Matrix widget can visualize the distance measurement results in a ......................\n\n",
    "jawaban": "The correct answer is: distance matrix"
  },
  "0710ffb5f9a358af899d01b7cd47b2d48d76dfd0aa6137f24035fc39c82297c2": {
    "soal": "The ..................................... widget can visualize distances between objects. The visualization is the same as printing a table of numbers, except the numbers are replaced by colored dots.\n\n\n",
    "jawaban": "The correct answer is: Distance Map"
  },
  "9b75ce2090f95618b825b293443e8c7754fc27dc230a1cf4408beb4f80c82fb8": {
    "soal": "Salah satu URL tempat lokasi contoh2 pengembangan / penggunaan ORANGE ada di\n\n\u00a0https://www.youtube.com/user/realannoyingorange\n",
    "jawaban": "The correct answer is 'False'."
  },
  "325ca293f2388c921d443291d1b4c2c2b7302e615c6fc9a739e2033329c9cae7": {
    "soal": "Most visualizations in Orange are interactive. In the workflow below, the Scatter Plot Widget for example. Double-click the icon to open it and click-and-drag to select several data points from the plot. The selected data will automatically enter the ............................. Widget. Double-click to check which data was selected. Change the selection and observe the changes in the Data Table Widget.\n\n\n\n",
    "jawaban": "The correct answer is: Data Table"
  },
  "8a798172b741408596b85586b6a7a3418d4b6ca11ec8ed890bd10c8e1c230496": {
    "soal": "The workflow below demonstrates the use of the Confusion Matrix widget\n\nTest & Score obtains data from File and two learning algorithms from Naive Bayes and Tree. Test & Score performs cross-validation or other train-and-test procedures to obtain class prediction by both algorithms for all (or some) data instances. The test results are sent to the Confusion Matrix, where we can observe how many instances were misclassified and why.\n\n\nIn the output, we use the Data Table to show examples selected in the confusion matrix. If we, for example, click Misclassified, the table will contain all instances that were misclassified by the selected method.\n\n\nThe Scatter Plot receives two sets of data. From the File widget, it gets the full data, while the confusion matrix only sends selected data, such as misclassified data. The Scatter Plot will display all data, with bold symbols representing selected data.\n\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "2b66b43ecc2b1daed4cd9ec97a91dbddec45c227e9f034686e1461cb1c24466f": {
    "soal": "Widget Data Info displays information from the unselected dataset.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "44083c5ba8e9acf3131da194df39224a761499d4e3682bbbc8b6c2799127323c": {
    "soal": "from google.colab import drive\ndrive.mount('/content/drive')\nWhat happens after executing drive.mount('/content/drive') in Colab?",
    "jawaban": "The correct answer is: You're prompted to authorize access to your Google account."
  },
  "6131bd5132cf67556248386339f4e109fd9531a5f7adb764a9a774142db8c98f": {
    "soal": "The Import Documents widget takes text files from a folder and creates a corpus. The ....................... widget can read .txt, .docx, .odt, .pdf, and .xml. If there are subfolders in the folder, they can be used to label the class.\n",
    "jawaban": "The correct answer is: Import Documents"
  },
  "5cc5bded5687cf102908d689f43dd4d612731e89c9f71ea31865cde613a72bf9": {
    "soal": "import pm4py\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\n# Import XES log\nlog = xes_importer.apply(xes_filename)\n# Cek jumlah kasus dan aktivitas\nprint(f\"Jumlah kasus: \")\nactivities = set()\nfor trace in log:\nfor event in trace:\nactivities.add(event['concept:name'])\nprint(f\"Jumlah aktivitas unik: {len(activities)}\")\nfrom pm4py.algo.discovery.dfg import algorithm as dfg_discovery\nfrom pm4py.visualization.dfg import visualizer as dfg_visualization\n# DFG berdasarkan frekuensi\ndfg = dfg_discovery.apply(log,\nvariant=dfg_discovery.Variants.FREQUENCY)\n# Tampilkan visualisasi\ndfg_vis = dfg_visualization.apply(dfg, log=log,\nvariant=dfg_visualization.Variants.FREQUENCY)\ndfg_visualization.view(dfg_vis)\nWhich Python library is specifically used for analyzing process bottlenecks using XES files?\n{\n=PM4Py\n-Pandas\n-NumPy\n-Scikit-learn\n-Matplotlib\n-OpenCV\n}",
    "jawaban": "The correct answer is: len(log)"
  },
  "1fc1875a7045cbdb03da352335784544722f1cf8b46af0a805474f3dcbdb30ba": {
    "soal": "In this example, we will just check what a bag of words model looks like. Load book-excerpts.tab using the Corpus widget and connect it to the Bag of Words widget. Here, we deliberately use the default parameters - the simplest count is term frequency. Check the output of the ............... widget using the Data Table widget. The last column represents the term frequency of each document.\n\n\n",
    "jawaban": "The correct answer is: Bag of Words"
  },
  "f6acadcaba6cc13d84e239f665ae80a186aa99b2ab3114b97fb42e0a1d049d85": {
    "soal": "Which of the following is an example of a dataset's row?",
    "jawaban": "The correct answer is: A single user's transaction record."
  },
  "6e9a3ad48394be2c7b568873ac8b9e946539a10fe92278702a09b3464a57d67b": {
    "soal": "The following regular expression (regex)\n\nmatches only words, no punctuation\nis\n",
    "jawaban": "The correct answer is:\n\\w+"
  },
  "8c44211e3c802cecb365f85456ebf06ed4787a3464eba9792f46f69dc35b6017": {
    "soal": "import pm4py\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\n# Import XES log\nlog = xes_importer.apply(xes_filename)\n# Cek jumlah kasus dan aktivitas\nprint(f\"Jumlah kasus: \")\nactivities = set()\nfor trace in log:\nfor event in trace:\nactivities.add(event['concept:name'])\nprint(f\"Jumlah aktivitas unik: {len(activities)}\")\nfrom pm4py.algo.discovery.dfg import algorithm as dfg_discovery\nfrom pm4py.visualization.dfg import visualizer as dfg_visualization\n# DFG berdasarkan frekuensi\ndfg = dfg_discovery.apply(log,\nvariant=dfg_discovery.Variants.FREQUENCY)\n# Tampilkan visualisasi\ndfg_vis = dfg_visualization.apply(dfg, log=log,\nvariant=dfg_visualization.Variants.FREQUENCY)\ndfg_visualization.view(dfg_vis)\nWhich PM4Py feature assists directly in identifying bottlenecks by visualizing time distributions?\n{\n=Performance Spectrum\n-DFG frequency\n-Histogram plot\n-Scatter plot\n-Regression analysis\n-Clustering map\n}",
    "jawaban": "The correct answer is: len(log)"
  },
  "aa5c09165aba961a9497942cf86233e1d81fd19124835a81d869d056859a18e9": {
    "soal": "The Box Plot widget shows the ........................ attribute values. It is good practice to examine any new data with this widget to quickly spot anomalies, such as duplicate values (e.g., gray or grey), outliers, and the like.\n",
    "jawaban": "The correct answers are: distribution, distribution"
  },
  "f832395d18e92d2cbc3b071d563b110492943a2ae3edef5d020f177d3ee8a264": {
    "soal": "from googlesearch import search\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef get_page_content(url):\ntry:\nheaders =\nresponse = requests.get(url, headers=headers, timeout=10)\nsoup = BeautifulSoup(response.content, 'html.parser')\n# Ambil judul halaman\ntitle = soup.title.string if soup.title else 'No Title'\n# Ambil konten paragraf utama\nparagraphs = soup.find_all('p')\ntext_content = ' '.join([p.get_text() for p in paragraphs[:5]]) # Batasi 5 paragraf pertama\nreturn title.strip(), text_content.strip()\nexcept Exception as e:\nreturn 'Error', f\"Failed to fetch content: {e}\"\ndef google_scrape_with_content(keywords, num_results=5, output_file='scraped_results.csv'):\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'Rank', 'Title', 'URL', 'Content'])\nfor keyword in keywords:\nprint(f\"\n\ud83d\udd0d Searching for: {keyword}\")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nprint(f\" \u2192 Fetching: {url}\")\ntitle, content = get_page_content(url)\nwriter.writerow([keyword, i+1, title, url, content])\ntime.sleep(2) # Delay biar aman\nexcept Exception as e:\nprint(f\"\u274c Error while searching '{keyword}': {e}\")\nprint(f\"\n\u2705 All results + content saved to '{output_file}'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\ngoogle_scrape_with_content(keywords, num_results=5)\nWhat does the script do if an error occurs during the search for a particular keyword?\n{\n=It prints an error message and continues with the next keyword.\n~It stops execution and exits the program.\n~It retries the search for the same keyword immediately.\n~It logs the error to a separate log file and continues.\n~It prompts the user to enter a new keyword.\n~It skips the keyword and moves to the next without any notification.\n}",
    "jawaban": "The correct answer is: \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\""
  },
  "3d39cc07fe980b21ec0e0ebb3076fd20641018dfa9a88631efc3b667b4e1ce77": {
    "soal": "The core of Apache Hadoop consists of the storage part (Hadoop Distributed File System (HDFS)) and the processing part (MapReduce). Hadoop splits files into large blocks and distributes them among the nodes in the cluster. To process the data, Hadoop MapCluster transfers code to the nodes for parallel processing, based on the data that needs to be processed at each node. This approach takes advantage of the data-node locality manipulating the data it holds in hand\u2014to allow data to be processed faster and more efficiently than conventional supercomputer architectures that rely on parallel file systems where computations and data are connected through high-speed networks.",
    "jawaban": "The correct answer is 'False'."
  },
  "cacfc18879f8810be0c712ae561f562ea01a5a32d8363204a6596294a4995e34": {
    "soal": "Which of the following does the self-attention mechanism do?",
    "jawaban": "The correct answer is: Identifies relationships between words"
  },
  "d972051d57ad220a90f1173d85155e92a8677ea0c862aa70f35c22424fe1c62f": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nWhat is the role of the Dense layer with 10 units in the model?",
    "jawaban": "The correct answer is: To produce 10 logits corresponding to the 10 classes for classification."
  },
  "1ce669ce5f16ea36568118ec4f2c34fbfde9c272a6904ae8c077b2117e44e35d": {
    "soal": "Data attributes in Orange have types discrete, continuous, or character image. Attribute types are marked with symbols that appear before the attribute name (D, C, S).\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "0f1747c0f4ca61db400274377cb0f3dc0cb1a5d733e84dd6746393e869d2aa2a": {
    "soal": "import tensorflow as tf\nfrom tensorflow.keras import layers, models\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nmodel = models.Sequential([\nlayers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(64, (3,3), activation='relu'),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(128, (3,3), activation='relu'),\nlayers.Flatten(),\nlayers.Dense(128, activation='relu'),\nlayers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5, batch_size=64)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nWhich function is used to flatten the output before Dense layers?",
    "jawaban": "The correct answer is: Flatten()"
  },
  "7d47fd87b5f52e2ddbd30604c395fa938b41e1bcebe7fb184dd8ccabaace6f20": {
    "soal": "Widget Calibrated Learner menghasilkan sebuah model yang mengkalibrasi distribusi dari class probabilities dan memaksimalkan decision threshold.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "87d9e46ad4dfe0316e57a1cffc3f27f9701d5ccb4c6a4d6a7d662cb819b744e8": {
    "soal": "Principal Component Analysis (PCA) transforms data into a dataset with uncorrelated variables, also called principal components. The PCA widget displays a graph (scree diagram) that shows the amount of variance explained by the best .................. components and allows you to interactively adjust the number of components to include in the output dataset. In this workflow, we can observe the transformation in the Data Table and in the Scatter Plot.\n\n\n",
    "jawaban": "The correct answer is: principal"
  },
  "17859f1f8e7d0a7372d21889bcea990a04a3d18dbed9952e29090ddf4bb598e9": {
    "soal": "Where is the prompt first received in the LLM process?",
    "jawaban": "The correct answer is: Context window"
  },
  "a9d82d1d0663b0e4d254edab6d6593eaa4fc1e95204b9714be15229f45a22920": {
    "soal": "import tensorflow as tf\nfrom tensorflow.keras import layers, models\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nmodel = models.Sequential([\nlayers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(64, (3,3), activation='relu'),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(128, (3,3), activation='relu'),\nlayers.Flatten(),\nlayers.Dense(128, activation='relu'),\nlayers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5, batch_size=64)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nWhat activation function is used in the output Dense layer?",
    "jawaban": "The correct answer is: Softmax"
  },
  "216243fc4619f8ff22d251ae3c5ffb60942b5ef1740f4a1ef36ba496739e1c98": {
    "soal": "Widget CN2 Rule Induction akan meng-induce (menginduksi) rule dari data menggunakan algoritma CN3.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "294979d5b8b74f5dc29875a8bb88b2c87aba37d3cc89bc5cbbe34550d3459598": {
    "soal": "The ......................... widget calculates the distance between rows or columns in the dataset. By default, the data will be normalized to ensure equal treatment of individual features. Normalization is always done column-wise (using columns as a reference).\n",
    "jawaban": "The correct answer is: Distances"
  },
  "9cdff15216f49e79dd86848b45f22b8a0c628beda3db344e4a60fe4ee11e0375": {
    "soal": "The following ticker code used in the Yahoo Finance Stock Data Widget (ORANGE)\n\nSPY\n\n\nBelongs to the company,\n\n\n",
    "jawaban": "The correct answer is: SPDR S&P 500 Trust ETF"
  },
  "bde478c8626853ef32acd38ae4779ce00941388fcf006dd2303ef70bfd946dc1": {
    "soal": "import pandas as pd\nfile_path = \"/content/drive/MyDrive/your_file.csv\"\ndf = pd.read_csv(file_path)\nprint(df.head())\nWhat is the purpose of the 'import' statement in Python?",
    "jawaban": "The correct answer is: It allows access to external modules or libraries."
  },
  "c18ef461e6834690b8064589b529c4b60dcddc1d9e2aac3cd7633e1138604604": {
    "soal": "import os\nimport string\nfrom collections import Counter\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\n# Buat daftar stopwords Bahasa Indonesia\nstop_words = set(stopwords.words('indonesian'))\ndef bersihkan_teks(teks):\n# Lowercase + hilangkan tanda baca\nteks = teks.lower()\nteks = teks.translate(str.maketrans('', '', string.punctuation))\nreturn teks\ndef analisis_folder(folder='outputs', top_n=20):\nsemua_konten = ''\nfor filename in os.listdir(folder):\nif filename.endswith('.txt'):\nfilepath = os.path.join(folder, filename)\nwith open(filepath, 'r', encoding='utf-8') as f:\nisi = f.read()\n# Ambil hanya bagian isi, setelah header\nif '\n\n' in isi:\nisi = isi.split('\n\n', 1)[1]\nsemua_konten += isi + ' '\n# Bersihkan teks dan tokenisasi\nteks_bersih = bersihkan_teks(semua_konten)\nkata_kata = teks_bersih.split()\n# Hapus stopwords\nkata_kata = [kata for kata in kata_kata if kata not in stop_words and kata.isalpha()]\n# Hitung frekuensi\ncounter = Counter(kata_kata)\ntop_kata = counter.most_common(top_n)\nprint(f\"\n\ud83d\udd0d Top Kata Paling Sering Muncul:\")\nfor kata, jumlah in top_kata:\nprint(f\"{kata}: {jumlah}\")\n# Buat Word Cloud\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(counter)\n# Tampilkan\nplt.figure(figsize=(12, 6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud dari Semua Konten')\nplt.tight_layout()\nplt.savefig('wordcloud.png')\nplt.show()\nprint(\"\n\u2705 Word Cloud disimpan sebagai 'wordcloud.png'\")\n# Jalankan analisis\nif __name__ == '__main__':\nanalisis_folder('outputs', top_n=30)\nWhich directory does the 'analisis_folder' function analyze by default?\n{\n=outputs\n~results\n~data\n~analysis\n~texts\n~documents\n}",
    "jawaban": "The correct answer is: top_n"
  },
  "71936d882c6c3314aef52436b5399986d084830dd3f654b37ba2cdd2dba1f633": {
    "soal": "import tensorflow as tf\nfrom tensorflow.keras import layers, models\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nmodel = models.Sequential([\nlayers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(64, (3,3), activation='relu'),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(128, (3,3), activation='relu'),\nlayers.Flatten(),\nlayers.Dense(128, activation='relu'),\nlayers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5, batch_size=64)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nWhich metric is used during model compilation?",
    "jawaban": "The correct answer is: Accuracy"
  },
  "d69b0ac0683755849c2f2502486e6d2dfe38453cd9d2c876b876eb97ef36c18b": {
    "soal": "100 (binary) OR 111 (binary) =",
    "jawaban": "The correct answer is: 111"
  },
  "b7089ac6485b891e665a82923f1537aed02f9877b6e92f39dd299f349b7e0290": {
    "soal": "\n\n\nIn the process of building a machine learning model, initial data can be explored using PCI or SOM techniques.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "9f6544aab7a304b6ba8b1b48362dfab1dfe94e97f9e9209ed117de366e1430c4": {
    "soal": "110 (binary) = ...... decimal",
    "jawaban": "The correct answer is: 6"
  },
  "09271e7c4317b5f71c8f1a5dca2ac673e2523f5705d9bf1f0deb96813dcf157d": {
    "soal": "The ...................... widget is used to manually arrange the data domain. The user can decide which attributes to use and how to do it. Orange distinguishes between regular attributes, class attributes (optional), and meta attributes. For example, to build a classification model, the domain will consist of a set of attributes and discrete class attributes. Meta attributes are not used in modeling, but some widgets can use them to label instances.\n",
    "jawaban": "The correct answer is: Select Columns"
  },
  "fa9eeca9b2bf8664fb02bf363611371dd331567722e7ab7cf67f7440d8448dff": {
    "soal": "To cluster based on similarity, we can measure the distance between images. The numerical representation of the images obtained from the Image Embedding widget, we measure the distance using the Distances widget. Typically, cosine distance works best for images, but you can experiment yourself. We then send the distance matrix to Hierarchical Clustering to visualize similar pairs in the dendrogram.\n\nThe result is that all the animals are correctly grouped. In the dendrogram, we do not see the images of the animals. We can use the ................... widget to view the images.\n\n\n\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Image Viewer"
  },
  "282dc126d398d47b838e51472c2e8765a5abfe21477a2596e6256dfe8eafea5e": {
    "soal": "Widget Network Clustering can help us uncover clusters and highly connected groups in a network. First, we will use the Widget Network File to load the lastfm.net dataset. Then we will send this network to the Widget Network Clustering. The Network Clustering widget finds 79 clusters in the network. To visualize the results, we use the Network Explorer Widget and set the attribute ............(1).............. to Cluster. This will color the network nodes according to the cluster colors - this is a good way to visualize .................(2)............... in a dense network.\n\nKeep in mind that the .................(3)............... Widget here will color the 10 largest clusters and color the rest as 'Other'.\n\n\n\n",
    "jawaban": "The correct answer is: \u2192 Network Clustering, (1) \u2192 Color, (3) \u2192 Network Explorer, (2) \u2192 highly connected group"
  },
  "1dddd257680edb16ed69b1c16a67cb85dca009bda1d6ba884ad29947ffd585a0": {
    "soal": "In the following workflow example, we will show how to quickly visualize a corpus with the Word Cloud widget. We can connect the Word Cloud widget directly to the Corpus widget, but instead, we decide to apply some preprocessing with the Text Preprocess widget. We are working with the dataset book-excerpts.tab. We can convert all text to lowercase, tokenize (split) the text into just words, filter out English stopwords, and select the 100 most frequent tokens.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "08df45374893194352d20c73beccb0a08f7f289c34a230d4a07083745650385e": {
    "soal": "In the Import Documents Widget, to retrieve data, select (click) the folder icon on the right side of the widget. Select (choose) the folder that we want to make corpus. After loading is complete, we will see how many documents were successfully retrieved by the Import Documents Widget. To observe the obtained corpus, connect the Import Documents widget to the ...................... widget. In this workflow, we are using a set of speeches by President Kennedy in plain text format.\n\n\n\n",
    "jawaban": "The correct answer is: Corpus Viewer"
  },
  "6c3807753f665fed11512c93d8d4833f33386e847ac18742a3f81946002d20e6": {
    "soal": "What is the role of a dataset in statistical analysis?",
    "jawaban": "The correct answer is: To provide data for identifying trends and making inferences."
  },
  "2c4f17ab873af5bd9d96aad19d45341e9d53ae897d20427b96f1ec6faa5b8984": {
    "soal": "Widget .................... reads the input data file (data table with data instances) and sends the dataset to its output channel. The history of the last opened file is saved in the widget. This widget also includes a directory with sample pre-installed datasets in Orange.\n",
    "jawaban": "The correct answer is: File"
  },
  "ce4311dd993479f32db2e646914b929d3ff5090c79816f3da4d6fa4249f23d9a": {
    "soal": "Widget kNN mem-prediksi berdasarkan instance training terjauh.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "407faae58fa9348438df64a6eaa5c67664e4889bc272e6005da9fa02616999a0": {
    "soal": "Orange3 installation on Ubuntu 18.04 can use the command\n\n\u00a0apt-get install orange3\n",
    "jawaban": "The correct answer is 'False'."
  },
  "87472ae9f0ec97e1a0e240fcd30d28b30f728f0c3c8bdf15bdc146001b551926": {
    "soal": "Sebagai contoh penggunaan kNN untuk klasifikasi kita mengggunakan dataset iris. Kita bandingkan hasil dari k-Nearest Neighbors dengan default model Constant, yang akan memprediksi class majoritas. Tampak kNN lebih ................\n\n\n\n\n\n",
    "jawaban": "The correct answer is: baik"
  },
  "bee31eb62877cd8dd807805a4f16fab1d3a54a6904704182b0c3798dd4a6f771": {
    "soal": "Below, we will use the Attrition - Train data from the Datasets widget. This data is about employee attrition. In other words, we want to know whether a certain employee will leave their job or not. We will create a prediction model with the Tree widget and observe the probabilities in Predictions.\n\nFor prediction, we need training data, which we have loaded in the first Datasets widget, and data to predict, which we will load in another Datasets widget. We will now use the Attrition - Predict data. Connect the second dataset to Predictions. Now we can see the predictions for three data instances from the second dataset.\n\nThe Tree model predicts that no employees will leave the company. We can try another model and see if the predictions change. Or, test the predictive score first in the Test & Score widget.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "5bc45a51ee2056c17ef4ee1243ad703070f0aa1cbe7b45db85f1529b47e27bed": {
    "soal": "The Distance Map widget can visualize distances between items.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "35af9012f4edccd1b05abdac2e2f6bae69fe3f9b8392941703a71bcd41e530c1": {
    "soal": "What is the main reason to use GPU or TPU in Google Colab?",
    "jawaban": "The correct answer is: To accelerate deep learning computations"
  },
  "892e95925a6e62cc183557969cbb68cce82bcac5ebbda5f4c2114d5acf904fde": {
    "soal": "import numpy as np\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n# Contoh data: nilai aktual dan prediksi\ny_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0]) # Nilai aktual\ny_pred = np.array([1, 0, 1, 0, 0, 1, 0, 1, 1, 0]) # Prediksi model\n# Menghitung confusion matrix\ncm = confusion_matrix(y_true, y_pred)\n# Menampilkan confusion matrix dalam bentuk teks\nprint(\"Confusion Matrix:\n\", cm)\n# Menampilkan confusion matrix dalam bentuk grafik\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.show()\nApa yang direpresentasikan oleh variabel y_true dalam contoh kode?",
    "jawaban": "The correct answer is: Nilai aktual dari kelas dalam dataset."
  },
  "7ba41b9c246c76638d14721dfb38cca7ce39c3112458569bd7c01125a35057a4": {
    "soal": "The Data Info widget is a simple widget that presents information about the dataset size, features, ..............., meta attributes, and location.\n\n\n",
    "jawaban": "The correct answer is: target"
  },
  "733a06a90d64e2b1afc4a387de02fa1291c255faa83326f06a951aed1aad0084": {
    "soal": "The smallest unit that makes up binary data in a computer is called:",
    "jawaban": "The correct answer is: Bit"
  },
  "c0be4e15d1d3853cabca828666025f514b5e168545b074df850480e7c2f43c23": {
    "soal": "What is the primary purpose of TensorFlow Serving?",
    "jawaban": "The correct answer is: Deploying machine learning models in production"
  },
  "d5467787d91024e9dcd59190b4213ba0061975b20c96089f9d7f68a49abada68": {
    "soal": "Why is SparseCategoricalCrossentropy suitable for a model?",
    "jawaban": "The correct answer is: It is appropriate when dealing with integer labels for multi-class classification"
  },
  "57becacc6ddae00a6ef84cd523d25e9f9f010f9ee76a6d6478f41c001269ffbe": {
    "soal": "import tensorflow as tf\nfrom tensorflow.keras import layers, models\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nmodel = models.Sequential([\nlayers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(64, (3,3), activation='relu'),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(128, (3,3), activation='relu'),\nlayers.Flatten(),\nlayers.Dense(128, activation='relu'),\nlayers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5, batch_size=64)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nWhat loss function is used for the model?",
    "jawaban": "The correct answer is: sparse_categorical_crossentropy"
  },
  "2e69d9005b6221f4291711501af3c0075e87812887838e1ad4c5e3e2dfb2b810": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=10000)\nWhich of the following is a common use of the IMDB dataset?",
    "jawaban": "The correct answer is: Text classification"
  },
  "bb988ece10aea777e618bd4e9c8e9a25c7c9cabed9a53154c8759862a4918a51": {
    "soal": "The Bag of Words model creates a corpus with word counts for each data instance (document). The count can be absolute, binary (present or not) or sublinear (logarithmic of term frequency). The Bag of Words model is required in combination with the ................... widget and can be used for predictive modelling.\n\n\n",
    "jawaban": "The correct answer is: Word Enrichment"
  },
  "d505316dda25bd941ecc2ec499434dbb9967a36da16e88614ae5aa66169ca39e": {
    "soal": "Which attribute is NOT mandatory in an event log for process mining?",
    "jawaban": "The correct answer is: Cost"
  },
  "7b90a4c02fbbdb169b95f6813ad3c20dc414f95f1c425e7ee8542f4c0446555b": {
    "soal": "What is the primary goal of process mining?",
    "jawaban": "The correct answer is: Transform event data into insights and actions"
  },
  "14be72b8367cd6e80f5b9c2c340bdf733f561388a41771da4f3366edb4f79c08": {
    "soal": "In the workflow below, we use the Zoo dataset. We load the data into the Scatter Plot widget, which allows us to select a subset of ...................... data instances. Then, we can send the selected data instances to the Save Data widget to save them into a file.\n\n\n\n\n",
    "jawaban": "The correct answer is: data"
  },
  "f1010fd4c9d11bf8a47184783d645a40ca96e89514c75fa467b2672e3ed28086": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#_xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nmap = pm4py.discover_heuristics_net(log)\npm4py.view_heuristics_net(map)\nWhich filename specifically represents a hospital-related event log?",
    "jawaban": "The correct answer is: Cross_Hospital.xes -BPIC_2012_A.xes -training_log_1.xes -PM4PY-running-example.xes -BPIC_2012_O.xes -excercise.xes"
  },
  "f36490185039cb5f858ed1aeb1e481fbd111188b6d874f32120b9a9003c1e656": {
    "soal": "\n\n\nThe workflow above can be used to perform item clustering based on purchases.\n\n\nItems purchased in an online shop, as seen in the data above, will become categorical data. The distance calculation process for clustering will work well if the data matrix is transposed (columns become rows).\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "40a45d7ff086d8f5ab822325648cf4306f76570564c2af849d059933b3f139d6": {
    "soal": "What permission change is necessary after downloading Docker Compose?",
    "jawaban": "The correct answer is: chmod +x /usr/local/bin/docker-compose"
  },
  "e49ddce8e6a730f22d4331cb50996851f431038d52094b9262abebf1b34bf3ac": {
    "soal": "Causal relationship reconstruction is central to:",
    "jawaban": "The correct answer is: Alpha Miner"
  },
  "2c31f4d8871c723ae8695224887fe14b657a7e7d214e3470b454a02a2251e2d7": {
    "soal": "\n\n\n\n\n\nWorkFlow di atas dalam dunia machine learning merupakan bagian dari supervised learning.\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "aafb8649418724e664d15e0b9a0e2db0303d0a39cfdbde157f8c45d66d576aed": {
    "soal": "import pandas as pd\nfile_path = \"/content/drive/My Drive/NamaFolder/NamaFile.xlsx\"\ndf = pd.read_excel(file_path)\nprint(df.head())\nIn the code, what does the variable 'file_path' represent?",
    "jawaban": "The correct answer is: The path to the Excel file stored in Google Drive."
  },
  "91a9a397a85f8a1ba350b2d40b62abb3d80eec05f4fa44e95425f3173228b362": {
    "soal": "A Pivot Table can help us collect and transform data. This workflow takes Kickstarter projects and aggregates them by seconds. We can check the frequency of projects published per month and observe the differences between funded and non-funded projects.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "ce7154cbb174421da304f31c54aa3a4242ab5ce0d5bf1e5b2b4467008b3496a4": {
    "soal": "Why is the transformer architecture important in LLMs?",
    "jawaban": "The correct answer is: It allows better handling of language context"
  },
  "d30b4309bf9e4a83d1bc6b77527e854bbfaf729a4a7919af32c5ac8d2eea5c3a": {
    "soal": "The Distances widget ignores discrete values and calculates distances only for continuous data, so the Distance Map widget can only display distance maps for discrete data.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "eaffc0bda296f15dd7f35dd05da48e17261622ee5bc7cac9dba0a915a4097f88": {
    "soal": "The Line Chart widget can visualize the sequence and movement of the time series in the most ......................... visualization that is easy to imagine.\n\n",
    "jawaban": "The correct answer is: time series"
  },
  "d6f4eb17f1be006810e9418b81d05f381a46c1ad1bca60db7ba05db79bcd4db5": {
    "soal": "import pandas as pd\ndf = pd.read_excel('path_ke_file.xlsx', sheet_name=0)\ndf_selected = df[['Kolom1', 'Kolom2']]\nprint(df_selected.head())\nHow do you select specific columns, 'Column1' and 'Column2', from a DataFrame?",
    "jawaban": "The correct answers are: df['Column1', 'Column2'], df.loc[:, ['Column1', 'Column2']], df_selected, df[['Column1', 'Column2']], df.iloc[:, ['Column1', 'Column2']], df.select(['Column1', 'Column2']), df.get(['Column1', 'Column2'])"
  },
  "3d043462d825850a9cb0097b048582c39f30e406caef7698670759d8fa928abf": {
    "soal": "from googlesearch import search\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef get_page_content(url):\ntry:\nheaders =\nresponse = requests.get(url, headers=headers, timeout=10)\nsoup = BeautifulSoup(response.content, 'html.parser')\n# Ambil judul halaman\ntitle = soup.title.string if soup.title else 'No Title'\n# Ambil konten paragraf utama\nparagraphs = soup.find_all('p')\ntext_content = ' '.join([p.get_text() for p in paragraphs[:5]]) # Batasi 5 paragraf pertama\nreturn title.strip(), text_content.strip()\nexcept Exception as e:\nreturn 'Error', f\"Failed to fetch content: {e}\"\ndef google_scrape_with_content(keywords, num_results=5, output_file='scraped_results.csv'):\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'Rank', 'Title', 'URL', 'Content'])\nfor keyword in keywords:\nprint(f\"\n\ud83d\udd0d Searching for: {keyword}\")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nprint(f\" \u2192 Fetching: {url}\")\ntitle, content = get_page_content(url)\nwriter.writerow([keyword, i+1, title, url, content])\ntime.sleep(2) # Delay biar aman\nexcept Exception as e:\nprint(f\"\u274c Error while searching '{keyword}': {e}\")\nprint(f\"\n\u2705 All results + content saved to '{output_file}'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\ngoogle_scrape_with_content(keywords, num_results=5)\nHow does the `load_keywords` function handle empty lines in the `keywords.txt` file?\n{\n=It ignores them and does not include them in the list of keywords.\n~It raises an error and stops execution.\n~It includes them as empty strings in the list of keywords.\n~It replaces them with a default keyword.\n~It logs a warning message but continues execution.\n~It prompts the user to enter a replacement keyword.\n}",
    "jawaban": "The correct answer is: \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\""
  },
  "4ab4de3de409b7421a4ac5ce8e647f23f9b94b2d1f779b423d373e9d9762df7f": {
    "soal": "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target biner\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(0, 2, 100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi data latih dan data uji (80% latih, 20% uji)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi dan pelatihan model klasifikasi (misalnya RandomForest)\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n# Prediksi pada data uji\ny_pred = model.predict(X_test)\n# Menghitung Precision, Recall, dan F1-Score\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nprint(f'Precision: ')\nprint(f'Recall: {recall:.2f}')\nprint(f'F1-Score: {f1:.2f}')\n# Alternatif: Menggunakan classification_report untuk ringkasan metrik\nprint('\nClassification Report:')\nprint(classification_report(y_test, y_pred))\nWhich function is used to calculate Precision in scikit-learn?\n{\n~recall_score\n=precision_score\n~f1_score\n~accuracy_score\n~classification_report\n~confusion_matrix\n}",
    "jawaban": "The correct answer is: precision:.2f"
  },
  "b7529d66ad50177ab66a41d86bea1b6513a89a02f77819731c894eb96b176527": {
    "soal": "The Tree widget can work for regression tasks. In the workflow below, we use the housing dataset and give the dataset to the Tree widget. The selected Tree node in the Tree Viewer will be displayed in the Scatter Plot, and we can see that the selected example has the same feature.\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "fe00d9054b1a42c91945c5d579d3ebad033a826a18d3abe077a26595da92a98b": {
    "soal": "The core of Apache Hadoop consists of the storage part (Hadoop Distributed \nAnswer Question 44\n System (HDFS)) and the processing part (MapReduce). Hadoop splits files into large blocks and distributes them across nodes in the cluster. To process the data, Hadoop MapReduce transfers code to the nodes to process in parallel, based on the data that needs to be processed at each node. This approach takes advantage of data-node locality, manipulating the data they hold in hand to enable faster and more efficient processing than more conventional supercomputer architectures relying on parallel file systems where computation and data are linked through high-speed networks.",
    "jawaban": "The correct answer is: File"
  },
  "2c5dbfe5d65516d5e2e8db7046d6b2549bd4b499723c6845c03995df7dc6ba5b": {
    "soal": "import pandas as pd\nfile_path = \"/content/drive/My Drive/NamaFolder/NamaFile.xlsx\"\ndf = pd.read_excel(file_path)\nprint(df.head())\nWhat is the result of executing 'df = pd.read_excel(file_path)'?",
    "jawaban": "The correct answer is: It creates a DataFrame by reading the Excel file located at 'file_path'."
  },
  "e5f4b5bd6ca849b330d8690efeea6c64218310845ee1aaa8ab41f21b908cfd41": {
    "soal": "from google.colab import drive\ndrive.mount('/content/drive')\nHow can you unmount Google Drive in a Colab session?",
    "jawaban": "The correct answer is: Restart the runtime."
  },
  "e5385c8d133a76734840dc083a3dd7083bcea2fdd36c9f851e77f3c57a7a7367": {
    "soal": "120 (decimal) = .......... (binary)",
    "jawaban": "The correct answer is: 1111000"
  },
  "9578a246e1ae6a8f228b0b5cac7233633533816312659280091c74522a0b2ffe": {
    "soal": "In the following workflow example, we will show the very simple use of the Corpus widget. Place the Corpus widget on the canvas and connect it to the Corpus Viewer widget. We use the dataset book-excerpts.tab, which is available in the add-on, and examine it in the Corpus Viewer widget.\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "a1c9d454061fd7dded2cfcef6f2d24cc53bf42e6cd28d237c470f8620f40d2d5": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nplt.imshow(x_train[0], cmap='gray')\nplt.title(f\"Label: \")\nplt.show()\nmodel = keras.Sequential([\nkeras.layers.Flatten(input_shape=(28, 28)),\nkeras.layers.Dense(128, activation='relu'),\nkeras.layers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nHow many epochs is the model trained for?\n{\n= 5\n~ 10\n~ 1\n~ 3\n~ 7\n~ 15\n}",
    "jawaban": "The correct answer is: y_train[0]"
  },
  "e0c18d4753bf0394bcce186aa3060a173aedca5bda71315fe5ae540aaa34fc78": {
    "soal": "Which command exports a model for sharing or backup?",
    "jawaban": "The correct answer is: ollama export model"
  },
  "b979225a7564c3a24d4a88f74d5afe9a0ff276548c136c88aa080a3be2179db5": {
    "soal": "In the following example workflow, we show how to quickly visualize ..........(1)............ with the Word Cloud widget. We can connect the Word Cloud widget directly to the Corpus widget, but instead, we decide to apply some preprocessing with the ..............(2)............ widget. We are working with the book-excerpts.tab dataset. We can convert all text to lowercase, tokenize (split) text into words only, filter English stopwords, and select the 100 most frequent tokens.\n\n\n\n",
    "jawaban": "The correct answer is: (2) \u2192 Preprocess Text, (1) \u2192 corpus"
  },
  "a75d59423bfee0880b31225df8ce027afd03fa3e3f3254a427834b4714e419da": {
    "soal": "Which of the following is NOT a TensorFlow optimizer?",
    "jawaban": "The correct answer is: GradientBoosting"
  },
  "e8f36d899974d0acee859a06d16f32db9f2931e3fc29522b5385db6978c91230": {
    "soal": "import numpy as np\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n# Contoh data: nilai aktual dan prediksi\ny_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0]) # Nilai aktual\ny_pred = np.array([1, 0, 1, 0, 0, 1, 0, 1, 1, 0]) # Prediksi model\n# Menghitung confusion matrix\ncm = confusion_matrix(y_true, y_pred)\n# Menampilkan confusion matrix dalam bentuk teks\nprint(\"Confusion Matrix:\n\", cm)\n# Menampilkan confusion matrix dalam bentuk grafik\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.show()\nBagaimana cara menampilkan confusion matrix dalam bentuk teks menggunakan scikit-learn?",
    "jawaban": "The correct answer is: print(confusion_matrix(y_true, y_pred))"
  },
  "ef847927dd659aa4671ef4ce511ca288e518db772eb5718adc7f34b72a349e02": {
    "soal": "import pm4py\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\n# Import XES log\nlog = xes_importer.apply(xes_filename)\n# Cek jumlah kasus dan aktivitas\nprint(f\"Jumlah kasus: \")\nactivities = set()\nfor trace in log:\nfor event in trace:\nactivities.add(event['concept:name'])\nprint(f\"Jumlah aktivitas unik: {len(activities)}\")\nfrom pm4py.algo.discovery.dfg import algorithm as dfg_discovery\nfrom pm4py.visualization.dfg import visualizer as dfg_visualization\n# DFG berdasarkan frekuensi\ndfg = dfg_discovery.apply(log,\nvariant=dfg_discovery.Variants.FREQUENCY)\n# Tampilkan visualisasi\ndfg_vis = dfg_visualization.apply(dfg, log=log,\nvariant=dfg_visualization.Variants.FREQUENCY)\ndfg_visualization.view(dfg_vis)\nWhich PM4Py function is used to load XES event logs?\n{\n=xes_importer.apply()\n-read_csv()\n-read_json()\n-read_xml()\n-import_log()\n-load_data()\n}",
    "jawaban": "The correct answer is: len(log)"
  },
  "bbce052428d2616cbc4d01b767a07126156b8c431dda738b5bd83961ea503059": {
    "soal": "Only ..................... input is suitable for the Distance Matrix widget, which are the Distances widget and the Distance Transformation widget. The output from the Distance Matrix widget is a data table containing the distance matrix. Users can decide how to label the table and distance matrix (or examples in the distance matrix), which can then be visualized or displayed in a separate data table.\n",
    "jawaban": "The correct answer is: two"
  },
  "fd5dc9ac43cd61a4c0ee1cf6098ff6fbb8e2a12afe1d012362a2baf45396aa57": {
    "soal": "Which of the following is a common data augmentation technique?",
    "jawaban": "The correct answer is: Generating new data samples by applying transformations like rotation or scaling."
  },
  "ca205397c743a46d47924e6d40996cbadfcaa6a688c872ceb91f286150a101ab": {
    "soal": "In the Correspondence Analysis Widget - Correspondence Analysis (CA) computes the linear CA transformation of input data. Although similar to PCA, CA computes a linear transformation on discrete data, not on ......................\n",
    "jawaban": "The correct answer is: continuous"
  },
  "2ba61faa2da542774113f90eacf662947c2cd4b1491c546b8a022aaa294d276c": {
    "soal": "Salah satu URL tempat lokasi contoh2 pengembangan / penggunaan ORANGE ada di\n\n\u00a0https://orange.biolab.si/categories/examples/\n",
    "jawaban": "The correct answer is 'True'."
  },
  "84283cdbfb4af94b1407bf40cb2aa4e35808d3e247deefa174883f78f7441e63": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nWhich activation function is applied in the output layer of the model?",
    "jawaban": "The correct answer is: None"
  },
  "2ed3c088502c3037829f5dc41c495b2acf36c3cab55b85f8e98209d957997f39": {
    "soal": "In the workflow below, we use the heart disease data and select the women subset from the Scatter Plot. Then, we integrate the distances between columns in the Distance Map. Since the subset also contains some discrete data, the Distances widget warns us that it will ignore the discrete features, so we will only see continuous instances/attributes in the map.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "ced2df2faa658eb2aff84ba694f539ee0d2ff74ebc1ad852f1e78fb6fcb41d8d": {
    "soal": "from sklearn.model_selection import train_test_split\nimport pandas as pd\n# Contoh: Membuat DataFrame dengan fitur 'X' dan target 'y'\ndata =\ndf = pd.DataFrame(data)\n# Memisahkan fitur dan target\nX = df[['X1', 'X2']]\ny = df['y']\n# Membagi dataset menjadi data pelatihan dan pengujian dengan stratifikasi\nX_train, X_test, y_train, y_test = train_test_split(\nX, y, test_size=0.3, stratify=y, random_state=42\n)\n# Menampilkan proporsi kelas di setiap subset\nprint(\"Proporsi kelas di y_train:\n\", y_train.value_counts(normalize=True))\nprint(\"\nProporsi kelas di y_test:\n\", y_test.value_counts(normalize=True))\nWhat is the purpose of using `value_counts(normalize=True)` on a pandas Series?\n{\n~To count the total number of unique values.\n~To sort the values in ascending order.\n=To display the proportion of each unique value relative to the total count.\n~To normalize the data to a standard scale.\n~To identify missing values in the Series.\n~To convert categorical data into numerical codes.\n}",
    "jawaban": "The correct answer is: 'X1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'X2': [11, 12, 13, 14, 15, 16, 17, 18, 19, 20], 'y': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]"
  },
  "d6ce8746a8fa1088279f4e7fd5d5cee20ccf6e9dc0369934c23049bc5b9f34cb": {
    "soal": "\n\n\nThe most popular programming language widely used to support data science is C++.",
    "jawaban": "The correct answer is 'False'."
  },
  "443c6cfb28c6d67e7bae8c0c35cf1497d98d6ccfccb6c11b63788d2e8a931747": {
    "soal": "Some visualization widgets, such as Scatter Plot and several data projection widgets, can expose data instances within a data subset. In this workflow, the Scatter Plot visualizes data from the input data file, but also marks the data points selected in the Data Table (the selected rows).\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "c82493ad2c7693e1d37c035b8d84df453b19bab6693fcd4e6990e0a687f07201": {
    "soal": "The Bag of Words model creates a corpus with word counts for each data instance (document). The count can be ......................., binary (present or not) or sublinear (logarithmic of term frequency). The Bag of Words model is required in combination with the Word Enrichment widget and can be used for predictive modelling.\n\n\n",
    "jawaban": "The correct answer is: absolute"
  },
  "79d4034851a4a63b6293124e26c72811bfdd96b466e0485d0889bcae830d621d": {
    "soal": "Widget Dataset retrieves the selected dataset from the server and sends it to the output. The file is downloaded to local memory and is thus immediately available but requires an internet connection. Each dataset is accompanied by a description and information about data size, number of instances, number of variables, target, and tags.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "ff6a1b9e1bd0cbb39444d1848b6614563c48f7bab8bc2d4c09b6583e6fcea32b": {
    "soal": "The Distributions widget displays the distribution of continuous attribute values. If the data contains a class variable, the distribution can be conditioned on the class.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "535ede0f380602ef00f21c46dc9058f1bc4394f44e1f805bea3e935a8b3f7e03": {
    "soal": "What is the 'curse of dimensionality' in machine learning?",
    "jawaban": "The correct answer is: The phenomenon where the feature space becomes sparse as dimensions increase"
  },
  "48594377a7bb0a01cf355b1200581be452fbd597dde85afc1145ffa975ed4fb6": {
    "soal": "Which of the following is a characteristic of reinforcement learning?",
    "jawaban": "The correct answer is: Learning through interaction with an environment to maximize cumulative reward"
  },
  "9eaad0a78926e9fe771ebf21dada8bb162559dfa75de7f125421acbc9770e86e": {
    "soal": "What is the benefit of using Petri Nets?",
    "jawaban": "The correct answer is: Ability to represent complex parallel structures"
  },
  "ec6d0dce5e8d6e8b90679be23df50eb9e8ba207748328af8812e426724bf5b90": {
    "soal": "The ORANGE Constant Widget is typically used as a reference for the minimum results that will be obtained for other model learners.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "df88e037e2c7692b64a85983f9d2c547fa8e9e9d01e7ed6dbfc57dedb8927940": {
    "soal": "import pandas as pd\nfile_path = \"/content/drive/My Drive/NamaFolder/NamaFile.xlsx\"\ndf = pd.read_excel(file_path)\nprint(df.head())\nWhich method can be used to display the number of rows and columns in the DataFrame?",
    "jawaban": "The correct answer is: df.shape"
  },
  "c19f937ac0f553eef648afc9b914c758aeffc53a858b8a9fda01c0cb0d094c1b": {
    "soal": "\n\n\nIn general, machine learning techniques are divided into three (3), which are:\n\nSupervised Learning.\n............................. Learning.\nReinforced / Reinforcement Learning\n\n\n",
    "jawaban": "The correct answer is: unsupervised"
  },
  "9943e7816b0a760c463d454358688f6e7cf74e8371f08bbfd425cb7ce8b15d04": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nplt.imshow(x_train[0], cmap='gray')\nplt.title(f\"Label: \")\nplt.show()\nmodel = keras.Sequential([\nkeras.layers.Flatten(input_shape=(28, 28)),\nkeras.layers.Dense(128, activation='relu'),\nkeras.layers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nWhat activation function is used in the output layer?\n{\n= softmax\n~ relu\n~ sigmoid\n~ tanh\n~ linear\n~ none\n}",
    "jawaban": "The correct answer is: y_train[0]"
  },
  "5b2c88b365c9de97951b5252ec5b1523c1a9c4c840c90ead43635820d94ac3bc": {
    "soal": "Big data is a term referring to large and complex datasets obtained from various sources, including business transactions, social data, census data, medical data, scientific data, and others. The main characteristics of big data are volume, velocity, and variety.\n- Volume refers to the very large amount of data generated constantly.\n- Velocity (\nAnswer Question 76\n in English) refers to the speed at which data is generated and updated, as well as the ability to process data in real-time.\n- Variety refers to the various sources of data and different types of data that can be collected.\nBig data typically requires specialized technology and techniques to process, analyze, and interpret it, including machine learning, data mining, and statistical analysis techniques. The goal of big data analysis is to gain valuable insights and information, such as trends and patterns useful for business, scientific, or social decision-making.",
    "jawaban": "The correct answer is: Velocity"
  },
  "32e8d0b0b9bd9147521cb4afd212d231337a860d8b4ce4616858c7850cc99bf0": {
    "soal": "What port is mapped to the host for the Ollama service in the `docker-compose.yml` configuration?",
    "jawaban": "The correct answer is: 11434"
  },
  "e413ae40c92ff6496f4861fd16543c62fe686bbe8b86d4654f80b5c9e7655c26": {
    "soal": "Which command installs PM4Py within an activated virtual environment?",
    "jawaban": "The correct answer is: pip install pm4py"
  },
  "a0b5c79630aceefa9ddd842a8e319d86225ae77c1594a40a1b0c512ad958d364": {
    "soal": "Which URL should Open WebUI connect to for Ollama's API?",
    "jawaban": "The correct answer is: http://ollama:11434"
  },
  "2c6607898e1f93cf8f8988abf143ce0fee95f295af782b4e0122e821a10a6654": {
    "soal": "Column attribute names can be preceded by a label followed by a hash. Use c for class and m for meta attributes, i for ignoring columns, w for weight (of a column), and ....... , D, T, S for continuous, discrete, time, and string attribute types. Example: C#mph, mS#name, i#dummy.\n\nWhere ........ is,\n",
    "jawaban": "The correct answer is: C"
  },
  "76c49412b102e91833b3df16fb443ed5bfe1714224c6d314cf1f097642d6cae8": {
    "soal": "Big data refers to the massive and complex amount of data obtained from various sources, including business transactions, social data, census data, medical data, scientific data, and more. The main characteristics of big data are volume, velocity, and variety.\n- Volume refers to the very large amount of data generated at any given moment.\n- Velocity refers to the speed at which data is generated and updated, and the ability to process the data in real-time.\n- Variety refers to the different data sources and types of data that can be collected.\nBig data typically requires special technology and techniques to process, analyze, and interpret the data, including machine learning techniques, data mining, and statistical analysis. The goal of big data analysis is to derive valuable insights and information, such as trends and patterns useful for business, scientific, or social decision-making.",
    "jawaban": "The correct answer is: Volume"
  },
  "1a1dcebfa03b35d16f16b20afc878479f8c33ebc346cf670f7528b5604fbb3c1": {
    "soal": "model.save('my_model')\nfrom google.colab import files\nfiles.download('my_model')\nnew_model = tf.keras.models.load_model('my_model')\nWhich Python library provides the download function shown in the example?",
    "jawaban": "The correct answer is: google.colab"
  },
  "68336ed8626b11f241fe096f9c8b79bb7af33148a264edd7ee5689e6de7f8e76": {
    "soal": "For Deep Learning models like LSTM or BERT analyzing TikTok comments, what is the minimum number of comments recommended?",
    "jawaban": "The correct answer is: 10,000 comments"
  },
  "830ab7d96b8f1dc160e1f02504fc06e349b68d754c9afb3c5148200260b01072": {
    "soal": "The CN2 algorithm is a classification technique designed to efficiently induce simple and comprehensive rules in the form of \u201cif true then predict class,\u201d even in domains where there may be noise.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "f54fea8c4ced7fa10e4f7236aa60a6816b0c2226344d50a59e5515b8b7a3e9b5": {
    "soal": "Most Time Series algorithms assume that we do not have missing values in our data. In this widget, we can select the interpolation method to estimate the missing values. By default, the .............. widget will use linear interpolation (quick and reasonable).\n",
    "jawaban": "The correct answer is: Interpolate"
  },
  "e134cbfdc9b31075cc35ef71382b17e816d971ada2dde128bf6168a0c2215d3c": {
    "soal": "Currently, the only widget that provides the correct signal needed by the ............ widget is the Test & Score widget. Therefore, the ................. widget always follows the Test & Score widget, and since it has no output, no other widgets follow it.\n",
    "jawaban": "The correct answer is: Calibration Plot"
  },
  "3b86dda0f939552520156cd26efaaf184143189ef3df6b23c1a664a392f72084": {
    "soal": "Pada Widget Constant ORANGE - untuk classification, ketika memprediksi nilai class dengan Prediction, widget ini akan menghasilkan frekuensi min dari class yang ada di training set. Jika ada dua atau lebih majority class, classifier akan memilih secara analytic dari predicted class, tapi akan selalu menghasilkan class yang sama untuk contoh tersebut.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "8519eb10a2df87639e5b9bfa3501a1f742c1c7de7d08e85a7d1a64fb73f350d4": {
    "soal": "In the Interpolate widget, which is part of the Time Series add-on, the interpolation type - Nearest interpolation replaces missing values with previously defined values.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "10b6c63457256241ca80a0e4924bfc0803e216fe316ccdfb2766cf604848b5f4": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\n# xes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\nxes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nprocess_tree = pm4py.discover_bpmn_inductive(log)\nbpmn_model = pm4py.convert_to_bpmn(process_tree)\npm4py.view_bpmn(bpmn_model)\nWhat file is actually used for processing (uncommented)?",
    "jawaban": "The correct answer is: PM4PY-running-example.xes"
  },
  "c0ae91107b4b36d02d38e389b2325afe2a3f7b74ab74e233aa1ea9f2d6132f37": {
    "soal": "What initiates the LLM processing cycle?",
    "jawaban": "The correct answer is: User prompt"
  },
  "e00cbdeff393989c4a6bf5e6cf4a2a1900dea5a2745b1b90eba26ab79ff0ef27": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nWhich layer in the model is considered a fully connected (dense) layer?",
    "jawaban": "The correct answer is: Dense"
  },
  "de274b36dd6eb86a1c3b0e0430e9bc39d5a1db4f28dbc03f800e389942b80927": {
    "soal": "The Import Documents widget takes text files from a folder and creates a corpus. The Import Network widget can read .txt, .docx, .odt, .pdf, and .xml. If there are subfolders in the folder, they can be used to label classes.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "84e79a8a150996f213175a3f1e1ed30665ab7d0f80d07a4dc0271aeb92cb7a24": {
    "soal": "What is the main purpose of stratifying data when splitting a dataset?",
    "jawaban": "The correct answer is: To ensure that class proportions in subsets reflect those in the original dataset."
  },
  "8e0a9c173590c075cb0492c8d06f0938aca449e0d4f177fffd26097c43eae928": {
    "soal": "Before running any models, which command should you execute to start the Ollama server?",
    "jawaban": "The correct answer is: ollama serve"
  },
  "2096279f3ab72beeffa5736b839bf34ddade06dee8dcc96336af44072859f0d4": {
    "soal": "What is a dataset?",
    "jawaban": "The correct answer is: A structured collection of data used for analysis, model training, or testing."
  },
  "1a532b2fb04c868d736fe8bfe8b5df870dc49e645f0416901bbeafd41980ccd5": {
    "soal": "A heat map is a graphical method for visualizing attribute values based on class in a ............-way matrix. Heat maps can only be used on datasets that contain continuous variables. The values are represented by colors: the higher the value, the darker the color. By combining class and attributes on the x and y axes, we see where which attribute values are strongest and weakest, allowing us to find typical features (discrete) or value ranges (continuous) for each class.\u00a0\n\n\n",
    "jawaban": "The correct answer is: two"
  },
  "4e545bafb213c07cf26f15c608d5453d73a781c8eb4527ffda918d1e7f678461": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nWhat is the resolution of each image in the MNIST dataset?",
    "jawaban": "The correct answer is: 28x28 pixels"
  },
  "c925576eeb1f55c605c4515f9cccf22f749976102a0bc2de0c6d8f931f1211fc": {
    "soal": "The following workflow is an example where we can compare three (3) classifiers (i.e., Naive Bayes, Tree, and Constant) and input them into the Test & Score widget. We are using the Titanic dataset. The Test & Score widget will display the evaluation results for each classifier. Then, we can plot through the Calibration Plot widget and the ROC Analysis widget from the output of the Test & Score widget for further analysis of the performance of each classifier. The Calibration Plot widget allows us to see the accuracy predictions of the class probability in the form of a plot/image.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "d9c20e6dfbb356aeaaa835da46d90deeb956c75741ba052e98938696a3c31251": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.rand(100) # 100 nilai target\n# Inisialisasi model regresi linier\nmodel = LinearRegression()\n# Tentukan jumlah fold (misalnya, 5-fold cross-validation)\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n# Evaluasi model menggunakan cross-validation\nscores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n# Menampilkan skor R^2 untuk setiap fold dan rata-ratanya\nprint(\"Skor R^2 untuk setiap fold:\", scores)\nprint(\"Rata-rata skor R^2:\", np.mean(scores))\nIn the code, how many folds are specified for K-Fold Cross-Validation?",
    "jawaban": "The correct answer is: 5"
  },
  "3aa7431e766cdb745cabc4f87669a04196a109596a690a3393f1c6304cd0ac44": {
    "soal": "Data does not always come in a nice table format. Data can also be in the form of text, audio recordings, video materials, or even images. However, computers can only work with numbers, so for any data mining, we need to transform the unstructured data into a vector representation.\n\n\nTo extract numbers from unstructured data, Orange can use a deep network embedder. Orange has just started integrating various embedders in the Image Embedding widget, and for now, they are available for text, images, and audio.\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "194531bcc68eb7af42b443e893036905bc527063e42a885a6c56935b83db5fce": {
    "soal": "The Tree widget can only be used for regression tasks.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "1a8d10f57a23eb31392161a371e15006d47ba015f2778a36ef9d07e1b1152486": {
    "soal": "Widget Select Columns is used to automatically organize data domains. Users can decide which attributes to use and how. Orange differentiates between regular attributes, class attributes (optional), and meta attributes. For example, to build a classification model, the domain will consist of a set of attributes and discrete class attributes. Meta attributes are not used in modeling, but some widgets can use them to label instances.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "c2f458c6f5022f08e6ae25f070b49d9204bcde2baf34156f509922ee2c61756a": {
    "soal": "The Preprocess Text widget in ORANGE applies preprocessing steps in sequence. This means it will first transform the text, then apply tokenization, POS tagging, normalization, filtering, and finally build n-grams from the given tokens. This is crucial for the WordNet Lemmatizer because it does not require POS tags for proper normalization.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "f2475de9cbeee1bf203a199978d0749e7a548b673d49caa48fec895f45590878": {
    "soal": "The Predictions widget shows the probability and final decision of the prediction model. The output of the Predictions widget is another dataset, where predictions are added as a new meta attribute. We can choose which features we want to output (original data, predictions, probabilities). The results can be observed in the Data Table. If the predicted data includes the actual class values, the prediction results can also be observed in the Data Save widget.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "530fb6ffadd138006715ba354706024edf342a35103de4e1b14e6b39e11ca6cf": {
    "soal": "What is the purpose of the 'softmax' activation function?",
    "jawaban": "The correct answer is: To convert raw scores into probability distributions"
  },
  "3a1a31aaaf4810cfa48edcca6ecea29f98f7bcf0540765adef12779b07141ec7": {
    "soal": "The Distance Matrix widget creates a distance matrix, which is a two-dimensional array containing distances, taken pairwise, between elements of the dataset. The number of elements in the dataset determines the size of the matrix. Data matrices are very important for the Hierarchical Clustering Widget.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "7cad71be2a86f56c958aa92c0310b9a95cecdb24b28994774c5102b209c72c04": {
    "soal": "import pandas as pd\ndf = pd.read_excel('path_ke_file.xlsx', sheet_name=0)\ndf_selected = df[['Kolom1', 'Kolom2']]\nprint(df_selected.head())\nWhich parameter in `read_excel()` specifies additional strings to recognize as NA/NaN?",
    "jawaban": "The correct answer is: na_values"
  },
  "5702bb3f385e0851fbe3ee606b123cdaba776d39688888501b83147ca21f9f1f": {
    "soal": "Which platform is known for hosting public datasets for machine learning?",
    "jawaban": "The correct answer is: Kaggle."
  },
  "fb653f5dec45eec5075c1eeb8cefa20bb43cac0e007e75373fc41bae6fa262e6": {
    "soal": "In the Import Documents Widget, to retrieve data, select (click) the folder icon on the right side of the widget. Select (choose) the folder that we want to make corpus. After loading is complete, we will see how many documents were successfully retrieved by the Widget ..................... To observe the obtained corpus, connect the Import Documents widget to the Corpus Viewer Widget. In this workflow, we are using a set of speeches by President Kennedy in plain text format.\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Import Documents"
  },
  "4b991876e0996750ec791e31859772ad0b4dd28271c92c524c23533b1893d5ab": {
    "soal": "To obtain numerical representations of the images from Image Import, we need to send the images to the Text Embedding widget as shown in the workflow below.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "bd9ca96e713a0cc2f6e366033273e40574cda2e8da07a61a6cfab8bb66332df5": {
    "soal": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n# Contoh: Memuat dataset dari file CSV\n# Gantilah 'data.csv' dengan path ke dataset Anda\ndf = pd.read_csv('data.csv')\n# Memisahkan fitur (X) dan target \n# Asumsikan kolom terakhir adalah target\nX = df.iloc[:, :-1]\ny = df.iloc[:, -1]\n# Membagi dataset menjadi data latih dan data uji (80% : 20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model Decision Tree\nmodel = DecisionTreeClassifier()\n# Melatih model dengan data latih\nmodel.fit(X_train, y_train)\n# Melakukan prediksi pada data uji\ny_pred = model.predict(X_test)\n# Menghitung akurasi\nakurasi = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model: ')\n# Menampilkan laporan klasifikasi yang mencakup presisi, recall, dan F1-score\nprint(classification_report(y_test, y_pred))\nWhat does the `test_size=0.2` parameter specify in the `train_test_split` function?\n{\n~That 20% of the features are used for testing.\n~That the test set will contain 20 samples.\n=That 20% of the dataset is allocated for the test set.\n~That the training set will contain 20 samples.\n~That the test set will have a fixed size of 20 units.\n~That 80% of the dataset is allocated for the test set.\n}",
    "jawaban": "The correct answer is: akurasi:.2f"
  },
  "a6711f9819ab99423b3a4987f0ad0cfa87dfc87e9999bb2eac03750d28bd5bc4": {
    "soal": "Which function is used to create a tensor with a normal distribution?",
    "jawaban": "The correct answer is: random_normal()"
  },
  "d35644fc5b959b07811d13b02e5e448c2a3fdea66ce97f834d222c6289708fbb": {
    "soal": "In the following workflow example, we will show the very simple use of the Corpus widget. Place the Corpus widget on the canvas and connect it to the Corpus Viewer widget. We use the dataset book-excerpts.jpg, which is available in the add-on, and examine it in the Corpus Viewer widget.\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "639cc2df52c5d5028669343be5a59a9214771a50221a88fe1763b299e672b19c": {
    "soal": "The .................... widget displays the model predictions on the data.\n",
    "jawaban": "The correct answer is: Predictions"
  },
  "81e66db9bd497409f50ad746972472b491a34538fcd4c1a42aad4c5f0f4feec9": {
    "soal": "According to the official Zoho website, Zoho Analytics is a comprehensive, reliable, and scalable analytics platform. Developers and system integrators (SI) can use this platform to develop and deploy custom analytic applications and integrations.\nAnother advantage of Zoho Analytics is that it is user-friendly, making it easy for users to upload and control data. Using Zoho Analytics, data practitioners can create multifaceted and custom dashboards. The platform is easy to use and implement.",
    "jawaban": "The correct answer is 'True'."
  },
  "2cde941fb8dc8cd7107a234710d12c48a3b6324757c8201622b47604953f2553": {
    "soal": "If\n\n1 AND 1 = 1 and 1 OR 1 = 1\n\n1 AND 0 = 0 and 1 OR 0 = 1\n\n0 AND 1 = 0 and 0 OR 1 = 1\n\n0 AND 0 = 0 and 0 OR 0 = 0\n\nthen in binary\n\n( 100 AND 000 ) OR 111 = ..... (binary)",
    "jawaban": "The correct answer is: 111"
  },
  "e61de5dc33292fe511f7acc24d232a3bbc6459f47ec3164b5af56bff871d7534": {
    "soal": "Recursive splitting of event logs is characteristic of:",
    "jawaban": "The correct answer is: Inductive Miner"
  },
  "7ab78d1f8b22882d260cb2fb8dad0326d0268d3d3063fc94da2b73db928ebc5c": {
    "soal": "The Tree widget can only be used for classification tasks.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "8b30585ab8402611290f7ea0768477c43ce216992fc5d4701b0a9d9f8b72be4f": {
    "soal": "How does process mining contribute to business process management?",
    "jawaban": "The correct answer is: By providing data-driven insights into actual process executions"
  },
  "4487a0be07c04eafb62f891206ecebd1890e06093df762d79ee55ac78c23caea": {
    "soal": "The following workflow is an example where we compare three (3) classifiers (Naive Bayes, Tree, and Constant) and input them into the Test & Score widget. We are using the Titanic dataset. The Test & Score widget will show the evaluation results for each classifier. Then, we can plot through the .............. widget and ROC Analysis widget from the Test & Score output for further analysis of the performance of each classifier. The Calibration Plot widget allows us to visualize the predicted accuracy of class probabilities in a plot/image form.\n\n\n",
    "jawaban": "The correct answer is: Calibration Plot"
  },
  "c10c30b230ec64ac6052d614217dad5350c1285fede534be3cbf0d17a8d0ce38": {
    "soal": "The Distance Transform widget transforms the distances present in the ..........................\n",
    "jawaban": "The correct answer is: dataset"
  },
  "ab206fb658e70875fddbb4d63e8bf135d843d50dd005e844f39b65d60f161ce5": {
    "soal": "The Select Columns widget is used to manually arrange the data domain. The user can decide which attributes to use and how to do it. Orange distinguishes between regular attributes, class attributes (optional), and ............................. meta attributes. For example, to build a classification model, the domain will consist of a set of attributes and discrete class attributes. Meta attributes are not used in modeling, but some widgets can use them to label instances.\n",
    "jawaban": "The correct answer is: meta"
  },
  "e752b7b1e7c481daa088947c113e0df292c707448186cebade99d62ac214720d": {
    "soal": "The following workflow is an example where we compare three (3) classifiers (Naive Bayes, Tree, and Constant) and input them into the .............. widget. We are using the Titanic dataset. The Test & Score widget will show the evaluation results for each classifier. Then, we can plot through the Calibration Plot widget and ROC Analysis widget from the Test & Score output for further analysis of the performance of each classifier. The Calibration Plot widget allows us to visualize the predicted accuracy of class probabilities in a plot/image form.\n\n\n",
    "jawaban": "The correct answer is: Test & Score"
  },
  "dfc879b82b8c631a1131e2de19a44759ce73244c6b2354e07afef755c1b21c9c": {
    "soal": "In the Interpolate widget, which is part of the Time Series add-on, the interpolation type - Linear interpolation replaces missing values with ........................... values between two nearest and defined data points.\n",
    "jawaban": "The correct answer is: linearly-spaced"
  },
  "c882c7c23d5cc6300962710a78be2022025ceaad1ea1504caccd78a3f31127e7": {
    "soal": "If\n\n1 AND 1 = 1\n\n1 AND 0 = 0\n\n0 AND 1 = 0\n\n0 AND 0 = 0\n\nThen 110 AND 110 = ......... (binary)",
    "jawaban": "The correct answer is: 110"
  },
  "e075e3cfc2b9326d97766bac173d8d6f6ba16bbd82bd2ea100b9be151a0571cf": {
    "soal": "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target biner\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(0, 2, 100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi data latih dan data uji (80% latih, 20% uji)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi dan pelatihan model klasifikasi (misalnya RandomForest)\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n# Prediksi pada data uji\ny_pred = model.predict(X_test)\n# Menghitung Precision, Recall, dan F1-Score\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nprint(f'Precision: ')\nprint(f'Recall: {recall:.2f}')\nprint(f'F1-Score: {f1:.2f}')\n# Alternatif: Menggunakan classification_report untuk ringkasan metrik\nprint('\nClassification Report:')\nprint(classification_report(y_test, y_pred))\nWhich function is used to split data into training and testing sets in scikit-learn?\n{\n~cross_val_score\n~train_test_split\n=train_test_split\n~split_data\n~data_partition\n~dataset_split\n}",
    "jawaban": "The correct answer is: precision:.2f"
  },
  "ad701c981f6d31cc022515ded1aa31a157276da0edc11c08313273f564f5b5fb": {
    "soal": "The Tree widget can be used for ....................... tasks and regression tasks.\n",
    "jawaban": "The correct answer is: classification"
  },
  "1f55a545460e25b9d86195424e9db1fe98e67d5ad8e8a38d55ca8cdbd25e5b6d": {
    "soal": "from google.colab import drive\ndrive.mount('/content/drive')\nWhy does Google Colab require authentication when mounting Google Drive?",
    "jawaban": "The correct answer is: All of the above."
  },
  "30b0dff59470ad0223944fa9f771458c57515ed800b41a3b338fb98430ce2658": {
    "soal": "The Predictions widget displays model predictions on ....................\n",
    "jawaban": "The correct answer is: data"
  },
  "001ad438b4b865e3d2f00ef25d175284b5f9382170ef71bb3ea1b7e99d58e517": {
    "soal": "One example of using the ................. widget is shown in the workflow below. Using the Tree widget, we can induce a model and check it using a view similar to the Tree Viewer widget.\n\n\n\n",
    "jawaban": "The correct answer is: Tree"
  },
  "a032655e9235f8700bbef1792b272fc8c03fe668ce1da99ac7b0cb5ebda6bfab": {
    "soal": "The ............... widget is a simple widget that presents information about the dataset size, features, target, meta attributes, and location.\n\n\n",
    "jawaban": "The correct answer is: Data Info"
  },
  "64e4a410eee15829cc35632ca40478cb725d1846be133f654ac9c7a969e922a8": {
    "soal": "import pandas as pd\n# Contoh: Membuat DataFrame dengan missing values\ndata =\ndf = pd.DataFrame(data)\n# Menampilkan jumlah missing values di setiap kolom\nprint(\"Jumlah missing values per kolom:\")\nprint(df.isnull().sum())\n# Menghapus baris yang memiliki missing values\ndf_cleaned = df.dropna()\nprint(df_cleaned)\n# Mengisi missing values pada kolom 'Usia' dengan mean\ndf['Usia'].fillna(df['Usia'].mean(), inplace=True)\n# Mengisi missing values pada kolom 'Usia' dengan median\ndf['Usia'].fillna(df['Usia'].median(), inplace=True)\n# Mengisi missing values pada kolom 'Kota' dengan modus\ndf['Kota'].fillna(df['Kota'].mode()[0], inplace=True)\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n# Inisialisasi imputer untuk mengisi missing values dengan mean\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n# Mengimputasi kolom 'Usia'\ndf[['Usia']] = imputer.fit_transform(df[['Usia']])\nIn the provided code, which column contains a missing value represented by None?\n{\n~Nama\n=Usia\n~Kota\n~Semua kolom\n~Tidak ada kolom\n~Hanya kolom numerik\n}",
    "jawaban": "The correct answer is: 'Nama': ['Andi', 'Budi', 'Citra', 'Dewi'], 'Usia': [25, 30, None, 22], 'Kota': ['Jakarta', None, 'Bandung', 'Surabaya']"
  },
  "433b76a396e975141a667b7f49f601fe7d3491dcc2b223daa483809dedc0e115": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#-xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\ndfg, start_activities, end_activities = pm4py.discover_dfg(log)\npm4py.view_dfg(dfg, start_activities, end_activities)\nWhat does the 'view_dfg' function do?",
    "jawaban": "The correct answer is: Visualizes the Directly-Follows Graph -Exports data to Excel -Trains a model -Prints summary statistics -Cleans data -Reads file data"
  },
  "c30723f1f2f4b3c7f45b6659d2c1a5cff2bb49c2588e23731be0f3e9f8b44b1e": {
    "soal": "The following regular expression (regex)\n\nmatches words that are longer than 4 characters\n\nis\n",
    "jawaban": "The correct answer is: \\w{4,}"
  },
  "9aa6655df1ec440334808ee8b568d615a347de45a1cc75a4e1c6f78cf3e4f5f2": {
    "soal": "Principal Component Analysis (PCA) is typically performed on multivariable (multivariate) systems. In simple terms, PCA looks for the least dominant variable from the categories (classes) present in the data. The technique used is to perform a coordinate transformation of the data to obtain new coordinates (new variables), usually fewer than the number of coordinates/variables in the original data, to better represent the categories (classes) in the data.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "96f8897ce6754798306abcbda75249e30117f06a5381b663b996cb0b5660238b": {
    "soal": "Salah satu URL tempat lokasi contoh2 pengembangan / penggunaan ORANGE ada di\n\n\u00a0http://www.englishcollocation.com/how-to-use/orange\n",
    "jawaban": "The correct answer is 'False'."
  },
  "3c3ac17af06f70faa93e9baabbf151e1d031d343add62952c1e5d67493785d3d": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nIn the example code for using Fashion-MNIST, how are the pixel values of the images normalized?",
    "jawaban": "The correct answer is: Dividing by 255.0"
  },
  "1b637aca1b54dbec24bb4f889b4ed3b6c7140606ec918c109f6b274b38f48d40": {
    "soal": "What command lists available models in the Ollama container?",
    "jawaban": "The correct answer is: ollama list"
  },
  "201465f72ef1ea96c2abc2fe2bd6e9be28f8137a778970299471964fb8c31006": {
    "soal": "In the ..............(1)................. widget, to integrate the time series\u2019 difference from the .................(2)............. widget, use .............(3)............. on a wide enough window to capture the full series.\n\n\n",
    "jawaban": "The correct answer is: (3) \u2192 Cumulative sum aggregation, (1) \u2192 Moving Transform, (2) \u2192 Difference"
  },
  "4dfa5ed03adfb2044223d167c41caaefd36f413715662c7aa3aaceaa84f7e02b": {
    "soal": "Pada Widget Constant ORANGE - untuk classification, ketika memprediksi nilai class dengan Prediction, widget ini akan menghasilkan frekuensi relative dari class yang ada di training set. Jika ada dua atau lebih majority class, classifier akan memilih secara manual dari predicted class, tapi akan selalu menghasilkan class yang sama untuk contoh tersebut.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "71b8dc54a6c82af4cfd5bab75dcbadc633fc0bc1adfcd4bc0a02e7f0c31b298d": {
    "soal": "Data does not always come in a nice table format. Data can also be in the form of text, audio recordings, video materials, or even images. However, computers can only work with numbers, so for any data mining, we need to transform the unstructured data into a vector representation.\n\n\nTo extract numbers from unstructured data, Orange can use a deep network embedder. Orange has just started integrating various embedders in the Image Embedding widget, and for now, they are available for text and images.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "48ec788e2fb4312fc6e03c997f28c50438ecffd5fade5fc667eaea007068d5af": {
    "soal": "Data Science is an interdisciplinary field that uses methods, processes, ...................... and scientific systems to extract knowledge and insights from data in various forms, both structured and unstructured, similar to data mining.\n",
    "jawaban": "The correct answer is: algorithm"
  },
  "a86d1cc9e104f697632aa45fbd4ea003728df9aec04f86b7be4b396328c12628": {
    "soal": "In the workflow below, the Tree widget will train a model and evaluate it against Logistic Regression. The accuracy comparison of the Tree widget and the Logistic Regression widget is done through the Scattering Plot widget. The results of the Test & Score widget can be viewed through the Confusion Matrix widget to compare the classification errors that occurred.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "f9e4e16773e315f5b2f73596fee220c6e307530b81cdb822d69fca1ec45cf0d5": {
    "soal": "\n\n\n\n\n\n\n\n\nIn general, machine learning techniques are divided into three (3) categories:\n\nSupervised Modeling.\nUnsupervised Learning.\nReinforced / Reinforcement Learning\n",
    "jawaban": "The correct answer is 'False'."
  },
  "c72237e4bfb4e3ef18999b3d18aa169933bdc9f1c9b63640a76679c73b301108": {
    "soal": "Which activation function is commonly used to convert logits to probabilities in multi-class classification?",
    "jawaban": "The correct answer is: Softmax"
  },
  "8f7ce721f64d6a080784f40057dd4c2704d59aa86be91b932bd4d968c1bfcf97": {
    "soal": "In the Line Chart widget, we can input forecast signals from the forecast model results. The forecast is drawn with a dashed line and confidence intervals (...................) as a range area.\n\n\n\n\n",
    "jawaban": "The correct answer is: confidence"
  },
  "790fcd90e2658aaec2ac47580dcb1fde7b12ed051b90337cf0c5994255339ff8": {
    "soal": "The Import Images widget is probably the first widget we will use in image analysis. The Import Images widget loads images and creates a class value from the folder. In this example, we use the Import Images widget to load 26 paintings by Monet or Manet.\n\nWe can observe the results in the ...................... widget. It is clear how Orange adds an additional class attribute with values Monet and Manet.\n\nNext, we can continue with standard machine learning methods. We send the images to the Image Embedding widget, where we will use the Painters embedder to get the image vector.\n\nThen we will use the Test & Score widget and the Logistic Regression widget to create a model to predict the artist of a painting (in this case Monet or Manet). We get a very good score. How could that be? It turns out these images were already trained on the Painters embedder, so the accuracy achieved will be high.\n\n\n",
    "jawaban": "The correct answer is: Data Table"
  },
  "87c5c8585529d1ede12a15c2bd856a8ceb4bc81c206da93a7bc5a6ae73d980b1": {
    "soal": "How does K-Fold Cross Validation work?",
    "jawaban": "The correct answer is: Divides the dataset into K subsets and iteratively uses each subset as a Validation set while the remaining K-1 subsets are used for training"
  },
  "e83e117c01e61d2322755a407582e6615938eb524e32d8605314876b79cfa235": {
    "soal": "Which repository offers tools for data research, preparation, and manipulation for model trainers and artists using ComfyUI?",
    "jawaban": "The correct answer is: ComfyUI-DataSet"
  },
  "a2214ddcb9e1c8828c935cf36ee3f220505b59527959da8ff5a2328bf69a4d01": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nWhat is a potential consequence of not using Dropout in a neural network model?",
    "jawaban": "The correct answer is: The model may overfit to the training data"
  },
  "310f64d8a5c46b7c20fbd1c07a1193a5d4fa1c4a6e7fa8c1fcaead830247318f": {
    "soal": "This workflow example shows how Network Analysis can enrich the workflow. We have used the lastfm.net data as input network from Network File and sent it to Network Analysis. We can decide to calculate the degree, degree centrality, and closeness centrality at node-level.\n\nWe can then visualize the network in Network Explorer. In the Network Explorer widget, we color it with the best tags, such as the default for this dataset. But now we can also set the node size to match the decentrality degree calculation results. This is a good way to visualize network properties.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "6f00ed1d60e4be7c84c95da140189a88bd9453a22dbad284f98195c3d0127b96": {
    "soal": "We use the zoo data set in combination with the .....(1)..... widget to find animal groups. Now we have clusters that we want to identify and what is significant for each cluster! Provide the cluster to the .....(2)..... widget and use \u2018Order by relevance\u2019 to find what defines the cluster. It seems they are separated by their type, although clustering was done without ....(3).....! This is an example of unsupervised learning.\n\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 Hierarchical Clustering, (2) \u2192 Box Plot, (3) \u2192 class label"
  },
  "5c942c3e661250794814069e43e36b358fe88dcc9b3ab56e0385ad77dd3fee56": {
    "soal": "111 (binary) + 1 (binary) = ?? (binary)",
    "jawaban": "The correct answer is: 1000"
  },
  "d98371dea6f7261c846358f12e883dd4fa3f38bf5b668b12a27b7aef7ef0a2d5": {
    "soal": "Below, we will use the Attrition - Train data from the Datasets widget. This data is about employee attrition. In other words, we want to know whether a certain employee will leave their job or not. We will create a prediction model with the Tree widget and observe the probabilities in Predictions.\n\nFor prediction, we need training data, which we have loaded in the first Datasets widget, and data to predict, which we will load in another Datasets widget. We will now use the Attrition - Predict data. Connect the second dataset to Predictions. Now we can see the predictions for three data instances from the second dataset.\n\nThe Tree model predicts that no employees will leave the company. We can try another model and see if the predictions change. Or, test the predictive score first in the ROC Analysis widget.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "207c215e327fe7436fbfa5ad20c24cdc337985ac62eb2d235d573a24caee9e5f": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\n# xes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\nxes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nprocess_tree = pm4py.discover_bpmn_inductive(log)\nbpmn_model = pm4py.convert_to_bpmn(process_tree)\npm4py.view_bpmn(bpmn_model)\nThe file 'BPIC_2012_A.xes' is currently:",
    "jawaban": "The correct answer is: commented out"
  },
  "ee34c0746b41f01c8257aecdff511f7630e43155c9b26f3287acb0498638fc5a": {
    "soal": "In the `docker-compose.yml` file, which service has a volume mounted at `/app/backend/data`?",
    "jawaban": "The correct answer is: Open-WebUI"
  },
  "52d7af05d829c39e4ec863c29fa4b562829625726aa7c31fe82e040634bc3d1b": {
    "soal": "We use the zoo dataset in combination with Hierarchical Clustering to find animal groups. Now we have the clusters we want to identify and what is significant for each cluster! Provide the cluster to the Box Plot widget and use \u2018Order by relevance\u2019 to find out what defines the cluster. It appears they are separated by their type, even though the clustering was done without ..................... class labels! This is an example of unsupervised learning.\n\n\n\n\n",
    "jawaban": "The correct answer is: class"
  },
  "77d0f73297f7b1f0aeebbde9becbbea812d592bc8f92871b36110cc7390f78af": {
    "soal": "In the Distance Map widget, usually, a color palette is used to visualize the entire range of distances that appear in the matrix. This can be changed by setting low and high thresholds. In this way, we can include the distance differences outside this range and visualize the interesting part of the distribution.\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "d91a2183fc8f93da127d49c489ce2f77f601f64512d205134b16be28a7460021": {
    "soal": "Example data selected in the first Data Table widget is passed to the second Data Table widget. Note that we can choose which dataset to view (iris or glass). Changing from one dataset to another changes the selection of data instances communicated if Commit on any change is selected.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "a45e49819e17f053990540315b3f3ccabeb698f682ff1a6cfaaabd6d4aaa6beb": {
    "soal": "In the workflow below, the Import Documents widget can load subfolders. We store President Kennedy's speeches in two (2) subfolders, pre-1962 and post-1962. If we load the parent folder, all three subfolders will be used as class labels. Check the output of the Import Documents widget in the Data Table widget.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "bed9c41cc2024199962a037e98e8b9373ba32b31c5a0e19fc6cafd830eba8850": {
    "soal": "Widget File reads input data files (data tables with data instances) and sends the dataset to its output channel. The history of the last opened file is stored in the widget. This widget also includes a directory with sample datasets that have not been pre-installed in Orange.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "7e5d56b5ae28eace59527ab7cbe2000e3bb77403cfbbb3809b4dd24c013861c8": {
    "soal": "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target biner\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(0, 2, 100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi data latih dan data uji (80% latih, 20% uji)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi dan pelatihan model klasifikasi (misalnya RandomForest)\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n# Prediksi pada data uji\ny_pred = model.predict(X_test)\n# Menghitung Precision, Recall, dan F1-Score\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nprint(f'Precision: ')\nprint(f'Recall: {recall:.2f}')\nprint(f'F1-Score: {f1:.2f}')\n# Alternatif: Menggunakan classification_report untuk ringkasan metrik\nprint('\nClassification Report:')\nprint(classification_report(y_test, y_pred))\nIf `X = np.random.rand(100, 10)`, what is the shape of `X`?\n{\n~(10, 100)\n~(100, 1)\n=(100, 10)\n~(10, 10)\n~(100, 100)\n~(1, 100)\n}",
    "jawaban": "The correct answer is: precision:.2f"
  },
  "60ea95056c9e42e66c0aadcbd03f7f1670713420bcd0643235c13e94536e35ba": {
    "soal": "import pandas as pd\n# Contoh: Membuat DataFrame dengan missing values\ndata =\ndf = pd.DataFrame(data)\n# Menampilkan jumlah missing values di setiap kolom\nprint(\"Jumlah missing values per kolom:\")\nprint(df.isnull().sum())\n# Menghapus baris yang memiliki missing values\ndf_cleaned = df.dropna()\nprint(df_cleaned)\n# Mengisi missing values pada kolom 'Usia' dengan mean\ndf['Usia'].fillna(df['Usia'].mean(), inplace=True)\n# Mengisi missing values pada kolom 'Usia' dengan median\ndf['Usia'].fillna(df['Usia'].median(), inplace=True)\n# Mengisi missing values pada kolom 'Kota' dengan modus\ndf['Kota'].fillna(df['Kota'].mode()[0], inplace=True)\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n# Inisialisasi imputer untuk mengisi missing values dengan mean\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n# Mengimputasi kolom 'Usia'\ndf[['Usia']] = imputer.fit_transform(df[['Usia']])\nWhat does the method df.isnull().sum() return?\n{\n=The number of missing values in each column.\n~The total number of missing values in the DataFrame.\n~A DataFrame without missing values.\n~The sum of all numerical columns.\n~The indices of rows with missing values.\n~A boolean mask of missing values.\n}",
    "jawaban": "The correct answer is: 'Nama': ['Andi', 'Budi', 'Citra', 'Dewi'], 'Usia': [25, 30, None, 22], 'Kota': ['Jakarta', None, 'Bandung', 'Surabaya']"
  },
  "be7243ffd926e64bbc4bf0be16074e457371c5465b6907359ce7f2b5f3f814e1": {
    "soal": "The output data type from the Time Series Widget ORANGE is a real number, so we can connect this data to other widgets/modules that can accept data.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "63872d985c2b9f180e955ec4e2dc5c4c15a0ce01f5a812c8778cbfbc724486ad": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nplt.imshow(x_train[0], cmap='gray')\nplt.title(f\"Label: \")\nplt.show()\nmodel = keras.Sequential([\nkeras.layers.Flatten(input_shape=(28, 28)),\nkeras.layers.Dense(128, activation='relu'),\nkeras.layers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nWhich Python package is used for plotting the sample image?\n{\n= matplotlib.pyplot\n~ seaborn\n~ plotly\n~ cv2\n~ PIL\n~ tkinter\n}",
    "jawaban": "The correct answer is: y_train[0]"
  },
  "ec44000d88a49c28ded0bdd98174e1ea55fd5da7057c2b90a56473dc37491af9": {
    "soal": "When there is no data in the input, the Corpus widget reads the text corpus from a file and sends the corpus instance to the output channel. ....................... the last opened file is stored in the Corpus widget. In the Corpus widget, there is also a directory with sample corpora pre-installed with the add-on. The widget can read data from Excel (.xlsx), comma-separated (.csv), and tab-delimited (.tab).\n",
    "jawaban": "The correct answer is: History"
  },
  "0366b39d45f558654e80d0be73db17dada9f85718625aa938bc2dcfd0750957b": {
    "soal": "import pandas as pd\nfrom pm4py.objects.conversion.log import converter as log_converter\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom pm4py.visualization.petrinet import visualizer as pn_visualizer\nfrom pm4py.objects.log.util import dataframe_utils\n# csv_filename='Insurance_claims_event_log.csv'\n# csv_filename='Business Travel Permits.csv'\n# csv_filename='Parent-Child Union Output.csv'\n# csv_filename='PM4PY_Dummy_Event_Log_100_Cases_pm4py.csv'\n# csv_filename='PM4Py_Dummy_Event_Log-congested-business-process.csv'\n# 1. Load CSV event log\ndf = pd.read_csv(csv_filename)\n# df = pd.read_csv(csv_filename,sep=';')\n# print(df.head())\n# 2. Format timestamps properly\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n# 3. Ensure column names match PM4Py expectations\ndf = dataframe_utils.convert_timestamp_columns_in_df(df)\n# 4. Convert to event log object\nevent_log = log_converter.apply(df, parameters=)\n# 5. Discover process model using Alpha Miner\nnet, initial_marking, final_marking = alpha_miner.apply(event_log)\n# 6. Visualize Petri Net\ngviz = pn_visualizer.apply(net, initial_marking, final_marking)\npn_visualizer.view(gviz)\nIf you encounter timestamp errors in PM4Py, what should you verify first?\n{\n=Timestamp formatting in DataFrame\n~Installation of Graphviz\n~PM4Py version\n~Process ID column\n~Activity names\n~Python version\n}",
    "jawaban": "The correct answer is: log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case_id'"
  },
  "953428725e8c9c18195c25b05d28facb80e2de2363bb4763b89deb4b20cd0959": {
    "soal": "To extract numbers from unstructured data, Orange can use .................. deep network embedder.\n",
    "jawaban": "The correct answer is: deep"
  },
  "ecd6e1e6f9b5e2f13b259bedf638458f5bb9cebca1c842570a1ac26531af6a6e": {
    "soal": "A pivot table is a statistical table that summarizes data from a larger table. This summary might include totals, averages, or other statistics, with the pivot table grouping them together in a meaningful way. Pivot tables are a technique in data processing.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "6a5ad1b47b7913bf7bb3da9d159124b9a04cee146bb98632d2c49f7dd3e1850a": {
    "soal": "import pandas as pd\nfile_path = \"/content/drive/MyDrive/your_file.csv\"\ndf = pd.read_csv(file_path)\nprint(df.head())\nWhich function would you use to read an CSV file into a DataFrame using 'pandas'?",
    "jawaban": "The correct answer is: pd.read_csv()"
  },
  "4ae3ac6c0b05c5ed19e1b73e3d4cbe12f51a100af101408f2eafbcabe81c688c": {
    "soal": "192 (decimal) AND 255 (decimal) = ...... (decimal)",
    "jawaban": "The correct answer is: 192"
  },
  "936e29df4839ded72e59487746af08be40e6bd8d27fcffa361b7b1caf8864056": {
    "soal": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n# Contoh: Memuat dataset dari file CSV\n# Gantilah 'data.csv' dengan path ke dataset Anda\ndf = pd.read_csv('data.csv')\n# Memisahkan fitur (X) dan target \n# Asumsikan kolom terakhir adalah target\nX = df.iloc[:, :-1]\ny = df.iloc[:, -1]\n# Membagi dataset menjadi data latih dan data uji (80% : 20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model Decision Tree\nmodel = DecisionTreeClassifier()\n# Melatih model dengan data latih\nmodel.fit(X_train, y_train)\n# Melakukan prediksi pada data uji\ny_pred = model.predict(X_test)\n# Menghitung akurasi\nakurasi = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model: ')\n# Menampilkan laporan klasifikasi yang mencakup presisi, recall, dan F1-score\nprint(classification_report(y_test, y_pred))\nWhich function is used to read a CSV file into a DataFrame?\n{\n~read_table()\n~read_excel()\n=read_csv()\n~read_json()\n~read_html()\n~read_sql()\n}",
    "jawaban": "The correct answer is: akurasi:.2f"
  },
  "66fee3c2a6901a1e05ea50f901c98685e3cf8ce3b3cd51d53cce5377b2b258bc": {
    "soal": "Which TensorFlow function is used to compute the softmax activation?",
    "jawaban": "The correct answer is: softmax()"
  },
  "a68c5a5f625feb2dee84adb4875cbc1160c72144d3c74ddc3f584e057f738f2e": {
    "soal": "Now, each core widget in Orange can function/interact smoothly with ................. so you can mix and match widgets as you like. Previously, one could not forward output from Select Columns (data table) to Preprocess Text (corpus), but this is no longer an issue.",
    "jawaban": "The correct answer is: Text"
  },
  "67cbee633026a301ef921b4a0b0afb413cf7cc0f854a97edb9fea031047d51e2": {
    "soal": "A Pivot Table can help us collect and transform data. This workflow takes Kickstarter projects and aggregates them by month. We can check the frequency of projects published per month and observe the differences between funded and non-funded projects.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "adb4ffff2bcc2db9b10d20b45318462c5487e871c8f9a33e8ec42fb6fabc827e": {
    "soal": "In the Interpolate widget, which is part of the Time Series add-on, the interpolation type - Mean interpolation replaces missing values with ....................... values of the series.\n",
    "jawaban": "The correct answer is: average"
  },
  "5522f69febe544f10e308c06d3396f483423f762483bc0dfbd1c322586f51a8f": {
    "soal": "The workflow clusters data items in the iris dataset by first checking the distances between data instances using the Distances widget. The Distance Matrix is passed to the ................................. widget, which creates a dendrogram. We can select different parts of the dendrogram to further analyze the related data.\n\n\n\n",
    "jawaban": "The correct answer is: Hierarchical Clustering"
  },
  "b2dd8eb4271736ae334fef7bf675aa74962490213af8aa7747f6986d9acf092c": {
    "soal": "The Test & Score widget receives a dataset and one or more predictors (predictive models, not learner algorithms). The Test & Score widget generates data and predictions.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "0778a7ef24a8cf405a5d3e6c9a9a4847393891643fc11542b396d3568d26b1b4": {
    "soal": "The Import Documents widget takes text files from a folder and creates a corpus. The Import Documents widget can read .txt, .docx, .odt, .pdf, and .xml. If there are subfolders in the folder, they can be used to label ..........................\n",
    "jawaban": "The correct answer is: class"
  },
  "6c404d00ed6998393153d39a242bd74805b99a460528e73b5fecdffab2a11d02": {
    "soal": "Output from the Yahoo Finance Widget is a time series table of open, high, below, close (OHLC), volume, and adjusted close prices.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "145d74636072a52cdfe1b0664ad33fcbaae9b5f480dc4e3df1a83d450bb15f3e": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#_xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nmap = pm4py.discover_heuristics_net(log)\npm4py.view_heuristics_net(map)\nThe 'log' variable is expected to contain what kind of data?",
    "jawaban": "The correct answer is: Event logs -Numeric arrays -JSON data -XML documents -Image data -Database records"
  },
  "e7811a4727377b63260f60ca3ca05ccdf1e6e479ab6f4b4ae67634203a53e339": {
    "soal": "How can you check the installed version of Ollama on your system?",
    "jawaban": "The correct answer is: ollama --version"
  },
  "896883dafb32809241501fb57beb9d476997d9083d7762e8236672cefbc4961f": {
    "soal": "The Tree widget can work for ................. tasks. In the workflow below, we use the housing dataset and give the dataset to the Tree widget. The selected Tree node in the Tree Viewer will be displayed in the Scatter Plot, and we can see that the selected example has the same feature.\n\n\n\n",
    "jawaban": "The correct answer is: regression"
  },
  "7c4f4ff1a47face18b583bb3d34e603ead7998d9e1252b94a64dc8a6463f9d45": {
    "soal": "The kNN widget uses the kNN algorithm, which searches for the n nearest training instances in the feature space and uses the average of the nearest features to predict.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "f6d74217570618eda7356a7270e310912744ec08d950ab5895a7167f5717f26b": {
    "soal": "What is the primary function of the Testing Set in dataset partitioning?",
    "jawaban": "The correct answer is: To evaluate the final model performance on unseen data"
  },
  "2986581001e08a15fe9d68e1519cdbaa32a42ac169268d2c5c5a3aba70100293": {
    "soal": "In the Interpolate widget, which is part of the Time Series add-on, the interpolation type - Mean interpolation replaces missing values with the maximum values of the series.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "8bb8298fc093745a38a438b582e94e6cd83213dc52d62cfa39e2bb3fb5212bf6": {
    "soal": "Widget Constant akan memprediksi most frequent class atau mean value dari sebuah training set.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "9e7d352a8c09977d3b5880d4de505c35694a16bd8bbd8b816b419b88746c9f43": {
    "soal": "If\n\n1 AND 1 = 1\n\n1 AND 0 = 0\n\n0 AND 1 = 0\n\n0 AND 0 = 0\n\nThen 111 AND 110 = ......... (binary)",
    "jawaban": "The correct answer is: 110"
  },
  "b7404066b2ad32686a1da2560bd05284fe4d7459f1aa596cb51b1c04b7476fc8": {
    "soal": "For journal publication or achieving high accuracy, what is the suggested minimum dataset size?",
    "jawaban": "The correct answer is: More than 10,000 data points"
  },
  "9c02058743958ca4bdf3c9b6b26e5d42c2a7fd2af3b33a8cc60cd8d2f17573fd": {
    "soal": "from googlesearch import search\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef get_page_content(url):\ntry:\nheaders =\nresponse = requests.get(url, headers=headers, timeout=10)\nsoup = BeautifulSoup(response.content, 'html.parser')\n# Ambil judul halaman\ntitle = soup.title.string if soup.title else 'No Title'\n# Ambil konten paragraf utama\nparagraphs = soup.find_all('p')\ntext_content = ' '.join([p.get_text() for p in paragraphs[:5]]) # Batasi 5 paragraf pertama\nreturn title.strip(), text_content.strip()\nexcept Exception as e:\nreturn 'Error', f\"Failed to fetch content: {e}\"\ndef google_scrape_with_content(keywords, num_results=5, output_file='scraped_results.csv'):\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'Rank', 'Title', 'URL', 'Content'])\nfor keyword in keywords:\nprint(f\"\n\ud83d\udd0d Searching for: {keyword}\")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nprint(f\" \u2192 Fetching: {url}\")\ntitle, content = get_page_content(url)\nwriter.writerow([keyword, i+1, title, url, content])\ntime.sleep(2) # Delay biar aman\nexcept Exception as e:\nprint(f\"\u274c Error while searching '{keyword}': {e}\")\nprint(f\"\n\u2705 All results + content saved to '{output_file}'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\ngoogle_scrape_with_content(keywords, num_results=5)\nWhat is the primary purpose of the provided Python script?\n{\n=To scrape Google search results for multiple keywords and save the title and content of each URL to a CSV file.\n~To perform sentiment analysis on Google search results.\n~To visualize search trends over time.\n~To automate the submission of search queries to Google.\n~To monitor changes in Google's search algorithms.\n~To translate search results into multiple languages.\n}",
    "jawaban": "The correct answer is: \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\""
  },
  "545e3b232bb0fd9206ee71c14efe0dbda0473bb22dd138bafca9862a0e0b8456": {
    "soal": "import pandas as pd\nfrom pm4py.objects.conversion.log import converter as log_converter\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom pm4py.visualization.petrinet import visualizer as pn_visualizer\nfrom pm4py.objects.log.util import dataframe_utils\n# csv_filename='Insurance_claims_event_log.csv'\n# csv_filename='Business Travel Permits.csv'\n# csv_filename='Parent-Child Union Output.csv'\n# csv_filename='PM4PY_Dummy_Event_Log_100_Cases_pm4py.csv'\n# csv_filename='PM4Py_Dummy_Event_Log-congested-business-process.csv'\n# 1. Load CSV event log\ndf = pd.read_csv(csv_filename)\n# df = pd.read_csv(csv_filename,sep=';')\n# print(df.head())\n# 2. Format timestamps properly\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n# 3. Ensure column names match PM4Py expectations\ndf = dataframe_utils.convert_timestamp_columns_in_df(df)\n# 4. Convert to event log object\nevent_log = log_converter.apply(df, parameters=)\n# 5. Discover process model using Alpha Miner\nnet, initial_marking, final_marking = alpha_miner.apply(event_log)\n# 6. Visualize Petri Net\ngviz = pn_visualizer.apply(net, initial_marking, final_marking)\npn_visualizer.view(gviz)\nIn PM4Py, what does an event log specifically represent?\n{\n~Individual timestamps\n=Collection of process instances with activities\n~Database of employee actions\n~Visual graph\n~Activity durations\n~Timestamp records\n}",
    "jawaban": "The correct answer is: log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case_id'"
  },
  "05a0dfc76c9a378240f698b05f9a9030507672a27bf8b2897960cdda8b20fc51": {
    "soal": "Widget CN2 Rule Induction akan meng-induce (menginduksi) rule dari data menggunakan algoritma CNNN.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "7f9ae7089a71a85a69568777fd91ab727a0aa4b303d8de459333cef1f34f4bf6": {
    "soal": "\n\n\nThe workflow above tests/evaluates CN2 Rule Induction and Tree in the Widget Test & Score. It appears that CN2 Rule Induction is better.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "432b7939ee22cf8bc20d114f92b5575cb63de9c3b5c52fd72a446dcd2949f580": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#_xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nmap = pm4py.discover_heuristics_net(log)\npm4py.view_heuristics_net(map)\nWhat type of file is being loaded in the script?",
    "jawaban": "The correct answer is: .xes -.csv -.json -.xml -.txt -.xls"
  },
  "7061a093439b12a374098b85bbba0caf7c975daf0d04de90defbdda8a5cdf4ec": {
    "soal": "1111 1111 (binary) AND 1010 1010 (binary) AND 0101 0101 (binary) =",
    "jawaban": "The correct answer is: 0"
  },
  "c807bbd004645287b32cdbef934e96914e15122ee0e07d348119fc7b09c397af": {
    "soal": "from googlesearch import search\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef get_page_content(url):\ntry:\nheaders =\nresponse = requests.get(url, headers=headers, timeout=10)\nsoup = BeautifulSoup(response.content, 'html.parser')\n# Ambil judul halaman\ntitle = soup.title.string if soup.title else 'No Title'\n# Ambil konten paragraf utama\nparagraphs = soup.find_all('p')\ntext_content = ' '.join([p.get_text() for p in paragraphs[:5]]) # Batasi 5 paragraf pertama\nreturn title.strip(), text_content.strip()\nexcept Exception as e:\nreturn 'Error', f\"Failed to fetch content: {e}\"\ndef google_scrape_with_content(keywords, num_results=5, output_file='scraped_results.csv'):\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'Rank', 'Title', 'URL', 'Content'])\nfor keyword in keywords:\nprint(f\"\n\ud83d\udd0d Searching for: {keyword}\")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nprint(f\" \u2192 Fetching: {url}\")\ntitle, content = get_page_content(url)\nwriter.writerow([keyword, i+1, title, url, content])\ntime.sleep(2) # Delay biar aman\nexcept Exception as e:\nprint(f\"\u274c Error while searching '{keyword}': {e}\")\nprint(f\"\n\u2705 All results + content saved to '{output_file}'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\ngoogle_scrape_with_content(keywords, num_results=5)\nWhich columns are included in the output CSV file?\n{\n=Keyword, Rank, Title, URL, Content\n~Keyword, Position, Link, Description\n~Search Term, Order, Web Address, Snippet\n~Term, Index, Hyperlink, Summary\n~Phrase, Number, Site, Extract\n~Query, Result Number, Webpage, Details\n}",
    "jawaban": "The correct answer is: \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\""
  },
  "3235f54096b9894ec8d13ac1ed08eb81d312c236e560cc638152b362395100a3": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.reuters.load_data(num_words=10000)\nIn the Reuters Newswire dataset, what do 'x_train' and 'x_test' represent?",
    "jawaban": "The correct answer is: Lists of news articles represented as sequences of word indices"
  },
  "b676ab0e535944b05c450c918a3bd3fface1e1a9652ea2772b74ba38e4faaf1e": {
    "soal": "What is the purpose of the 'embedding' layer in neural networks?",
    "jawaban": "The correct answer is: To convert categorical variables into dense vector representations"
  },
  "57cad35858d30e45976f751baca327a609815d7cb36d2e0aa9bd3c05d517413b": {
    "soal": "Widget ............(1)............. menggunakan algoritma Tree dengan kemampuan untuk melakukan ...........(2).......... pruning (pemangkasan ke depan).\n",
    "jawaban": "The correct answer is: (1) \u2192 Tree, (2) \u2192 forward"
  },
  "e4ae1c9866552d85a5e94136a91981898c16c6d7a9c76600de88f329dd1dc904": {
    "soal": "Inductive Miner is especially good at:",
    "jawaban": "The correct answer is: Complex nested process handling"
  },
  "2d545ef807589b7a58e3b409d17b043704178f7bad61c4928eb41196d6e8f7f2": {
    "soal": "Apache Hadoop is an open-source software framework written in Java for distributed storage and processing of very large data sets on clusters of computers built from commodity hardware. All modules in Hadoop are designed with the fundamental assumption that hardware failures (individual machines or racks of machines) are common and should thus be handled automatically in software by the framework.",
    "jawaban": "The correct answer is 'True'."
  },
  "85ab913e5a790568f0b7d89e265d05c9592e3e9ed2ba1839ef68ddbcd98b7c6c": {
    "soal": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n# Contoh: Memuat dataset dari file CSV\n# Gantilah 'data.csv' dengan path ke dataset Anda\ndf = pd.read_csv('data.csv')\n# Memisahkan fitur (X) dan target \n# Asumsikan kolom terakhir adalah target\nX = df.iloc[:, :-1]\ny = df.iloc[:, -1]\n# Membagi dataset menjadi data latih dan data uji (80% : 20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model Decision Tree\nmodel = DecisionTreeClassifier()\n# Melatih model dengan data latih\nmodel.fit(X_train, y_train)\n# Melakukan prediksi pada data uji\ny_pred = model.predict(X_test)\n# Menghitung akurasi\nakurasi = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model: ')\n# Menampilkan laporan klasifikasi yang mencakup presisi, recall, dan F1-score\nprint(classification_report(y_test, y_pred))\nWhat assumption is made about the position of the target variable in the DataFrame?\n{\n~That it is the first column.\n~That it is in the middle of the DataFrame.\n=That it is the last column.\n~That it is named 'target'.\n~That it is indexed by a specific number.\n~That it is spread across multiple columns.\n}",
    "jawaban": "The correct answer is: akurasi:.2f"
  },
  "d1a684f9a44aa01573bc92bf602cc5d5f4fb44173b10c2e921d61197fc147241": {
    "soal": "\n\n\n\n\n\nUnsupervised Learning techniques are divided into:\n\nClustering.\nAssociation Analysis.\nDimensionality Reduction.",
    "jawaban": "The correct answer is 'True'."
  },
  "89f267317ba760ade244fab53dc107c1bc92f21b04d01d181a89b6b059465e7c": {
    "soal": "In the Interpolate widget, which is part of the Time Series add-on, the interpolation type - Spline interpolation fits with a cubic polynomial to the values around the missing value. Therefore, this technique will be very ................. but will give the best results.\n",
    "jawaban": "The correct answer is: slow"
  },
  "a838754ec27129a6064e81998a0bfa81fc45d68a754e04989a9cfeff7ba6da8f": {
    "soal": "import pandas as pd\nfrom pm4py.objects.conversion.log import converter as log_converter\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom pm4py.visualization.petrinet import visualizer as pn_visualizer\nfrom pm4py.objects.log.util import dataframe_utils\n# csv_filename='Insurance_claims_event_log.csv'\n# csv_filename='Business Travel Permits.csv'\n# csv_filename='Parent-Child Union Output.csv'\n# csv_filename='PM4PY_Dummy_Event_Log_100_Cases_pm4py.csv'\n# csv_filename='PM4Py_Dummy_Event_Log-congested-business-process.csv'\n# 1. Load CSV event log\ndf = pd.read_csv(csv_filename)\n# df = pd.read_csv(csv_filename,sep=';')\n# print(df.head())\n# 2. Format timestamps properly\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n# 3. Ensure column names match PM4Py expectations\ndf = dataframe_utils.convert_timestamp_columns_in_df(df)\n# 4. Convert to event log object\nevent_log = log_converter.apply(df, parameters=)\n# 5. Discover process model using Alpha Miner\nnet, initial_marking, final_marking = alpha_miner.apply(event_log)\n# 6. Visualize Petri Net\ngviz = pn_visualizer.apply(net, initial_marking, final_marking)\npn_visualizer.view(gviz)\nWhat type of visualization is generated by PM4Py?\n{\n~Histogram\n=Petri Net\n~Scatter plot\n~Pie chart\n~Line chart\n~Boxplot\n}",
    "jawaban": "The correct answer is: log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case_id'"
  },
  "eb9298d1f55258015b93195f3c9b9387bd0441a5a407acbc139fc5877f6ec757": {
    "soal": "The kNN widget predicts based on the nearest training instance..........................\n\n\n\n",
    "jawaban": "The correct answer is: nearest"
  },
  "7dbb731c42acfc398de2e8322cdd4bb652c28b3b8702bbaba93484e2d651414a": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(2, size=100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi training (80%) dan testing (20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model, misalnya RandomForestClassifier\nmodel = RandomForestClassifier(random_state=42)\n# Melatih model pada data training\nmodel.fit(X_train, y_train)\n# Memprediksi data testing\ny_pred = model.predict(X_test)\n# Mengevaluasi akurasi model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model pada data testing: ')\nIn the code, what percentage of the data is allocated for training purposes?\n{\n~10%\n~20%\n=80%\n~50%\n~70%\n~90%\n}",
    "jawaban": "The correct answer is: accuracy:.2f"
  },
  "97df8ed2f4faafc98894f10a2f9ff91f2cb895bb863a07be146720506533e857": {
    "soal": "The Bag of Words model creates a corpus with word counts for each data instance (document). The count can be absolute, ........................ (present or not) or sublinear (logarithmic of term frequency). The Bag of Words model is required in combination with the Word Enrichment widget and can be used for predictive modelling.\n\n\n",
    "jawaban": "The correct answer is: binary"
  },
  "8771e9d8566f5f3bedfe3d093e83008c95afeebbb539e6a1e53cd3e677fd110f": {
    "soal": "The Distance Map widget can visualize distances between objects. The visualization is the same as if we printed a table of numbers, but the numbers are replaced with colored dots.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "7ccdc99cfa7ed7e70e6e91b72fc2b1f04d49fc24ac6c486c60a7a2014493df5a": {
    "soal": "import pandas as pd\n# Contoh: Membuat DataFrame dengan missing values\ndata =\ndf = pd.DataFrame(data)\n# Menampilkan jumlah missing values di setiap kolom\nprint(\"Jumlah missing values per kolom:\")\nprint(df.isnull().sum())\n# Menghapus baris yang memiliki missing values\ndf_cleaned = df.dropna()\nprint(df_cleaned)\n# Mengisi missing values pada kolom 'Usia' dengan mean\ndf['Usia'].fillna(df['Usia'].mean(), inplace=True)\n# Mengisi missing values pada kolom 'Usia' dengan median\ndf['Usia'].fillna(df['Usia'].median(), inplace=True)\n# Mengisi missing values pada kolom 'Kota' dengan modus\ndf['Kota'].fillna(df['Kota'].mode()[0], inplace=True)\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n# Inisialisasi imputer untuk mengisi missing values dengan mean\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n# Mengimputasi kolom 'Usia'\ndf[['Usia']] = imputer.fit_transform(df[['Usia']])\nWhich pandas function is used to fill missing values with the mean of a column?\n{\n=fillna()\n~dropna()\n~isnull()\n~notnull()\n~replace()\n~interpolate()\n}",
    "jawaban": "The correct answer is: 'Nama': ['Andi', 'Budi', 'Citra', 'Dewi'], 'Usia': [25, 30, None, 22], 'Kota': ['Jakarta', None, 'Bandung', 'Surabaya']"
  },
  "880ffcf1087b8160d3406a7197f1e2ba8874f65cfc5ff277776fb435006fa8f7": {
    "soal": "The result of 8 (decimal) OR 2 (decimal) is",
    "jawaban": "The correct answer is: 10"
  },
  "48114740c8874411627d23f4631f2fbd8f0c31fae855a26eb0d9c7aa263e2dcd": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nplt.imshow(x_train[0], cmap='gray')\nplt.title(f\"Label: \")\nplt.show()\nmodel = keras.Sequential([\nkeras.layers.Flatten(input_shape=(28, 28)),\nkeras.layers.Dense(128, activation='relu'),\nkeras.layers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nHow many units are in the first Dense layer?\n{\n= 128\n~ 10\n~ 256\n~ 64\n~ 28\n~ 512\n}",
    "jawaban": "The correct answer is: y_train[0]"
  },
  "626244f00c2acaf4d798cf38c67bd29027cfa34c2e2dfe2368300ba6fb7a690a": {
    "soal": "import pandas as pd\nfile_path = \"/content/drive/My Drive/NamaFolder/NamaFile.xlsx\"\ndf = pd.read_excel(file_path)\nprint(df.head())\nIn a Google Colab environment, which library is typically used to mount Google Drive?",
    "jawaban": "The correct answer is: google.colab.drive"
  },
  "995586c1ea4e4dce2fedd1ab9ea6022a8ed094d3f7fac202f50b72f462d8c320": {
    "soal": "The basic framework of Apache Hadoop consists of the following modules:\n- Hadoop Common - contains libraries and utilities required by other Hadoop modules;\n- Hadoop Distributed File System (HDFS) - a distributed file system that stores data on commodity machines, providing very high aggregate bandwidth in the cluster;\n- Hadoop YARN - a resource management platform responsible for managing computational resources in the cluster and using them for scheduling user applications; and\n- Hadoop MapReduce - a programming model for large-scale data processing.",
    "jawaban": "The correct answer is 'False'."
  },
  "6261d9cf03408d801491f03ad585c6430d5c0b03ecc73a672de2189f2d4a9d60": {
    "soal": "In the workflow below, we use the heart disease data and select the women subset from the Scatter Plot. Then, we visualize the distances between columns in the Distance Map. Since the subset also contains some discrete data, the Distances widget warns us that it will account for the discrete features, so we will only see continuous instances/attributes in the map.\u00a0\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "2f8cb0e4a2ba4d15157fa8ed04187c61ead7aa2fbab2ccb30c710ad46883fb9c": {
    "soal": "import pm4py\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\n# Import XES log\nlog = xes_importer.apply(xes_filename)\n# Cek jumlah kasus dan aktivitas\nprint(f\"Jumlah kasus: \")\nactivities = set()\nfor trace in log:\nfor event in trace:\nactivities.add(event['concept:name'])\nprint(f\"Jumlah aktivitas unik: {len(activities)}\")\nfrom pm4py.algo.discovery.dfg import algorithm as dfg_discovery\nfrom pm4py.visualization.dfg import visualizer as dfg_visualization\n# DFG berdasarkan frekuensi\ndfg = dfg_discovery.apply(log,\nvariant=dfg_discovery.Variants.FREQUENCY)\n# Tampilkan visualisasi\ndfg_vis = dfg_visualization.apply(dfg, log=log,\nvariant=dfg_visualization.Variants.FREQUENCY)\ndfg_visualization.view(dfg_vis)\nAn XES file typically includes:\n{\n=Event logs of processes\n-Python code snippets\n-HTML code\n-JSON formatted API calls\n-Java applications\n-C++ libraries\n}",
    "jawaban": "The correct answer is: len(log)"
  },
  "850cee92df0b2ad4c7d7f3085e2037d34ebd308bfafa57bd277b82f5627205e0": {
    "soal": "Widget Network Analysis can be used for statistical analysis of network ......................\n",
    "jawaban": "The correct answer is: data"
  },
  "f9191531244c406cfba6b70b81d4fead494630efecf753e357c6238aea29028c": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#_xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nmap = pm4py.discover_heuristics_net(log)\npm4py.view_heuristics_net(map)\nHow many alternative filenames are listed as commented out in total?",
    "jawaban": "The correct answer is: 10 -6 -8 -7 -9 -11"
  },
  "97e92a832ddbf9e0c50b073f5bd1cd1b5d55fe21a6becada203181e3794bb22c": {
    "soal": "In the following workflow example, a very simple use of the Corpus widget will be shown. Place the ...................... widget onto the canvas and connect it to the Corpus Viewer widget. We are using the book-excerpts.tab dataset, which is available in the add-on, and examining it in the Corpus Viewer widget.\n\n\n\n\n",
    "jawaban": "The correct answer is: Corpus"
  },
  "befa896ce96711643a042bd2ae08fddaeaf23cedaa27b0f0fe923310885dfca2": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nWhat is the shape of the `x_train` array in the CIFAR-100 dataset?",
    "jawaban": "The correct answer is: (50000, 32, 32, 3)"
  },
  "d990b1603508fe948be796fb5cd66326bad3082c707e92f68fd72e6f22901dc2": {
    "soal": "This workflow combines the classification tree interface and visualization with a scatter plot. When both the tree viewer and scatter plot are open, selecting any node from the network will send the related data example to the scatter plot. In the workflow, the selected data is treated as part of the entire dataset and displayed in the scatter plot. With a simple combination of widgets, we have built an interactive classification tree browser.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "65e1258eb85d9b1c014e83d74a372a33a0c898c2ef2e252a0884cc2b09939474": {
    "soal": "\n\n\nThe image above shows the sequence of processes in data science, which are:\n\nASK - Ask, interesting questions.\nGET - Obtain data.\n................... - Explore data, whether there are any anomalies / interesting patterns, etc.\nMODEL - Build, fit, and validate the model.\nVISUALIZATION - Communicate and visualize the data.\n\n",
    "jawaban": "The correct answer is: EXPLORE"
  },
  "e93536a1bc40884b1db85c3d856c20acb7766acafd936ed573aedfaa20329d90": {
    "soal": "What is the purpose of installing 'graphviz' in the context of PM4Py?",
    "jawaban": "The correct answer is: To enable process visualization capabilities"
  },
  "e124408b1a0e3b6494baaab070cccdc58d695f956b678abbf1a8148b49cd8521": {
    "soal": "Widget Import Images allows us to import videos from a directory.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "47615aeef0b35cd9aee9b166eeafeb505dbc3275b6fe525cb95e6d33e0d16746": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nWhich of the following is a correct way to load the CIFAR-10 dataset using TensorFlow?",
    "jawaban": "The correct answers are: tf.keras.datasets.cifar10.load_dataset(), tf.keras.datasets.load_data('cifar10'), import tensorflow as tf\n(x_train, y_train), (x_test, y_test), tf.keras.datasets.cifar10.load_data(), tf.keras.datasets.cifar10.load(), tf.keras.datasets.load_cifar10(), tf.keras.datasets.cifar.load_data()"
  },
  "a0c27d7104513355e4a5895161bc6e625c7d2f407e7ec07ca9827514de1eb5ef": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nplt.imshow(x_train[0], cmap='gray')\nplt.title(f\"Label: \")\nplt.show()\nmodel = keras.Sequential([\nkeras.layers.Flatten(input_shape=(28, 28)),\nkeras.layers.Dense(128, activation='relu'),\nkeras.layers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nWhat does `x_train[0]` represent?\n{\n= The first training image in the dataset\n~ The first label in the test set\n~ A model layer\n~ The training accuracy\n~ The shape of the data\n~ The loss value\n}",
    "jawaban": "The correct answer is: y_train[0]"
  },
  "c870113853477c83be8259f2c66a31af3d4699559bb9e1550c9dbe9debcf3bab": {
    "soal": "\n\n\nMetoda klasifikasi yang digunakan di workflow di atas adalah\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: logistic regression"
  },
  "9fef857904c66a6c4b461b2f384482cc8e2c708781c408d4f17652d3d07174cf": {
    "soal": "Which programming language is most commonly used with TensorFlow?",
    "jawaban": "The correct answer is: Python"
  },
  "d03702ef94cdc4996f80beb7255024af89036c0586bd9030cbc3468655ee8b43": {
    "soal": "1010 (binary) + 100 (binary) + 1 (binary) - 1111 (binary) = ...... (decimal)",
    "jawaban": "The correct answer is: ZERO"
  },
  "247fa8fe14a03f00eabbe8f0a96c77f5506cdf597e3e7910897c21107d458509": {
    "soal": "import pm4py\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\n# Import XES log\nlog = xes_importer.apply(xes_filename)\n# Cek jumlah kasus dan aktivitas\nprint(f\"Jumlah kasus: \")\nactivities = set()\nfor trace in log:\nfor event in trace:\nactivities.add(event['concept:name'])\nprint(f\"Jumlah aktivitas unik: {len(activities)}\")\nfrom pm4py.algo.discovery.dfg import algorithm as dfg_discovery\nfrom pm4py.visualization.dfg import visualizer as dfg_visualization\n# DFG berdasarkan frekuensi\ndfg = dfg_discovery.apply(log,\nvariant=dfg_discovery.Variants.FREQUENCY)\n# Tampilkan visualisasi\ndfg_vis = dfg_visualization.apply(dfg, log=log,\nvariant=dfg_visualization.Variants.FREQUENCY)\ndfg_visualization.view(dfg_vis)\nThe Performance Spectrum visualizer is part of:\n{\n=pm4py.visualization.performance_spectrum.visualizer\n-pm4py.analysis.performance\n-pm4py.log.importer\n-pm4py.algo.discovery.dfg\n-pm4py.objects.log.importer\n-pm4py.visualization.dfg\n}",
    "jawaban": "The correct answer is: len(log)"
  },
  "1cb639a2bb0d31aea2181d5eb154500ca53b233eefecfba712e9d5226bb3c58a": {
    "soal": "The ....................... widget will induce rules from the data using the CN2 algorithm.\n\n\n\n",
    "jawaban": "The correct answer is: CN2 Rule Induction"
  },
  "eebfdf1335894267e27914844bed4f4c2751b109a0ecf3c18c6eac26e298eddb": {
    "soal": "import os\nimport requests\nfrom googlesearch import search\nfrom bs4 import BeautifulSoup\nimport re\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef clean_filename(text):\n# Hilangkan karakter ilegal untuk nama file\nreturn re.sub(r'[\\/*?:\"<>|]', , text).strip().replace(' ', '_')[:50]<%1\u001f%\u001f0%> def get_page_content(url):\ntry:\nheaders =\nresponse = requests.get(url, headers=headers, timeout=10)\nsoup = BeautifulSoup(response.content, 'html.parser')\ntitle = soup.title.string if soup.title else 'No Title'\nparagraphs = soup.find_all('p')\ncontent = '\n\n'.join([p.get_text() for p in paragraphs[:5]])\nreturn title.strip(), content.strip()\nexcept Exception as e:\nreturn 'Error', f\"Gagal mengambil konten: {e}\"\ndef save_to_txt(keyword, rank, title, url, content, folder='outputs'):\nos.makedirs(folder, exist_ok=True)\nfilename = f\"{clean_filename(keyword)}_{rank}_{clean_filename(title)}.txt\"\nfilepath = os.path.join(folder, filename)\nwith open(filepath, 'w', encoding='utf-8') as f:\nf.write(f\"Keyword : {keyword}\n\")\nf.write(f\"Peringkat : {rank}\n\")\nf.write(f\"Judul : {title}\n\")\nf.write(f\"URL : {url}\n\n\")\nf.write(content)\ndef scrape_and_save_txt(keywords, num_results=5):\nfor keyword in keywords:\nprint(f\"\n\ud83d\udd0d Searching: {keyword}\")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nprint(f\" \u2192 ({i+1}) Fetching: {url}\")\ntitle, content = get_page_content(url)\nsave_to_txt(keyword, i+1, title, url, content)\ntime.sleep(2)\nexcept Exception as e:\nprint(f\"\u274c Error saat mencari '{keyword}': {e}\")\nprint(\"\n\u2705 Semua konten telah disimpan di folder 'outputs/'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\nscrape_and_save_txt(keywords, num_results=5)\nWhat HTTP header does the script set when making requests to URLs?\n{\n=User-Agent\n~Content-Type\n~Accept-Encoding\n~Authorization\n~Referer\n~Cache-Control\n}",
    "jawaban": "The correct answer is: \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\""
  },
  "f48587b4652cb676280af4c48465ebed5fdb9d31c559fffffa117eee0d09a4c5": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nWhat does the output of a `Dense` layer with a softmax activation represent in a classification model?",
    "jawaban": "The correct answer is: The probabilities of the input belonging to each class"
  },
  "18a5f80c8035554c31a3b6c936401a9f4e8cc81323fd45d52b38da1344f25a4d": {
    "soal": "What does the term \"web scraping\" usually refer to?",
    "jawaban": "The correct answer is: Automated processes using bots or crawlers"
  },
  "c6a7358b2cee169376516a3fa22daa77c7ba53a1243fc48adacc964c667d7276": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nWhy is `SparseCategoricalCrossentropy` used as a loss function in multi-class classification tasks?",
    "jawaban": "The correct answer is: It is suitable when class labels are provided as integers"
  },
  "504ad3935b683c55221d1b6a7763ed0b1234fe70fbf8da1d70000674ac919cc8": {
    "soal": "How can you view all models that are currently running?",
    "jawaban": "The correct answer is: ollama ps"
  },
  "cb9d80b7ab8ce8f4f838f50630e579c3d4b5ed4a62bd84de9c39d9504f38bf46": {
    "soal": "Noise sensitivity is highest in:",
    "jawaban": "The correct answer is: Alpha Miner"
  },
  "222313a8ae612cc1d71f984708871ed856a40330bba602fffcc50a3f8fecbe6b": {
    "soal": "In real-world production applications, how does dataset size impact model performance?",
    "jawaban": "The correct answer is: Larger datasets generally lead to better performance"
  },
  "32688e79c5a7466a3557c6d13a260943b60f6a7aeb33a3ffabebb7d5f8fa9d0a": {
    "soal": "The CN2 Rule Induction widget will ........................ (induce) rules from the data using the CN2 algorithm.\n\n\n\n",
    "jawaban": "The correct answer is: induce"
  },
  "0cf192c65fae9b3e3a05f5f765abaee51f912b302358b047d4724ee7d07757dc": {
    "soal": "\n\n\nThe image above shows the sequence of processes in data science, which are:\n\nASK - Ask, interesting questions.\nGET - Obtain data.\nEXPLORE - Explore data, whether there are any anomalies / interesting patterns, etc.\nMODEL - Build, fit, and validate the model.\n.......................... - Communicate and visualize the data.\n\n",
    "jawaban": "The correct answer is: VISUALIZATION"
  },
  "55dfcb0d1bbeaee4623eeda9aacd4a3540778ca4662571b82845a8d537fc87be": {
    "soal": "What happens if the Colab environment does not detect a GPU?",
    "jawaban": "The correct answer is: The code will run using the CPU only"
  },
  "3ec2aef8756c9aa174b3214b89d78c1a80a3dcf82edd9b2b7a30d84d85905039": {
    "soal": "ProM can animate processes after:",
    "jawaban": "The correct answer is: Process discovery"
  },
  "a07882d972c9f29a6c27f43625f31c3964f9e184e2f2a9a7979cf5fc7cd4b9bc": {
    "soal": "Which package is NOT installed with the command 'sudo apt -y install libxrandr2 libxss1 libxcursor1 libxcomposite1 libxi6 libxtst6 graphviz python3-tk libblas-dev liblapack-dev libsuitesparse-dev gcc g++'?",
    "jawaban": "The correct answer is: libssl-dev"
  },
  "d88b7bd63079627722d2d6f2a5fcd9f5b794b6657e0202a9338a84b627db5ce3": {
    "soal": "A drawback of Alpha Miner is:",
    "jawaban": "The correct answer is: Poor handling of short loops and complex structures"
  },
  "9c145a761b6c9bff5eeee300590fc0547e3a767a0d1b3cca52d5cef7b8842c17": {
    "soal": "In the workflow below, Iris data from the File widget is passed to the ..................... widget, where we choose to display only two attributes (i.e., petal width and petal length). Then, we can view the original dataset and the dataset with the selected columns in the Data Table widget.\n\n\n",
    "jawaban": "The correct answer is: Select Columns"
  },
  "51ab47f8acd1a0b568440f201ea0bd17604687d66068770ed0e8f709b439449e": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nWhy might the softmax activation function be applied outside the `Dense` layer in a model?",
    "jawaban": "The correct answer is: Because the loss function (e.g., `SparseCategoricalCrossentropy`) can apply softmax internally"
  },
  "f619764f8abb1d36dc6305f192a9b6409e60a58b61d9153c49016d5c995b07df": {
    "soal": "The Image Embedding widget in Orange is very easy to use. Technically, Orange sends the image to the server, where the server pushes the image through a pre-trained deep neural network, such as Google Inception v3. Deep networks are often trained with a specific purpose. Inception v3, for example, can classify images into one of 1,000 image classes. We may ignore the suggested classifications and instead consider the penultimate layer of the network with 2048 nodes (numbers) and use it for the image\u2019s vector-based representation.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "5057876a179e757b5719d7909af9d472b61957f88dd7a714827f149b6fe7bdb5": {
    "soal": "import os\nimport requests\nfrom googlesearch import search\nfrom bs4 import BeautifulSoup\nimport re\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef clean_filename(text):\n# Hilangkan karakter ilegal untuk nama file\nreturn re.sub(r'[\\/*?:\"<>|]', , text).strip().replace(' ', '_')[:50]<%1\u001f%\u001f0%> def get_page_content(url):\ntry:\nheaders =\nresponse = requests.get(url, headers=headers, timeout=10)\nsoup = BeautifulSoup(response.content, 'html.parser')\ntitle = soup.title.string if soup.title else 'No Title'\nparagraphs = soup.find_all('p')\ncontent = '\n\n'.join([p.get_text() for p in paragraphs[:5]])\nreturn title.strip(), content.strip()\nexcept Exception as e:\nreturn 'Error', f\"Gagal mengambil konten: {e}\"\ndef save_to_txt(keyword, rank, title, url, content, folder='outputs'):\nos.makedirs(folder, exist_ok=True)\nfilename = f\"{clean_filename(keyword)}_{rank}_{clean_filename(title)}.txt\"\nfilepath = os.path.join(folder, filename)\nwith open(filepath, 'w', encoding='utf-8') as f:\nf.write(f\"Keyword : {keyword}\n\")\nf.write(f\"Peringkat : {rank}\n\")\nf.write(f\"Judul : {title}\n\")\nf.write(f\"URL : {url}\n\n\")\nf.write(content)\ndef scrape_and_save_txt(keywords, num_results=5):\nfor keyword in keywords:\nprint(f\"\n\ud83d\udd0d Searching: {keyword}\")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nprint(f\" \u2192 ({i+1}) Fetching: {url}\")\ntitle, content = get_page_content(url)\nsave_to_txt(keyword, i+1, title, url, content)\ntime.sleep(2)\nexcept Exception as e:\nprint(f\"\u274c Error saat mencari '{keyword}': {e}\")\nprint(\"\n\u2705 Semua konten telah disimpan di folder 'outputs/'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\nscrape_and_save_txt(keywords, num_results=5)\nBy default, how many top search results does the script retrieve for each keyword?\n{\n=5\n~10\n~15\n~20\n~25\n~30\n}",
    "jawaban": "The correct answer is: \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\""
  },
  "92bdb4981aaf85c7f66609735885f1bca41ef267c5ac414c94af5e3812df6b9a": {
    "soal": "The CN2 algorithm is a classification technique designed to efficiently perform induction of simple and comprehensive rules in the form of \u201cif cond then predict ................... \u201d, even in domains where there may be noise.\n",
    "jawaban": "The correct answer is: class"
  },
  "9f1e91e808dae5281f1d4e6a9f6197ec0bb7bead24d09bcb5ea42495e59aa8cf": {
    "soal": "In the workflow below, the Import Documents widget can load subfolders. We store President ................(1).................'s speeches in two (2) subfolders, namely, pre-1962 and post-1962. If we load the parent folder, both subfolders will be used as ................(2)............... labels. Check the output of the Import Documents widget in the Data Table widget.\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 Kennedy, (2) \u2192 class"
  },
  "93d9afae9f1b0ea90d522513728f41e53f8bee429733e5cb1856724065f6a2f5": {
    "soal": "import pm4py\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\n# Import XES log\nlog = xes_importer.apply(xes_filename)\n# Cek jumlah kasus dan aktivitas\nprint(f\"Jumlah kasus: \")\nactivities = set()\nfor trace in log:\nfor event in trace:\nactivities.add(event['concept:name'])\nprint(f\"Jumlah aktivitas unik: {len(activities)}\")\nfrom pm4py.algo.discovery.dfg import algorithm as dfg_discovery\nfrom pm4py.visualization.dfg import visualizer as dfg_visualization\n# DFG berdasarkan frekuensi\ndfg = dfg_discovery.apply(log,\nvariant=dfg_discovery.Variants.FREQUENCY)\n# Tampilkan visualisasi\ndfg_vis = dfg_visualization.apply(dfg, log=log,\nvariant=dfg_visualization.Variants.FREQUENCY)\ndfg_visualization.view(dfg_vis)\nWhat does PM4Py's performance analysis typically reveal?\n{\n=Bottleneck points in the process\n-Number of users involved\n-Total cost of a process\n-Expected future process steps\n-Types of software used\n-Network latency\n}",
    "jawaban": "The correct answer is: len(log)"
  },
  "4c233499bc3740b53558e43d80e08e5d66e41aa84288e8eb9b3cedd6b890e0cd": {
    "soal": "Which of the following is an example of LLM in everyday language?",
    "jawaban": "The correct answer is: ChatGPT"
  },
  "bbfb9be05953c3059ba1f7c30473e260595b037af9865a0eeb7bd11114f5ce02": {
    "soal": "Cassandra, or fully known as Apache Cassandra, is one of the open-source products for distributed database management by Apache. Cassandra is designed to manage large (\nAnswer Question 70\n data) structured data that is spread across many servers. This software is highly scalable, so it is no wonder that many large companies have trusted Cassandra as one of their work solutions, such as Facebook, Twitter, and Apple.",
    "jawaban": "The correct answer is: big"
  },
  "be55e9795722e8c7e34d172fa4ac6fce61032fea97880bb41d098761097e9269": {
    "soal": "import os\nimport string\nfrom collections import Counter\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\n# Buat daftar stopwords Bahasa Indonesia\nstop_words = set(stopwords.words('indonesian'))\ndef bersihkan_teks(teks):\n# Lowercase + hilangkan tanda baca\nteks = teks.lower()\nteks = teks.translate(str.maketrans('', '', string.punctuation))\nreturn teks\ndef analisis_folder(folder='outputs', top_n=20):\nsemua_konten = ''\nfor filename in os.listdir(folder):\nif filename.endswith('.txt'):\nfilepath = os.path.join(folder, filename)\nwith open(filepath, 'r', encoding='utf-8') as f:\nisi = f.read()\n# Ambil hanya bagian isi, setelah header\nif '\n\n' in isi:\nisi = isi.split('\n\n', 1)[1]\nsemua_konten += isi + ' '\n# Bersihkan teks dan tokenisasi\nteks_bersih = bersihkan_teks(semua_konten)\nkata_kata = teks_bersih.split()\n# Hapus stopwords\nkata_kata = [kata for kata in kata_kata if kata not in stop_words and kata.isalpha()]\n# Hitung frekuensi\ncounter = Counter(kata_kata)\ntop_kata = counter.most_common(top_n)\nprint(f\"\n\ud83d\udd0d Top Kata Paling Sering Muncul:\")\nfor kata, jumlah in top_kata:\nprint(f\"{kata}: {jumlah}\")\n# Buat Word Cloud\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(counter)\n# Tampilkan\nplt.figure(figsize=(12, 6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud dari Semua Konten')\nplt.tight_layout()\nplt.savefig('wordcloud.png')\nplt.show()\nprint(\"\n\u2705 Word Cloud disimpan sebagai 'wordcloud.png'\")\n# Jalankan analisis\nif __name__ == '__main__':\nanalisis_folder('outputs', top_n=30)\nWhat is the function of the 'Counter' class from the 'collections' module in the script?\n{\n=To count the frequency of each word in the text.\n~To remove duplicate words from the text.\n~To sort words alphabetically.\n~To store unique words found in the text.\n~To tokenize the text into individual words.\n~To generate random samples of words from the text.\n}",
    "jawaban": "The correct answer is: top_n"
  },
  "0145dc60a26590ca12466d86725d5a058ede15ad3d5484c0604835ff69fb5ffc": {
    "soal": "To extract numbers from unstructured data, Orange can use deep network ............... .\n",
    "jawaban": "The correct answer is: embedder"
  },
  "fa44ff735cf36f10873fc53582fbe162f4810d7e6e25868b2d24f50ab7c27da8": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.rand(100) # 100 nilai target\n# Inisialisasi model regresi linier\nmodel = LinearRegression()\n# Tentukan jumlah fold (misalnya, 5-fold cross-validation)\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n# Evaluasi model menggunakan cross-validation\nscores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n# Menampilkan skor R^2 untuk setiap fold dan rata-ratanya\nprint(\"Skor R^2 untuk setiap fold:\", scores)\nprint(\"Rata-rata skor R^2:\", np.mean(scores))\nWhich function is used to split the dataset into K folds for cross-validation in the code?",
    "jawaban": "The correct answer is: KFold"
  },
  "b4d2d8f8f6d4bc29de3dc286c9f15e2c15c2fbe2690d7f2a809d915e037cc182": {
    "soal": "This workflow example shows how Network Analysis can enrich the workflow. We have used the lastfm.net data as the input network from the Widget Network File and sent it to the Widget Network Analysis. We can decide to compute the degree, degree centrality, and closeness centrality at the node-level.\n\nWe can then visualize the network in the Widget Network Explorer. In the .................. widget, it colors with the best tags, like the default for this dataset. But now we can also adjust the node size to fit the degree centrality results. This is a good way to visualize network properties.\n\n",
    "jawaban": "The correct answer is: Network Explorer"
  },
  "252387da6714adfaa1c78c7943fc4c0344aaf6b9ede9750d08deed9f74bfa388": {
    "soal": "According to the official Apache website, Apache Spark is a framework used to analyze big data. Data processing through the Apache Spark framework is considered faster than other frameworks like MapReduce because the data is processed out-of-memory. The growth of data at the terabyte level produced daily requires solutions that provide real-time analysis with high speed, one of which is using Apache Spark.\nThe advantages of Apache Spark include:\n- Faster performance compared to traditional data processing frameworks.\n- Easy to use, data processing applications built with Spark can be written in programming languages like Python, R, Java, and Scala.\n- Equipped with SQL Library, Streaming, and Graph Analysis, which makes it easier for data processing and analysis.",
    "jawaban": "The correct answer is 'False'."
  },
  "b02754936340d7baa8296986c55adb032cb2198d689622692c0ed56f0dd64fa8": {
    "soal": "Below, we will use the Attrition - Train data from the Datasets widget. This data is about employee addition. In other words, we want to know whether a certain employee will leave their job or not. We will create a prediction model with the Tree widget and observe the probabilities in Predictions.\n\nFor prediction, we need training data, which we have loaded in the first Datasets widget, and data to predict, which we will load in another Datasets widget. We will now use the Attrition - Predict data. Connect the second dataset to Predictions. Now we can see the predictions for three data instances from the second dataset.\n\nThe Tree model predicts that no employees will leave the company. We can try another model and see if the predictions change. Or, test the predictive score first in the Test & Score widget.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "94aeb47699b55391ccbf89a0804d27532ff07b72d7fc2ce08235ea83203e9606": {
    "soal": "What is a potential drawback of the Holdout Validation method when dealing with small datasets?",
    "jawaban": "The correct answer is: The model may not have enough data to learn effectively"
  },
  "f23ad8d9ccecd70e1b0dcc6712be72ae74691dcebc287160c97a16f6a112fda6": {
    "soal": "What is the main advantage of using cloud computing for Big Data analytics?",
    "jawaban": "The correct answer is: Scalability and flexibility in resource management"
  },
  "7c24e8540e4e9a90b8a9c112264d6512a660384fa9c0d30be4d4335a2aaf84e3": {
    "soal": "from sklearn.model_selection import train_test_split\nimport pandas as pd\n# Contoh: Membuat DataFrame dengan fitur 'X' dan target 'y'\ndata =\ndf = pd.DataFrame(data)\n# Memisahkan fitur dan target\nX = df[['X1', 'X2']]\ny = df['y']\n# Membagi dataset menjadi data pelatihan dan pengujian dengan stratifikasi\nX_train, X_test, y_train, y_test = train_test_split(\nX, y, test_size=0.3, stratify=y, random_state=42\n)\n# Menampilkan proporsi kelas di setiap subset\nprint(\"Proporsi kelas di y_train:\n\", y_train.value_counts(normalize=True))\nprint(\"\nProporsi kelas di y_test:\n\", y_test.value_counts(normalize=True))\nWhy is the `random_state=42` parameter used in the `train_test_split` function?\n{\n~To increase the randomness of data splitting.\n~To ensure the data is split differently each time.\n=To ensure reproducibility of the data split.\n~To limit the size of the dataset.\n~To set the seed for the model's random number generator.\n~To specify the number of splits.\n}",
    "jawaban": "The correct answer is: 'X1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'X2': [11, 12, 13, 14, 15, 16, 17, 18, 19, 20], 'y': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]"
  },
  "beff00ae9808b5f7208175d6fe069c401602aace78db231e8c612ef1cd9dd5ba": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#_xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nmap = pm4py.discover_heuristics_net(log)\npm4py.view_heuristics_net(map)\nHow are alternative filenames represented in the script?",
    "jawaban": "The correct answer is: Commented lines with '"
  },
  "46ed34fc266a5b3296e73a825f113b53129dd7fbdb3f6d04f1a881044b4613c1": {
    "soal": "What does the prompt represent in the LLM workflow?",
    "jawaban": "The correct answer is: User input"
  },
  "41f9f5f88e15a953229f4c7d1b96c1d6e7df07aa02d34085ed53e8c798afa793": {
    "soal": "The Venn Diagram widget can also be used to explore different prediction models. In the following example, we analyze 3 prediction methods, namely Naive Bayes, SVM, and Random Forest, according to their classification errors on data instances.\n\nBy selecting misclassification in three Confusion Matrix widgets and sending them to the Venn diagram widget, we can see all misclassification examples visualized per method used. Then we open the Venn Diagram widget and select, for example, misclassified instances identified by all three methods. This is represented as the intersection of the three circles. Click on the intersection to see these two examples marked in the Line Plot widget. Try selecting different parts of the diagram to see how the scatter plot visualization changes.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "f2d00043f56c183fe36697d238b6cb99c9d8aa26af1b5910e4218f988d06a7b9": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nWhich of the following is NOT a class in the CIFAR-10 dataset?",
    "jawaban": "The correct answer is: Elephant"
  },
  "d7805c16bf46047fe978b13dc18d73cde050483183dd257959be1757c41dac48": {
    "soal": "The ....................... widget can apply the rolling window function to the time series. Use the Moving Transform widget to get the average value of the series.\n",
    "jawaban": "The correct answer is: Moving Transform"
  },
  "1c84e502766525a0aa785e564e6359f12ede8efc50ec6664024afa869a425aab": {
    "soal": "The Image Viewer widget will display images from the dataset, stored either locally or on the internet. The Image Viewer widget will look for attributes with type=png in the third header row.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "754d5d6b2383a951b82e61aec7df93b4a1200565e385aae61f2d25908fc8beb9": {
    "soal": "The following workflow demonstrates the use of the Distances widget. We use the iris.tab data from the File widget. We calculate ................... between instances (rows) and pass the result to Hierarchical Clustering. This workflow is useful for finding groups/clusters in the data instances.\n\n\n\n",
    "jawaban": "The correct answer is: distance"
  },
  "aa54a3104503e9f060b446790f46e7170bdf095277d235868dce254c92a18550": {
    "soal": "The Distances widget also works well with other Orange add-ons. The Distance Matrix widget can be fed into the Network from Distances Widget (.................... add-on) to convert the matrix into a graph and into the Duplicate Detection Widget (Text add-on) to find duplicate documents in the corpus.\n",
    "jawaban": "The correct answer is: Network"
  },
  "102a19721c61a7d77c700d23f8294bedef44917433f041af7ea9a26d039f0f53": {
    "soal": "In the following example, we will see how to properly use Preprocess with Widget ....(1).... or Widget ....(2).....\n\nThis time we use the heart disease.tab data from the File widget. We can access the data through the dropdown menu. This is a dataset with 303 patients who came to the doctor suffering from chest pain. After tests, some patients were found to have diameter narrowing and others did not (this is our class variable).\n\nThe heart disease data has several missing values, and we want to handle them. First, we will split the dataset into training and testing data using the Data Sampler.\n\nNext, we will send the Data Sample to Preprocess. We will use the Impute Widget to address missing values, but we can try combinations of preprocessors on our data. We will send the processed data to Widget ....(3).... and the model built to Widget ....(1).....\n\nFinally, the Predictions Widget also needs data to predict. We will use the output from the Data Sampler widget for prediction, but this time not the Data Sample, but the Remaining Data (remaining data), which is the data not used to train the model.\n\nNote how we send the remaining data (unprocessed data) directly to the Predictions Widget without applying any preprocessing. This is because Orange internally handles preprocessing on new data to prevent errors in model construction. The exact preprocessor used on the training data will be used for prediction. The same process applies to Widget ....(2)....\n\n\n",
    "jawaban": "The correct answer is: (3) \u2192 Logistic Regression, (2) \u2192 Test & Score, (1) \u2192 Predictions"
  },
  "a2a068e19e7d403d1464a68cbc952acb6ebb120e21740916f05374d8e088c64b": {
    "soal": "In the ORANGE Constant Widget - for classification, when predicting class values with Prediction, this widget will generate the relative frequency of the classes present in the training set. If there are two or more majority classes, the classifier will manually choose from the predicted class, but it will always produce the same class for that instance.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "6861bdde58e6889dacaf26f7fa3b1939d26a2c4613df066ef75f047f10825eea": {
    "soal": "Pada Widget Constant, Learner akan menghasilkan model yang selalu memprediksi mayoritas untuk ..................\u00a0 atau nilai rata-rata untuk regresi.\n",
    "jawaban": "The correct answer is: classification"
  },
  "799cbc9a349a6409e12c6526dcbedc29943826f85fcd17c3d9934ff75bc56dc3": {
    "soal": "The Calibration Plot widget cannot show the match between predicted probability by the classifier and the actual class probability.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "7bc3ad81ac2082e800b61e3c13989f8f8931f5014cb18dfc104a673bd45a033f": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#_xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nmap = pm4py.discover_heuristics_net(log)\npm4py.view_heuristics_net(map)\nWhich of these filenames is uncommented and active?",
    "jawaban": "The correct answer is: BPIC_2012_A.xes -BPIC_2012_W.xes -BPIC_2012_O.xes -BPI_Challenge_2019.xes -excercise.xes -Cross_Hospital.xes"
  },
  "7b865f4ea59c028a0b62243ef31ec742e2da51bb30fa7de7624286ec97b1d76f": {
    "soal": "Supported data formats supported by ORANGE3 (based on its add-ons):\n\n\ndistance matrix: Distance File\npredictive model: Load Model\nnetwork: Network File from Network add-on\nimages: Import Images from Image Analytics add-on\ntext/corpus: Corpus or Import Documents from Text add-on\nsingle cell data: Load Data from Single Cell add-on\nseveral spectroscopy files: Multifile from Spectroscopy add-on",
    "jawaban": "The correct answer is 'True'."
  },
  "7796ffb65cfae4111c61582483193445e05fb70e10b1550a0d97b04188fb47a4": {
    "soal": "\n\n\nThe workflow above can be used to perform item clustering based on purchases.\n\n\nItems purchased in an online shop, as seen in the data above, will become categorical data. The distance calculation process for clustering will work well if the data matrix is not transposed (columns remain as columns).\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "fbd413b99f467ab34df452d199b95c4793990f6dc9fab68488ed38106a5077f6": {
    "soal": "Cassandra, or fully known as Apache Cassandra, is one of the \nAnswer Question 95\n source products for distributed database management by Apache. Cassandra is designed to manage large (big data) structured data that is spread across many servers. This software is highly scalable, so it is no wonder that many large companies have trusted Cassandra as one of their work solutions, such as Facebook, Twitter, and Apple.",
    "jawaban": "The correct answer is: open"
  },
  "73a663c9987c6b57749384924e6f81c45631f426a4a266bf7a9669042cfaaede": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nplt.imshow(x_train[0], cmap='gray')\nplt.title(f\"Label: \")\nplt.show()\nmodel = keras.Sequential([\nkeras.layers.Flatten(input_shape=(28, 28)),\nkeras.layers.Dense(128, activation='relu'),\nkeras.layers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nWhat does the MNIST dataset contain?\n{\n= Handwritten digits from 0 to 9\n~ Color images of animals\n~ Text sentences for NLP\n~ Sound waveforms\n~ 3D models\n~ Time series data\n}",
    "jawaban": "The correct answer is: y_train[0]"
  },
  "3ca332a88e670fe3f64e41c646378332ac910784a1a785ec8e1484f89dd66783": {
    "soal": "In the use of the Constant Widget for regression, we use the Constant Widget to create a predictor in Prediction. We use the housing dataset. In Prediction, we can see that the Mean Learner returns .... (mean) value for all instances.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: one"
  },
  "f985775dffe92e0264a51ec9fd63ce73ae633d56287e1213ae946e3ae9911286": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=10000)\nHow is the IMDB dataset typically divided for training and testing purposes?",
    "jawaban": "The correct answer is: 25,000 reviews for training and 25,000 reviews for testing"
  },
  "c734b37fc99650435cd034ea182a667d1816f60c56d415f326be2c48439fcf5c": {
    "soal": "Widget Data Table receives one or more datasets through its input and presents them as a spreadsheet. Data instances can be sorted by attribute values. This widget also supports manual selection of data instances.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "115c46794db85809427f5e0e57d4b928d4f7bd4879b92fcabff548b955522019": {
    "soal": "Which optimizer is known for adapting the learning rate during training?",
    "jawaban": "The correct answer is: Adam"
  },
  "005ad3dfa7fe47d05ac630130b4de1b2124f9bbf44cbfe3361f54e13a64b7eb6": {
    "soal": "The Test & Score widget will test the learning algorithm. Different sampling schemes are available, including using separate test data. The widget does two things. First, it will display a table with performance metrics for different classifiers, such as classification accuracy and area under the curve. Second, the evaluation output, which cannot be used by other widgets to analyze classifier performance, such as ROC Analysis or Confusion Matrix.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "5c827d774b0999eb141f56f0bca4cb5cd6c5b9e7d0411f51bf2af5bb5dbeb6d1": {
    "soal": "In the workflow below, the Import Documents widget can load subfolders. We store President Kennedy's speeches in two (2) subfolders, pre-1962 and post-1962. If we load the parent folder, both subfolders will be used as class labels. Check the output of the Import Documents widget in the Data Table widget.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "e7c0dc2b687a7e4c42c518232b6c9972ae5b998a209830bbfc27b1b9cc987e40": {
    "soal": "Which of the following is a common usage of LLMs in communication?",
    "jawaban": "The correct answer is: Language translation"
  },
  "feb2ebac5ba8d361c9c60a6df5333275697113177cb3679217a7219fe21b36d8": {
    "soal": "The Sentiment Analysis widget enables basic sentiment analysis of corpora. It works for English and uses two techniques supported by NLTK - ....................... and Vader. Both techniques are lexicon-based. Liu Hu calculates a single normal score for sentiment in text (negative score for negative sentiment, positive for positive, 0 for neutral), while Vader produces scores for each category (positive, negative, neutral) and adds a total sentiment score called compound.\n\n\n\n\nLiu Hu score\nVader score\n\n\nAnswer: \nQuestion 6",
    "jawaban": "The correct answer is: Liu Hu"
  },
  "1809ec9788e6868ba9963673f1749f33384269f0e3102074b0759fc7930664c1": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nHow many grayscale images are contained in the Fashion-MNIST dataset?",
    "jawaban": "The correct answer is: 70,000"
  },
  "f0c1c29abe30f410c48982cb15c6988f0e829ece92acbc3b7cb37ccfb7f15b02": {
    "soal": "In the workflow below, the Import Documents widget can load subfolders. We store President Kennedy's speeches in two (2) subfolders, pre-1962 and post-1962. If we load the parent folder, both subfolders will be used as class labels. Check the output of the Import Documents widget in the Data Mining widget.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "d5e5461b46ee437ba2342c4844ba988894b4d263038efc6f4e5fc4704b8c6f98": {
    "soal": "In the PCA Widget in ORANGE2 - the number of transformation components can be selected from the Components Selection input box or by dragging the vertical show-off line on the graph.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "719703e9b663a031586252f366eccb4bc400b9ad78cfc33396348a3509fb3b19": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nIn Keras, what is the correct way to implement a Dropout layer with a 25% rate?",
    "jawaban": "The correct answers are: `tf.keras.layers.Dropout(0.25)`, 25%)`, 0.25)`"
  },
  "9edc745c3bb3ff4d17cbc929c304014e1768091f3368cb3eb68a30d5b4167d52": {
    "soal": "If the Import Documents widget fails to read a specific file for any reason, that file will be skipped. Files that are successfully read will be sent to the output.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "bc8a9d312cbe5106c3bbdaea5bdfa7874c600f0abf5809f014bba353ccf3850c": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nWhat is the range of class labels in the CIFAR-100 dataset?",
    "jawaban": "The correct answer is: 0-99"
  },
  "12f91917ef4971e2b3f9848540fb2ffb7c282fc7e24aec5ecf0a2bbfb0aa9505": {
    "soal": "20::Which command pulls the 'mistral' model in the Ollama container?",
    "jawaban": "The correct answer is: ollama pull mistral"
  },
  "b7a591b68deece7125c45447f75f06fbf782627d797d7ee1279163b4ed28b06a": {
    "soal": "In the workflow below, we use the heart disease data and select the subset only for women from the Scatter Plot. Then, we visualize the distance between columns in the Distance Map. Since the subset also contains some discrete data, the ..................... widget warns us that it will ignore discrete features, so we will only see continuous instances/attributes in the map.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Distances"
  },
  "f738296a1a264ae5e2c19700ddb753b73a99097b22afeb23e46ec661f5bed752": {
    "soal": "10.1.1.1 (decimal) AND 255.255.255.255 (decimal)",
    "jawaban": "The correct answer is: 10.1.1.1"
  },
  "745c91999c4f3989f0307cb294f0682531a230068c4403a23a720991656866fb": {
    "soal": "Which visualization is recommended for formal compliance verification?",
    "jawaban": "The correct answer is: Petri Net"
  },
  "72f1f8498c987141bfce599ba751a1ef790f470e315ec3be0542f51090779dfc": {
    "soal": "The .................... widget can display attribute-value data in a spreadsheet.\n",
    "jawaban": "The correct answer is: Data Table"
  },
  "8eaa4a1e8d1448cfaa9d7ebbd03f70995f30d136381bc4789fa94a4fba2dd56b": {
    "soal": "In the ORANGE Constant Widget - for classification, when predicting class values with Prediction, this widget will generate the max frequency of the classes present in the training set. If there are two or more majority classes, the classifier will randomly choose from the predicted class, but it will always produce the same class for that instance.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "f8e43656586f84db4eb72a269526511d27b978bbb2e739f7a2370f609f5a8ab4": {
    "soal": "The function pm4py.discover_petri_net_inductive(log) returns:",
    "jawaban": "The correct answer is: Petri net, initial marking, final marking"
  },
  "8b2c878835be02a26c18c6e13b88f8786c1786a18e618bae10cdaf602bf14496": {
    "soal": "Data Science is an interdisciplinary field that uses ............... , processes, algorithms, and scientific systems to extract knowledge and insights from data in various forms, both structured and unstructured, similar to data mining.\n",
    "jawaban": "The correct answer is: method"
  },
  "d752332bfcc5a5458d047ea16fc2bb513c4cd9126c5660502101791f7793dfbc": {
    "soal": "We can measure the distance between embedded images and see which ones are the most similar. We can use the Distances widget to measure the distance. Typically, ................\u00a0 distance works best for images. We send the distance matrix to Hierarchical Clustering to visualize similar pairs in the dendrogram.\n\n\n",
    "jawaban": "The correct answer is: cosine"
  },
  "c76d4ed7e33128c2ba04b3d5ec980d347aa00f7426e688a7e9035a3e259a1669": {
    "soal": "Where does the LLM place the new response?",
    "jawaban": "The correct answer is: In the context window"
  },
  "6ae5b9fa4381481673bc6f5ee47d30ce7cb37c72ff49157c66f6f3d5ee7aebd7": {
    "soal": "Machine learning is a branch of computer science that focuses on developing algorithms and techniques to create computer systems that can learn from data. In machine learning, computers are trained to recognize patterns and rules in data so that they can make intelligent and accurate predictions or decisions based on that data.\nThe learning process in machine learning can be done with three main methods: supervised learning, unsupervised learning, and reinforcement learning.\n- Supervised learning involves using labeled or annotated data to train the algorithm so that the computer can recognize patterns or relationships between data features and labels. Examples of supervised learning applications are image classification, stock price prediction, and spam email classification.\n- Unsupervised learning, on the other hand, involves using data with labels to identify hidden patterns or structures in the data. Examples of unsupervised learning applications are data clustering and dimensionality reduction.\n- Reinforcement learning involves computer systems learning through experience by taking actions and receiving feedback on whether the actions were right or wrong. The goal is to find the optimal decision or action that results in maximum benefit.\nMachine learning has various applications, including in facial recognition, image analysis, speech recognition, forecasting, anomaly detection, and more.",
    "jawaban": "The correct answer is 'False'."
  },
  "a3050e8c2d9374d1a677374478e9b090a124b8656b11cfc1c73797e6c6ffc468": {
    "soal": "import pandas as pd\n# Contoh: Membuat DataFrame dengan missing values\ndata =\ndf = pd.DataFrame(data)\n# Menampilkan jumlah missing values di setiap kolom\nprint(\"Jumlah missing values per kolom:\")\nprint(df.isnull().sum())\n# Menghapus baris yang memiliki missing values\ndf_cleaned = df.dropna()\nprint(df_cleaned)\n# Mengisi missing values pada kolom 'Usia' dengan mean\ndf['Usia'].fillna(df['Usia'].mean(), inplace=True)\n# Mengisi missing values pada kolom 'Usia' dengan median\ndf['Usia'].fillna(df['Usia'].median(), inplace=True)\n# Mengisi missing values pada kolom 'Kota' dengan modus\ndf['Kota'].fillna(df['Kota'].mode()[0], inplace=True)\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n# Inisialisasi imputer untuk mengisi missing values dengan mean\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n# Mengimputasi kolom 'Usia'\ndf[['Usia']] = imputer.fit_transform(df[['Usia']])\nAfter fitting a SimpleImputer, which method is used to apply the imputation to the data?\n{\n=transform()\n~fit()\n~fit_transform()\n~apply()\n~impute()\n~replace()\n}",
    "jawaban": "The correct answer is: 'Nama': ['Andi', 'Budi', 'Citra', 'Dewi'], 'Usia': [25, 30, None, 22], 'Kota': ['Jakarta', None, 'Bandung', 'Surabaya']"
  },
  "751c43c283ac212c3b6f7eb7dc2b73d9b527a358556271e7610712231d047dd5": {
    "soal": "Principal ................. Analysis (PCA) transforms data into a dataset with uncorrelated variables, also called principal components. The PCA widget displays a graph (scree diagram) that shows the amount of variance explained by the best principal components and allows you to interactively adjust the number of components to include in the output dataset. In this workflow, we can observe the transformation in the Data Table and in the Scatter Plot.\n\n\n\n\n",
    "jawaban": "The correct answer is: Component"
  },
  "010d6f74e7fd5d15fbd362ff4854d8af628302ea66db78fde12bc358502b0b9d": {
    "soal": "Inductive Visual Miner excels at representing:",
    "jawaban": "The correct answer is: Complex structured processes"
  },
  "11cf61388c8c8da599ab3c0e84867123c83c09f965ac1357cc9214268e86479e": {
    "soal": "Most visualizations in Orange are interactive. For example, the Scatter Plot widget. Double-click the icon to open it, and click-and-drag to select multiple data points from the plot. The selected data will automatically enter the Data Table widget. Double-click to inspect which data was selected. Change the selection and observe the changes in the Data Table widget . This works well if both widgets are opened.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "1bdca03d02dea7f423b9763453c2fe024f3c3a84d68dbe547055f73dc83c9d6f": {
    "soal": "Widget File can read attribute-value data from streaming input.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "2d48707014ae2fd084e1de5fb5519122ffdea6409739edf29895b02c6ab5c96e": {
    "soal": "Column attribute names can be preceded by a label followed by a hash. Use .... for class and m for meta attributes, i for ignoring columns, w for weight (of a column), and C, D, T, S for continuous, discrete, time, and string attribute types. Example: C#mph, mS#name, i#dummy.\n\nWhere ........ is,\n",
    "jawaban": "The correct answer is: c"
  },
  "32d4368460c3d05c01db422fabe78558888b4b9dea4d8adc1f44094366e98824": {
    "soal": "The CN2 algorithm is a regression technique designed to efficiently induce simple and comprehensive rules in the form of \u201cif cond then predict class,\u201d even in domains where there may be noise.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "87e5a607f025e6f089ea1b7eeef997056f50475b9333b589788facc8c0a85491": {
    "soal": "Orange is an open-source Machine Learning and data visualization tool for experts only.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "39f8598d13cf3aadc9f8501485c7956d6c9d8b3c1586dee96d5cf9c4aea7176c": {
    "soal": "Contoh penggunaan Widget kNN untuk task regresi di perlihatkan pada workflow di bawah ini. Workflow ini menunjukkan cara menggunakan Learner output. Untuk tujuan contoh ini, kita menggunakan dataset housing. Kita memasukkan model prediksi kNN ke dalam Predictions dan mengamati nilai yang diprediksi.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "8ff38228118c0a4adec27714ce16e2a13c85f7112a9feb0fa4e6feaba47214f6": {
    "soal": "Contoh sederhana dengan Calibrated Learner. Kita menggunakan dataset titanic karena widget ini membutuhkan nilai binary class (dalam hal ini mereka adalah 'survived\u2019 atau \u2018not survived\u2019).\n\nKita menggunakan Logistic Regression sebagai base learner yang akan dikalibrasi dengan nilai setting default, yaitu dengan sigmoid optimization dari distribusi nilai dan di optimasi dengan CA.\n\nMembandingkan hasil dari uncalibrated Logistic Regression model kita akan melihat dengan jelas bahwa\u00a0Logistic Regression model lebih ..................\u00a0\n\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: buruk"
  },
  "d2f7b960ee58e2c2bb37b86343ab7c3118a68a0ed61915e3aa64267f420f8474": {
    "soal": "Widget Select Columns is used to manually organize data domains. Users can decide which attributes to use and how. Orange differentiates between regular attributes, class attributes (optional), and meta attributes. For example, to build a classification model, the domain will consist of a set of attributes and continuous class attributes. Meta attributes are not used in modeling, but some widgets can use them to label instances.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "44948b5341a7e2c5d1d9f94eb08e4faca9af0d46cfae120793bf29783c89e776": {
    "soal": "from googlesearch import search\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef get_page_content(url):\ntry:\nheaders =\nresponse = requests.get(url, headers=headers, timeout=10)\nsoup = BeautifulSoup(response.content, 'html.parser')\n# Ambil judul halaman\ntitle = soup.title.string if soup.title else 'No Title'\n# Ambil konten paragraf utama\nparagraphs = soup.find_all('p')\ntext_content = ' '.join([p.get_text() for p in paragraphs[:5]]) # Batasi 5 paragraf pertama\nreturn title.strip(), text_content.strip()\nexcept Exception as e:\nreturn 'Error', f\"Failed to fetch content: {e}\"\ndef google_scrape_with_content(keywords, num_results=5, output_file='scraped_results.csv'):\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'Rank', 'Title', 'URL', 'Content'])\nfor keyword in keywords:\nprint(f\"\n\ud83d\udd0d Searching for: {keyword}\")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nprint(f\" \u2192 Fetching: {url}\")\ntitle, content = get_page_content(url)\nwriter.writerow([keyword, i+1, title, url, content])\ntime.sleep(2) # Delay biar aman\nexcept Exception as e:\nprint(f\"\u274c Error while searching '{keyword}': {e}\")\nprint(f\"\n\u2705 All results + content saved to '{output_file}'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\ngoogle_scrape_with_content(keywords, num_results=5)\nWhat are the headers written to the output CSV file?\n{\n=Keyword, Rank, Title, URL, Content\n~Search Term, Position, Link, Description\n~Query, Result Number, Web Address, Snippet\n~Term, Order, Hyperlink, Summary\n~Phrase, Index, Site, Extract\n~Keyword, Number, Page, Details\n}",
    "jawaban": "The correct answer is: \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\""
  },
  "1f2bfbaa83c26ad4cefc81f3f8da20e0b970505361b86b6931ece440474f5228": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\n# xes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\nxes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nprocess_tree = pm4py.discover_bpmn_inductive(log)\nbpmn_model = pm4py.convert_to_bpmn(process_tree)\npm4py.view_bpmn(bpmn_model)\nWhat variable holds the loaded event log?",
    "jawaban": "The correct answer is: log"
  },
  "3507382ee8019a33e0813785b9449566834c065b60db42f9c2a7c5bd47e54ce4": {
    "soal": "result 8 (decimal) AND 3 (decimal) is",
    "jawaban": "The correct answer is: 0"
  },
  "028d26e5a0fa81db0525b1941ae58b75ef1df1b8abef5987bc28319edb12c541": {
    "soal": "In the image below, we use the Network Generator Widget to create a Grid graph with a height of 4 and a width of 5 and send it to Network Analysis. We can calculate node degrees and send the data to Network Explorer. Finally, we observe the resulting graph in the visualization and set the node size and color to Degree. Network Explorer is a good tool for observing and explaining network throughput.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "aa3f066ada15b17ef4434f65143bdd646e32c3a53d891f17941587e4ffe2fc49": {
    "soal": "In a typical classification example, we will use the Constant widget to compare the scores of other learning algorithms (such as kNNN) with the default score. Use the iris dataset and connect to Test & Score. Then, connect Constant and kNN to Test & Score and observe how well kNNN performs against the baseline Constant.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "bbdedfd02f72b00c6aa769bc9ddc9b9096c949eaeba8f2c085989854c2cdd71b": {
    "soal": "Data does not always come in the form of neat tables. Data can also be in the form of text, audio recordings, video content, or even images. However, computers can only work with numbers, so for any data mining, we need to transform that unstructured data into a .............(1)................ representation.\n\nTo extract numbers from unstructured data, Orange can use .................(2)................ embedder. Orange has just started incorporating various embedders in Orange in the Image Embedding widget, and currently, they are available for text and images.\n",
    "jawaban": "The correct answer is: (1) \u2192 vector, (2) \u2192 deep network"
  },
  "ce635e2f696662a8d1eed57a5c408117c11ed30f47eb7176c951ac7a50b0af83": {
    "soal": "\n\n\nThe workflow above tests/evaluates CN2 Rule Induction and Tree in the Widget Test & Score. It appears that Tree is worse.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "cf203685847ddc41854e54697c5d00d7234e4c3025182467dfe98d297b4159c0": {
    "soal": "import pandas as pd\nfile_path = \"/content/drive/MyDrive/your_file.csv\"\ndf = pd.read_csv(file_path)\nprint(df.head())\nWhich type of data structure is primarily provided by the 'pandas' library in Python?",
    "jawaban": "The correct answer is: DataFrames"
  },
  "1f0cd17fd7befad605380bace4efc8de0d396326a1c990f340a42b2cd300018c": {
    "soal": "The Widget .......(1) ............... takes the selected dataset from the server and sends it to the output. The file is downloaded to local memory and is immediately available even without an internet connection .........(2)............ Each dataset is accompanied by a description and information about the data size, the number of ...........(3).............., the number of variables, target, and tags.\n",
    "jawaban": "The correct answer is: (3) \u2192 instance, (2) \u2192 internet, (1) \u2192 Dataset"
  },
  "00cd934fbf63fd50758678481c91a23a026a37bc3b51bc88cbf0971eeb4a7139": {
    "soal": "In the ORANGE Constant Widget - for classification, when predicting class values with Prediction, this widget will generate the relative frequency of the classes present in the training set. If there are two or more majority classes, the classifier will randomly choose from the predicted class, but it will always produce the same class for that instance.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "7d659a45c1c064eba041fbd6088878d093e68b4d611999a08d9c20db6b6eebf3": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.rand(100) # 100 nilai target\n# Inisialisasi model regresi linier\nmodel = LinearRegression()\n# Tentukan jumlah fold (misalnya, 5-fold cross-validation)\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n# Evaluasi model menggunakan cross-validation\nscores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n# Menampilkan skor R^2 untuk setiap fold dan rata-ratanya\nprint(\"Skor R^2 untuk setiap fold:\", scores)\nprint(\"Rata-rata skor R^2:\", np.mean(scores))\nWhich machine learning model is initialized in the code?",
    "jawaban": "The correct answer is: LinearRegression"
  },
  "e1e5eee411d7102a6b840a4208a0ed98cc8f58b22a1a02e8326e3b4be74a9bae": {
    "soal": "Network Clustering Widget can help us reveal clusters and highly connected groups in a network. First, we will use the Network File Widget to load the lastfm.net dataset. Then, we will send the network to the Network Clustering Widget. The Network Clustering Widget finds 79 clusters in the network. To visualize the results, we use the Network Explorer Widget and set the Color attribute to Cluster. This will color the network nodes with the corresponding cluster color - this is a good way to visualize highly connected groups in a dense network.\n\nNote that the Network Explorer Widget here will color the 10 largest clusters and color the rest as 'Other'.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "c744eaef57ecf7790593323ce23eeb8d5f10fd6a617a9d6fd77b31162ec1da9b": {
    "soal": "import numpy as np\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n# Contoh data: nilai aktual dan prediksi\ny_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0]) # Nilai aktual\ny_pred = np.array([1, 0, 1, 0, 0, 1, 0, 1, 1, 0]) # Prediksi model\n# Menghitung confusion matrix\ncm = confusion_matrix(y_true, y_pred)\n# Menampilkan confusion matrix dalam bentuk teks\nprint(\"Confusion Matrix:\n\", cm)\n# Menampilkan confusion matrix dalam bentuk grafik\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.show()\nDalam confusion matrix untuk klasifikasi biner, apa yang direpresentasikan oleh elemen di baris kedua dan kolom pertama?",
    "jawaban": "The correct answer is: False Negatives (FN)"
  },
  "98a6f12137afaf3563ceb87621014ae1e5bd796fd274dc5d90b6536e11ca8b7a": {
    "soal": "The following workflow is an example where we compare three (3) classifiers (Naive Bayes, Tree, and Constant) and input them into the Test & Score widget. We are using the Titanic dataset. The Test & Score widget will display the evaluation results for each classifier. Then, we can plot through the Calibration Plot widget and the ..................... widget from the Test & Score output for further analysis of the performance of each classifier. The Calibration Plot widget allows us to see the predicted accuracy of class probabilities in a plot/image form.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: ROC Analysis"
  },
  "33f47cb6ff46425129f8c5dce9687cc23a8f9d98830ae97efcf2d79516f19352": {
    "soal": "Since the output data type from the Time Series Widget ORANGE is a table, we can connect this data to other widgets/modules that can accept tables. Of course, we can use the output from the Time Series Widget ORANGE to test various functions available in other time series add-ons in ORANGE.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "d991bce724319a5471e0a48b640c47ae287d68233fd043aa5a9ffbcbda80f3f3": {
    "soal": "Widget Network Analysis calculates statistical summaries from the node-level and graph-level for the network. Widget ........................ will output the network with the computed statistics and an extended item data table (only node-level index).\n",
    "jawaban": "The correct answer is: Network Analysis"
  },
  "90e3a106654106df4f32ad4194926d649f799b50b0f083edfeb5d0685144ef5d": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#-xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\ndfg, start_activities, end_activities = pm4py.discover_dfg(log)\npm4py.view_dfg(dfg, start_activities, end_activities)\nWhat does \"__name__ == '__main__'\" indicate?",
    "jawaban": "The correct answer is: The script is executed as a standalone program -The script is imported as a module -Checks variable name -Defines a main class -Declares global variables -Specifies filename"
  },
  "f21362d52afa1e8aee51f9ee3c99e720f54c7fc80e0bbb948318a9667ff5d8d9": {
    "soal": "from googlesearch import search\nimport csv\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef google_scrape_bulk(keywords, num_results=10, output_file='results.csv'):\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'Rank', 'URL'])\nfor keyword in keywords:\nprint(f\"\ud83d\udd0d Searching for: \")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nwriter.writerow([keyword, i+1, url])\ntime.sleep(1) # Delay agar tidak dianggap bot\nexcept Exception as e:\nprint(f\"\u274c Error while searching '{keyword}': {e}\")\nprint(f\"\n\u2705 All results saved to '{output_file}'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\ngoogle_scrape_bulk(keywords, num_results=10)\nWhat does the script do if an error occurs during the search for a particular keyword?\n{\n=It prints an error message and continues with the next keyword.\n~It stops execution and exits the program.\n~It retries the search for the same keyword immediately.\n~It logs the error to a separate log file and continues.\n~It prompts the user to enter a new keyword.\n~It skips the keyword and moves to the next without any notification.\n}",
    "jawaban": "The correct answer is: keyword"
  },
  "4dd1d890e71dd89effd23cee51fb87cbb4a8025f199dc7e1c4d1fd15fe043d4c": {
    "soal": "In the Cross Validation workflow above, the learner used is,\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answers are: Logistic Regression, Random Forest Classification, SVM"
  },
  "2cddb3e04ef41a6bcb593e1dcbc9160b717d56fc9e5245b23ca828387be10a63": {
    "soal": "import os\nimport requests\nfrom googlesearch import search\nfrom bs4 import BeautifulSoup\nimport re\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef clean_filename(text):\n# Hilangkan karakter ilegal untuk nama file\nreturn re.sub(r'[\\/*?:\"<>|]', , text).strip().replace(' ', '_')[:50]<%1\u001f%\u001f0%> def get_page_content(url):\ntry:\nheaders =\nresponse = requests.get(url, headers=headers, timeout=10)\nsoup = BeautifulSoup(response.content, 'html.parser')\ntitle = soup.title.string if soup.title else 'No Title'\nparagraphs = soup.find_all('p')\ncontent = '\n\n'.join([p.get_text() for p in paragraphs[:5]])\nreturn title.strip(), content.strip()\nexcept Exception as e:\nreturn 'Error', f\"Gagal mengambil konten: {e}\"\ndef save_to_txt(keyword, rank, title, url, content, folder='outputs'):\nos.makedirs(folder, exist_ok=True)\nfilename = f\"{clean_filename(keyword)}_{rank}_{clean_filename(title)}.txt\"\nfilepath = os.path.join(folder, filename)\nwith open(filepath, 'w', encoding='utf-8') as f:\nf.write(f\"Keyword : {keyword}\n\")\nf.write(f\"Peringkat : {rank}\n\")\nf.write(f\"Judul : {title}\n\")\nf.write(f\"URL : {url}\n\n\")\nf.write(content)\ndef scrape_and_save_txt(keywords, num_results=5):\nfor keyword in keywords:\nprint(f\"\n\ud83d\udd0d Searching: {keyword}\")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nprint(f\" \u2192 ({i+1}) Fetching: {url}\")\ntitle, content = get_page_content(url)\nsave_to_txt(keyword, i+1, title, url, content)\ntime.sleep(2)\nexcept Exception as e:\nprint(f\"\u274c Error saat mencari '{keyword}': {e}\")\nprint(\"\n\u2705 Semua konten telah disimpan di folder 'outputs/'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\nscrape_and_save_txt(keywords, num_results=5)\nHow does the script name each saved text file?\n{\n=keyword_rank_title.txt\n~title_keyword_rank.txt\n~rank_keyword_title.txt\n~keyword_title_rank.txt\n~rank_title_keyword.txt\n~title_rank_keyword.txt\n}",
    "jawaban": "The correct answer is: \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\""
  },
  "fb3c78944e094be87ad751a841e212af9d66df0b00037f82ebbb99e4f5cf419d": {
    "soal": "In the following example, we will see how to properly use Preprocess with the Predictions widget or the Test & Score widget.\n\nThis time we are using the heart disease.tab data from the File widget. We can access the data through the dropdown menu. This dataset consists of 303 patients who visited a doctor with chest pain. After the test was conducted, some patients were found to have narrowed arteries while others did not (this is our class variable).\n\nThe heart disease data has some missing values, and we want to handle that. First, we will split the dataset into train and test data using the Data Sampler.\n\nNext, we will send the Data Sample to Preprocess. We will use the Impute widget to handle the missing values, but we can try combinations of preprocessors on our data. We will send the processed data to the Logistic Regression widget and the model built to the Predictions widget.\n\nFinally, the Predictions widget also needs data to predict. We will use the output from the Data Sampler widget for prediction, but this time not the Data Sample, but the Remaining Data (remaining data), which is the data that was not used to train the model.\n\nNotice how we send the remaining data directly to the Predictions widget without applying any preprocessing. This is because Orange handles preprocessing on new data internally to avoid errors in model construction. The same preprocessor used on the training data will be used for plotting. The same process applies to the Test & Score widget.\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "0b3d3ebd57f995b0b0dce190ea4ada0bdb77e26f8a18dbf8238a9d810e4b4bf4": {
    "soal": "A limitation of Inductive Miner is:",
    "jawaban": "The correct answer is: Potential for overly generalized models"
  },
  "80d77162442cd7a9252eb5a6e1d4ae30de0ab736619496c04c547f9162f17447": {
    "soal": "Which TensorFlow function is used to compute the inverse of a square matrix?",
    "jawaban": "The correct answer is: matrix_inverse()"
  },
  "e3102ae75acfcd413eaae1512ab6439c955fa44995fdb4c77d4627d4ae4c9dc4": {
    "soal": "Network Analysis Widget can be used for statistical analysis of network data.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "7a82fe83a374dd2e19289ac022b6d869d0e664706869fd1d72b6254692681ad8": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nHow is the CIFAR-10 dataset divided between training and testing?",
    "jawaban": "The correct answer is: 50,000 training images and 10,000 test images"
  },
  "86661513338ad6ee1a2772def2c9097f39f2dbbe2a4dd96fdbb710f321cf4c87": {
    "soal": "The Moving Transform widget uses the Aggregation function to aggregate values in time series windows. The available options are: .................... (average), sum, max, min, median, mode, standard deviation, variance, product, linearly-weighted moving average, exponential moving average, harmonic mean, geometric mean, non-zero count, cumulative sum, and cumulative product\n",
    "jawaban": "The correct answer is: mean"
  },
  "d9f17b340620fcda7168dcfe8f683ccc75bf49bb814bb58910bac9300982ecea": {
    "soal": "In the following workflow example, we will show how to quickly visualize a corpus with the Word Classifier widget. We can connect the Word Classifier widget directly to the Corpus widget, but instead, we decide to apply some preprocessing with the Text Preprocess widget. We are working with the dataset book-excerpts.tab. We can convert all text to lowercase, tokenize (split) the text into just words, filter out English stopwords, and select the 100 most frequent tokens.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "f26586390847a42d9a358d34a1eb9d118147fc92c6fe65a2c355631c0c59072e": {
    "soal": "In the ...................... widget, we can input forecast signals from the forecast model results. The forecast is drawn with a dashed line and confidence intervals as a range area.\n\n\n\n",
    "jawaban": "The correct answer is: Line Chart"
  },
  "af99283d3e2a4c22d4194afa8bed91e3279fce10813b9b55b5b488bf743796a7": {
    "soal": "When the user provides data to the input, the ...................... widget will convert the data into a corpus. The user can select which feature to use as the text feature.\n",
    "jawaban": "The correct answer is: Corpus"
  },
  "77165d2cfd5bb3aefb1b5d1926079be78713de8c275e52a824461f0e8f578965": {
    "soal": "from googlesearch import search\nimport csv\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef google_scrape_bulk(keywords, num_results=10, output_file='results.csv'):\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'Rank', 'URL'])\nfor keyword in keywords:\nprint(f\"\ud83d\udd0d Searching for: \")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nwriter.writerow([keyword, i+1, url])\ntime.sleep(1) # Delay agar tidak dianggap bot\nexcept Exception as e:\nprint(f\"\u274c Error while searching '{keyword}': {e}\")\nprint(f\"\n\u2705 All results saved to '{output_file}'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\ngoogle_scrape_bulk(keywords, num_results=10)\nWhich Python module is used in the script to perform Google searches?\n{\n=googlesearch from googlesearch-python\n~google-api-python-client\n~beautifulsoup4\n~requests\n~selenium\n~scrapy\n}",
    "jawaban": "The correct answer is: keyword"
  },
  "d4bdd35de34001b6e06e67812965308778072dbb450381c2a2798e696d04057a": {
    "soal": "Word Cloud can be created from the Documents we have. In the text mining toolbox, there is an Import Documents widget to read document files. Before displaying them as a word cloud, it is advisable to pass them through the Preprocess Text widget first, to reduce unnecessary words like conjunctions, etc. Then we can pass it through the Bag of Words widget first, or directly to the .................................... widget to display.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Word Cloud"
  },
  "bc0fb322df2f2c8f5457509f098efbde3764469a50228c6984a294d669b3cb62": {
    "soal": "One advantage of Orange is the interactive data analysis Workflow with a large toolbox.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "04df85efff2381892cb2690acea0e1654ba44ae8883b3283048a20335b1072ad": {
    "soal": "Which of the following is a characteristic of the K-Means clustering algorithm?",
    "jawaban": "The correct answer is: It requires the number of clusters to be specified beforehand"
  },
  "22cb3f2f343318adf0ddae4831a67f41c59c7c5645ed710eb95ae1c2cd1a9470": {
    "soal": "For supervised problems, where data instances are explained by class labels, we want to know which feature is the most informative. The Rank Widget provides a table ............. and their informativeness scores, and supports manual feature selection. In this workflow, we use it to find the two best features (from the 79 initial ones from the selected dataset) and display their scatter plot.\n\n\n\n",
    "jawaban": "The correct answer is: feature"
  },
  "2f7f673168e08ac6bfae1335ba08bda7cc90daef7f6a1b6ec8c777590ec3db05": {
    "soal": "The winner of the Turing award, Jim Gray, envisioned data science as the \"fourth paradigm\" of science:\n\nempirical\ntheoretical\ncomputational\ndata-base\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "d13461ccb3cde15b72ab8194b2a0c625772eabbfc3bb06020f1b373413db8517": {
    "soal": "import os\nimport requests\nfrom googlesearch import search\nfrom bs4 import BeautifulSoup\nimport re\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef clean_filename(text):\n# Hilangkan karakter ilegal untuk nama file\nreturn re.sub(r'[\\/*?:\"<>|]', , text).strip().replace(' ', '_')[:50]<%1\u001f%\u001f0%> def get_page_content(url):\ntry:\nheaders =\nresponse = requests.get(url, headers=headers, timeout=10)\nsoup = BeautifulSoup(response.content, 'html.parser')\ntitle = soup.title.string if soup.title else 'No Title'\nparagraphs = soup.find_all('p')\ncontent = '\n\n'.join([p.get_text() for p in paragraphs[:5]])\nreturn title.strip(), content.strip()\nexcept Exception as e:\nreturn 'Error', f\"Gagal mengambil konten: {e}\"\ndef save_to_txt(keyword, rank, title, url, content, folder='outputs'):\nos.makedirs(folder, exist_ok=True)\nfilename = f\"{clean_filename(keyword)}_{rank}_{clean_filename(title)}.txt\"\nfilepath = os.path.join(folder, filename)\nwith open(filepath, 'w', encoding='utf-8') as f:\nf.write(f\"Keyword : {keyword}\n\")\nf.write(f\"Peringkat : {rank}\n\")\nf.write(f\"Judul : {title}\n\")\nf.write(f\"URL : {url}\n\n\")\nf.write(content)\ndef scrape_and_save_txt(keywords, num_results=5):\nfor keyword in keywords:\nprint(f\"\n\ud83d\udd0d Searching: {keyword}\")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nprint(f\" \u2192 ({i+1}) Fetching: {url}\")\ntitle, content = get_page_content(url)\nsave_to_txt(keyword, i+1, title, url, content)\ntime.sleep(2)\nexcept Exception as e:\nprint(f\"\u274c Error saat mencari '{keyword}': {e}\")\nprint(\"\n\u2705 Semua konten telah disimpan di folder 'outputs/'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\nscrape_and_save_txt(keywords, num_results=5)\nWhat does the 'get_page_content' function return if an error occurs while fetching a URL?\n{\n='Error', followed by the error message.\n~'No Title', 'No Content'\n~None, None\n~'Error', None\n~'Failed', 'Failed to fetch content'\n~An empty string for both title and content.\n}",
    "jawaban": "The correct answer is: \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\""
  },
  "2b8a319231454ae84578f6d87c9b13ee94d2bf5ef64eb618c6e2d81325b6cccb": {
    "soal": "The Distance Matrix widget can visualize the measurement results of distances in a distance map.\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "e726182d2909e86ed733aafd2850fc425874da00f296befb1bef7066805969ef": {
    "soal": "Widget Network Analysis calculates statistical summaries from the node-level and ......................-level for the network. Widget Network Analysis will output the network with the computed statistics and an extended item data table (only node-level index).\n",
    "jawaban": "The correct answer is: graph"
  },
  "69572f6a5c00e6313f29434a87865ba1dde814ebea6ec893fb868a9a0f3a21eb": {
    "soal": "The Scatter Plot widget, like other Orange widgets, supports zooming in and out of the plot area and manual selection of data instances. These functions are available at the bottom left corner of the widget.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "ba7fa1f4ee7970c7e67963ef02c38431371b35d14ea886abbcb1e36e5bb3da23": {
    "soal": "Using the Yahoo Finance Widget allows us to generate time series data from Yahoo Finance stock market data.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "cbc577402155a5bd60b6962160144389c5452d4da2c9631512edd17808bb7bd2": {
    "soal": "perform the following operation 202 (decimal) AND 255 (decimal)",
    "jawaban": "The correct answer is: 202"
  },
  "1f8189ade1cd634a7afb7412609ca36204b7639959a435bf865076851959726b": {
    "soal": "\n\n\n\n\n\nIn the example above, we use the zoo dataset and send it to CN2 Rule Rotation. We can review and interpret the model created with the CN2 Rule Viewer widget.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "349d6ed741a4518f571cfd5eacbdb189f8199ccbbac0a81b75b8d63c6b64ac27": {
    "soal": "result 8 (decimal) AND 4 (decimal) is",
    "jawaban": "The correct answer is: 0"
  },
  "91a47251e96ccf7e5d9e24a094044e90a15aeb8d468ff11235cbcaab048f4662": {
    "soal": "Which command stops a specified model that is currently running?",
    "jawaban": "The correct answer is: ollama stop model"
  },
  "b5b5ce22b511053e5befafb0671cd6b47147e52177eb7cb1bdc3ceaeaede23d8": {
    "soal": "import pm4py\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\n# Import XES log\nlog = xes_importer.apply(xes_filename)\n# Cek jumlah kasus dan aktivitas\nprint(f\"Jumlah kasus: \")\nactivities = set()\nfor trace in log:\nfor event in trace:\nactivities.add(event['concept:name'])\nprint(f\"Jumlah aktivitas unik: {len(activities)}\")\nfrom pm4py.algo.discovery.dfg import algorithm as dfg_discovery\nfrom pm4py.visualization.dfg import visualizer as dfg_visualization\n# DFG berdasarkan frekuensi\ndfg = dfg_discovery.apply(log,\nvariant=dfg_discovery.Variants.FREQUENCY)\n# Tampilkan visualisasi\ndfg_vis = dfg_visualization.apply(dfg, log=log,\nvariant=dfg_visualization.Variants.FREQUENCY)\ndfg_visualization.view(dfg_vis)\nWhat type of graph does PM4Py utilize to analyze durations and frequency in business processes?\n{\n=Directly-Follows Graph (DFG)\n-Bar graph\n-Pie chart\n-Line graph\n-Histogram\n-Boxplot\n}",
    "jawaban": "The correct answer is: len(log)"
  },
  "493df14bcadbe37f96c1e8bfe2d37e5bca92dd9d88fef23bee8e4e68378f3636": {
    "soal": "Widget SQL Table can read data from an XLS database.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "5ae6da6c9264cacdc55b629ba458937a2f4e4c7814e9b90deddf961bdcbe4b75": {
    "soal": "In the following workflow example, it shows how to quickly visualize the corpus with the Word Cloud widget. We can connect the Word Cloud widget directly to the Corpus widget, but instead, we decided to apply some preprocessing with the ..................... widget. We are working with the book-excerpts.tab dataset. We can change all text to lowercase, tokenize (split) the text into words, filter out English stopwords, and select the 100 most frequently occurring tokens.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Text Preprocess"
  },
  "2bc5574373451e2561339e06f1a9a88588d3b361d0c6fdf6bdc4ebb86bcfc549": {
    "soal": "Bag of Words model creates a corpus with word counts for each data instance (document). The count can be absolute, ...............(1)............ (present or not), or sublinear (logarithmic of term frequency). .................(2)............. model is needed in combination with the Word Enrichment widget and can be used for predictive modeling.\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 binary, (2) \u2192 Bag of Words"
  },
  "b21f00a78ce4b330bcbfea38ab855048a3d6b4812e394aa446571d911b53bb4e": {
    "soal": "9 (decimal) + 10 (binary) = ....... (decimal)",
    "jawaban": "The correct answer is: 11"
  },
  "f680f82a704e6e3a57ffe234a997c410b299ba7cef5b62a72ad7abdec031f7b2": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nWhich of the following is NOT a category in the Fashion-MNIST dataset?",
    "jawaban": "The correct answer is: Hat"
  },
  "ef1d413a69debd5c5650c04ab6ba539ce1854f81d495a29d4becbd8a8396148e": {
    "soal": "Which method guarantees liveness in its models?",
    "jawaban": "The correct answer is: Inductive Visual Miner"
  },
  "d8843a90cbea704f93f32554a195c123a5de38250b8d75bde6f9ba8d193bfc47": {
    "soal": "In the following workflow example, we will show how to quickly visualize a corpus with the Word Cloud widget. We can connect the Word Cloud widget directly to the Corpus widget, but instead, we decide to apply some postprocessing with the Text Preprocess widget. We are working with the dataset book-excerpts.tab. We can convert all text to lowercase, tokenize (split) the text into just words, filter out English stopwords, and select the 100 most frequent tokens.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "1191a8be0dc08d333b2abf14d151841ead8d11ea813bee6ec07084e1a88a0c5f": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.reuters.load_data(num_words=10000)\nWhich Python library provides access to the Reuters Newswire dataset through a built-in function?",
    "jawaban": "The correct answer is: TensorFlow"
  },
  "5dfce9606662daec56e0ec66940f2a86794dcf80aa5cb5cae85f2ba48d5243a2": {
    "soal": "\n\n\nThe most popular programming language widely used to support data science is .....................\n\n\n\n",
    "jawaban": "The correct answer is: python"
  },
  "3cba94289848fff24222711ff3dcebec4bd47bc4c870430004eb146bfde16e62": {
    "soal": "import numpy as np\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n# Contoh data: nilai aktual dan prediksi\ny_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0]) # Nilai aktual\ny_pred = np.array([1, 0, 1, 0, 0, 1, 0, 1, 1, 0]) # Prediksi model\n# Menghitung confusion matrix\ncm = confusion_matrix(y_true, y_pred)\n# Menampilkan confusion matrix dalam bentuk teks\nprint(\"Confusion Matrix:\n\", cm)\n# Menampilkan confusion matrix dalam bentuk grafik\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.show()\nApa yang direpresentasikan oleh variabel y_pred dalam contoh kode?",
    "jawaban": "The correct answer is: Prediksi model terhadap data uji."
  },
  "c957219d45adf7cb6369f6729ec0315639c96f976b498f1e8a06a3866a57a5ec": {
    "soal": "Word Cloud data can be built from the text files (ASCII) we have, as in the workflow below. First, data from the Text Files Widget must be segmented into words using the Segment Widget. Then, the output segmented data needs to be converted from segmented data into a corpus so that it can be processed by the ............... toolbox using the Interchange Widget. Before displaying it as a word cloud, it is recommended to do preprocessing first to reduce unnecessary words, such as conjunctions, etc., using the Preprocess Text Widget.\n\n\n",
    "jawaban": "The correct answer is: text mining"
  },
  "b4ca14f8b49c569f7d0ea975e07a2130a492cf58d97ef0c911eb8ac481b0a8a5": {
    "soal": "What is the primary purpose of data mining in Big Data analytics?",
    "jawaban": "The correct answer is: To discover patterns and knowledge from large amounts of data"
  },
  "ad9a53b57200fc4378407c44576c5ec69a68ae4d2cf22f8f809d189824f663e4": {
    "soal": "The Select Columns widget is used to manually arrange the data domain. The user can decide which attributes to use and how to do it. Orange distinguishes between regular attributes, class attributes (optional), and meta attributes. For example, to build a classification model, the domain will consist of a set of attributes and discrete class attributes. Meta attributes are not used in modeling, but some widgets can use them to label instances.\n",
    "jawaban": "The correct answer is: manual"
  },
  "acf5203e32de965d1cdecf957460180785592d782d3054486d4e3cbaba841f32": {
    "soal": "The Venn Diagram widget can also be used to explore different prediction models. In the following example, we analyze 3 prediction methods, namely ......................, SVM, and Random Forest, based on their classification errors on data instances.\n\nBy selecting misclassification in the three Confusion Matrix widgets and sending them to the Venn Diagram widget, we can see all misclassification examples visualized by the method used. We then open the Venn Diagram widget and select, for example, misclassified instances identified by all three methods. This is represented as the intersection of the three circles. Click on the intersection to view these two examples highlighted in the Scatter Plot widget. Try selecting different parts of the diagram to see how the scatter plot visualization changes.\n\n\n\n",
    "jawaban": "The correct answer is: Naive Bayes"
  },
  "6c22bd3e3c5c97d47d35aad33c9028695ac45e28fff48b8f235cde185cc4285a": {
    "soal": "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target biner\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(0, 2, 100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi data latih dan data uji (80% latih, 20% uji)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi dan pelatihan model klasifikasi (misalnya RandomForest)\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n# Prediksi pada data uji\ny_pred = model.predict(X_test)\n# Menghitung Precision, Recall, dan F1-Score\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nprint(f'Precision: ')\nprint(f'Recall: {recall:.2f}')\nprint(f'F1-Score: {f1:.2f}')\n# Alternatif: Menggunakan classification_report untuk ringkasan metrik\nprint('\nClassification Report:')\nprint(classification_report(y_test, y_pred))\nWhat is the purpose of the `random_state` parameter in scikit-learn models?\n{\n~To randomize the order of data processing.\n~To ensure different results each time the code is run.\n=To ensure reproducibility of results.\n~To set the number of iterations for the algorithm.\n~To control the randomness of the model's predictions.\n~To initialize the model's weights randomly.\n}",
    "jawaban": "The correct answer is: precision:.2f"
  },
  "9921c2abfb8104f7b72ef1bd622e13b490c577c49d4772702ebebff4e770a619": {
    "soal": "The ........................................ widget is most commonly used immediately after the File widget to observe statistical properties of a dataset. In this example, we have used the heart-disease data to inspect our variables.\n\n\n\n\nThe Box Plot is also useful for finding properties of a specific dataset, such as a set of instances manually defined in another widget (e.g., Scatter Plot, or instances from multiple clusters, or classification tree nodes).\n",
    "jawaban": "The correct answer is: Box Plot"
  },
  "f83e8dbe6f9dc2d21a66fcd0ec6367f1902e3ff96c1817ed2380a0423d98b408": {
    "soal": "The Bag of Words model creates a corpus with word counts for each data instance (document). The count can be floating, binary (present or not), or sublinear (logarithmic of term frequency). The Bag of Words model is needed in combination with the Word Enrichment widget and can be used for predictive modeling.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "4686abc77617df614bce18b5a6b75f312b96cd8a09a59a0f7188a46896b41609": {
    "soal": "What is a key component of the transformer architecture?",
    "jawaban": "The correct answer is: Self-attention"
  },
  "5ebe7cf72ad5029c2847c5e52968293acf8b0f6ecbc6153adeff3199ac9a8273": {
    "soal": "Heuristic Miner improves upon Alpha Miner by:",
    "jawaban": "The correct answer is: Considering frequency of activity relations"
  },
  "65f75930f559e7c5a91d011e9ab54106bc926e62522183ed582141ef22ab8466": {
    "soal": "What enables LLMs to understand natural language?",
    "jawaban": "The correct answer is: Training on large datasets"
  },
  "ab81eb76d2ba4535d6ba86f685a9e6d0a9339afbd68246d13604b576e0f142fa": {
    "soal": "Widget CSV File Import reads comma-separated files and sends the dataset to the output channel. The separator can be a comma, semicolon, space, tab, or manually defined delimiter. The history of the newly opened file is maintained in the widget.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "68a87ad12046d5a0d9fb09eb0c30f4e609b83c242c0ab206c37eccc8d01eeb46": {
    "soal": "In the 'docker-compose.yaml' file, which device capability is reserved for the Ollama service?",
    "jawaban": "The correct answer is: gpu"
  },
  "66fc7b55d4ccf15d4e7e517f7553398df83d19aaefe092a6c7159d792cff644e": {
    "soal": "What is the role of the learning rate in training a neural network?",
    "jawaban": "The correct answer is: It controls the step size at each iteration while moving toward a minimum of the loss function."
  },
  "688e4467ef96c295d1e7110aaf00eac81bb01c0a972948fa043d1fc993d25332": {
    "soal": "The input for the Scatter Plot widget is:\n\nData: input dataset\nData Subset: subset of instances\nFeature: list of attributes\n",
    "jawaban": "The correct answer is 'True'."
  },
  "89eb6e1d0b0ea8e926072e667d7928c21fdc751b70809a2003d7fbff14e3de70": {
    "soal": "Only two inputs are suitable for the Distance Matrix widget, which are the Distances widget and the ....................... widget. The output from the Distance Matrix widget is a data table containing the distance matrix. Users can decide how to label the table and distance matrix (or examples in the distance matrix), which can then be visualized or displayed in a separate data table.\n",
    "jawaban": "The correct answer is: Distance Transformation"
  },
  "af7f464232f685230ef0d1acc0747188d76901e7373ea0e9775c5f070f2dd2a8": {
    "soal": "Transformer models like BERT or IndoBERT ideally need how many data points for training?",
    "jawaban": "The correct answer is: Hundreds of thousands to millions"
  },
  "1b499fd451d3eb2acd151d75ebb0a51df694fe1c74a673671d8da65d61c9a771": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nx_train = np.random.rand(1000, 10)\ny_train = np.random.randint(2, size=(1000, 1))\nmodel = keras.Sequential([\nkeras.layers.Dense(64, activation='relu', input_shape=(10,)),\nkeras.layers.Dense(32, activation='relu'),\nkeras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer='adam',\nloss='binary_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=10, batch_size=32)\nWhich metric is monitored during training?",
    "jawaban": "The correct answer is: accuracy"
  },
  "dfe883ceb4148e0cdadc366b5ec76ef8821c695bb1f1347b976daa9fe8a17260": {
    "soal": "\n\n\nThe image above shows the steps that need to be followed in building and evaluating the performance of a machine learning model.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "32f83c4e70014540f44d4197075ab8973ac52869ea2c41814c12418ad4b9e19a": {
    "soal": "import pandas as pd\nfile_path = \"/content/drive/MyDrive/your_file.csv\"\ndf = pd.read_csv(file_path)\nprint(df.head())\nWhat does the 'pd.read_csv()' function do in the given code?",
    "jawaban": "The correct answer is: Reads data from a CSV file into a DataFrame."
  },
  "42cad054e56d0a71802da9be9168e4d2950ff4e477910563588621103ac20e04": {
    "soal": "Widget Tree menggunakan algoritma .................... dengan kemampuan untuk melakukan forward pruning (pemangkasan ke depan).\n",
    "jawaban": "The correct answer is: Tree"
  },
  "59d81d675d7faf47fcbd7642afb1a488c70d9b28e1ee8852d383859b1e87f381": {
    "soal": "In the workflow below, the Tree widget will train a model and evaluate it against Logistic Regression. The accuracy comparison between the Tree widget and ................. is done through the Test & Score widget. The results from the Test & Score widget can be viewed through the Confusion Matrix widget to compare the classification errors that occurred.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Logistic Regression"
  },
  "efb1bec0c43eb80aabadccc0d9cc2a703cd945864cbf721670ce00c29b5d2d25": {
    "soal": "In the following workflow example, using the zoo dataset and creating a clustering workflow with Distances and Hierarchical Clustering. Now set the maximum value for cluster selection (click on the ruler above). Connect the Box Plot to Hierarchical Clustering, check Order by relevance, and select Cluster as a subgroup. This will sort the attributes by how well they define the selected subgroup, in our case, a cluster.\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "9000d77125cfe03d1001a8fac24101da1adcc1397882fc7cf227048772e869b2": {
    "soal": "How do you download a specific model using Ollama?",
    "jawaban": "The correct answer is: ollama pull model"
  },
  "855bcb1f1fb8ddc8023ecd00364c2a3413fc42d3ba88d3c75a0d285e98c4143c": {
    "soal": "Dalam penggunakan Widget Constant untuk regresi, kita menggunakan Widget Constant untuk membuat prediktor dalam Prediction. Kita menggunakan dataset housing. Dalam Prediction, kita dapat melihat bahwa Mean Learner mengembalikan dua (rata-rata) nilai untuk semua instance.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "faa8ce73f027a12dfc1764d4633c320e7f1bf34d678a6cfcfce63ac55d89c7b5": {
    "soal": "Network Clustering Widget tries to find clusters in the network. Network Clustering works with two algorithms, one from Raghavan et al. (2007), which uses label propagation to find suitable clusters, and one from Leung et al. (2009), which builds on Raghavan's work and adds graph attenuation as a parameter for cluster formation.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "e4e2a9ccc6c30caf81179ce378d9bee7882be6f98807b6491195b55404cb9bd7": {
    "soal": "The Distances widget calculates the distance between rows or columns in the dataset. By default, the data is minimized to ensure equal treatment of individual features. Minimization is always done column-wise (using columns as the reference).\n",
    "jawaban": "The correct answer is 'False'."
  },
  "6cad9f56c0e5abba57d13476afd6f4024e8433c71f1cf9cf4808b3c36fef83a3": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nWhich activation function is applied in the second Dense layer of the model?",
    "jawaban": "The correct answer is: No activation function specified"
  },
  "25b80c48ffb1fc5a27f12356b8c6dbbfab63cadff9f71d3f33054eb7f42ddaa1": {
    "soal": "\n\n\nThoughts & concepts supporting data science began to develop since 1943.",
    "jawaban": "The correct answer is 'True'."
  },
  "b68b109972c1dbbc49cfea0e45a8f43e5bace002943a023ea376a61ef1d66be7": {
    "soal": "The ................................... widget can manually select data attributes and the composition of the data domain.\n",
    "jawaban": "The correct answer is: Select Columns"
  },
  "c30cf4307760a6d036686d69b97c596a59f1f65cab98cb34d8790d43db0f309d": {
    "soal": "The Calibrated Learner widget wraps/continues the work of another learner with statistically calibration and decision threshold optimization.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "966e6f491402c19f3ab6ffc1a3c4235060e9237a7d0a8a3a0b6cebbf22b123e9": {
    "soal": "In the Moving Transform widget, to integrate the time series' difference from the Distances widget, use Cumulative sum aggregation on a wide enough window to capture the entire series.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "15405c0a9489034aa5ad613c4de4a814e63adfc9b7499e7e159cceac67785b78": {
    "soal": "The Moving Transform widget uses ...........(1).................. to aggregate values in time series windows. Available options include: mean (average), sum, max, min, median, mode, standard deviation, variance, product, linearly-weighted moving average, exponential moving average, harmonic mean, geometric mean, non-zero count, ...................(2)................., and cumulative product\n",
    "jawaban": "The correct answer is: (2) \u2192 cumulative sum, (1) \u2192 Aggregation function"
  },
  "1363a49fc0519751131c5c9a339f80b563ce84c46b10debf3814fc8d2b4d89af": {
    "soal": "import os\nimport requests\nfrom googlesearch import search\nfrom bs4 import BeautifulSoup\nimport re\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef clean_filename(text):\n# Hilangkan karakter ilegal untuk nama file\nreturn re.sub(r'[\\/*?:\"<>|]', , text).strip().replace(' ', '_')[:50]<%1\u001f%\u001f0%> def get_page_content(url):\ntry:\nheaders =\nresponse = requests.get(url, headers=headers, timeout=10)\nsoup = BeautifulSoup(response.content, 'html.parser')\ntitle = soup.title.string if soup.title else 'No Title'\nparagraphs = soup.find_all('p')\ncontent = '\n\n'.join([p.get_text() for p in paragraphs[:5]])\nreturn title.strip(), content.strip()\nexcept Exception as e:\nreturn 'Error', f\"Gagal mengambil konten: {e}\"\ndef save_to_txt(keyword, rank, title, url, content, folder='outputs'):\nos.makedirs(folder, exist_ok=True)\nfilename = f\"{clean_filename(keyword)}_{rank}_{clean_filename(title)}.txt\"\nfilepath = os.path.join(folder, filename)\nwith open(filepath, 'w', encoding='utf-8') as f:\nf.write(f\"Keyword : {keyword}\n\")\nf.write(f\"Peringkat : {rank}\n\")\nf.write(f\"Judul : {title}\n\")\nf.write(f\"URL : {url}\n\n\")\nf.write(content)\ndef scrape_and_save_txt(keywords, num_results=5):\nfor keyword in keywords:\nprint(f\"\n\ud83d\udd0d Searching: {keyword}\")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nprint(f\" \u2192 ({i+1}) Fetching: {url}\")\ntitle, content = get_page_content(url)\nsave_to_txt(keyword, i+1, title, url, content)\ntime.sleep(2)\nexcept Exception as e:\nprint(f\"\u274c Error saat mencari '{keyword}': {e}\")\nprint(\"\n\u2705 Semua konten telah disimpan di folder 'outputs/'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\nscrape_and_save_txt(keywords, num_results=5)\nIn what format does the script save each extracted content file?\n{\n=.txt\n~.csv\n~.json\n~.html\n~.xml\n~.docx\n}",
    "jawaban": "The correct answer is: \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\""
  },
  "c0338cfbc39b7b0038b422bd67e70704e0c1b15d4a3e55852edf7d4fa9f0e115": {
    "soal": "In the Periodogram widget, the periodogram for ........................ series is calculated using the Lomb-Scargle method.\n",
    "jawaban": "The correct answer is: non-equispaced"
  },
  "fe7809a1643e336786192f1675b67e608464eb76240cb5268bf6eb75d565f930": {
    "soal": "In process mining, what does an event log typically contain?",
    "jawaban": "The correct answer is: Case ID, activity, timestamp"
  },
  "a90b9f70335e6396583e1ac05f685afc81312270be93efd0b3fd4dd712e1e5a3": {
    "soal": "In the workflow below, we collect data using the Twitter widget. We collect tweets from users @HillaryClinton and @realDonaldTrump over two weeks, totaling 242 tweets.\n\n\n\n\n\n\n\nIn the Preprocess Text widget, tweet tokenization is available, which stores hashtags, emojis, mentions, etc. However, this tokenizer does not remove punctuation, so we expand the Regexp filtering with symbols we want to remove. We will obtain tokens with only words, which we display in the Word Cloud. Then we can create a scheme to predict the tweet author based on the tweet author, as described in more detail in the documentation for the Twitter widget.\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "4f8be8703feda6b1a845db0997647fec5522bb4b97ea7a09d947b9c7d79637e3": {
    "soal": "Instalasi Orange3 di Ubuntu 18.04 dapat menggunakan perintah\n\n\u00a0apt-get install orange3\n",
    "jawaban": "The correct answer is 'False'."
  },
  "8ba4e22dcd2774b057f546113f1b043fce071227a34a23d5296662377cab2aef": {
    "soal": "Widget Dataset retrieves the selected dataset from the server and sends it to the output. The file is downloaded to local memory and is thus immediately available even without an internet connection. Each dataset is accompanied by a description and information about data size, number of instances, number of variables, target, and twit.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "156605b5fe02af0d6324e15561d69895ef1d0f32d22b77790e2062649d054507": {
    "soal": "The Confusion Matrix widget provides the count/proportion of instances between ................. and actual class. Selecting elements within the matrix will provide the corresponding instances to the output. In this way, we can observe which specific instances were misclassified and how.\n",
    "jawaban": "The correct answer is: predicted"
  },
  "5ebf000ee1ee18002d5c4735368e873c7ddd59da83bedf6b5cdcb6f1ad09a976": {
    "soal": "In the ORANGE example below, we can predict the classification of a text. The classification model is obtained using the ................. learner with training data from the Corpus Grimms. The text prediction from Corpus Andersen is performed by the Predictions widget.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Logistic Regression"
  },
  "881144adb65c35df0b8b86b59ffc6cbf907ec0525f87504c4756a1ef10fe4334": {
    "soal": "Pada Widget Constant, Learner akan menghasilkan model yang selalu memprediksi mayoritas untuk classification atau nilai ..........\u00a0 untuk regresi.\n",
    "jawaban": "The correct answer is: rata2"
  },
  "a2c5cda0673a61ab69a3c909e4429d79691d16129d63000f0f940a5fbdd3ef32": {
    "soal": "\n\n\nThe image above shows the sequence of processes in data science, which are:\n\nASK - Ask interesting questions (such as, What do we want to achieve / scientific goal? What would we do if we had all the data? What do we want to predict / discard?\nGET - Obtain data.\nEXPLORE - Explore data, looking for anomalies, patterns, etc.\nMODEL - Create, fit, and validate models.\nVISUALIZATION - Communicate and visualize the data.",
    "jawaban": "The correct answer is 'False'."
  },
  "8b37cdd01f8f9c302dfc2a50d92b0ba344228e2c205b6306b4a89b5f9e4bef14": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nWhich Python library provides a function to load the Fashion-MNIST dataset?",
    "jawaban": "The correct answer is: TensorFlow"
  },
  "c81dfcaf063fc271408ff6dc86e94aba58a4d030bdb3bd7c048e82327e751ac0": {
    "soal": "The workflow clusters data items in the iris dataset by first checking the Distances between data instances using the Distances widget. The Distance Matrix is passed to the ............(1).............. widget, which produces a dendrogram. We can select different parts of the ...........(2).............. to further analyze the related data.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: (2) \u2192 dendrogram, (1) \u2192 Hierarchical Clustering"
  },
  "f075bb40348a216722964059f95439dfa7770e4a7bfc3d0b44effc9be100dd79": {
    "soal": "What is the purpose of the `ollama run model` command?",
    "jawaban": "The correct answer is: Runs the specified model for interaction."
  },
  "69952dbd43cd1173121971575f33b430c0fddcccde124b12193fe414199e06b4": {
    "soal": "Alpha Miner primarily produces:",
    "jawaban": "The correct answer is: Workflow nets"
  },
  "5ab8d2d09c37833d545d2b5846c0491b7115c9bd7fd579cf2d89fdcaa8e3f81e": {
    "soal": "The kNN widget uses the kNN algorithm, which searches for the k nearest training instances in the feature space and uses the values of the nearest features to predict.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "dedd1cdb19f6529b9e17e04764c124a220c0a672193d4acd5bae1a69f2bebdb4": {
    "soal": "The Image Viewer widget will display images from the dataset, stored either locally or on the internet. The Image Viewer widget will look for attributes with type=jpg in the third header row.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "9464d741ff0b43e74beea24bffb61759eb0ef8913a34175e6461a029a4351348": {
    "soal": "Only two inputs are suitable for the ..................... widget, which are the Distances widget and the Distance Transformation widget. The output from the Distance Matrix widget is a data table containing the distance matrix. Users can decide how to label the table and distance matrix (or examples in the distance matrix), which can then be visualized or displayed in a separate data table.\n",
    "jawaban": "The correct answer is: Distance Matrix"
  },
  "b5e6d252f19f08548086a236efca5900449b55643e0bccdd8209c23040207ee9": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nx_train = np.random.rand(1000, 10)\ny_train = np.random.randint(2, size=(1000, 1))\nmodel = keras.Sequential([\nkeras.layers.Dense(64, activation='relu', input_shape=(10,)),\nkeras.layers.Dense(32, activation='relu'),\nkeras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer='adam',\nloss='binary_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=10, batch_size=32)\nWhat is the loss function used in model compilation?",
    "jawaban": "The correct answer is: binary_crossentropy"
  },
  "e54bbc49b621c374e0298b9f3199982a1ad74a62f1e88e3fd1d56c8dc7e79228": {
    "soal": "The following workflow is an example where we compare three (3) classifiers (Naive Bayes, Tree, and Constant) and input them into the Test & Score widget. We are using the Titanic dataset. The Test & Score widget will display the evaluation results for each classifier. Then, we can plot through the Calibration Plot widget and the ROC Analysis widget from the Test & Score output for further analysis of the performance of each classifier. The ....................... widget allows us to see the predicted accuracy of class probabilities in a plot/image form.\n\n\n",
    "jawaban": "The correct answer is: Calibration Plot"
  },
  "ba45af8f14cfff0234db96096dc60656e25db34577acc2007bf4c0011a01a8bb": {
    "soal": "import pandas as pd\ndf = pd.read_excel('path_ke_file.xlsx', sheet_name=0)\ndf_selected = df[['Kolom1', 'Kolom2']]\nprint(df_selected.head())\nWhich method displays the first five rows of a DataFrame?",
    "jawaban": "The correct answer is: df.head()"
  },
  "7bcc04dd937d2fc5ecb7ea51a1c0469a9b1de5adb12835825f68a674080b210b": {
    "soal": "Which command creates a new directory named 'processmining' in the user's home directory?",
    "jawaban": "The correct answer is: mkdir"
  },
  "b7fb9b079a0b3b5e8a487c5d34b4048c23828da11148c2c334f345224fdd7cf1": {
    "soal": "Data science is \"a concept to unify statistics, data analysis, machine learning, and related methods\" to \"understand and analyze real-world phenomena\" with data. It uses techniques and theories drawn from many fields in the context of mathematics, statistics, information science, and computer science.",
    "jawaban": "The correct answer is 'True'."
  },
  "351b4a8b0ffb69f3d845cbdcaee272077578c13d9c85efd4cfc9253a0149456f": {
    "soal": "import pandas as pd\nfrom pm4py.objects.conversion.log import converter as log_converter\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom pm4py.visualization.petrinet import visualizer as pn_visualizer\nfrom pm4py.objects.log.util import dataframe_utils\n# csv_filename='Insurance_claims_event_log.csv'\n# csv_filename='Business Travel Permits.csv'\n# csv_filename='Parent-Child Union Output.csv'\n# csv_filename='PM4PY_Dummy_Event_Log_100_Cases_pm4py.csv'\n# csv_filename='PM4Py_Dummy_Event_Log-congested-business-process.csv'\n# 1. Load CSV event log\ndf = pd.read_csv(csv_filename)\n# df = pd.read_csv(csv_filename,sep=';')\n# print(df.head())\n# 2. Format timestamps properly\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n# 3. Ensure column names match PM4Py expectations\ndf = dataframe_utils.convert_timestamp_columns_in_df(df)\n# 4. Convert to event log object\nevent_log = log_converter.apply(df, parameters=)\n# 5. Discover process model using Alpha Miner\nnet, initial_marking, final_marking = alpha_miner.apply(event_log)\n# 6. Visualize Petri Net\ngviz = pn_visualizer.apply(net, initial_marking, final_marking)\npn_visualizer.view(gviz)\nWhat file formats are commonly used as inputs for PM4Py?\n{\n~JSON, XML, YAML\n=XES, CSV, Parquet\n~TXT, DOC, PDF\n~XLSX, PPTX, DOCX\n~JPEG, PNG, SVG\n~SQL, SQLite, MDB\n}",
    "jawaban": "The correct answer is: log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case_id'"
  },
  "66acde7ba2553159f9b203b127b4e45835fbf764cb81658ebe4a6b57d537990a": {
    "soal": "How can testing a model on new data help detect overfitting?",
    "jawaban": "The correct answer is: By revealing if the model performs well on training data but poorly on unseen data."
  },
  "ce590a16db67a72837c169d9300dab5c243d61ee8c5561d4a4e691884dff86a1": {
    "soal": "Widget Constant ORANGE biasanya digunakan sebagai referensi akan hasil minimal yang akan di peroleh untuk model learner lainnya.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "1c3de798271356eeb7950f017251cad82f45cd3a2f44647c9be9b0b8cd66417e": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.reuters.load_data(num_words=10000)\nIn the context of the Reuters Newswire dataset, what does the 'num_words' parameter in the load_data function control?",
    "jawaban": "The correct answer is: The maximum number of words to include based on frequency"
  },
  "731576669c8d9791b77fbeeeff2b8ee17398e31ec3acffc0e5fd9fe0929e2816": {
    "soal": "import os\nimport string\nfrom collections import Counter\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\n# Buat daftar stopwords Bahasa Indonesia\nstop_words = set(stopwords.words('indonesian'))\ndef bersihkan_teks(teks):\n# Lowercase + hilangkan tanda baca\nteks = teks.lower()\nteks = teks.translate(str.maketrans('', '', string.punctuation))\nreturn teks\ndef analisis_folder(folder='outputs', top_n=20):\nsemua_konten = ''\nfor filename in os.listdir(folder):\nif filename.endswith('.txt'):\nfilepath = os.path.join(folder, filename)\nwith open(filepath, 'r', encoding='utf-8') as f:\nisi = f.read()\n# Ambil hanya bagian isi, setelah header\nif '\n\n' in isi:\nisi = isi.split('\n\n', 1)[1]\nsemua_konten += isi + ' '\n# Bersihkan teks dan tokenisasi\nteks_bersih = bersihkan_teks(semua_konten)\nkata_kata = teks_bersih.split()\n# Hapus stopwords\nkata_kata = [kata for kata in kata_kata if kata not in stop_words and kata.isalpha()]\n# Hitung frekuensi\ncounter = Counter(kata_kata)\ntop_kata = counter.most_common(top_n)\nprint(f\"\n\ud83d\udd0d Top Kata Paling Sering Muncul:\")\nfor kata, jumlah in top_kata:\nprint(f\"{kata}: {jumlah}\")\n# Buat Word Cloud\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(counter)\n# Tampilkan\nplt.figure(figsize=(12, 6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud dari Semua Konten')\nplt.tight_layout()\nplt.savefig('wordcloud.png')\nplt.show()\nprint(\"\n\u2705 Word Cloud disimpan sebagai 'wordcloud.png'\")\n# Jalankan analisis\nif __name__ == '__main__':\nanalisis_folder('outputs', top_n=30)\nWhat does the command 'plt.savefig('wordcloud.png')' do in the script?\n{\n=It saves the generated word cloud image as a PNG file named 'wordcloud.png'.\n~It displays the word cloud image on the screen.\n~It saves the word frequency data to a text file.\n~It exports the analysis results to a CSV file.\n~It prints the word frequency list to the console.\n~It saves the stopwords list to a file.\n}",
    "jawaban": "The correct answer is: top_n"
  },
  "9b9eb68558305ca415a313c396e79f22388aca179d032c7eca6f4e54ccc2cd9b": {
    "soal": "What is overfitting in machine learning?",
    "jawaban": "The correct answer is: When a model performs well on training data but poorly on new data"
  },
  "b42bcf5f084da7a2b2abb83752d6b31bd2cf6ae4771529639cd6b90881770759": {
    "soal": "import numpy as np\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n# Contoh data: nilai aktual dan prediksi\ny_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0]) # Nilai aktual\ny_pred = np.array([1, 0, 1, 0, 0, 1, 0, 1, 1, 0]) # Prediksi model\n# Menghitung confusion matrix\ncm = confusion_matrix(y_true, y_pred)\n# Menampilkan confusion matrix dalam bentuk teks\nprint(\"Confusion Matrix:\n\", cm)\n# Menampilkan confusion matrix dalam bentuk grafik\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.show()\nMengapa pustaka numpy diimpor dalam contoh kode untuk confusion matrix?",
    "jawaban": "The correct answer is: Untuk manipulasi array dan pembuatan data contoh."
  },
  "71b63b447480558094491409cc91f36d3a13b2fa9b029e35bb52ee8c5106694d": {
    "soal": "For datasets with parallelism, which visualization is NOT recommended?",
    "jawaban": "The correct answer is: Directly-Follows Graph (DFG)"
  },
  "dfa1abf38ec375c7c62d8f3682e6803acb17ce910f7ead103252ce29c03cc059": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=10000)\nThe IMDB dataset contains how many movie reviews?",
    "jawaban": "The correct answer is: 50,000"
  },
  "caba4c912bc585973d61ec5ee29f55fea5b44ae290cbb371e5ef6a040faf57e7": {
    "soal": "Which command provides detailed information about a specific model?",
    "jawaban": "The correct answer is: ollama show model"
  },
  "38f819acdb28f6eab341ebe7860e5a353733aff5ac8466e3c5faf755ade77e7e": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(2, size=100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi training (80%) dan testing (20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model, misalnya RandomForestClassifier\nmodel = RandomForestClassifier(random_state=42)\n# Melatih model pada data training\nmodel.fit(X_train, y_train)\n# Memprediksi data testing\ny_pred = model.predict(X_test)\n# Mengevaluasi akurasi model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model pada data testing: ')\nWhat does the `print(f'Akurasi model pada data testing: {accuracy:.2f}')` statement do?\n{\n~Prints the model's accuracy as an integer.\n=Prints the model's accuracy on the test data formatted to two decimal places.\n~Displays the confusion matrix of the model.\n~Outputs the model's precision and recall scores.\n~Saves the accuracy result to a file.\n~Plots the accuracy over training epochs.\n}",
    "jawaban": "The correct answer is: accuracy:.2f"
  },
  "8fb4607239b31da3e5bd90a709fb1c709a7d450ba17d1ded0faaa60ce62b432e": {
    "soal": "PM4Py\u2019s primary visualization type is:",
    "jawaban": "The correct answer is: Static visualization"
  },
  "837a0b73e0559ae1a3c5c06f3a3ae2e3cb9c6aec59aae84a2b13a5e49b9db142": {
    "soal": "The Distance Map widget can visualize distances between objects. The visualization is the same as if we printed a table of numbers, but the numbers are replaced with colored houses.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "ea05cda283b9598199d05a9a16275156689f58d9e31efd4de69f371bbebb0f2c": {
    "soal": "import os\nimport string\nfrom collections import Counter\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\n# Buat daftar stopwords Bahasa Indonesia\nstop_words = set(stopwords.words('indonesian'))\ndef bersihkan_teks(teks):\n# Lowercase + hilangkan tanda baca\nteks = teks.lower()\nteks = teks.translate(str.maketrans('', '', string.punctuation))\nreturn teks\ndef analisis_folder(folder='outputs', top_n=20):\nsemua_konten = ''\nfor filename in os.listdir(folder):\nif filename.endswith('.txt'):\nfilepath = os.path.join(folder, filename)\nwith open(filepath, 'r', encoding='utf-8') as f:\nisi = f.read()\n# Ambil hanya bagian isi, setelah header\nif '\n\n' in isi:\nisi = isi.split('\n\n', 1)[1]\nsemua_konten += isi + ' '\n# Bersihkan teks dan tokenisasi\nteks_bersih = bersihkan_teks(semua_konten)\nkata_kata = teks_bersih.split()\n# Hapus stopwords\nkata_kata = [kata for kata in kata_kata if kata not in stop_words and kata.isalpha()]\n# Hitung frekuensi\ncounter = Counter(kata_kata)\ntop_kata = counter.most_common(top_n)\nprint(f\"\n\ud83d\udd0d Top Kata Paling Sering Muncul:\")\nfor kata, jumlah in top_kata:\nprint(f\"{kata}: {jumlah}\")\n# Buat Word Cloud\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(counter)\n# Tampilkan\nplt.figure(figsize=(12, 6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud dari Semua Konten')\nplt.tight_layout()\nplt.savefig('wordcloud.png')\nplt.show()\nprint(\"\n\u2705 Word Cloud disimpan sebagai 'wordcloud.png'\")\n# Jalankan analisis\nif __name__ == '__main__':\nanalisis_folder('outputs', top_n=30)\nWhat is the effect of the command 'plt.axis('off')' in the script?\n{\n=It removes the axis labels and ticks from the matplotlib plot.\n~It turns off the display of the plot.\n~It resets the axis limits to their default values.\n~It changes the axis labels to be invisible but keeps the ticks.\n~It sets the axis labels to display in a different color.\n~It rotates the axis labels by 90 degrees.\n}",
    "jawaban": "The correct answer is: top_n"
  },
  "e7bfea2434c5594bb357161638d6dcf9fba78f4c3d322121c8367bd6f1edbacc": {
    "soal": "This workflow example shows how ................ can enrich the workflow. We used the lastfm.net data as the input network from Network File and sent it to Network Analysis. We can decide to compute the degree, degree centrality, and closeness centrality at the node-level.\n\nWe can then visualize the network in the Network Explorer. In the Network Explorer widget, it colors with the best tags, like the default for this dataset. But now we can also adjust the node size to fit the degree centrality results. This is a good way to visualize network properties.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Network Analysis"
  },
  "d089bf1de5db6dd296981acc5ec5bec8af8585c6176a96e4917c2f8fb43963a7": {
    "soal": "Widget File can read data from Excel (.xlsx), tab-delimited (.txt) simple, comma-separated file (.csv), or URL.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "6db7f83a4fce8c9b50fd8bd2aed37f65e7451ab1d3c23414568cf64de0e4c725": {
    "soal": "Some even call data science sexy, as stated by Hans Rosling, featured in a 2011 BBC documentary with the quote, \"Statistics is now the sexiest subject around.\" Nate Silver called data science a sexed-up version of statistics. In many ways, previous approaches have simply been rebranded as \"data science\" to make it less attractive, ultimately leading to the term becoming \"diluted beyond usefulness.\"",
    "jawaban": "The correct answer is 'False'."
  },
  "946caf354be65f800a240bb83c0c05cf37b14a98ff57b37a2aca109bc718c628": {
    "soal": "To obtain numerical representations of the images from Image Export, we need to send the images to the Image Embedding widget as shown in the workflow below.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "9d002f46187104cc8c145740c1f011e16cc375defa8e5a50c0049849a6951f38": {
    "soal": "\n\n\n\n\n\nThe image above shows the calculation technique choices depending on the data we have.",
    "jawaban": "The correct answer is 'False'."
  },
  "cfefe706a6a1145eb804acf37b5993463f1d2b9be798065903f7183c3ebf59b5": {
    "soal": "111 (binary) x 100 (decimal) = ....... (decimal)",
    "jawaban": "The correct answer is: 700"
  },
  "470207bd452530c4b8d76a82f1d9979282e6c2544bf07ca4e9b600063e0e3b30": {
    "soal": "Widget Save Data can consider the dataset provided in the input channel and save it to a data file with the specified name. It can save data as a file with tab-separated or comma-separated data.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "b2cc291d65e92927e2a0309f513c3a2bf6fd4327a1149a69fca536042a784dff": {
    "soal": "The core of Apache Hadoop consists of storage (Hadoop Distributed File System (HDFS)) and processing (MapReduce) parts. Hadoop splits files into large blocks and distributes them across nodes in the cluster. To process data, Hadoop MapReduce transfers code to nodes for parallel processing based on the data that needs to be processed at each node. This approach takes advantage of data-node locality manipulating the data it holds in hand\u2014allowing data to be processed faster and more efficiently than traditional supercomputer architectures that rely on parallel file systems where computations and data are connected through high-speed networks.",
    "jawaban": "The correct answer is 'True'."
  },
  "8a9a75ac47d39c154b27fd2dca99fd39954c0acaf8d87907b200b2154469bdb2": {
    "soal": "The easiest way to use the ......................... widget is to select a data subset and find matching examples in the visualization. We use the breast-cancer dataset to select two subsets with the Select Rows widget - the first subset is breast cancer patients aged between 40 and 49 years, and the second is patients with tumor sizes between 20 and 29. The Venn diagram helps us find examples that fit both criteria, which can be found at the intersection of the two circles.\n\n\n",
    "jawaban": "The correct answer is: Venn Diagram"
  },
  "29560ab926240a3ed13de9d3e2d0a0662d159652ef83729f950106cd634a190f": {
    "soal": "A pivot table is a statistical table that summarizes data from a larger table. This summary might include totals, averages, or other statistics, with the pivot table grouping them together in a non-meaningful way. Pivot tables are a technique in data processing.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "73e7b034847e26dbf28f30fe4d6f294581343da4b090ece982e54b4556376265": {
    "soal": "Deep Learning models such as LSTM or CNN typically require how many data points for optimal performance?",
    "jawaban": "The correct answer is: Tens of thousands to hundreds of thousands"
  },
  "338bf66de90349c2e00cd1fd8f216b0626086e45ea08aa4d7d807a4af7f427ec": {
    "soal": "For a more complex use of the Select Columns widget, we create a workflow to redefine the classification problem in the heart-disease dataset. Initially, the task was to predict whether the patient had coronary artery diameter narrowing. We changed the problem to gender regression, based on age, chest pain, and cholesterol levels, and informally kept diameter narrowing as a meta attribute.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "96eb2efcea5941c0aa844492926db8e867bf88490b128e3074cd25172a0bd301": {
    "soal": "When there is no data in the input, the Corpus widget reads ..............(1)........... corpus from a file and sends the corpus instance to the output channel. The history of the last opened file is saved in the Corpus widget. In the Corpus widget, there is also a directory with sample corpora that have been pre-installed with the add-on. The widget can read data from .............(2).............. files (.xlsx), comma-separated (.csv), and tab-delimited (.tab).\n",
    "jawaban": "The correct answer is: (1) \u2192 text, (2) \u2192 excel, \u2192 corpus"
  },
  "d415830abee4c081a38fbb78e0952c0d6af6bd07753dba6a27b9a1c40cacf4e6": {
    "soal": "import pandas as pd\n# Contoh: Membuat DataFrame dengan missing values\ndata =\ndf = pd.DataFrame(data)\n# Menampilkan jumlah missing values di setiap kolom\nprint(\"Jumlah missing values per kolom:\")\nprint(df.isnull().sum())\n# Menghapus baris yang memiliki missing values\ndf_cleaned = df.dropna()\nprint(df_cleaned)\n# Mengisi missing values pada kolom 'Usia' dengan mean\ndf['Usia'].fillna(df['Usia'].mean(), inplace=True)\n# Mengisi missing values pada kolom 'Usia' dengan median\ndf['Usia'].fillna(df['Usia'].median(), inplace=True)\n# Mengisi missing values pada kolom 'Kota' dengan modus\ndf['Kota'].fillna(df['Kota'].mode()[0], inplace=True)\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n# Inisialisasi imputer untuk mengisi missing values dengan mean\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n# Mengimputasi kolom 'Usia'\ndf[['Usia']] = imputer.fit_transform(df[['Usia']])\nWhich method is used to fit the SimpleImputer to the data?\n{\n=fit()\n~transform()\n~fit_transform()\n~apply()\n~impute()\n~replace()\n}",
    "jawaban": "The correct answer is: 'Nama': ['Andi', 'Budi', 'Citra', 'Dewi'], 'Usia': [25, 30, None, 22], 'Kota': ['Jakarta', None, 'Bandung', 'Surabaya']"
  },
  "eb3336c7e715bdc19bb5f968fe7b8396e5c397c4f8df7269468f219616bdad7e": {
    "soal": "Data does not always come in the form of neat tables. Data can also be in the form of text, audio recordings, video content, or even images. However, computers can only work with ................., so for any data mining, we need to transform that unstructured data into a vector representation.\n\nTo extract numbers from unstructured data, Orange can use a deep network embedder. Orange has just started incorporating various embedders in Orange in the Image Embedding widget, and currently, they are available for text and images.\n",
    "jawaban": "The correct answer is: numbers"
  },
  "6691db75dc786bd6c2927b13dcaa1a742008b7d5a3d9c67b3d717807b3c90247": {
    "soal": "Widget Save Data can consider the dataset provided in the input channel and save it to a data file with the specified name. Widget Save Data can save data as a file with tab-separated or comma-separated data.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "2d57f478334289f51b22ab132beae13b6947ac67f5b4c4413cfab4ca0bca86a6": {
    "soal": "010 (binary) = ...... decimal",
    "jawaban": "The correct answer is: 2"
  },
  "944f39c0e56e9e553da3231ecb60635aca369305b2803a3adff1c9286fe09fac": {
    "soal": "Statistical science and techniques are usually used at the beginning of the analysis process. Business Intelligence (BI) is usually performed after the data is collected and organized. Predictive Analytics, whether using traditional methods or machine learning/deep learning, is done after obtaining reports from Business Intelligence.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "38b2d3457511e2385071b84c444e16c002e653962262a4be4da3af583522d6eb": {
    "soal": "The Distances widget ignores discrete values and calculates distances only for continuous data, so the Distance Map widget can only display distance maps for continuous data.",
    "jawaban": "The correct answer is 'True'."
  },
  "84304c4aa202fabd389cc74395ef68233269470da9f886aef85fd6db36ca7c27": {
    "soal": "The core of Apache Hadoop consists of the storage part (Hadoop Distributed File System (HDFS)) and the processing part (MapReduce). Hadoop splits files into large blocks and distributes them across nodes in the cluster. To process the data, Hadoop \nAnswer Question 84\n transfers code to the nodes to process in parallel, based on the data that needs to be processed at each node. This approach takes advantage of data-node locality, manipulating the data they hold in hand to enable faster and more efficient processing than more conventional supercomputer architectures relying on parallel file systems where computation and data are linked through high-speed networks.",
    "jawaban": "The correct answer is: MapReduce"
  },
  "1987354838ac3d00bc39337543166e2957648f8a90ef7e424f4aea0b9cac9bd9": {
    "soal": "For a small-scale research project, what is the minimum recommended dataset size?",
    "jawaban": "The correct answer is: 500 data points"
  },
  "d501e3e91f1b1670a74d0fbbde87322896cd09e83a1ac746ac074db276d4b88d": {
    "soal": "from googlesearch import search\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef get_page_content(url):\ntry:\nheaders =\nresponse = requests.get(url, headers=headers, timeout=10)\nsoup = BeautifulSoup(response.content, 'html.parser')\n# Ambil judul halaman\ntitle = soup.title.string if soup.title else 'No Title'\n# Ambil konten paragraf utama\nparagraphs = soup.find_all('p')\ntext_content = ' '.join([p.get_text() for p in paragraphs[:5]]) # Batasi 5 paragraf pertama\nreturn title.strip(), text_content.strip()\nexcept Exception as e:\nreturn 'Error', f\"Failed to fetch content: {e}\"\ndef google_scrape_with_content(keywords, num_results=5, output_file='scraped_results.csv'):\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'Rank', 'Title', 'URL', 'Content'])\nfor keyword in keywords:\nprint(f\"\n\ud83d\udd0d Searching for: {keyword}\")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nprint(f\" \u2192 Fetching: {url}\")\ntitle, content = get_page_content(url)\nwriter.writerow([keyword, i+1, title, url, content])\ntime.sleep(2) # Delay biar aman\nexcept Exception as e:\nprint(f\"\u274c Error while searching '{keyword}': {e}\")\nprint(f\"\n\u2705 All results + content saved to '{output_file}'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\ngoogle_scrape_with_content(keywords, num_results=5)\nIn which format does the script save the search results and their content?\n{\n=CSV\n~JSON\n~XML\n~TXT\n~XLSX\n~HTML\n}",
    "jawaban": "The correct answer is: \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\""
  },
  "8446cc6f86d3445e8b73f102135d5031911cc3d532bb52c3d0b5a930ee6594b2": {
    "soal": "This workflow combines the classification tree interface and visualization with a scatter plot. When both the tree viewer and scatter plot are open, selecting any node from the tree will send the related data example to the scatter plot. In the workflow, the selected data is treated as part of the entire dataset and displayed in the scatter plot. With a simple combination of widgets, we have built an interactive classification tree browser.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "4c6e97760e1256cfa9c0c837d8748a2fde8d81705de60c3762dfb4a9db899621": {
    "soal": "import pandas as pd\n# Contoh: Membuat DataFrame dengan missing values\ndata =\ndf = pd.DataFrame(data)\n# Menampilkan jumlah missing values di setiap kolom\nprint(\"Jumlah missing values per kolom:\")\nprint(df.isnull().sum())\n# Menghapus baris yang memiliki missing values\ndf_cleaned = df.dropna()\nprint(df_cleaned)\n# Mengisi missing values pada kolom 'Usia' dengan mean\ndf['Usia'].fillna(df['Usia'].mean(), inplace=True)\n# Mengisi missing values pada kolom 'Usia' dengan median\ndf['Usia'].fillna(df['Usia'].median(), inplace=True)\n# Mengisi missing values pada kolom 'Kota' dengan modus\ndf['Kota'].fillna(df['Kota'].mode()[0], inplace=True)\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n# Inisialisasi imputer untuk mengisi missing values dengan mean\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n# Mengimputasi kolom 'Usia'\ndf[['Usia']] = imputer.fit_transform(df[['Usia']])\nWhich function in pandas is used to detect missing values in a DataFrame?\n{\n=isnull()\n~dropna()\n~fillna()\n~notnull()\n~replace()\n~interpolate()\n}",
    "jawaban": "The correct answer is: 'Nama': ['Andi', 'Budi', 'Citra', 'Dewi'], 'Usia': [25, 30, None, 22], 'Kota': ['Jakarta', None, 'Bandung', 'Surabaya']"
  },
  "63ec6a19ed6769283e8dbff1a9aef0426a519156392ab7a6141d9f52818319a8": {
    "soal": "101 (binary) + ?? (binary) = 111 (binary)",
    "jawaban": "The correct answer is: 10"
  },
  "bbe7078e1707fcd9bbca476ff926845e5be5acc41c934d224f728bdd3a74cdd0": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nWhich activation function is used in the first Dense layer of the model?",
    "jawaban": "The correct answer is: ReLU"
  },
  "b15b60e88aca5c042aa0f402b03da2738882d138d36bc509339312ea2f27a38b": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#-xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\ndfg, start_activities, end_activities = pm4py.discover_dfg(log)\npm4py.view_dfg(dfg, start_activities, end_activities)\nHow many XES filenames are commented out in the script?",
    "jawaban": "The correct answer is: 9 -6 -7 -8 -10 -5"
  },
  "615b6609584a42fe54452672d67a2a32329f0f95248c0aa32c9d12e87aa0b187": {
    "soal": "The Import Images widget scans .................... and returns one row per image taken. The columns include image name, directory path to the image, width, height, and image size. The column with the directory path of the image is then used as an attribute for visualization and Image Embedding.\n",
    "jawaban": "The correct answer is: direktori"
  },
  "b16bbd9cfb5f76173674d75dd9518f936a2608f9dd2f2e2480b280841d9a5d54": {
    "soal": "The Calibration Plot widget plots ............................ probability against the probability predicted by the classifier.\n",
    "jawaban": "The correct answers are: class, class"
  },
  "fc18f019e98b67678cdb829a204727a57f6038b163b6701aa971d5a84d13dc72": {
    "soal": "The ................... widget is a standard visualization widget, displaying the data profile, usually in the form of ordered numeric data. In this simple example, we will display the iris data in a line plot, grouped by iris attributes. The plot shows how petal length separates the class values well.\n\nIf we observe this in the Scatter Plot Widget, we can confirm this is indeed true. Petal length is an interesting attribute for class separation, especially when enhanced with petal width, which is also well separated in the line plot\n\n\n",
    "jawaban": "The correct answer is: Line Plot"
  },
  "68bcab1d618e4d4bff137e3899d6051d11bdc18d5a38803c5d6f373a842f9da7": {
    "soal": "Dalam contoh klasifikasi tipikal, kita akan menggunakan widget Contant untuk membandingkan skor algoritma pembelajaran lainnya (seperti kNN) dengan skor default. Gunakan dataset iris dan connect ke Test & Score. Kemudian hubungkan Constant dan kNN ke Test & Score dan amati seberapa baik kinerja kNN terhadap baseline Constant.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "38ec978262eb7652b7257f44fc0f0d05bcb794983e2bd499991c20ab14b2b357": {
    "soal": "When there is no data in the input, the Corpus widget reads the text corpus from a file and sends the corpus instance to the output channel. The history of the last opened file is saved in the Corpus widget. The Corpus widget also contains a directory with sample corpora that were previously installed with the canvas. The widget can read data from Excel files (.xlsx), comma-separated (.csv), and tab-delimited (.tab).\n",
    "jawaban": "The correct answer is 'False'."
  },
  "dcda8729827c4a3b0e4b8d74d5be526988bf753b7a1692f7606165397ff80969": {
    "soal": "Which of the following best describes 'Variety' in the context of Big Data?",
    "jawaban": "The correct answer is: Different types of data (structured, unstructured, semi-structured) from various sources"
  },
  "b0fdb67beaa1e647c990a56d4bfbfdb65849e616a56da368a5d009112bf97b5c": {
    "soal": "To perform machine learning, we need numbers. To obtain numerical representations of the images, we send the images to the Image Embedding widget.\n\nThe output data from the Image Embedding widget can be examined using the ............................ widget. Now we have numerical/pixel data for the images. Each image has 2048 numerical representations (columns n0 to n2047). With this numeric feature representation, we can apply all standard machine learning techniques, such as clustering.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Data Table"
  },
  "1baaf4a6714bf50d9e5ae5f99b8ddbbb66c7353efb9a7b61ed2b3842a53c3660": {
    "soal": "Answer could be more than one.\n\n101 (binary) + 10 (binary) = .............................. ?",
    "jawaban": "The correct answers are: 111 (binary), 7 (decimal)"
  },
  "d63e422a88e16f95ecf70582f97bc69aaef635d02a82368d4b4e56b4fad37b37": {
    "soal": "In the workflow below, we use the Zoo dataset. We load the data into the Scatter Plot widget, which allows us to select a subset of data instances. Then, we can send the selected data instances to the ............................ widget to save them into a file.\n\n\n\n",
    "jawaban": "The correct answer is: Save Data"
  },
  "f7b05cd7c9ddfd797624710253613d2ad39008392ed6144e66ae11190c56836b": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#_xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nmap = pm4py.discover_heuristics_net(log)\npm4py.view_heuristics_net(map)\nWhich Python variable stores the discovered heuristic net?",
    "jawaban": "The correct answer is: map -log -xes_filename -net -data -view"
  },
  "bdb8d09a83981386810489b6fe41570bf072184320c71be983e163163e8a7d25": {
    "soal": "\n\n\nIn preprocess text, we can do several things, such as\n\n\n\nConvert all letters to lowercase.\nRemove (stop words), words that are not useful, such as conjunctions like and, in, to, from, etc. Remove HTML tags\nProcess stopword filtering using Indonesian\n\nRemove URLs\netc.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "6dcb0931bad520834878f95f4957b14ed3eff3c1aa82a58acdedbf660a6a6bd1": {
    "soal": "To cluster based on ........................, we can measure the distance between images. The numerical representation of the images obtained from the Image Embedding widget can be measured using the Distances widget. Typically, cosine distance works best for images, but you can experiment yourself. We then send the distance matrix to Hierarchical Clustering to visualize the similar pairs in the dendrogram.\n\nThe result is that all the animals are correctly grouped. In the dendrogram, we don't see the images of the animals. We can use the Image Viewer widget to view the images.\n\n\n",
    "jawaban": "The correct answer is: similarity"
  },
  "60c122ba3d7c6e3ffa0065ee009b16d749ecac1da63fc7ce053fb6b3d420bf67": {
    "soal": "What is the purpose of using Anaconda for installing PM4Py?",
    "jawaban": "The correct answer is: To manage packages and dependencies in isolated environments"
  },
  "f9bb54c428f6f15e7184ce6e44d79b100ae39764e5b2a3928d9b36ca018b6e9c": {
    "soal": "011 (binary) = ...... (decimal)",
    "jawaban": "The correct answer is: 3"
  },
  "127515bab00396d48b00591d730dfa5d78fde2cfd65299b51016f3841582e896": {
    "soal": "In the next example, we will try to predict the category of documents. We use the dataset book-excerpts.tab, which we send through the Preprocess Text widget with default parameters. Then we connect the Preprocess Text widget to the Bag of Words widget to obtain term frequency. With term frequency information, we will calculate the model.\n\nConnect the Bag of Words widget to the Test & Score widget for predictive modeling. Connect the ...............(1)................ widget or another classifier widget to the Test & Score widget (all on the left). The Test & Score widget will calculate the performance score for each learner input. Here, we obtain very good results for the SVM widget.\n\nNext, we need to check where the model is making mistakes. Add the Confusion Matrix widget to the Test & Score widget. The Confusion Matrix widget will display the correctly and incorrectly classified documents. Select ...............(2).............. to output the misclassified documents, which we will examine further using the Corpus Viewer widget.\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 SVM, (2) \u2192 misclassified"
  },
  "57fb1180655f9e49e3f7550c0d652613cef82bb327898927371ebad0033ababc": {
    "soal": "The Select Columns widget can select data attributes and the composition of the data domain .......................\n",
    "jawaban": "The correct answer is: manual"
  },
  "5a4029f97357e865bcc9e29a164927201345c31b0901366b2fb3c529318b2ef6": {
    "soal": "Perform the following operation\n\n192 (decimal) AND 255 (decimal)",
    "jawaban": "The correct answer is: 192"
  },
  "e3e1abae0edb5c746b2f816daa5bda3dce6cb315228d7043a8a4437a2337108f": {
    "soal": "from googlesearch import search\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef get_page_content(url):\ntry:\nheaders =\nresponse = requests.get(url, headers=headers, timeout=10)\nsoup = BeautifulSoup(response.content, 'html.parser')\n# Ambil judul halaman\ntitle = soup.title.string if soup.title else 'No Title'\n# Ambil konten paragraf utama\nparagraphs = soup.find_all('p')\ntext_content = ' '.join([p.get_text() for p in paragraphs[:5]]) # Batasi 5 paragraf pertama\nreturn title.strip(), text_content.strip()\nexcept Exception as e:\nreturn 'Error', f\"Failed to fetch content: {e}\"\ndef google_scrape_with_content(keywords, num_results=5, output_file='scraped_results.csv'):\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'Rank', 'Title', 'URL', 'Content'])\nfor keyword in keywords:\nprint(f\"\n\ud83d\udd0d Searching for: {keyword}\")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nprint(f\" \u2192 Fetching: {url}\")\ntitle, content = get_page_content(url)\nwriter.writerow([keyword, i+1, title, url, content])\ntime.sleep(2) # Delay biar aman\nexcept Exception as e:\nprint(f\"\u274c Error while searching '{keyword}': {e}\")\nprint(f\"\n\u2705 All results + content saved to '{output_file}'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\ngoogle_scrape_with_content(keywords, num_results=5)\nWhich Python modules are required to run the script?\n{\n=googlesearch-python, requests, beautifulsoup4, csv, time\n~selenium, pandas, numpy, re, os\n~urllib, json, lxml, threading, datetime\n~scrapy, flask, sqlalchemy, matplotlib, sys\n~mechanize, xml, argparse, hashlib, itertools\n~pyquery, chardet, configparser, logging, subprocess\n}",
    "jawaban": "The correct answer is: \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\""
  },
  "820949e5e573e50184ba840b624937b73ce67503d13bf43fc7e20eb4dd702d1c": {
    "soal": "In the workflow below, we take data using the Twitter widget. We retrieve tweets from users @HillaryClinton and @realDonaldTrump from the internet and obtain their tweets for two weeks, totaling 242 tweets.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "9e89c846a6a668c4959ae0bae5d6faf546bd6cbac3ea3644778a52436034af0d": {
    "soal": "!pip install googlesearch-python nltk matplotlib wordcloud\nfrom googlesearch import search\nimport csv\ndef google_scrape(keyword, num_results=40,\noutput_file='results.csv'):\nprint(f\"Searching Google for: \")\n# Lakukan pencarian dan ubah hasil ke dalam list\nresults = list(search(keyword, num_results=num_results))\n# Simpan ke file CSV\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'URL'])\nfor url in results:\nwriter.writerow([keyword, url])\nprint(f\"Saved {len(results)} results to '{output_file}'\")\n# Contoh penggunaan\nkeyword = \"berita terbaru teknologi Indonesia\"\ngoogle_scrape(keyword, num_results=40)\nWhich command installs the necessary modules for the script, including 'googlesearch-python'?\n{\n=pip install googlesearch-python nltk matplotlib wordcloud\n~pip install google-search nltk matplotlib wordcloud\n~pip install googlesearch nltk matplotlib wordcloud\n~pip install google-api-python-client nltk matplotlib wordcloud\n~pip install google-search-api nltk matplotlib wordcloud\n~pip install googlesearch-python nltk seaborn wordcloud\n}",
    "jawaban": "The correct answer is: keyword"
  },
  "736cbd55ed3d45e453806b6e16d2d346a27f79e80a5f664a4fd63790d7e01346": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nplt.imshow(x_train[0], cmap='gray')\nplt.title(f\"Label: \")\nplt.show()\nmodel = keras.Sequential([\nkeras.layers.Flatten(input_shape=(28, 28)),\nkeras.layers.Dense(128, activation='relu'),\nkeras.layers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nWhat does the Flatten layer do?\n{\n= Converts 2D input to 1D\n~ Increases the input size\n~ Adds noise\n~ Applies dropout\n~ Normalizes the input\n~ Reduces overfitting\n}",
    "jawaban": "The correct answer is: y_train[0]"
  },
  "7adcc68e6c0c06b58fac3164d1ffeacb9c5ffc4cbf2a8ec8ef74b73d4b7c2ce5": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\n# xes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\nxes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nprocess_tree = pm4py.discover_bpmn_inductive(log)\nbpmn_model = pm4py.convert_to_bpmn(process_tree)\npm4py.view_bpmn(bpmn_model)\nWhich file is NOT listed as an option in the comments?",
    "jawaban": "The correct answer is: training_log_2.xes"
  },
  "77d0bbe33f8595c96c419d811f89e80530bef860ba1516d9019b5d11a20bb1e8": {
    "soal": "In the Moving Transform widget, to integrate the time series' difference from the Difference widget, use Cumulative node aggregation on a wide enough window to capture the entire series.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "885c2573eaca962e472a7bf354c027e55dcb4fa47698177c35752e96a2272182": {
    "soal": "Principal Component Analysis (PCA) computes the PCA linear transformation of input data. It outputs a transformed dataset with the weight of individual instances or the weight of principal components.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "09344c260e86b5350a2e064ea1f13b1dcef094cabcff801492b28e0b6279d896": {
    "soal": "The Preprocess Text widget in ORANGE applies random preprocessing steps. This means it will first transform the text, then apply tokenization, POS tagging, normalization, filtering, and finally build n-grams from the given tokens. This is crucial for the WordNet Lemmatizer because it requires POS tags for proper normalization.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "63546a8c368228680bf765e176c44304a8d4e6d9d92a8db56033590eb636cc27": {
    "soal": "In the following example workflow, we show the use of a very simple Corpus widget. Place the Corpus widget on the canvas and connect it to the ..............(1)................ widget. We use the book-excerpts...........(2).............. dataset, which is available in the add-on, and inspect it in the Corpus Viewer widget.\n\n\n",
    "jawaban": "The correct answer is: (2) \u2192 tab, (1) \u2192 Corpus Viewer"
  },
  "e0691c51346d582df6063246046fb123bc0889e83e2151184906d278898826eb": {
    "soal": "To group based on similarity, we can measure the distance between images. The numeric representation of images obtained from the Image Embedding widget can be used to measure the distance using the .............(1)............. widget. Usually, cosine distance works best for images, but you can experiment on your own. Then we will send the distance ...........(2).............. to Hierarchical Clustering to visualize similar pairs in the dendrogram.\n\nAs a result, all animals are correctly grouped. In the dendrogram of the clustering results, we do not see images of animals. We can use the Image Viewer widget to view the images.\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 Distances, (2) \u2192 matrix"
  },
  "5df5b09cd113b68b92eb48b9ec21b149e80c7996fda1893625d825bb44ec3164": {
    "soal": "Pada Widget Constant, Learner akan menghasilkan model yang selalu memprediksi ............\u00a0 untuk classification atau nilai rata-rata untuk regresi.\n",
    "jawaban": "The correct answer is: majority"
  },
  "42b1cbbe7b7c2e4c290d721766522e473498e0ee11a03763dd70b165ed6512fb": {
    "soal": "The Calibrated Learner widget generates a model that calibrates the distribution of class probabilities and optimizes the decision threshold. This widget only works for classification tasks.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "631c29b641169860a59cb1a3a1a682e2a2a2f8107d089379b5e38feea50a42a0": {
    "soal": "How do you remove a Docker container named 'ollama'?",
    "jawaban": "The correct answer is: docker rm ollama"
  },
  "574bcb76452af1763c2a0f32f91b13a9ec14b21e910782e4cf7638de08344a4a": {
    "soal": "What does the term 'epoch' mean in machine learning?",
    "jawaban": "The correct answer is: A complete pass through the entire training dataset"
  },
  "adc468a1ca47d477dbd29f49c2d36331404f6440384ff9eb27813b3f77bfdb41": {
    "soal": "What is the significance of 'resource' information in an event log?",
    "jawaban": "The correct answer is: Identifies who or what performed the activity"
  },
  "a50800188e4c8d02009497291a42fe1b63bf701426badcf43d663e3141f6e108": {
    "soal": "Line plot, a type of plot that displays data as a series of points, connected by straight line segments. Line Plot is used for numeric data. Meanwhile, for categorical data, Line Plot can be used for ........................ data points.\n",
    "jawaban": "The correct answer is: grouping"
  },
  "85df13662682ee899edca823c958e53cca73ad0ccf6f71d6d24e972ba16f5c29": {
    "soal": "In the Distributions widget, for discrete attributes, the graph displayed by the widget shows how many times (e.g., how many instances) each attribute value appears in the data. If the data contains a class variable, the class distribution for each attribute value will also be displayed (as shown in the snapshot below). To create this graph, we used the Zoo dataset.\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "56d8feb162c2146c19c4e010f76c9c32185bc0154425c7b04fd1dbd05db212be": {
    "soal": "!pip install googlesearch-python nltk matplotlib wordcloud\nfrom googlesearch import search\nimport csv\ndef google_scrape(keyword, num_results=40,\noutput_file='results.csv'):\nprint(f\"Searching Google for: \")\n# Lakukan pencarian dan ubah hasil ke dalam list\nresults = list(search(keyword, num_results=num_results))\n# Simpan ke file CSV\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'URL'])\nfor url in results:\nwriter.writerow([keyword, url])\nprint(f\"Saved {len(results)} results to '{output_file}'\")\n# Contoh penggunaan\nkeyword = \"berita terbaru teknologi Indonesia\"\ngoogle_scrape(keyword, num_results=40)\nWhat does the 'search' function from the 'googlesearch' module return?\n{\n=A list of URLs matching the search query.\n~The HTML content of the search results page.\n~A list of titles and snippets from the search results.\n~A dictionary containing URLs and their corresponding titles.\n~A JSON object with detailed search result information.\n~A Pandas DataFrame of search results.\n}",
    "jawaban": "The correct answer is: keyword"
  },
  "04021a30f7458bbd1c272667b7229430d493886a75f4a1f12b7d6f9f91e50ff8": {
    "soal": "Widget Line Plot can be used to visualize data profiles (e.g., Time series / .........................).\n",
    "jawaban": "The correct answer is: time series"
  },
  "14c4d4d2c456258e787d052a12f96fc5ea1e3325c7bb33ba2307e16764fbe5fb": {
    "soal": "The Image Viewer widget will display the images on the Internet.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "460cf8193c96156aece7187bb4160cc0df17f981e82cbebfa65286c131cb14cd": {
    "soal": "Widget SQL Table can read data from an SQL database.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "c9d7c29d98718f16fe2c903940c3d44d3685043143c435f435f506f16d7a49d3": {
    "soal": "The Select Columns widget is used to manually arrange the data domain. The user can decide which attributes to use and how to do it. Orange distinguishes between regular attributes, class attributes (optional), and meta attributes. For example, to build a ......................... model, the domain will consist of a set of attributes and discrete class attributes. Meta attributes are not used in modeling, but some widgets can use them to label instances.\n",
    "jawaban": "The correct answer is: classification"
  },
  "b3efcc85f5c3861e17f15d20aadc5e55c80ba20612d97bb1aceeb012b6613447": {
    "soal": "CN2 Rule Induction can only be used for ................. tasks.\n",
    "jawaban": "The correct answer is: classification"
  },
  "8ba59a0b9705c52d2db27cf8206d5a52cd0cd1e4914873772fd148cc0819c2e2": {
    "soal": "The kNN widget predicts based on the closest training instances.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "58daab3e9f2573625c2b9ed182e78f227030f3556f36b894b852895f9bfaa25a": {
    "soal": "\n\n\nExamples of statistics data professions include,\n\n\n\n\n\n\n",
    "jawaban": "The correct answers are: Data Architect, Data Engineer, Database Administrator"
  },
  "563042e3c3ea3c8a0d908c6801890c915d694912953317a482be92cbd0e9a752": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#_xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nmap = pm4py.discover_heuristics_net(log)\npm4py.view_heuristics_net(map)\nWhich of the following statements is true about the heuristic net?",
    "jawaban": "The correct answer is: It visualizes frequent behaviors in processes -It is a neural network -It clusters numerical data -It generates text summaries -It trains machine learning models -It predicts customer behaviors"
  },
  "56ed797380be256615342c14df029a4ee287c10321236630c858c08482cda544": {
    "soal": "ProM installation requires users to:",
    "jawaban": "The correct answer is: Follow official instructions"
  },
  "2f0047aec80c2dd1e4e32d8b51a13ab118dfec88103009524b820acb7e071976": {
    "soal": "Which optimizer is commonly used for training deep learning models in TensorFlow?",
    "jawaban": "The correct answer is: Adam Optimizer"
  },
  "1343492c79fee8ba972667693f8772997eaa36013a7cad5e86af1ed8cf062413": {
    "soal": "The Tree widget uses the Tree algorithm with the ability to perform inward pruning.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "384a8bc37551ff5a51bd9bbe99a125b9f7b8fc171e7491671c1e82c238c89a98": {
    "soal": "result 8 (decimal) OR 1 (decimal) is",
    "jawaban": "The correct answer is: 9"
  },
  "a5982001dd119c236c8f98a44c410bbf5ecfaecfa2dfa7d48e27ff5849e78872": {
    "soal": "In the example below, we will use the Attrition - Train data from the Datasets widget. This is data about employee attrition. In other words, we want to know whether a specific employee will leave their job or not. We will create a prediction model using the Tree widget and observe the probabilities in Predictions.\n\nFor predictions, we need training data, which we have loaded in the first Datasets widget, and data to predict, which we will load into another Datasets widget. We will now use the Attrition - Predict data. Connect the second dataset to the ...................... Widget. Now we can see the predictions for three instances of data from the second dataset.\n\nThe Tree model predicts that no employee will leave the company. We can try another model and see if the predictions change. Or test predictive scores first in the Test & Score widget.\n\n\n",
    "jawaban": "The correct answer is: Predictions"
  },
  "a528172895b8ab14f47eaf96ee16a5e6ee5494fa37eb972f09044f70fa3c335e": {
    "soal": "Principal Component Analysis (PCA) transforms data into a dataset with uncorrelated variables, also called principal components. The PCA widget displays a graph (scree plot) showing the degree of variance explained by the best principal components and allows for interactive adjustment of the number of components to include in the output dataset. In this workflow, we can observe the transformation in the Data Table and the Scatter Plot.\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "3f715df7415b9ad7866b17a863621f1b573c96c82607e2153c43a597537ff932": {
    "soal": "We can measure the distance between the embedded images and see which ones are most similar. We can use the Distances widget to measure the distance. Typically, cosine distance works best for images. We send the distance matrix to Hierarchical Clustering to visualize the similar pair in the dendrogram.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "db2bba8615edd56632fd4223820721642849c06913d7951b292a9d06c4957507": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#_xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nmap = pm4py.discover_heuristics_net(log)\npm4py.view_heuristics_net(map)\nWhat function should you run last in this script to see output?",
    "jawaban": "The correct answer is: pm4py.view_heuristics_net(map) -pm4py.read_xes(xes_filename) -pm4py.discover_heuristics_net(log) -import pm4py -print(log) -save(map)"
  },
  "23c9545e2fb582ef89ed66ceb029d58310bae4a1b65def92fc76644901bf6449": {
    "soal": "Network Clustering Widget tries to find clusters in the network. Network Clustering works with two algorithms, one from Raghavan et al. (2007), which uses label propagation to find suitable clusters, and one from Leung et al. (2009), which builds on Raghavan's work and adds hop amplification as a parameter for cluster formation.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "3286d5a0226a63ef1d03a95d86325e824877a80a4efc3e3256a5e8a4215cf9e2": {
    "soal": "In the Distributions widget, for discrete attributes, the graph displayed by the widget shows how many times (e.g., how many instances) each attribute value appears in the data. If the data contains a ............ variable, the ............ distribution for each attribute value will also be displayed (as shown in the snapshot below). To create this graph, we used the Zoo dataset.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: class"
  },
  "192042bcedbecf6a5ad4e7ddf07a54ed151dc81092282cb0eb1d723f8f2a5080": {
    "soal": "Which of the following best describes a feature in a dataset?",
    "jawaban": "The correct answer is: An individual measurable property or characteristic of a data entry."
  },
  "47b657ceb8fe8574b87a32883d7cd259eef88dd821e9af494afb89758799109b": {
    "soal": "The Constant widget will predict the most frequent ............ or mean value of a training set.\n",
    "jawaban": "The correct answer is: class"
  },
  "486d4ff87287a711327ed8a3d8b212b51f13973e8372a1cfb180ef73cbe2ace4": {
    "soal": "In the workflow below, the widget ............(1)........... will train a model and evaluate it against Logistic Regression. The accuracy comparison between the Tree widget and Logistic Regression widget is done through the Test & Score widget. The results from the Test & Score widget can be seen through the ..............(2).......... widget to compare the classification errors.\n\n\n",
    "jawaban": "The correct answer is: (2) \u2192 Confusion Matrix, (1) \u2192 Tree"
  },
  "186f7f8d9d6eb92c058d7c781d2d46ca5ee5faba73c1fd175986ec2e56c89281": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nplt.imshow(x_train[0], cmap='gray')\nplt.title(f\"Label: \")\nplt.show()\nmodel = keras.Sequential([\nkeras.layers.Flatten(input_shape=(28, 28)),\nkeras.layers.Dense(128, activation='relu'),\nkeras.layers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nWhat type of model is created?\n{\n= Sequential\n~ Functional API\n~ Subclassing API\n~ Decision Tree\n~ Ensemble Model\n~ Convolutional Network\n}",
    "jawaban": "The correct answer is: y_train[0]"
  },
  "04c6b41ce00b913db962e76cb5228ba7520a432a0eb740c984d84016feb5338a": {
    "soal": "In the workflow below, the Import Documents widget can load subfolders. We store President Kennedy's speeches in two (2) subfolders, pre-1962 and post-1962. If we load the parent folder, both subfolders will be used as ........................ label. Check the output of the Import Documents widget in the Data Table widget.\n\n\n\n\n",
    "jawaban": "The correct answer is: class"
  },
  "6650ff82b03b6f7493fe74e279e001c6d291398fddf7d190532c8b5cbb9eca2d": {
    "soal": "CN2 Rule Induction hanya bisa jalan untuk klasifikasi saja.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "5ccc0521a55223bc6335ab4909ba4ec87ba8a9a220e4045b923aa01075a7e83b": {
    "soal": "The Text Preprocessing widget in ORANGE can process ORANGE Python source code based on the method we choose.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "60b79eeb070f233fc63a0d9aed025795bf6066f43f5765be1c5ec30dab637791": {
    "soal": "The Import Images widget allows us to import ........................ (images) from a directory.\n",
    "jawaban": "The correct answer is: image"
  },
  "6fb7b04073bb4caa06a5dae1c797fa3c8f2c95cc4a93e2bb766c845c8b323e22": {
    "soal": "Which of the following is an example of unstructured data?",
    "jawaban": "The correct answer is: Social media posts"
  },
  "2c8baddb5efe3c38a316030045f571d3af8360733c9a44bd8cc631c068d6b9d5": {
    "soal": "Heuristic Net visualizer is especially tolerant to:",
    "jawaban": "The correct answer is: Noise"
  },
  "d8a59db00b0d43073b2ef07ef8426adf7adfc506a01f204ad312e08ce0ad2b5a": {
    "soal": "Which command pauses a model that is currently running?",
    "jawaban": "The correct answer is: ollama pause model"
  },
  "19506f8a7837710b6a6300063996f55a071498eb0078c2ec0350ead5cde393c6": {
    "soal": "Which statement is true about LLM responses?",
    "jawaban": "The correct answer is: They are based on previous prompts and outputs"
  },
  "97f28fa039757ca951effb00f90a0e7e182461fd5ea6054982ea670ae9d09fe3": {
    "soal": "\n\n\n\n\n\n\n\n\nThe image above shows the techniques/science/mindmap of deep learning in broad terms.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "eb97ceaec360940b09b70a2c229579678ea5f5167e315bca33881d505fda8d3f": {
    "soal": "A simple example with Calibrated Learner. We use the Titanic dataset because this widget requires binary class values (in this case, they are 'survived' or 'not survived').\n\nWe use Logistic Regression as the base learner, which will be calibrated with the default settings, with sigmoid optimization from the value distribution and optimized with CA.\n\nBy comparing the results of the uncalibrated Logistic Regression model, we will clearly see that the calibrated model is worse.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "4e73e2a02a7091ad7a75d7ebb10901b67c82421df3044ddb442442eb2ae17bb9": {
    "soal": "The Image Viewer widget will display images from the dataset, which can be stored locally or on the Internet. The Image Viewer widget will search for attributes with type= .................\u00a0 in the third row header.",
    "jawaban": "The correct answer is: image"
  },
  "14a920db005e9b6fdb8ec3454f38fe32f5e63db6bd3b509a3f8c4ffae343ff03": {
    "soal": "The Distributions widget displays the distribution of ................. or continuous attribute values. If the data contains a class variable, the distribution can be conditioned on the class.\n",
    "jawaban": "The correct answer is: discrete"
  },
  "688049f469f4a3538fe1ee975e4ee6daee94d7d5b4a36695cb00408207ca87bc": {
    "soal": "The Distances widget also works well with other Orange add-ons. The Distance Matrix widget can be fed into the Network from Distances Widget (Network add-on) to convert the matrix into a graph and into the Duplicate Detection Widget (Text add-on) to find duplicate documents in .....................\n",
    "jawaban": "The correct answer is: corpus"
  },
  "983c217c87b67469edbb4c580717e9ee51aa89d87feb5c9d7ccc85d3bba62853": {
    "soal": "import os\nimport string\nfrom collections import Counter\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\n# Buat daftar stopwords Bahasa Indonesia\nstop_words = set(stopwords.words('indonesian'))\ndef bersihkan_teks(teks):\n# Lowercase + hilangkan tanda baca\nteks = teks.lower()\nteks = teks.translate(str.maketrans('', '', string.punctuation))\nreturn teks\ndef analisis_folder(folder='outputs', top_n=20):\nsemua_konten = ''\nfor filename in os.listdir(folder):\nif filename.endswith('.txt'):\nfilepath = os.path.join(folder, filename)\nwith open(filepath, 'r', encoding='utf-8') as f:\nisi = f.read()\n# Ambil hanya bagian isi, setelah header\nif '\n\n' in isi:\nisi = isi.split('\n\n', 1)[1]\nsemua_konten += isi + ' '\n# Bersihkan teks dan tokenisasi\nteks_bersih = bersihkan_teks(semua_konten)\nkata_kata = teks_bersih.split()\n# Hapus stopwords\nkata_kata = [kata for kata in kata_kata if kata not in stop_words and kata.isalpha()]\n# Hitung frekuensi\ncounter = Counter(kata_kata)\ntop_kata = counter.most_common(top_n)\nprint(f\"\n\ud83d\udd0d Top Kata Paling Sering Muncul:\")\nfor kata, jumlah in top_kata:\nprint(f\"{kata}: {jumlah}\")\n# Buat Word Cloud\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(counter)\n# Tampilkan\nplt.figure(figsize=(12, 6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud dari Semua Konten')\nplt.tight_layout()\nplt.savefig('wordcloud.png')\nplt.show()\nprint(\"\n\u2705 Word Cloud disimpan sebagai 'wordcloud.png'\")\n# Jalankan analisis\nif __name__ == '__main__':\nanalisis_folder('outputs', top_n=30)\nHow does the script handle files in the 'outputs' directory that do not have a '.txt' extension?\n{\n=It ignores them and does not process their content.\n~It attempts to read and analyze them as text files.\n~It deletes them before proceeding with the analysis.\n~It moves them to a separate directory for non-text files.\n~It logs a warning message and continues processing other files.\n~It prompts the user to convert them to text files.\n}",
    "jawaban": "The correct answer is: top_n"
  },
  "13fd5d3745a4a9e915e2001cfac76bba2770bf0ad66f4a8e7c845b5c264517e8": {
    "soal": "How does the Flatten layer transform the input data?",
    "jawaban": "The correct answer is: Converts each 28x28 image into a 784-element 1D vector"
  },
  "c17bb3c9069a69c890a9b3d79565c2b1d53cf30b7f51a709908d50b8e90aa60f": {
    "soal": "With a dropout rate of 0.2, what percentage of neurons are deactivated during training?",
    "jawaban": "The correct answer is: 20%"
  },
  "af5b13983253996346c828782c909501dd1bd518673fae2074151d3504898927": {
    "soal": "One URL location for Orange development/use examples is\n\n\u00a0http://www.englishcollocation.com/how-to-use/orange\n",
    "jawaban": "The correct answer is 'False'."
  },
  "d05eaf5078e455a683e3f0f79f51d91e2acc38d884e846be7201b0f104133e64": {
    "soal": "Word Cloud data can be built from the text files (ASCII) we have, as in the workflow below. First, data from the Text Files Widget must be segmented into words using the Segment Widget. Then, the output segmented data needs to be converted from segmented data into a corpus so that it can be processed by the Widget Interchange in the text mining toolbox. Before displaying it as a word cloud, it is recommended to do preprocessing first to reduce unnecessary words, such as conjunctions, etc., using the Preprocess Text Widget.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Preprocess Text"
  },
  "10a9e9331fe9d8cfa1edcd18fc32cbe85da8cf2f30bf07f8129152a1bb206f4f": {
    "soal": "The Distributions widget displays the distribution of the minimum value for one attribute\n",
    "jawaban": "The correct answer is 'False'."
  },
  "dd1a9b4c32582b67d8046431941dbb13dab5ba832e7c2636ce8746fb074afb7d": {
    "soal": "The Distance Transform widget transforms the distances present in the distance matrix.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "27a7270adfdbf0b6f2e298f3135db5476ee850f8968ef425a64996831dce31a4": {
    "soal": "The Predictions widget receives a dataset and one or more ................. (predictive models, not algorithm learners). The Predictions widget generates data and predictions.\n",
    "jawaban": "The correct answer is: predictor"
  },
  "2e2ec9ca2e49ce7429236c8ad34ddc787fd4d600dda393fbe238b93bb6055de3": {
    "soal": "\n\n\n\n\n\nProses klasifikasi dilakukan menggunakan algoritma / modul logistric regression yang hasilnya langsung dinilai keakuratannya oleh modul Test & Score\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "50e5d28cc2cd3c4711172f3a506c109a028bea6634cc80b2bef2d0fecfd14246": {
    "soal": "Network Generator Widget creates an example network. It is intended for teaching/learning about networks.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "a532a84e8cd190a8d84e6d86ced81b350227632aa73dc32adda780e9491c9ed6": {
    "soal": "Widget Calibrated Learner menghasilkan sebuah model yang mengkalibrasi distribusi dari class probabilities dan meng-optimasi decision threshold.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "b5265b59174ae8855141a83ab89e4d7d85fa90b3ad4ca6beb34f639af4c58fdc": {
    "soal": "The Venn Diagram widget can also be used to explore different prediction models. In the following example, we analyze 3 prediction methods, namely Naive Bayes, SVM, and Random Forest, based on their classification errors on data instances.\n\nBy selecting misclassification in the three Confusion Matrix widgets and sending them to the Venn Diagram widget, we can see all misclassification examples visualized by the method used. We then open the Venn Diagram widget and select, for example, misclassified instances identified by all three methods. This is represented as the intersection of the three circles. Click on the intersection to view these two examples highlighted in the Scatter Plot widget. Try selecting different parts of the diagram to see how the scatter plot visualization changes.\n\n\n\n\n",
    "jawaban": "The correct answer is: Confusion Matrix"
  },
  "21384b99ea7b12500113f5aa1a496cfccc55ecc0fedb3234fc3f4a0c80785759": {
    "soal": "In the Correlogram widget, we will visualize the non-correlation coefficient for the time series we select.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "76128394a6fd6844c683c2a0c9497ba14d6c3e28d8809e7409bdc5efb1b92500": {
    "soal": "Widget CSV File Import can import data tables from CSV formatted files.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "f2e06ae6baf0c4a2c665cab8fe9aef8fbf8823585106745f638cd6fa053a2fb0": {
    "soal": "\n\n\nWorkflow di atas melakukan test / evaluasi CN2 Rule Induction dan Tree di Widget Test & Score. Terlihat Tree lebih baik.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "5108a1cd869364bc438e6aa7d5ea2bae4e4a9e31c6189294e419e8ba12bf358d": {
    "soal": "The Tree algorithm is a simple algorithm that can separate data into nodes based on class purity (class/category purity). The Tree algorithm is the predecessor of the Random Forest algorithm. The Tree widget in Orange is designed in-house and can handle discrete and floating point datasets.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "320a013efe8ad8024827b93ea957b78ea26378a3f903441ed8fec8eb528694e4": {
    "soal": "The workflow clusters data items in the iris dataset by first checking the ...........(1)............ between data instances using the Distances widget. The Distance ........(2)............ is passed to the Hierarchical Clustering widget, which produces a dendrogram. We can select different parts of the dendrogram to further analyze the related data.\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 Distances, (2) \u2192 Matrix"
  },
  "a082451b6a225371834524eaccbf5b17fa327a9bde3ba62c313c4594d2e5cc7d": {
    "soal": "The Confusion Matrix widget shows the proportion between predicted and network class.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "02260476a00789f6965b354f76b06c682fe688996304da36d437e08ee10afb83": {
    "soal": "Principal Component .................. (PCA) transforms data into a dataset with uncorrelated variables, also called principal components. The PCA widget displays a graph (scree diagram) that shows the amount of variance explained by the best principal components and allows you to interactively adjust the number of components to include in the output dataset. In this workflow, we can observe the transformation in the Data Table and in the Scatter Plot.\n\n\n\n",
    "jawaban": "The correct answer is: Analysis"
  },
  "a9cf0c1df5daf1a6ae771e4a13ec03c1df8d3fdc8332d090afd146b12c5cafb8": {
    "soal": "In the workflow below, the .................... widget can load subfolders. We store President Kennedy's speeches in two (2) subfolders, pre-1962 and post-1962. If we load the parent folder, both subfolders will be used as class labels. Check the output of the Import Documents widget in the Data Table widget.\n\n\n\n",
    "jawaban": "The correct answer is: Import Documents"
  },
  "fe79bfcc24fecbd46bf20d51063a09b412eab4224239fc03c49d16ba9428ad5f": {
    "soal": "In the Preprocess Text Widget, we can do several things, such as\n\nConvert all letters to lowercase.\nRemove .................., words that are less useful like conjunctions such as and, in, to, from, etc.\nSet preprocessing for stopwords in Indonesian.\nRemove HTML tags\nRemove URLs\n\n",
    "jawaban": "The correct answer is: stop word"
  },
  "73f3530c91c1a0553307ce327af6ee02ce4b302266ad010fc1395df9b0b340e5": {
    "soal": "The core of Apache \nAnswer Question 90\n consists of a storage part (Hadoop Distributed File System (HDFS)) and a processing part (MapReduce). Hadoop divides files into large blocks and distributes them among nodes in the cluster. To process data, Hadoop MapReduce transfers code to nodes for parallel processing, based on the data that needs to be processed at each node. This approach takes advantage of data locality\u2014nodes manipulate the data they hold\u2014to allow faster and more efficient data processing compared to more conventional supercomputer architectures relying on parallel file systems where computation and data are connected via high-speed networks.",
    "jawaban": "The correct answer is: Hadoop"
  },
  "ebc119430e6db0d1799199fff745958ace1ae2aabe52592f1d0bcb71d92c4f74": {
    "soal": "In the context of dataset splitting, what percentage of the total data is typically allocated to the Training Set?",
    "jawaban": "The correct answer is: 70-80%"
  },
  "5596954b45a67dcb751d8c28eb68c6c2f4c41ff5bfd8252b881496cb347ac8e9": {
    "soal": "The ...................... widget can visualize distances between items.\n",
    "jawaban": "The correct answer is: Distance Map"
  },
  "edea45f581b93704cf9839560ba28cde3270a0148a253dac92bd3e56d22721a7": {
    "soal": "What does LLM stand for?",
    "jawaban": "The correct answer is: Large Language Model"
  },
  "5c01bcb679b70e6ca427c30a02344993f42a82885e819733b025df02a55510ee": {
    "soal": "import os\nimport requests\nfrom googlesearch import search\nfrom bs4 import BeautifulSoup\nimport re\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef clean_filename(text):\n# Hilangkan karakter ilegal untuk nama file\nreturn re.sub(r'[\\/*?:\"<>|]', , text).strip().replace(' ', '_')[:50]<%1\u001f%\u001f0%> def get_page_content(url):\ntry:\nheaders =\nresponse = requests.get(url, headers=headers, timeout=10)\nsoup = BeautifulSoup(response.content, 'html.parser')\ntitle = soup.title.string if soup.title else 'No Title'\nparagraphs = soup.find_all('p')\ncontent = '\n\n'.join([p.get_text() for p in paragraphs[:5]])\nreturn title.strip(), content.strip()\nexcept Exception as e:\nreturn 'Error', f\"Gagal mengambil konten: {e}\"\ndef save_to_txt(keyword, rank, title, url, content, folder='outputs'):\nos.makedirs(folder, exist_ok=True)\nfilename = f\"{clean_filename(keyword)}_{rank}_{clean_filename(title)}.txt\"\nfilepath = os.path.join(folder, filename)\nwith open(filepath, 'w', encoding='utf-8') as f:\nf.write(f\"Keyword : {keyword}\n\")\nf.write(f\"Peringkat : {rank}\n\")\nf.write(f\"Judul : {title}\n\")\nf.write(f\"URL : {url}\n\n\")\nf.write(content)\ndef scrape_and_save_txt(keywords, num_results=5):\nfor keyword in keywords:\nprint(f\"\n\ud83d\udd0d Searching: {keyword}\")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nprint(f\" \u2192 ({i+1}) Fetching: {url}\")\ntitle, content = get_page_content(url)\nsave_to_txt(keyword, i+1, title, url, content)\ntime.sleep(2)\nexcept Exception as e:\nprint(f\"\u274c Error saat mencari '{keyword}': {e}\")\nprint(\"\n\u2705 Semua konten telah disimpan di folder 'outputs/'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\nscrape_and_save_txt(keywords, num_results=5)\nWhy does the script include 'time.sleep(2)' after processing each URL?\n{\n=To prevent being flagged as a bot by introducing a delay between requests.\n~To allow the server time to process the previous request.\n~To synchronize with the rate limits of the Google API.\n~To ensure that the extracted content is saved properly.\n~To give the system time to free up resources before the next request.\n~To reduce the load on the local machine's CPU.\n}",
    "jawaban": "The correct answer is: \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\""
  },
  "863e3123168ec642f9fecfa7ffc462bb9391713225ff36e24b235c23708afc94": {
    "soal": "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target biner\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(0, 2, 100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi data latih dan data uji (80% latih, 20% uji)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi dan pelatihan model klasifikasi (misalnya RandomForest)\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n# Prediksi pada data uji\ny_pred = model.predict(X_test)\n# Menghitung Precision, Recall, dan F1-Score\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nprint(f'Precision: ')\nprint(f'Recall: {recall:.2f}')\nprint(f'F1-Score: {f1:.2f}')\n# Alternatif: Menggunakan classification_report untuk ringkasan metrik\nprint('\nClassification Report:')\nprint(classification_report(y_test, y_pred))\nWhat is the purpose of the `train_test_split` function in scikit-learn?\n{\n~To train the model on the dataset.\n~To test the model on new data.\n=To split the dataset into training and testing subsets.\n~To evaluate the model's performance.\n~To preprocess the data before training.\n~To combine multiple datasets into one.\n}",
    "jawaban": "The correct answer is: precision:.2f"
  },
  "f8003ff3f64c63b56db70db3e5000ae6e48c87066241d9751a3756784447fb3a": {
    "soal": "Widget Line Plot is a standard visualization widget, displaying the data profile, usually in the form of ordered numeric data. In this simple example, we will display the iris data in a line plot, grouped by .............(1).......... iris. The plot shows how petal length separates the class values well.\n\nIf we observe this in the ..............(2)........... Widget, we can confirm this is indeed true. Petal length is an interesting attribute for class separation, especially when enhanced with petal width, which is also well separated in the line plot.\n\n",
    "jawaban": "The correct answer is: (1) \u2192 attribute, (2) \u2192 Scatter Plot"
  },
  "418ade40da197e4dfe521d97188701e1f9954541534c05637dddb87630ae8318": {
    "soal": "Alpha Miner is known for:",
    "jawaban": "The correct answer is: Simplicity"
  },
  "e537ab85b11b2cedb1432d2276e82d838f7517fcad9510da6ad041659c4f5f9c": {
    "soal": "Data Mining is critical to supporting the success of modern data-driven organizations. An IDG survey of 70 IT and business leaders found that 92% of respondents want to apply advanced analytics more broadly across their organizations. The same survey found that the benefits of data mining are deep and wide.\nIn fact, respondents identified no fewer than 30 different ways in which data mining positively impacts their business. Here are the top 10:\n- Improving decision-making processes\n- Improving security risk posture\n- Improving Planning and Forecasting\n- Competitive advantage\n- Cost reduction/savings\n- Customer acquisition\n- New revenue streams\n- Acquisition/retention of new customers\n- Strengthening customer relationships\n- New product development",
    "jawaban": "The correct answer is 'True'."
  },
  "82616aea5fb4251bdfc1d1b920ab044f7cb064122611fe6ab204d763c5af598f": {
    "soal": "In the workflow below, we use two File widgets to read the Iris and Glass datasets (provided in the Orange distribution), and send them to the Data Table widget.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "7c00334d2702d4c82a91ec06ac1b49dca78bd16bd0e07675cf9f353b77971192": {
    "soal": "11111 (binary) + 1 (binary) = ........ (binary)",
    "jawaban": "The correct answer is: 100000"
  },
  "8f8be6a40ab2522e2829ecdc7c276e11b57303028209e8184d83876dd69c4d47": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#_xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nmap = pm4py.discover_heuristics_net(log)\npm4py.view_heuristics_net(map)\nWhich filename has an underscore at the start of the comment line?",
    "jawaban": "The correct answer is: _xes_filename"
  },
  "558634400d5180cdcc991b9107bfc3e0e0cd5b0fe7c36e479d74d304f6e955d3": {
    "soal": "The Calibrated Learner widget generates a model that calibrates the distribution of class probabilities and optimizes the .......... threshold. This widget only works for binary classification tasks.\n",
    "jawaban": "The correct answer is: decision"
  },
  "bbb90bba9daa86860ee38e197bed2eb2986c40f48dcf4e181f4cab830f2b794a": {
    "soal": "The ....................... widget plots class probability against the probability predicted by the classifier.\n",
    "jawaban": "The correct answer is: Calibration Plot"
  },
  "3bb32188c6ebaf4a0797cab3b8310c3001e8704139382a8edb40af0a01aad61c": {
    "soal": "from google.colab import drive\ndrive.mount('/content/drive')\nWhich command is used to import the drive module from google.colab?",
    "jawaban": "The correct answer is: from google.colab import drive"
  },
  "a18e29f76d1a8c644c9618597f52eb6ac4c5dbdb9b0cc7a15a559ad5d5cfa31f": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nWhat is the input shape specified for the Flatten layer in the given model?",
    "jawaban": "The correct answer is: (28, 28)"
  },
  "c62d90a8ccc1ab1f4ad2c66b4165b446642a772dcb2161d4c10ab092983030b0": {
    "soal": "In the snapshot below, we can see how ............... affects the distance matrix. We load the Iris dataset and calculate the distance between rows with the help of the Distances widget. To show how Distance Transformation affects the Distance Matrix, we create the workflow below and compare the transformed distance matrix with the \"original\".\n\n\n",
    "jawaban": "The correct answer is: transformation"
  },
  "6ee747ee3cfdfb7471a8f3b9d86871c5e876edbde9235febc945e4846c262bb9": {
    "soal": "If you have multiple categorical features to use for stratification, what should you do?",
    "jawaban": "The correct answer is: Combine the categorical features into a single feature before applying stratification."
  },
  "a0300abcf2c73ead905470bac0adcb2066c2d4953bf9b09c3f3ecb8d4a8919af": {
    "soal": "In the Import Documents Widget, to retrieve data, select (click) the folder icon on the right side of the widget. Select (choose) the folder that we want to make corpus. After loading is complete, we will see how many documents were successfully retrieved by the Import Documents Widget. To observe ................... the obtained corpus, connect the Import Documents widget to the Corpus Viewer Widget. In this workflow, we are using a set of speeches by President Kennedy in plain text format.\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: corpus"
  },
  "cfff0b23366aed3fbddd0573ba145d9bc220e78edabab4010b8da0aa85b5356d": {
    "soal": "What is the primary purpose of a confusion matrix in model evaluation?",
    "jawaban": "The correct answer is: To display the number of true and false predictions for each class."
  },
  "a91649591b85ecf5936ebef5fb4b15eb7aee8406374cd65bc87aa6fda366c946": {
    "soal": "What does the 'Volume' aspect of Big Data refer to?",
    "jawaban": "The correct answer is: The amount of data generated and stored"
  },
  "e9c9680dff8671b3a7ab341161c58a2f98d8fbffa94a97b7875801a2b3c9306f": {
    "soal": "The Import Documents widget can export text documents from a folder to become a corpus.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "5515f69ad5e07ea2b03d42c3cdbbd43c3de9ecd6fdca66de2abde28c17e7f9d4": {
    "soal": "Widget Data Info displays information from the selected dataset.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "1df1c19767c193e242a7f953dfa99ff9d7a47baf8489b93c17397646ec7c034c": {
    "soal": "Data Mining has the power to transform businesses; however, implementing processes that meet the needs of all stakeholders often hinders successful data mining investments \u2014 78% of respondents say they do not struggle to find the right data mining strategies or solutions.",
    "jawaban": "The correct answer is 'False'."
  },
  "1514cf52e59086612ab691c45d80cbe30b63394cde6665baaa17e363053c3695": {
    "soal": "In the workflow below, we collect data using the Twitter widget. We collect tweets from users @HillaryClinton and @realDonaldTrump over two weeks, totaling 242 tweets.\n\n\n\n\n\n\n\nIn the Preprocess Text widget, tweet tokenization is available, which stores hashtags, emojis, mentions, etc. However, this tokenizer will remove punctuation, so we expand the Regexp filtering with symbols we want to remove. We will obtain tokens with only words, which we display in the Word Cloud. Then we can create a scheme to predict the tweet author based on tweet content, as described in more detail in the documentation for the Twitter widget.\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "76a79e271591f91013b770bcb4e168d1aa5f73fc77bd2a9a569a5cacbb19c298": {
    "soal": "Handling ................... data in the Distances Widget can only be used with Euclidean, Manhattan, and Cosine metrics.\n",
    "jawaban": "The correct answer is: Sparse"
  },
  "590c2608390f7d0d4915114697518146e0bf0f9c0b37130b950676f2509dfb52": {
    "soal": "In Boolean Algebra\nIf\n1 AND 1 = 1\n1 AND 0 = 0\n0 AND 1 = 0\n0 AND 0 = 0\n\nThen\n11 AND 10 = 11 (binary)",
    "jawaban": "The correct answer is 'False'."
  },
  "79d798c515eeb8980adf39837f53877537816da0cc6e621ad2094a6e3e6e2b04": {
    "soal": "What is the primary purpose of installing Ollama, Open-WebUI, and PostgreSQL using Docker Compose?",
    "jawaban": "The correct answer is: To set up a vector database environment"
  },
  "1a9b854c5b790d684cb161923888debfce9dc0a8b77830c8ca8a035218db4227": {
    "soal": "The ......................... widget can create a bag of words from the input corpus.\n",
    "jawaban": "The correct answer is: Bag of Words"
  },
  "d7cac0ac4ff2f415f5ec090ab7fe48e3a8d3fd180668f03ca95a05c800f1cafb": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.rand(100) # 100 nilai target\n# Inisialisasi model regresi linier\nmodel = LinearRegression()\n# Tentukan jumlah fold (misalnya, 5-fold cross-validation)\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n# Evaluasi model menggunakan cross-validation\nscores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n# Menampilkan skor R^2 untuk setiap fold dan rata-ratanya\nprint(\"Skor R^2 untuk setiap fold:\", scores)\nprint(\"Rata-rata skor R^2:\", np.mean(scores))\nWhich module from `sklearn` provides the `KFold` and `cross_val_score` functions?",
    "jawaban": "The correct answer is: sklearn.model_selection"
  },
  "ea57f3af3955bbda59a8bbefbe05c00448897d3e5c7adf2ceee61b3db8bb4c6c": {
    "soal": "According to the official Zoho website, Zoho Analytics is a comprehensive, reliable, and scalable e-commerce platform. Developers and system integrators (SI) can use this platform to develop and deploy custom analytic applications and integrations.\nAnother advantage of Zoho Analytics is that it is user-friendly, making it easy for users to upload and control data. Using Zoho Analytics, data practitioners can create multifaceted and custom dashboards. The platform is easy to use and implement.",
    "jawaban": "The correct answer is 'False'."
  },
  "156bf71218c745f366fa42f4e2dd357334a733e82a870fb8429c27ecc3280722": {
    "soal": "In the workflow below, the Tree widget will train a model and evaluate it against Logistic Regression. The accuracy comparison of the Tree widget and the Logistic Regression widget is done through the Test & Score widget. The results of the Test & Score widget can be viewed through the Confusion Matrix widget to compare the classification errors that occurred.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "e4e7826d8e8183ee75ce639e98c6218aabf620ce659fd7c9cd30918601a48624": {
    "soal": "Currently, the only widget that provides the correct signal required by the ............ widget is the Test & Score widget. Therefore, the ................. widget always follows the Test & Score widget, and since it has no output, no other widgets follow it.\n",
    "jawaban": "The correct answer is: Calibration Plot"
  },
  "103a587e98792805adaa9dc34111cee56722603338c462144850c435373171c0": {
    "soal": "The following ticker code used in the Yahoo Finance Stock Data Widget (ORANGE)\n\nAMZN\n\n\nBelongs to the company,\n\n\n",
    "jawaban": "The correct answer is: Amazon.com"
  },
  "6b076af5f964c9e0704b8e5255c46324f0d2a4bc3c333dd9c52ab25be41b64f3": {
    "soal": "The Import Documents widget takes text files from a folder and creates a corpus. The Import Documents widget can read .txt, .docx, .odt,....................... and .xml. If there are subfolders in the folder, they can be used to label the class.\n",
    "jawaban": "The correct answer is: pdf"
  },
  "4b6b2fbba13955c0c466a41c5c41ad708c90f2991c1eef9b37d4744a5a8fa424": {
    "soal": "The ROC Analysis widget receives a dataset and one or more predictors (predictive models, not learner algorithms). The ROC Analysis widget generates data and predictions.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "657b805075aeda5565c280e05d688278e085f113eb474786aad54ec32f399b57": {
    "soal": "In the following workflow example, we will show how to quickly visualize a corpus with the Word Cloud widget. We can connect the Word Cloud widget directly to the Corpus widget, but instead, we decide to apply some preprocessing with the Text Preprocess widget. We are working with the dataset book-excerpts.tab. We can convert all text to Arabic, tokenize (split) the text into just words, filter out English stopwords, and select the 100 most frequent tokens.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "729d2da43997c27275c3c14f5dc2ab3525472f4b11fa3892f4a103d04b71ae39": {
    "soal": "Column attribute names can be preceded by a label followed by a hash. Use c for class and m for meta attributes, i for ignoring columns, w for weight (of a column), and C , D , T , ........ for continuous, discrete, time, and string attribute types. Example: C#mph, mS#name, i#dummy.\n\nWhere ........ is,\n",
    "jawaban": "The correct answer is: S"
  },
  "98be6c4ad7aae30ad4fa2814f8ee033cd53e10c80d281e0f0c2878f42e8000d4": {
    "soal": "In the Text Preprocessing Widget, Information on preprocessed data, document count calculates and reports the number of words in the input documents. The total token count calculates all tokens in the corpus. Unique tokens exclude duplicate tokens and report only unique tokens in the corpus.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "6b0a4203a48fc4035df1aff28455175ad176d7b688dbf357620b22cb0172a4d0": {
    "soal": "Data attributes in Orange have types discrete, continuous or character string. The attribute type is marked by a symbol that appears before the attribute name (D, C, ...............................).\n\n\n\nAnswer: \nQuestion 31",
    "jawaban": "The correct answer is: S"
  },
  "ffa938e6247f30dad58538432bcf5ded429da787e06fd374a3f55898197dd5de": {
    "soal": "The Venn Diagram widget can plot a Venn diagram for two or more data subsets.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "390e1677998b2dddcb672097cee856a9b1f6b252e79be3a98d110129183f53e6": {
    "soal": "1111 (binary) + 1 =",
    "jawaban": "The correct answer is: 10000"
  },
  "f5123bb2f7dc421414b2ea871d5c79ffc740e703110a905d100186e4a1f1ab09": {
    "soal": "In the Import Documents Widget, to retrieve data, select (click) the folder icon on the right side of the widget. Select (choose) the folder that we want to make corpus. After loading is complete, we will see how many documents were successfully retrieved by the Import Documents Widget. To observe the obtained corpus, connect the ..................... widget to the Corpus Viewer Widget. In this workflow, we are using a set of speeches by President Kennedy in plain text format.\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Import Documents"
  },
  "ddbde35afbc096cfba716983bae3dca8242557e7fcd243a50afe462e16994964": {
    "soal": "from googlesearch import search\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef get_page_content(url):\ntry:\nheaders =\nresponse = requests.get(url, headers=headers, timeout=10)\nsoup = BeautifulSoup(response.content, 'html.parser')\n# Ambil judul halaman\ntitle = soup.title.string if soup.title else 'No Title'\n# Ambil konten paragraf utama\nparagraphs = soup.find_all('p')\ntext_content = ' '.join([p.get_text() for p in paragraphs[:5]]) # Batasi 5 paragraf pertama\nreturn title.strip(), text_content.strip()\nexcept Exception as e:\nreturn 'Error', f\"Failed to fetch content: {e}\"\ndef google_scrape_with_content(keywords, num_results=5, output_file='scraped_results.csv'):\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'Rank', 'Title', 'URL', 'Content'])\nfor keyword in keywords:\nprint(f\"\n\ud83d\udd0d Searching for: {keyword}\")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nprint(f\" \u2192 Fetching: {url}\")\ntitle, content = get_page_content(url)\nwriter.writerow([keyword, i+1, title, url, content])\ntime.sleep(2) # Delay biar aman\nexcept Exception as e:\nprint(f\"\u274c Error while searching '{keyword}': {e}\")\nprint(f\"\n\u2705 All results + content saved to '{output_file}'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\ngoogle_scrape_with_content(keywords, num_results=5)\nWhat is the purpose of the `load_keywords` function in the script?\n{\n=To read search keywords from a file and return them as a list.\n~To perform Google searches for each keyword.\n~To write search results to a CSV file.\n~To clean and preprocess the keywords.\n~To validate the format of the keywords.\n~To remove duplicate keywords from the list.\n}",
    "jawaban": "The correct answer is: \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\""
  },
  "1465224d338ed19351035e6fae8c0b1b59ed9952cc8fe6a1e49938bb31dc84cb": {
    "soal": "import os\nimport string\nfrom collections import Counter\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\n# Buat daftar stopwords Bahasa Indonesia\nstop_words = set(stopwords.words('indonesian'))\ndef bersihkan_teks(teks):\n# Lowercase + hilangkan tanda baca\nteks = teks.lower()\nteks = teks.translate(str.maketrans('', '', string.punctuation))\nreturn teks\ndef analisis_folder(folder='outputs', top_n=20):\nsemua_konten = ''\nfor filename in os.listdir(folder):\nif filename.endswith('.txt'):\nfilepath = os.path.join(folder, filename)\nwith open(filepath, 'r', encoding='utf-8') as f:\nisi = f.read()\n# Ambil hanya bagian isi, setelah header\nif '\n\n' in isi:\nisi = isi.split('\n\n', 1)[1]\nsemua_konten += isi + ' '\n# Bersihkan teks dan tokenisasi\nteks_bersih = bersihkan_teks(semua_konten)\nkata_kata = teks_bersih.split()\n# Hapus stopwords\nkata_kata = [kata for kata in kata_kata if kata not in stop_words and kata.isalpha()]\n# Hitung frekuensi\ncounter = Counter(kata_kata)\ntop_kata = counter.most_common(top_n)\nprint(f\"\n\ud83d\udd0d Top Kata Paling Sering Muncul:\")\nfor kata, jumlah in top_kata:\nprint(f\"{kata}: {jumlah}\")\n# Buat Word Cloud\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(counter)\n# Tampilkan\nplt.figure(figsize=(12, 6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud dari Semua Konten')\nplt.tight_layout()\nplt.savefig('wordcloud.png')\nplt.show()\nprint(\"\n\u2705 Word Cloud disimpan sebagai 'wordcloud.png'\")\n# Jalankan analisis\nif __name__ == '__main__':\nanalisis_folder('outputs', top_n=30)\nWhich command is used to download the stopwords necessary for the script?\n{\n=nltk.download('stopwords')\n~pip install stopwords\n~import stopwords from nltk\n~download('stopwords')\n~nltk.import('stopwords')\n~stopwords.download('nltk')\n}",
    "jawaban": "The correct answer is: top_n"
  },
  "8db745476ad17fc6848755bb32b68194e33d861916f50d3151dfc2bcc6280cf3": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.reuters.load_data(num_words=10000)\nInto how many different topics are the news articles in the Reuters Newswire dataset categorized?",
    "jawaban": "The correct answer is: 46"
  },
  "5b5098e9c407739a0214a159ce903a66ea446f19639bfc673f4f8e2e32338c00": {
    "soal": "The basic framework of Apache Hadoop consists of the following modules:\n- Hadoop Common - contains libraries and utilities required by other Hadoop modules;\n- Hadoop Distributed File System (HDFS) - a distributed file system that stores data on commodity machines, providing very high aggregate bandwidth in clusters;\n- Hadoop YARN - a resource management platform responsible for managing computing resources in the cluster and using them for user application scheduling; and\n- Hadoop MapReduce - a programming model for large-scale data processing.\nThe term \"Hadoop\" has come to refer not only to the basic modules above but also to \"\nAnswer Question 38\n\", or a collection of additional software packages that can be installed on top of or alongside Hadoop, such as Apache Pig, Apache Hive, Apache HBase, Apache Phoenix, Apache Spark, Apache Zookeeper, Impala, Apache Flume, Apache Sqoop, Apache Oozie, Apache Storm, and others.",
    "jawaban": "The correct answer is: ecosystem"
  },
  "7444cd5e120d4b9ebed5011ecbd51e69b806ff02c39583b0c4977551618584fa": {
    "soal": "What is the core ability of LLMs?",
    "jawaban": "The correct answer is: Understanding and generating language"
  },
  "2b7e29647ba2c78c2095a5641d81a4630fc83ea41b2f49a469beedd04117be7a": {
    "soal": "Currently, the only widget that provides the correct signal needed by the Calibration Plot widget is the Test & Score widget. Therefore, the Calibration Plot widget always follows the Test & Score widget, and since it has no output, many other widgets follow it.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "59cf8e43e716b4ae394fb4190e9c09e3ed37bd1007219a7ce190e3a836cf3104": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.reuters.load_data(num_words=10000)\nWhat is the total number of news articles in the Reuters Newswire dataset?",
    "jawaban": "The correct answer is: 11,228"
  },
  "e1f957c4bd3ba7c04eb9ebf5f07cafa51f6eae8868d6949373c498cf62379917": {
    "soal": "\n\n\nIn the process of building a machine learning model, initial data can be explored using PCA or SOM techniques.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "d8e2c575c9b20f6f6d0e545f29b31d6cb800f7a03268d587d62ce3d39af5d9b6": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.reuters.load_data(num_words=10000)\nWhat is the default number of most frequent words considered when loading the Reuters Newswire dataset with TensorFlow's load_data function?",
    "jawaban": "The correct answer is: 10,000"
  },
  "f8b0642423dea85024c91aef3b47becb54582b6d410c6fd53140f01463dda3d2": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=10000)\nIn the IMDB dataset, what does each integer in a review sequence represent?",
    "jawaban": "The correct answer is: A specific word in the dataset's vocabulary"
  },
  "3cd8f0f889f6fce232b1e3a9984027d599d61cb8b517fba93d42c93b5a865d06": {
    "soal": "In the Constant Widget, the Learner will produce a model that always predicts the majority for .................. or the mean value for regression.\n",
    "jawaban": "The correct answer is: classification"
  },
  "783ad76e828c2b88c0f4b99241f62cb50f66c356538dac275411e297a1aa26a9": {
    "soal": "\n\n\n\n\n\nIn the confusion matrix in the above Workflow, we can see the classification errors that occurred.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "c46c9532f16b18039ced27bb2ed32a1b37177fa5ab46486ef9d1de903b532215": {
    "soal": "\n\n\n\n\n\nWorkFlow berikut dapat digunakan untuk melakukan feature ranking dari data yang ada terhadap masing-masing kategori yang di inginkan.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "fc68a1a44e049a2c78fcfff1b6706aa35b6d437a7a516e7d97abfafa96b79e78": {
    "soal": "What allows LLMs to process relationships between words?",
    "jawaban": "The correct answer is: Self-attention mechanism"
  },
  "ffe97c26407dab708e15cb2ed41bca06dd36214b3ad04b097d6b7de5b2a113d0": {
    "soal": "Principal Component Analysis (PCA) transforms data into a dataset with uncorrelated variables, also called principal components. The PCA widget displays a graph (scree plot) showing the degree of variance explained by the best principal components and allows for interactive adjustment of the number of components to include in the output dataset. In this workflow, we can observe the transformation in the Matrix Table and the Scatter Plot.\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "a91a479e0708009bdda9bd2b2d6292e2fa35bc21fd737c428417ec18c17d5dc8": {
    "soal": "The following workflow is an example where we can compare three (3) classifiers (i.e., Naive Bayes, Tree, and Constant) and input them into the Predictions widget. We are using the Titanic dataset. The Test & Score widget will display the evaluation results for each classifier. Then, we can plot through the Calibration Plot widget and the ROC Analysis widget from the output of the Test & Score widget for further analysis of the performance of each classifier. The ROC Analysis widget allows us to see the accuracy predictions of the class probability in the form of a plot/image.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "8f8c55bea25e64a498665db02e64d8243578792085538dc6ef7bdd7c8c231624": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\n# xes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\nxes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nprocess_tree = pm4py.discover_bpmn_inductive(log)\nbpmn_model = pm4py.convert_to_bpmn(process_tree)\npm4py.view_bpmn(bpmn_model)\nHow many log file options are commented out in the script?",
    "jawaban": "The correct answer is: 10"
  },
  "05c106a0f1d9367f64589f4220c03f16f4017a0ba1eb78748a349ae536588d73": {
    "soal": "The Constant Widget will predict the most frequent class or mean value from a training set.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "65da56da44c189916f45f49e3d59ef4d0fa018315fc3add3da04611a8fc0d339": {
    "soal": "In the Periodogram widget, the periodogram for non-equispaced series is calculated using the Lomb-Newton method.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "ba3d987d0718a37e9148edb66b9a9d108fa88ff0a9f6c2856dc856e849fc4333": {
    "soal": "The Data Info widget is a simple widget that presents information about ................ dataset, features, target, meta attributes, and location.\n\n\n",
    "jawaban": "The correct answer is: size"
  },
  "415a6bbc3f1c7af182a779b17d5cc243037b2e01ccb804387755527e37976f0e": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(2, size=100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi training (80%) dan testing (20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model, misalnya RandomForestClassifier\nmodel = RandomForestClassifier(random_state=42)\n# Melatih model pada data training\nmodel.fit(X_train, y_train)\n# Memprediksi data testing\ny_pred = model.predict(X_test)\n# Mengevaluasi akurasi model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model pada data testing: ')\nWhat is the purpose of the `model.fit(X_train, y_train)` statement in the code?\n{\n~To evaluate the model's performance on the training data.\n~To split the data into training and testing sets.\n=To train the model using the training data.\n~To predict the target values for the test data.\n~To initialize the machine learning model.\n~To calculate the accuracy of the model.\n}",
    "jawaban": "The correct answer is: accuracy:.2f"
  },
  "6a238c329e7e5659a7efecb43964651f8445a2378b28ad8c2a7c2cd3c3f5d709": {
    "soal": "In the ORANGE Constant Widget - for classification, when predicting class values with Prediction, this widget will generate the min frequency of the classes present in the training set. If there are two or more majority classes, the classifier will randomly choose from the predicted class, but it will always produce the same class for that instance.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "03da3dd9af75227ed02a425aef8863c856e79310c06abe10cbf80e47a20859ba": {
    "soal": "In the Correspondence Analysis Widget - Correspondence Analysis (CA) will compute the CA linear transformation of the input data. Although similar to PCA, CA computes the linear transformation on discrete data, not continuous data.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "84cc71b44d4ecf4092249d7b2342a590b2858f9e11ff6c554fe19aa7348cd4e5": {
    "soal": "Which of the following is a web crawling and scraping framework in Python?",
    "jawaban": "The correct answer is: Scrapy"
  },
  "c04222ecbfe72436c3f5af0cb7496570b474570aadacb2b7a2a9fdf61258545c": {
    "soal": "In the Periodogram widget, the periodogram for non-equispaced series is calculated using the Lomb-Scargle method.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "71b94ada401c25422a698d51df192700aaf6d87c24eb278e42d1b5ffd14cd123": {
    "soal": "1110 (binary) + 0 (binary) =",
    "jawaban": "The correct answer is: 1110"
  },
  "ad7fec3fb67444840737fed1d31287d9ab234e2bfde1c49824cc930e8cdd3a29": {
    "soal": "Which function is used to randomly shuffle the elements of a tensor?",
    "jawaban": "The correct answer is: random_shuffle()"
  },
  "2e0bbe6f19515bd10e24b0631f4a29b5c9db596d40f2746029a8fceab30eae48": {
    "soal": "In the workflow below, we use the heart disease data and select the subset only for women from ......(1).......... Then, we visualize the distances between the columns in ......(2)........... Since the subset also contains some discrete data, the ......(3)....... widget warns us that it will ignore discrete features, so we will only see continuous instances/attributes in the map.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: (2) \u2192 Distance Map, (1) \u2192 Scatter Plot, (3) \u2192 Distances"
  },
  "8bce371f40c043f6d02fd08732550832fef196eda3de3e78ebeccd0e6402cebf": {
    "soal": "Network Clustering Widget can help us reveal clusters and highly connected groups in a network. First, we will use the Network File Widget to load the lastfm.net dataset. Then, we will send the network to the Network Clustering Widget. The Network Clustering Widget finds 79 nodes in the network. To visualize the results, we use the Network Explorer Widget and set the Color attribute to Cluster. This will color the network nodes with the corresponding cluster color - this is a good way to visualize highly connected groups in a dense network.\n\nNote that the Network Explorer Widget here will color the 10 largest clusters and color the rest as 'Other'.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "1fb99a9420e4bdf037df60bda4a4d19350d160aeeadfcff6e70249276be2bc0c": {
    "soal": "result 8 (decimal) AND 1 (decimal) is",
    "jawaban": "The correct answer is: 0"
  },
  "c6575c699adcec701dc540495706f3b27163b57ca9a74ea7b4ce45b7cb26b9b3": {
    "soal": "The following workflow demonstrates the use of the Distances widget. We use the iris.tab data from the File widget. We calculate the distance between data instances (rows) and pass the result to Hierarchical Clustering. This workflow is useful for finding ................. in data instances.\n\n\n\n",
    "jawaban": "The correct answer is: group"
  },
  "489e2d8708faf44562c8f5fbde11976fc70e353a57d46db30dd86e78a772c700": {
    "soal": "PCA can be used to simplify the visualization of large datasets. Below, we use the Iris dataset to show how we can improve the visualization of the dataset with PCA. The unchanged data in the Scatter Plot shows much clearer differences between classes than the default settings.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "8acdbfdb9c5ddee03e66d2d52ca1ffc560ddde62ba60996292992930e86362dc": {
    "soal": "from google.colab import drive\ndrive.mount('/content/drive')\nCan you mount multiple Google Drive accounts simultaneously in a single Colab session?",
    "jawaban": "The correct answer is: No, but you can mount shared drives from different accounts."
  },
  "d825eb9e1af94eff7263d5f2ce15fbb486d0266fdb375a676d6aa74f2f2590b2": {
    "soal": "In the workflow below, the Tree widget will ............ a model and evaluate it against Logistic Regression. The accuracy comparison between the Tree widget and Logistic Regression widget is done through the Test & Score widget. The results from the Test & Score widget can be viewed through the Confusion Matrix widget to compare the classification errors that occurred.\n\n\n\n\n",
    "jawaban": "The correct answer is: train"
  },
  "c184623a7324509afee69774ef2dbb64c899e92a9d93a096175690e246cf8a27": {
    "soal": "Which of the following best describes 'conformance checking'?",
    "jawaban": "The correct answer is: Assessing how well event logs align with an existing process model"
  },
  "b0fac9cd36caf645caf3f312829eb0710ca8c1a4754fee8709701f70a3958451": {
    "soal": "The following workflow demonstrates the use of the Confusion Matrix widget\n\n\nTest & Score obtains data from File and two learning algorithms from Naive Bayes and Tree. Test & Score performs cross-validation or other train-and-test procedures to obtain class predictions by both algorithms for all (or some) data instances. The test results are sent to .............. Matrix, where we can observe how many instances were misclassified and why.\n\nIn the output, we use the Data Table to show the examples we selected in the confusion matrix. If we, for example, click on Misclassified, the table will contain all the instances that were misclassified by the selected method.\n\nScatter Plot receives two sets of data. From the File widget, it gets the complete data, while the confusion matrix only sends selected data, for instance, misclassified data. The Scatter Plot will display all data, with bold symbols representing the selected data.\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Confusion"
  },
  "eb7910d54995c26558f697d38b12bd220c9cc6fb1a4a4f4d933a74f1f524f497": {
    "soal": "from googlesearch import search\nimport csv\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef google_scrape_bulk(keywords, num_results=10, output_file='results.csv'):\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'Rank', 'URL'])\nfor keyword in keywords:\nprint(f\"\ud83d\udd0d Searching for: \")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nwriter.writerow([keyword, i+1, url])\ntime.sleep(1) # Delay agar tidak dianggap bot\nexcept Exception as e:\nprint(f\"\u274c Error while searching '{keyword}': {e}\")\nprint(f\"\n\u2705 All results saved to '{output_file}'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\ngoogle_scrape_bulk(keywords, num_results=10)\nHow is the output CSV file opened in the script?\n{\n=In write mode with UTF-8 encoding.\n~In append mode with ASCII encoding.\n~In read mode with UTF-16 encoding.\n~In write mode with default system encoding.\n~In binary mode with ISO-8859-1 encoding.\n~In exclusive creation mode with UTF-8 encoding.\n}",
    "jawaban": "The correct answer is: keyword"
  },
  "0e9ec716856753ab19b1e5789d5660e6c1d4ed9d20b00710672b26e1aa669eb1": {
    "soal": "The Calibrated Learner widget generates a model that calibrates the distribution of class probabilities and optimizes the decision threshold. This widget only works for logistics tasks.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "f16daffeaa13550adf362f2c9b4e6cf48b9fc1fc636f419d7ab951b798f18bc3": {
    "soal": "In the snapshot workflow ORANGE below, we will observe the effects of preprocessing on text. For example, we are working with book-excerpts.tab, which we load with the Corpus widget. We have connected Preprocess Text to the Corpus and maintained the default preprocessing method (lowercase, word tokenization, and stopword removal). The only additional parameter we added as output is the first 100 most frequent tokens. Then we connect Preprocess Text with Word Cloud to observe the most frequent words in the text.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "97526b7ef5c143217d22699f66d5023c3b77525388cc4f0724bc6bd0a6905aa5": {
    "soal": "from googlesearch import search\nimport csv\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef google_scrape_bulk(keywords, num_results=10, output_file='results.csv'):\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'Rank', 'URL'])\nfor keyword in keywords:\nprint(f\"\ud83d\udd0d Searching for: \")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nwriter.writerow([keyword, i+1, url])\ntime.sleep(1) # Delay agar tidak dianggap bot\nexcept Exception as e:\nprint(f\"\u274c Error while searching '{keyword}': {e}\")\nprint(f\"\n\u2705 All results saved to '{output_file}'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\ngoogle_scrape_bulk(keywords, num_results=10)\nHow does the `load_keywords` function handle empty lines in the `keywords.txt` file?\n{\n=It ignores them and does not include them in the list of keywords.\n~It raises an error and stops execution.\n~It includes them as empty strings in the list of keywords.\n~It replaces them with a default keyword.\n~It logs a warning message but continues execution.\n~It prompts the user to enter a replacement keyword.\n}",
    "jawaban": "The correct answer is: keyword"
  },
  "126be3f5fe8a023d699ba4dd356ef18cdc8365fa06a0e9b120ca94fe99d7767f": {
    "soal": "The ........................... widget transforms the distances present in the dataset.\n",
    "jawaban": "The correct answer is: Distance Transform"
  },
  "c85fa7649a87499021dd98fe2794a3cfb0efae6fc13b5526f993fbe44682391f": {
    "soal": "In the ....................... widget, typically, a color palette is used to visualize the entire range of distances that appear in the matrix. This can be adjusted by setting low and high thresholds. This way, we ignore the distance differences outside this range and visualize the interesting parts of the distribution.\n\n",
    "jawaban": "The correct answer is: Distance Map"
  },
  "a0b1783ecbb8946228e10203f0b4fcc15cd9a20b3ffd6a1d6d6c0b22965ee8d3": {
    "soal": "To cluster based on similarity, we can measure the distance between images. The numerical representation of the images obtained from the Image Embedding widget, we measure the distance using the Distances widget. Typically, cosine distance works best for images, but you can experiment yourself. We then send the ................................. to Hierarchical Clustering to visualize similar pairs in the dendrogram.\n\nThe result is that all the animals are correctly grouped. In the dendrogram, we do not see the images of the animals. We can use the Image Viewer widget to view the images.\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: distance matrix"
  },
  "a628219bdb4866013ae47182f26dca1925cb0da63b511e090bb3b0896187e91a": {
    "soal": "Handling sparse data in the Distances Widget can only be used with Euclidean, Manhattan, and Cosine metrics.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "73f58a37af3854b3fd9bebf477dadec60f852992b273d6e3929558a7ca4d34d6": {
    "soal": "Widget kNN mem-prediksi berdasarkan instance training terbesar.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "aa5549eff58a287380257915c596bd2dbc18398a76fdabfcddbbb9d64e2f417f": {
    "soal": "Which service is suggested as a legal and more stable alternative to direct scraping of Google search results?",
    "jawaban": "The correct answer is: SerpApi"
  },
  "72a2d993e1506b37c7e53b2968af13998bbcb8263869c09d1cdbaee7ddcc709b": {
    "soal": "A heat map is a graphical method for visualizing attribute values based on class in a three-way matrix. Heat maps can only be used on datasets that contain continuous variables. The values are represented by colors: the higher the value, the darker the color. By combining class and attributes on the x and y axes, we see where which attribute values are strongest and weakest, allowing us to find typical features (discrete) or value ranges (continuous) for each class.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "8ec6a187dd38bc14ea52c79363ade97e5214e688c72c73750d6d70464bf84e07": {
    "soal": "\n\n\nIn general, machine learning techniques are divided into three (3), namely:\n\n............................ Learning.\nUnsupervised Learning.\nReinforced / Reinforcement Learning\n\n\n",
    "jawaban": "The correct answer is: supervised"
  },
  "807a0af881b1f0dece6e59af246747a0bdd607dd820f991ee12164381d47d9fb": {
    "soal": "Which command lists all Docker images on the system?",
    "jawaban": "The correct answer is: docker images"
  },
  "2e46f8038390a0ac0e5f4577f1fd7f7388aecabe59bd1818e3e653e7b0062663": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(2, size=100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi training (80%) dan testing (20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model, misalnya RandomForestClassifier\nmodel = RandomForestClassifier(random_state=42)\n# Melatih model pada data training\nmodel.fit(X_train, y_train)\n# Memprediksi data testing\ny_pred = model.predict(X_test)\n# Mengevaluasi akurasi model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model pada data testing: ')\nWhat is the effect of changing the `random_state` parameter in `train_test_split`?\n{\n~It changes the proportion of training and testing data.\n~It affects the performance of the RandomForestClassifier.\n=It alters the way data is split, leading to different training and testing subsets.\n~It normalizes the dataset before splitting.\n~It determines the number of features selected.\n~It has no effect on the data splitting process.\n}",
    "jawaban": "The correct answer is: accuracy:.2f"
  },
  "bcd7b186db73620f001b23c003e4ec992d42cb61fe1db49e7ac0d1227aad695c": {
    "soal": "Most visualizations in Orange are interactive. In the workflow below, the Scatter Plot widget, for example. Double-click the icon to open it, and click-and-drag to select multiple data points from the plot. The selected data will automatically enter the Data Transpose widget. Double-click to inspect which data was selected. Change the selection and observe the changes in the Data Table widget . This works well if both widgets are opened.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "f704c70b49e185824cf40334e3e31b52309649b9ff00bb1716362d54c890ba71": {
    "soal": "The Distances widget also works well with other Orange add-ons. The Distance Matrix widget can be fed into the Network from Distances widget (.....(1)..... add-on) to transform the matrix into ....(2)..... and into the Duplicate Detection widget (Text add-on) to find document duplicates in .....(3).......\n",
    "jawaban": "The correct answer is: (1) \u2192 Network, (3) \u2192 corpus, (2) \u2192 graph"
  },
  "57d8402a9dbbe5800cae601e964963dcc1b858f1b75737ac7d126ec1259d5c18": {
    "soal": "!pip install googlesearch-python nltk matplotlib wordcloud\nfrom googlesearch import search\nimport csv\ndef google_scrape(keyword, num_results=40,\noutput_file='results.csv'):\nprint(f\"Searching Google for: \")\n# Lakukan pencarian dan ubah hasil ke dalam list\nresults = list(search(keyword, num_results=num_results))\n# Simpan ke file CSV\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'URL'])\nfor url in results:\nwriter.writerow([keyword, url])\nprint(f\"Saved {len(results)} results to '{output_file}'\")\n# Contoh penggunaan\nkeyword = \"berita terbaru teknologi Indonesia\"\ngoogle_scrape(keyword, num_results=40)\nWhich Python module is used in the script to perform Google searches without an API?\n{\n=googlesearch from googlesearch-python\n~google-api-python-client\n~beautifulsoup4\n~requests\n~selenium\n~scrapy\n}",
    "jawaban": "The correct answer is: keyword"
  },
  "09ccb7e5e7a864f38193c4efda14183c8b0029d8916fa15ccd04c18ad5ef39d1": {
    "soal": "In the example workflow below, using the zoo dataset and creating a clustering workflow with Distances and ............................. Now define the threshold for selecting the cluster (click on the ruler above). Connect the Box Plot to ............................., check Order by relevance and select Cluster as a subgroup. This will sort the attributes based on how well they define the selected subgroup, in our case a cluster.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Hierarchical Clustering"
  },
  "e35753c880329f1e26cadb509ec535cee0827c196a0027f4c2ac7da18091d4f1": {
    "soal": "This workflow example shows how Network Analysis can enrich the workflow. We have used the lastfm.net data as input network from Network File and sent it to Network Analysis. We can decide to calculate the degree, degree centrality, and closeness centrality at node-level.\n\nWe can then visualize the network in Network Explorer. In the Network Explorer widget, we color it with the best tags, such as the default for this dataset. But now we can also set the graph size to match the degree centrality calculation results. This is a good way to visualize network properties.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "70d54c8d449c0b0947f248896f63fd39291f04a7339b717efc2b409e57a0268d": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nHow many images are there per class in the CIFAR-100 dataset?",
    "jawaban": "The correct answer is: 600"
  },
  "291902f38d84564a6e8b6bcc57c37e3bd18c02df65f07ce7c92cf05638c7db1e": {
    "soal": "In the workflow below, Iris data from the File widget is passed to the Select Columns widget, where we choose to display only two attributes (i.e., petal width and ..................). Then, we can view the original dataset and the dataset with the selected columns in the Data Table widget.\n\n\n",
    "jawaban": "The correct answer is: petal width"
  },
  "071d5b20c06f0ac2833568c7bbb84de26c7ade6bbb30b2ebe812a9966aecd750": {
    "soal": "The Corpus widget can load a collection of text documents, optionally tagged with categories, or convert input data into a corpus.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "208873163f36793de7b912d60298e6fc7fea692b5ad715cda45526c0f8e2c3c0": {
    "soal": "\n\n\nWorkflow di atas melakukan test / evaluasi CN2 Rule Induction dan Tree di Widget Test & Score. Terlihat CN2 Rule Induction lebih ......................\n\n\n\n",
    "jawaban": "The correct answer is: baik"
  },
  "221b8373e017d050f5345f4af881e3056db870c816c2625a93bba213f05254db": {
    "soal": "In the 'docker-compose.yaml' file, what is the name of the service for Ollama?",
    "jawaban": "The correct answer is: ollama"
  },
  "03662ec9b4ee2978e1a99f731f6ff21867b2fff37ccda4eadefac4bb85f77e6d": {
    "soal": "This workflow example shows how Network Analysis can enrich the workflow. We have used the lastfm.net data as the input network from the Widget Network File and sent it to the Widget Network Analysis. We can decide to compute the degree, degree centrality, and closeness centrality at ...................-level.\n\nWe can then visualize the network in the Widget Network Explorer. In the Network Explorer widget, it colors with the best tags, like the default for this dataset. But now we can also adjust the node size to fit the degree centrality results. This is a good way to visualize network properties.\n\n\n\n",
    "jawaban": "The correct answer is: node"
  },
  "6e451960c046b4ecde5628b98a42e74061351c3f93477b54ec200df24d11a0bd": {
    "soal": "In the workflow below, we use two File widgets to read the Iris and Glass datasets (provided in the Orange distribution), and send them to the widget ..............................\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Data Table"
  },
  "c592d232a46d3e43cd7721730e9bbd185175aae6e9dff0e43460e02ad0684cb8": {
    "soal": "Widget Constant ORANGE biasanya digunakan sebagai baseline untuk model learner lainnya.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "67657281ae3c4a3a8b614f3aebcb19f1409844291386cc0cce90381877203220": {
    "soal": "Which term was historically used synonymously with process discovery?",
    "jawaban": "The correct answer is: Automated Business Process Discovery (ABPD)"
  },
  "89a6848acfff84aa28ce4a40265741c4368725b4884a0d44886b8ecb4e8413db": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nWhich loss function is appropriate when the labels are provided as sparse integers?",
    "jawaban": "The correct answer is: SparseCategoricalCrossentropy"
  },
  "d1f62a36d531a0f6dddce586f2a01016a55b78f00452faabdede0ba1a2fdf1a3": {
    "soal": "Principal Component Analysis (PCA) computes the PCA ......... transformation of input data. It outputs a transformed dataset with the weight of individual instances or the weight of principal components.\n",
    "jawaban": "The correct answer is: linear"
  },
  "6249a88fdfc8b151cd217e419689930fb6a3a80494ee8098fc2cb10d2151203e": {
    "soal": "import pm4py\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\n# Import XES log\nlog = xes_importer.apply(xes_filename)\n# Cek jumlah kasus dan aktivitas\nprint(f\"Jumlah kasus: \")\nactivities = set()\nfor trace in log:\nfor event in trace:\nactivities.add(event['concept:name'])\nprint(f\"Jumlah aktivitas unik: {len(activities)}\")\nfrom pm4py.algo.discovery.dfg import algorithm as dfg_discovery\nfrom pm4py.visualization.dfg import visualizer as dfg_visualization\n# DFG berdasarkan frekuensi\ndfg = dfg_discovery.apply(log,\nvariant=dfg_discovery.Variants.FREQUENCY)\n# Tampilkan visualisasi\ndfg_vis = dfg_visualization.apply(dfg, log=log,\nvariant=dfg_visualization.Variants.FREQUENCY)\ndfg_visualization.view(dfg_vis)\nThe command to load a log from a file using PM4Py explicitly is:\n{\n=log = xes_importer.apply(xes_filename)\n~log = pm4py.load_file(xes_filename)\n~import xes from file\n~log = pm4py.read(xes_filename)\n~log = load(log_filename)\n~import_log(file)\n}",
    "jawaban": "The correct answer is: len(log)"
  },
  "6e9ef94f881ef20161df98f491c1ca7a084b743a4a49b56a18b53a13177b9299": {
    "soal": "import pandas as pd\ndf = pd.read_excel('path_ke_file.xlsx', sheet_name=0)\ndf_selected = df[['Kolom1', 'Kolom2']]\nprint(df_selected.head())\nHow can you set custom column names while reading an Excel file?\n)\n}",
    "jawaban": "The correct answers are: pd.read_excel('file.xlsx', names, ['A', 'B', 'C']), pd.read_excel('file.xlsx', set_columns, ['A', 'B', 'C']), df, pd.read_excel('file.xlsx', header, None, names, ['A', 'B', 'C']), pd.read_excel('file.xlsx', columns, ['A', 'B', 'C']), pd.read_excel('file.xlsx', col_names, ['A', 'B', 'C']), pd.read_excel('file.xlsx', rename, {'A', 'B', 'C'"
  },
  "82f47eaca8ffbbfeb20351b33fa65e022301661b65455c32e1daded8e7ffa435": {
    "soal": "Dalam penggunakan Widget Constant untuk regresi, kita menggunakan Widget Constant untuk membuat prediktor dalam Prediction. Kita menggunakan dataset housing. Dalam Prediction, kita dapat melihat bahwa Mean Learner mengembalikan satu (rata-rata) nilai untuk semua instance.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "5b47773df2fa44e9fe71e30ec96f0afa67eaf1b269650fd50fe5c1c85b0b61aa": {
    "soal": "The Heat Map widget can offer a heat map for a pair of attributes.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "5c966159d02ed98372619b82f927858220c30d88439679707c78a6572ae488f0": {
    "soal": "Line plot, a type of plot that displays data as a series of points connected by straight line segments. Line Plot works for numeric data. For categorical data, Line Plot can be used for grouping data points.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "1f41ce5ec74aba9d45646b9f7161d9b7988e21317872d7bbd412f07359f066a9": {
    "soal": "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target biner\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(0, 2, 100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi data latih dan data uji (80% latih, 20% uji)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi dan pelatihan model klasifikasi (misalnya RandomForest)\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n# Prediksi pada data uji\ny_pred = model.predict(X_test)\n# Menghitung Precision, Recall, dan F1-Score\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nprint(f'Precision: ')\nprint(f'Recall: {recall:.2f}')\nprint(f'F1-Score: {f1:.2f}')\n# Alternatif: Menggunakan classification_report untuk ringkasan metrik\nprint('\nClassification Report:')\nprint(classification_report(y_test, y_pred))\nWhich parameter sets the random seed in `RandomForestClassifier`?\n{\n~n_estimators\n~max_depth\n~min_samples_split\n=random_state\n~criterion\n~max_features\n}",
    "jawaban": "The correct answer is: precision:.2f"
  },
  "187df881cf2a1758245d79f999fb511189f3fe4769414a2cc5177f056da93314": {
    "soal": "For supervised problems, where data instances are described with class labels, we want to know which feature is the most informative. The Rank widget provides a table of features and their informativeness scores, and supports manual feature selection. In the workflow, we use it to find the top two features (out of 79 initial features from the selected dataset) and display their scatter plot.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "72ccde896dad2dddfb48dbee8e335151eba4d4907dc986c650dae93e07e5d8b1": {
    "soal": "MapReduce and HDFS components of Apache Hadoop are inspired by Google's papers on MapReduce and Google File System.",
    "jawaban": "The correct answer is: Google"
  },
  "3eaad7a26f4c9bc60cd1862dd2e82ca6214ad21953336c17bed4dff97aa5b82c": {
    "soal": "\n\n\nIn the process of building a machine learning model, model performance evaluation using regression is usually measured by R2 error, RMSE, and MSE.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "e447369d2f92555adf83c1759e25388e76477e7e200ccefeea99f5329169c895": {
    "soal": "What is 'process discovery' in the context of process mining?",
    "jawaban": "The correct answer is: Deriving a process model from event logs without prior models"
  },
  "1b81ef194f252b27b9ec1c82cd78714f637ff58cbac878e73a453aa2319f47c4": {
    "soal": "Widget Data Info is a simple widget that presents information about dataset size, feature, target, meta attributes, and location.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "1ce57703893f9e8d3e17a72a456bfa15b1086c813419b35bf3703e7165620795": {
    "soal": "The winner of the Turing award, Jim Gray, envisioned data science as the \"fourth paradigm\" of science:\n\nempirical\ntheoretical\ncomputational\ndata-driven\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "10db2559e7f7741493394944cf24f9b631950233cf76edb3697a3c31e48e25dc": {
    "soal": "Orange adalah Machine Learning open source dan visualisasi data untuk pemula dan ahli.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "2eb65b0e965bfc0c98f14afa80982d0b89075beb3e76f248cd6af3183d56868e": {
    "soal": "In the Moving Transform widget, we can determine which aggregation function to run on the time series and the window size.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "0d176544ab48cbf940aa1bc6fdcd0fed76571168df801c267c063bcf788678c3": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(2, size=100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi training (80%) dan testing (20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model, misalnya RandomForestClassifier\nmodel = RandomForestClassifier(random_state=42)\n# Melatih model pada data training\nmodel.fit(X_train, y_train)\n# Memprediksi data testing\ny_pred = model.predict(X_test)\n# Mengevaluasi akurasi model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model pada data testing: ')\nWhich module from `sklearn` contains the `RandomForestClassifier`?\n{\n~sklearn.tree\n~sklearn.linear_model\n=sklearn.ensemble\n~sklearn.svm\n~sklearn.neighbors\n~sklearn.cluster\n}",
    "jawaban": "The correct answer is: accuracy:.2f"
  },
  "3d84664f66f054d818558b818b56cc47643986d6dd46558a1d4ab915285b1a94": {
    "soal": "ProM visualizes animations using data from:",
    "jawaban": "The correct answer is: XES logs"
  },
  "92f55bf895034395e6cfbe5bd1c65d0bb23ccdca94c22d84d374fc435d5108e7": {
    "soal": "Data does not always come in the form of neat tables. Data can also be in the form of text, audio recordings, video content, or even images. However, computers can only work with numbers, so for any data mining, we need to transform that unstructured data into a ...........\n\nTo extract numbers from unstructured data, Orange can use a deep network embedder. Orange has just started incorporating various embedders in Orange in the Image Embedding widget, and currently, they are available for text and images.\n",
    "jawaban": "The correct answer is: vector"
  },
  "7a29b4b61122bd77a4c80f0c49aa0d47ece0f50d7c73088ed33f029a451c4984": {
    "soal": "Which PM4Py visualizer shows activity connection strength and probabilities?",
    "jawaban": "The correct answer is: Heuristic Net"
  },
  "76aa47766b98aae74dc13aa87f6c8badffb14cff7ffd997bc6787621bc4ced65": {
    "soal": "In the .......................... widget, typically, a color palette is used to visualize the entire range of distances that appear in the matrix. This can be altered by setting low and high thresholds. This way, we ignore the distance differences outside this range and visualize the interesting parts of the distribution.\n\n",
    "jawaban": "The correct answer is: Distance Map"
  },
  "0beaa2866927941e921f8f24b20cb2e044354e9a3cf17c73e055f2536b57c3ae": {
    "soal": "import pm4py\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\n# Import XES log\nlog = xes_importer.apply(xes_filename)\n# Cek jumlah kasus dan aktivitas\nprint(f\"Jumlah kasus: \")\nactivities = set()\nfor trace in log:\nfor event in trace:\nactivities.add(event['concept:name'])\nprint(f\"Jumlah aktivitas unik: {len(activities)}\")\nfrom pm4py.algo.discovery.dfg import algorithm as dfg_discovery\nfrom pm4py.visualization.dfg import visualizer as dfg_visualization\n# DFG berdasarkan frekuensi\ndfg = dfg_discovery.apply(log,\nvariant=dfg_discovery.Variants.FREQUENCY)\n# Tampilkan visualisasi\ndfg_vis = dfg_visualization.apply(dfg, log=log,\nvariant=dfg_visualization.Variants.FREQUENCY)\ndfg_visualization.view(dfg_vis)\nWhat is crucial to ensure before analyzing bottlenecks with PM4Py?\n{\n=The XES file has a correct format and necessary data\n-The Python version is latest\n-Pandas is updated\n-Numpy is installed\n-Graphviz is installed\n-Internet connection is stable\n}",
    "jawaban": "The correct answer is: len(log)"
  },
  "9720a4952bf57de39585f11126282adbcde89e23b4296f157e04b6cd6c81a3e0": {
    "soal": "Which URL should you access in your browser to use Open-WebUI after starting the services?",
    "jawaban": "The correct answer is: http://localhost:3000"
  },
  "8be81af31af6b5c1d7676e64bee286f7f0f5970c2907c8d0fef15648c80d511d": {
    "soal": "Just like R-programming, Apache Hadoop is open source. It is a tool framework created by Google and Apache. The Hadoop framework allows for processing more data, storing heterogeneous data, and accelerating its processing.\nAccording to AWS, Hadoop is an open-source framework that is very effective for storing very large datasets. In addition to storing, this framework can also process data ranging from gigabytes to kilobytes efficiently.",
    "jawaban": "The correct answer is 'False'."
  },
  "fa75752c017710165e6044c42bb532a21177eea5c026197472e035aaa0614ddd": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nWhat is the resolution of each image in the Fashion-MNIST dataset?",
    "jawaban": "The correct answer is: 28x28 pixels"
  },
  "5c040bf8a50fe1d1f37f81856423c93a21624b4b8faf2de00b22066b92b741b1": {
    "soal": "Data mining is the process of extracting valuable information or patterns from large and complex datasets. Data mining uses techniques and algorithms to analyze data and identify hidden patterns, relationships, correlations, anomalies, or trends in the data. The main goal of data mining is to generate valuable and usable information from large and complex data.\nSome common data mining techniques include clustering, classification, association, regression, and anomaly detection.\n- Clustering is a technique used to group data based on similarities in features or characteristics, so that similar data is grouped together.\n- Classification is a technique used to classify data into known categories or classes.\n- Association is a technique used to find relationships between features or items in the dataset, helping with analysis and recommendations.\n- Regression is a technique used to predict continuous variable values based on other variables.\n- Anomaly detection is a technique used to identify data that differs from existing patterns and can help in detecting fraud or errors.\nData mining can be used in various fields, including business, healthcare, education, science, and technology. Examples of data mining applications are sales prediction, credit analysis, health risk analysis, text mining, and social media analysis.",
    "jawaban": "The correct answer is 'False'."
  },
  "c78c8cdddad1ed35c8cd465754af37186300f54b0d00819cbc540142fc0f0984": {
    "soal": "Distances are most commonly calculated between instances (\"....(1).... (row)\" in the Distances widget) or attributes (\"....(2).... (column)\" in the Distances widget). The only input suitable for the Distance Map widget is ......(3)..... widget. In the output, users can select regions on the map, and the widget will display the corresponding instances or attributes.\n",
    "jawaban": "The correct answer is: (1) \u2192 rows, (2) \u2192 columns, (3) \u2192 Distances"
  },
  "4c4b261f6b1339eba6ac5150f06622e61f23da5a40f0720835ca5486254d7523": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(2, size=100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi training (80%) dan testing (20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model, misalnya RandomForestClassifier\nmodel = RandomForestClassifier(random_state=42)\n# Melatih model pada data training\nmodel.fit(X_train, y_train)\n# Memprediksi data testing\ny_pred = model.predict(X_test)\n# Mengevaluasi akurasi model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model pada data testing: ')\nWhich function is used to split the dataset into training and testing subsets in the code?\n{\n~split_data\n~train_test_divide\n=train_test_split\n~data_partition\n~dataset_splitter\n~data_separator\n}",
    "jawaban": "The correct answer is: accuracy:.2f"
  },
  "778d13c55be9d25009a25e388e755a78e43007bed78c803fbb36ecf5d22815a1": {
    "soal": "Which of the following is a high-level API for building and training deep learning models in TensorFlow?",
    "jawaban": "The correct answer is: Keras"
  },
  "60e2ddf3a2c650eda8ee022f64c71ae9a1cf17fe74896327f0eeab269ba81237": {
    "soal": "Which command updates the package lists on Ubuntu 24.04?",
    "jawaban": "The correct answer is: apt update"
  },
  "389997976e3babf6238728144db9a062d42dd8d052fa6ddd64c2d16ff31494e9": {
    "soal": "The ............................ widget allows us to import images from a directory.\n",
    "jawaban": "The correct answer is: Import Images"
  },
  "561277fef35391c4e8ed796da038398e4f93c9d4cc8544d61c8d3fa27447281c": {
    "soal": "11 (binary) x 11 (decimal) = ..... (decimal)",
    "jawaban": "The correct answer is: 33"
  },
  "3e5169c19974d712bc013676757eebef6621c3227f06689e744becfb855f13c7": {
    "soal": "What does a high F1-Score indicate about a model's performance?",
    "jawaban": "The correct answer is: Both high precision and high recall."
  },
  "b999ddd40f4d86bf2c60bdec224efe172e3595c0fb89b2aaf07ac1768c7c94ce": {
    "soal": "Using the Yahoo Finance Widget allows us to generate time series data from Jakarta Stock Exchange (JEX) data.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "9cc113e47225f0c78a70b64dbe3cb60ccdae9c4e5df260e048b75a37117581f3": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#-xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\ndfg, start_activities, end_activities = pm4py.discover_dfg(log)\npm4py.view_dfg(dfg, start_activities, end_activities)\nWhat is the format of the file being read by the script?",
    "jawaban": "The correct answer is: .xes -.csv -.json -.xml -.txt -.xlsx"
  },
  "8e8ff68347a6e2a946cbf4cdfee54451c2d494967c4bf792f51a1c78257e0993": {
    "soal": "In the Text Preprocessing widget in ORANGE, N-grams Range will create n-grams from tokens. The Number specifies the range of n-grams. The default is 10-gram and 20-gram.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "324539863f77037d973447f01cb264793fa2aff34dc279d915b9b5dcc0e44d0e": {
    "soal": "The Calibrated Learner widget generates a model that calibrates the distribution of class statistics and optimizes the decision threshold.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "54cb374c9078c55da5b5c275fdc518e8ff56383e0c6a752bda78fbb6227f3c78": {
    "soal": "In the Interpolate widget, which is part of the Time Series add-on, the interpolation type - Linear interpolation replaces missing values with linearly-spaced values between two nearest and defined data points.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "198c6ba916e981ee2289f5c36baa28f4eea910acb99f3f92bd342bf993b2b5dd": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#-xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\ndfg, start_activities, end_activities = pm4py.discover_dfg(log)\npm4py.view_dfg(dfg, start_activities, end_activities)\nWhat are 'start_activities' and 'end_activities' used for?",
    "jawaban": "The correct answer is: Identifying beginning and ending activities in processes -Training machine learning models -Calculating statistics -Loading data files -Cleaning data -Converting file formats"
  },
  "83167a5685f3e5f1d357dbcac8e3d3b76090f4a41764b65e4eabb77c9a6a429e": {
    "soal": "Short loop handling is weak in:",
    "jawaban": "The correct answer is: Alpha Miner"
  },
  "a568d39e660cb9811032c698fe711f1b69f0d61beae69b5c3c44a28f22029986": {
    "soal": "Which of the following metrics provides a detailed view of a model's performance, especially on imbalanced datasets?",
    "jawaban": "The correct answer is: Precision, Recall, and F1-Score"
  },
  "49173b4ff35c17e7c03d8cf04c36d529cf11bf40ca9e127c85e906e0e4be75ec": {
    "soal": "We use the zoo dataset in combination with Hierarchical Clustering to find animal groups. Now we have the clusters that we want to find, and what is significant for each cluster! Give the cluster to the Box Plot and use \u2018Order by relevance\u2019 to find what defines the cluster. It seems they are separated by name, even though the grouping is done without any class labels! This is an example of unsupervised learning.\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "ee43c49f401841987b91670ddf27cff918b98182fe4bbc91de15c46c6017a16b": {
    "soal": "One use of the Tree widget is shown in the workflow below. By using the Tree widget, we can induce a model and check it using a view similar to the Tree Viewer widget.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "b4620021e1017101d80535054aa7996d20bc39d9dccb9aecb7be13f3f6a0956c": {
    "soal": "In the ............ Widget, to retrieve data, select (click) the folder icon on the right side of the widget. Select (choose) the folder that we want to use as a corpus. After loading is complete, we will see how many documents were successfully retrieved by the Import Documents Widget. To observe the obtained corpus, connect the Import Documents widget to the Corpus Viewer Widget. In this workflow, we are using a set of speeches by President Kennedy in plain text format.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Import Documents"
  },
  "0670662d7676bf102b0c87015285438b5e403475d31b6f5a97db12566b2b6f25": {
    "soal": "Widget CSV File Import reads comma-separated files and sends the dataset to the output channel. The separator can be a comma, semicolon, space, tab, jpg, or manually defined delimiter. The history of the newly opened file is maintained in the widget.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "ef721a67f4239f452bcf1cde183af2a8f9ab216a07217dbfc54419310fdd2362": {
    "soal": "The Import Documents widget can import text documents from a folder to become ............................\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: corpus"
  },
  "ea156c0abf5e260d42fb1ca694d2a70dfad3fa23f48001902e6106142144d28e": {
    "soal": "!pip install googlesearch-python nltk matplotlib wordcloud\nfrom googlesearch import search\nimport csv\ndef google_scrape(keyword, num_results=40,\noutput_file='results.csv'):\nprint(f\"Searching Google for: \")\n# Lakukan pencarian dan ubah hasil ke dalam list\nresults = list(search(keyword, num_results=num_results))\n# Simpan ke file CSV\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'URL'])\nfor url in results:\nwriter.writerow([keyword, url])\nprint(f\"Saved {len(results)} results to '{output_file}'\")\n# Contoh penggunaan\nkeyword = \"berita terbaru teknologi Indonesia\"\ngoogle_scrape(keyword, num_results=40)\nWhat does the script do if the specified output file already exists?\n{\n=It overwrites the existing file with new data.\n~It appends the new data to the existing file.\n~It creates a new file with a numbered suffix.\n~It prompts the user for a new filename.\n~It raises an error and stops execution.\n~It merges the new data with the existing file content.\n}",
    "jawaban": "The correct answer is: keyword"
  },
  "ba3fddf32a7fdb1d6efe9046e7e48a7d05830a87a5402a6d6f8cea443fe22793": {
    "soal": "What type of memory is the context window most similar to?",
    "jawaban": "The correct answer is: Short-term memory"
  },
  "2cf3e97de07ff1d7e87d74d6c34b4bdae71feb9cbdfe1194df935d009d1158d9": {
    "soal": "101 (binary) AND 111 (binary) = .... (binary)",
    "jawaban": "The correct answer is: 101"
  },
  "2ec11e7336bf64c57444d2e4ea8a37f8692ed7aed9335d8a10566c56d124bde1": {
    "soal": "As an example of using kNN for classification, we use the iris dataset. We compare the results of k-Nearest Neighbors with the default Constant model, which will predict the minority class. It appears that kNN is better.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "066a37f159e342de159f0bb65c4412d434c6809937db07d3e64341f2074b0ef2": {
    "soal": "Directly-Follows Graph (DFG) is suitable for which complexity level of datasets?",
    "jawaban": "The correct answer is: Low to Medium Complexity"
  },
  "63d67861340c96b14be3006ffbba6566dbb59966edb90d234646e85551976819": {
    "soal": "PM4Py installation primarily requires:",
    "jawaban": "The correct answer is: Python environment"
  },
  "47aea79a4b53e7dcb0d0441c95fb4db4e2286770377e785085c13bb51d63560e": {
    "soal": "In TensorFlow, which function is used to concatenate tensors along an existing axis?",
    "jawaban": "The correct answer is: concat()"
  },
  "2391ec69784e47a12019f4c3fb597dc1034576da92ee2c41d4d2fa3ded3ba088": {
    "soal": "Which visualization method guarantees a deadlock-free and live model?",
    "jawaban": "The correct answer is: Inductive Visual Miner"
  },
  "e263329c22ab64d812bdb420017be6e2c9b242ea6d58e199fe1e06e89f10d67a": {
    "soal": "What file format does ProM commonly analyze?",
    "jawaban": "The correct answer is: XES"
  },
  "0bc1bce1ae81dc6747bd39aa286f32ec168f822301610e030c367d867238bf2e": {
    "soal": "\n\n\nData visualization with many variables (multivariate), not same, unordered can use ..............................\n\n\n\n",
    "jawaban": "The correct answers are: scatter plot, area chart, connected scatter plot"
  },
  "6f15fac40dd730bd0e330e377e3c2aec46bd5087f88dbd262cd0be2644cf0d57": {
    "soal": "Principal Component Analysis (........(1)...........) transforms data into a dataset with uncorrelated variables, also called principal components. The PCA widget displays a graph (scree diagram) that shows the degree of variance explained by the best .............(2)......... components and allows you to interactively adjust the number of components to be included in the output dataset. In this workflow, we can observe the transformation in the Data Table and in ..........(3)...............\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 PCA, (3) \u2192 Scatter Plot, (2) \u2192 principal"
  },
  "17132e1a99d26449fd829f5b3b4cbfd8f3890513ea9c692f3b7bc3a7f37897f4": {
    "soal": "\n\n\nExamples of big data statistics professions include,\n\n\n\n\n\n\n",
    "jawaban": "The correct answers are: Big Data Architect, Big Data Engineer"
  },
  "96239531ec26eb2b236338f864e4bad9805d6899b3b4d209e72ee7513bc2d16e": {
    "soal": "In the Moving Transform widget, to integrate the time series' difference from the Difference widget, use Cumulative sum aggregation on a wide enough window to capture the entire series.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "0399768eaae24bf1de6917be60b38bb40fce8ee9df4e0a548bdeb1c3b23f5979": {
    "soal": "The kNN widget predicts based on the most frequent training instances.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "77c3c8a0db1bdbc2577957537e9695361c746e69fa96d7afcf20437007caa32f": {
    "soal": "If TensorFlow detects a GPU in Google Colab, what is displayed?",
    "jawaban": "The correct answer is: Information about the available GPU"
  },
  "a477ebe2114bc61268914512cd88ed1b3b94caf2bdcd1866a760eafea59bc5bd": {
    "soal": "In the next example, we will try to predict the category of a document. We will use the dataset book-excerpts.tab, which we will send through the Preprocess Text widget with default parameters. Then we will connect the Preprocess Text widget to the Bag of Words widget to get term frequency. With the term frequency information, we will calculate the model.\n\nConnect the Bag of Words widget to the Test & Score widget for predictive modeling. Connect the SVM widget or another classifier widget to the Test & Score widget (all on the left side). The Test & Score widget will calculate the performance score for each learner in the input. Here we get excellent results with the Linear Regression widget.\n\nNext, we need to check where the model made mistakes. Add the Confusion Matrix widget to the Test & Score widget. The Confusion Matrix widget will display documents that were correctly classified and misclassified. Select misclassified to output misclassified documents, which we will review further using the Corpus Viewer.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "ba024c86b8553677c55d0d159bd91794e9e09e7fcd3732b78174858a5c967977": {
    "soal": "The Tree algorithm is a simple algorithm that can separate data into nodes based on class purity (class/category purity). The Tree algorithm is the predecessor of the Random Forest algorithm. The Tree widget in Orange is designed in-house and can handle integer and continuous datasets.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "aa1d9a78da4c0d505140904c3235f91b81ce8b9a65140e8a2862ddaaab33ff9f": {
    "soal": "The Venn Diagram widget can also be used to explore different prediction models. In the following example, we analyze 3 prediction methods, namely Naive Bayes, SVM, and Random Tree, according to their classification errors on data instances.\n\nBy selecting misclassification in three Confusion Matrix widgets and sending them to the Venn diagram widget, we can see all misclassification examples visualized per method used. Then we open the Venn Diagram widget and select, for example, misclassified instances identified by all three methods. This is represented as the intersection of the three circles. Click on the intersection to see these two examples marked in the Scatter Plot widget. Try selecting different parts of the diagram to see how the scatter plot visualization changes.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "cb529820006bf5da4d8f057f5742557779c739af8164cdcbf4525c1c394ccf98": {
    "soal": "What does the context window do?",
    "jawaban": "The correct answer is: Stores relevant information for understanding"
  },
  "a713f2c5a5519fb3bd83f2581d24ebb46c8057ce759060eda3ad9c197769142f": {
    "soal": "We use the zoo data set in combination with the .................... widget to find animal groups. Now we have clusters that we want to identify and what is significant for each cluster! Provide the cluster to the Box Plot widget and use \u2018Order by relevance\u2019 to find what defines the cluster. It seems they are separated by their type, although clustering was done without class labels! This is an example of unsupervised learning.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Hierarchical Clustering"
  },
  "73c2bc4cbdd76676c85677098a9dacf650b4626f01e07409b7bbd996dafe7eb1": {
    "soal": "The Select Columns widget is used to manually arrange the data domain. The user can decide which attributes to use and how to do it. Orange distinguishes between regular attributes, class attributes ........................... (optional), and meta attributes. For example, to build a classification model, the domain will consist of a set of attributes and discrete class attributes. Meta attributes are not used in modeling, but some widgets can use them to label instances.\n",
    "jawaban": "The correct answer is: class"
  },
  "254aaa0c2cd5093fc9aaac32e757c29d3fc72795e089a71bb37c6b9494456fe8": {
    "soal": "Which visualization method does PM4Py NOT support directly?",
    "jawaban": "The correct answer is: Animation"
  },
  "87a5801805366636df5bbf15de234bdd076bf7a546ee86a3f9d7e533e22379dd": {
    "soal": "URL yang _BUKAN_ merupakan tempat contoh / tutorial orange data mining adalah\n",
    "jawaban": "The correct answers are: https://www.youtube.com/c/OrangeWorkFlow, https://www.merriam-webster.com/dictionary/orange, http://www.englishcollocation.com/how-to-use/orange,\nPlay Video\n, https://www.youtube.com/c/OrangeData"
  },
  "eca665bbd1cbc067e3030b9966ae231c3f83630e1aab85d18aea92b6dbc18a77": {
    "soal": "import tensorflow as tf\nfrom tensorflow.keras import layers, models\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nmodel = models.Sequential([\nlayers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(64, (3,3), activation='relu'),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(128, (3,3), activation='relu'),\nlayers.Flatten(),\nlayers.Dense(128, activation='relu'),\nlayers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5, batch_size=64)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nWhat is TensorFlow primarily used for in this context?",
    "jawaban": "The correct answer is: Deep Learning with CNN"
  },
  "0279e9a990329cbfb247e3be73081bec76680984408b3ce905aa4fe97b70a5af": {
    "soal": "To handle complex, structured data, use:",
    "jawaban": "The correct answer is: Inductive Miner"
  },
  "eb4085bdb7a97ba8a890047b158712c2f9641d2545d5f815a1bdf06991796eda": {
    "soal": "Which of the following is a key benefit of process mining?",
    "jawaban": "The correct answer is: Identifying execution paths and addressing performance issues"
  },
  "8a97c48bc8d336503634327b5eab376f40a9d365a400f521fc7d71bb6a8b0926": {
    "soal": "\n\n\n\n\n\nUnsupervised Learning techniques are divided into:\n\nClustering.\nAssociation Analysis.\nRegression.",
    "jawaban": "The correct answer is 'False'."
  },
  "20bcf214262dece2e1bc698099451e23ac9c54d9feeb525c46a4c2d2f6f8d85a": {
    "soal": "Which metric represents the percentage of correct predictions out of all predictions made?",
    "jawaban": "The correct answer is: Accuracy"
  },
  "21a9aaef980b95a218212b7a427ddc5fcfb31b2e11f3690517610a7ecb66053c": {
    "soal": "The ................................. widget shows the match between the classifier's predicted probability and the actual class probability.\n",
    "jawaban": "The correct answer is: Calibration Plot"
  },
  "f1c940b92e464c2410849c79cad291a65c8a472362b7c40cca8c42a9c243752e": {
    "soal": "Most visualizations in Orange are interactive. In the workflow below, the Scatter Plot widget, for example. Double-click the icon to open it, and click-and-drag to select multiple data points from the plot. The selected data will automatically enter the Data Table widget. Double-click to inspect which data was selected. Change the selection and observe the changes in the Data Table widget . This works well if both widgets are opened.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "f567bad96b057ff91d83ed1acf33fc01a17f7e75eaf95c2e50a78ae232cf40ef": {
    "soal": "The Save Data widget can consider the dataset provided in the input channel and save it to a data file with a specified name. It can save the data as a file with data separated by ............................ or separated by commas.\n",
    "jawaban": "The correct answer is: tab"
  },
  "dfdbefd7b1e0355b2442949d220825bc1e0ac8f34a6ca2a65b62e31f6572e929": {
    "soal": "In the following workflow example, it shows how to quickly visualize the corpus with the Word Cloud widget. We can connect the Word Cloud widget directly to the Corpus widget, but instead, we decided to apply some preprocessing with the Text Preprocess widget. We are working with the book-excerpts.tab dataset. We can change all ................... to lowercase, tokenize (split) the text into words, filter out English stopwords, and select the 100 most frequently occurring tokens.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: text"
  },
  "99e9e1fb908fda951c789eb1f56f4835d3e0dcb77e4761982f7b8714284261db": {
    "soal": "What does the Holdout Validation method involve?",
    "jawaban": "The correct answer is: Splitting the dataset into Training and Testing sets"
  },
  "316cf348bf2ac8511e39f71e10b0dfe81358c6d65e3bf70bcaf72c611f0a75f5": {
    "soal": "The Corpus widget can load a collection of text documents, optionally tagged with categories, or convert input data to ......................\n",
    "jawaban": "The correct answer is: corpus"
  },
  "2af14988f0a193cc9108b6de6de8071588205aded5f721bc26fea51dce591cf2": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nHow many distinct clothing categories are represented in the Fashion-MNIST dataset?",
    "jawaban": "The correct answer is: 10"
  },
  "089d49532d92d47c7422bd69e74bd5aa4d185a4e905d19f1b77387f9fd5a57f0": {
    "soal": "In the Interpolate widget, which is part of the Time Series add-on, the interpolation type - Spline interpolation fits a quadrature polynomial to the values around the missing values. Therefore, this technique will be very slow but will provide the best results.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "c79fcbe4f4fcddebcf2e9c674853c7cbb76ba209afa9d5af9c32821c1119080e": {
    "soal": "The ...............(1)............ widget enables basic sentiment analysis of corpora. So far it works for English and uses two techniques supported by NLTK - Liu Hu and ..............(2)........... Both of these techniques are lexicon-based. Liu Hu calculates a single normal score for sentiment in the text (negative score for negative sentiment, positive for positive, 0 is neutral), while Vader generates scores for each category (positive, negative, neutral) and adds a total sentiment score called compound.\n\n\n\n\nLiu Hu score\nVader score\n",
    "jawaban": "The correct answer is: (1) \u2192 Sentiment Analysis, (2) \u2192 Vader"
  },
  "fb74e7dc257fe9fdcbb81c5e9a7cfa0664b09bae679b2211f235a0f605ab62ca": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\n# xes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\nxes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nprocess_tree = pm4py.discover_bpmn_inductive(log)\nbpmn_model = pm4py.convert_to_bpmn(process_tree)\npm4py.view_bpmn(bpmn_model)\nWhich extension indicates the file is an event log suitable for process mining?",
    "jawaban": "The correct answer is: .xes"
  },
  "9adc3d5b9336287926b76bd80a722707b85cfde9149f5753ef9595bb4d85f886": {
    "soal": "Widget kNN mem-prediksi berdasarkan instance training ..........................\n\n\n\n",
    "jawaban": "The correct answer is: terdekat"
  },
  "389931e06e358b62163dae2a2598b5cbe1667e8d6a45e39a56b92528b28ae455": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\n# xes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\nxes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nprocess_tree = pm4py.discover_bpmn_inductive(log)\nbpmn_model = pm4py.convert_to_bpmn(process_tree)\npm4py.view_bpmn(bpmn_model)\nWhich function visualizes the BPMN model?",
    "jawaban": "The correct answer is: view_bpmn"
  },
  "7da92d13ed6eae082010b5c7a083a23810de3f39beef5feecb865a83eec00949": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nplt.imshow(x_train[0], cmap='gray')\nplt.title(f\"Label: \")\nplt.show()\nmodel = keras.Sequential([\nkeras.layers.Flatten(input_shape=(28, 28)),\nkeras.layers.Dense(128, activation='relu'),\nkeras.layers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nHow many output units are in the final Dense layer?\n{\n= 10\n~ 1\n~ 28\n~ 128\n~ 256\n~ 5\n}",
    "jawaban": "The correct answer is: y_train[0]"
  },
  "9eecaf7fcb33aa45b8d3a708f6126ba35a77e7028ea949c5c712d2af5a8fe1c5": {
    "soal": "The easiest way to use the Venn Diagram widget is to select a data subset and find matching examples in the visualization. We use the breast-cancer dataset to select two subsets with ...................... widget - the first subset is breast cancer patients aged between 40 and 49 years, and the second is patients with tumor sizes between 20 and 29. The Venn diagram helps us find examples that fit both criteria, which can be found at the intersection of the two circles.\n\n\n",
    "jawaban": "The correct answer is: Select Rows"
  },
  "ab047b11c69e5a493632d88b5d8cb79982725bb019d840128c60ee06ae209b3d": {
    "soal": "The Box Plot widget shows the distribution of attribute values. It is good practice to check old data with this widget to quickly find anomalies such as duplicate values (e.g., gray or grey), outliers, and the like.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "fc0ca52dad6c28ff15225b78a1a9e01ea10d6aae78bb3d452065ca9a4068855a": {
    "soal": "Widget Network Analysis calculates a summary ...........(1)............ at the node-level and graph-level for a network. The Widget Network Analysis will output .............(2).............. with the results of .............(3).............. statistics and an extended item data table (only node-level index).\n",
    "jawaban": "The correct answer is: (2) \u2192 network, (1) \u2192 statistics, (3) \u2192 computation"
  },
  "079c9a7f1b4472c5680719e1fcccad2e4882916d8b5b6dbac7b47a49279fa5dc": {
    "soal": "Which restart policy is set for the Ollama service in the 'docker-compose.yaml' file?",
    "jawaban": "The correct answer is: unless-stopped"
  },
  "b798974e7695af87d52ab4684c6c222618bf557293ac5b6c90ee095dc4a9e8d8": {
    "soal": "Data Mining has the power to transform businesses; however, implementing processes that meet the needs of all stakeholders often hinders successful data mining investments \u2014 78% of respondents say they struggle to find the right data mining strategies or solutions.\nDespite these obstacles, businesses that are able to effectively mine widespread data have several key similarities. Successful companies:\n- Do not know the core needs of their business, both tactical and strategic, that can be met by data mining;\n- Identify and evaluate data sources to be used by data mining tools for accuracy and relevance;\n- Define applications, including Business Intelligence (BI) systems, where data mining tools should be interoperable;\n- Identify available data mining solutions that meet the entire scope of the organization's requirements, from budget to end-user technical capabilities;\n- Use one standard data mining tool that meets the needs of IT, data scientists, and analysts, while also meeting the consumption and visualization needs of business users.",
    "jawaban": "The correct answer is 'False'."
  },
  "de8f021918359ec0ac6cb3d6ed26890a7a1e4c9daa5ec711aa8c0cef594392fc": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nWhat serves as the input to the first Dense layer in the model?",
    "jawaban": "The correct answer is: Output from the Flatten layer"
  },
  "d815ff8213ba8e59151da30ab6eb6698fcc458629a692ead724ff6b062907720": {
    "soal": "Why is a timestamp important in an event log?",
    "jawaban": "The correct answer is: It records the exact time an event occurred"
  },
  "efbbd268b425e62550e013dd37cdaf26baf77a4515dcede979bcc70e9bfb403e": {
    "soal": "After logging into Open-WebUI, what is a recommended next step?",
    "jawaban": "The correct answer is: Pull models from Ollama or other sources"
  },
  "7fee061d74b3bb7ed84474a7fc696cee500b89d4af198a5e1fb94a7739f1faa2": {
    "soal": "import pandas as pd\nfrom pm4py.objects.conversion.log import converter as log_converter\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom pm4py.visualization.petrinet import visualizer as pn_visualizer\nfrom pm4py.objects.log.util import dataframe_utils\n# csv_filename='Insurance_claims_event_log.csv'\n# csv_filename='Business Travel Permits.csv'\n# csv_filename='Parent-Child Union Output.csv'\n# csv_filename='PM4PY_Dummy_Event_Log_100_Cases_pm4py.csv'\n# csv_filename='PM4Py_Dummy_Event_Log-congested-business-process.csv'\n# 1. Load CSV event log\ndf = pd.read_csv(csv_filename)\n# df = pd.read_csv(csv_filename,sep=';')\n# print(df.head())\n# 2. Format timestamps properly\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n# 3. Ensure column names match PM4Py expectations\ndf = dataframe_utils.convert_timestamp_columns_in_df(df)\n# 4. Convert to event log object\nevent_log = log_converter.apply(df, parameters=)\n# 5. Discover process model using Alpha Miner\nnet, initial_marking, final_marking = alpha_miner.apply(event_log)\n# 6. Visualize Petri Net\ngviz = pn_visualizer.apply(net, initial_marking, final_marking)\npn_visualizer.view(gviz)\nWhat does the 'activity' column contain?\n{\n=Activity names within a process\n~Unique ID for each process\n~Duration of each activity\n~Start and end times\n~Cost data\n~Resource information\n}",
    "jawaban": "The correct answer is: log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case_id'"
  },
  "71c48bf3c5c613c172b4f7e71acc6b69f95c9dee4cdfa71fb8bf04644186ffc9": {
    "soal": "What do you use to load a XES log in Python with PM4Py?",
    "jawaban": "The correct answer is: pm4py.read_xes('file.xes')"
  },
  "42f1fdbcab0ca511895c35dc7e6ef2786a26ef0d0adcc5d93b6fabbe36fe6b8f": {
    "soal": "What is the primary purpose of the Training Set in dataset partitioning?",
    "jawaban": "The correct answer is: To train the model"
  },
  "fb011770313bb4d3ce941954295e24fa7d035c7a65af6f8a91857cd9f1beafa8": {
    "soal": "Column attribute names can be preceded by a label followed by a hash. Use c for class and m for meta attributes, i for ............ , w for weight (of a column), and C, D, T, S for continuous, discrete, time, and string attribute types. Example: C#mph, mS#name, i#dummy.\n\nWhere ........ is,\n",
    "jawaban": "The correct answer is: ignore column"
  },
  "7b25e0877f7afd748473173a7129cdbb996c5fabddb5ad353351f57b1c47be18": {
    "soal": "What is the primary goal of machine learning?",
    "jawaban": "The correct answer is: To enable machines to learn from data"
  },
  "a43010caff294e9f2b162f298f08e249edd6296d0884673e3364fd682d8f3a28": {
    "soal": "Some formats supported by ORANGE3 (based on its add-ons):\n\n.............................. : Multifile from Spectroscopy add-on\n",
    "jawaban": "The correct answer is: multiple spectroscopy file"
  },
  "5349b1472ae1a90538f5f187719fdcbb1684b114dda399f75e5c0c7c3944efef": {
    "soal": "For unsupervised problems, where data instances are described with class labels, we want to know which feature is the most informative. The Rank widget provides a table of features and their informativeness scores, and supports manual feature selection. In the workflow, we use it to find the top two features (out of 79 initial features from the selected dataset) and display their scatter plot.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "b34c0cca338b19b993db462aa28e9742a3616e6ae91d2e73b463e53b0a9e33ed": {
    "soal": "Why is trying multiple discovery methods recommended?",
    "jawaban": "The correct answer is: To find the most suitable approach"
  },
  "3f2bb37ef5cf79d2079b20d136bfdf93bc9ebe344697be2fc02503a2104fe312": {
    "soal": "The Bag of Words widget can create a bag of words from the input image.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "10524bac98d1ba503ebc6a68da230e57d997eceac692dc1122249278457e99c1": {
    "soal": "In this example, we will just check what a bag of words model looks like. Load book-excerpts.tab using the Corpus widget and connect it to the ............ widget. Here, we deliberately use the default parameters - the simplest count is term frequency. Check the output of the Bag of Words widget using the Data Table widget. The last column represents the term frequency of each document.\n\n\n",
    "jawaban": "The correct answer is: Bag of Words"
  },
  "6a4c806227dbff5d61428b1551dd199430a645e34311ce3624005f9cf88e9574": {
    "soal": "Distances are most commonly calculated between instances (\"rows\" in the Distances widget) or attributes (\"columns\" in the Distances widget). The only compatible input for the Distance Map is the Distances widget. In the input, the user can select the map region, and the Distance Map widget will display the corresponding instances or attributes.",
    "jawaban": "The correct answer is 'False'."
  },
  "6c19dafb23c80a79187630e3a99282b334e62d9db874afd1af6fc975cc880623": {
    "soal": "One use of the Tree widget is shown in the workflow below. By using the Tree widget, we can integrate a model and check it using a view similar to the Tree Viewer widget.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "651c976d6936421620d914a6c93ef2b7dc845ecb167959bd0da92ba92e674e43": {
    "soal": "The Import Documents widget can import text documents from a folder to become articles.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "a842b72adb3bc4b0d6c223f5a1593053cf145b4c30ee6425b7dc8b4d26500c7b": {
    "soal": "import pandas as pd\nfile_path = \"/content/drive/MyDrive/your_file.csv\"\ndf = pd.read_csv(file_path)\nprint(df.head())\nWhat is the purpose of the 'df.head()' function in the provided code?",
    "jawaban": "The correct answer is: It displays the first 5 rows of the DataFrame."
  },
  "4f84cf489d499f6d181fcc1af6b8f96a29c6d11a5c769769ee650ea2bab636d8": {
    "soal": "Which command displays all models currently downloaded on your system?",
    "jawaban": "The correct answer is: ollama list"
  },
  "a29997c78e70e2d37fa7b40777daf48792a03496a3d69106877a03ae10c0bc0c": {
    "soal": "The Image Viewer widget will display the images in the dataset.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "aad4b82cdea84b0a0c5bceda86f208808f90d6bfdae09292f38ab9258a9753cd": {
    "soal": "import numpy as np\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n# Contoh data: nilai aktual dan prediksi\ny_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0]) # Nilai aktual\ny_pred = np.array([1, 0, 1, 0, 0, 1, 0, 1, 1, 0]) # Prediksi model\n# Menghitung confusion matrix\ncm = confusion_matrix(y_true, y_pred)\n# Menampilkan confusion matrix dalam bentuk teks\nprint(\"Confusion Matrix:\n\", cm)\n# Menampilkan confusion matrix dalam bentuk grafik\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.show()\nDalam confusion matrix untuk klasifikasi biner, apa yang direpresentasikan oleh elemen di baris pertama dan kolom pertama?",
    "jawaban": "The correct answer is: True Negatives (TN)"
  },
  "8329c0cc05b717bd96f8fc611c3b83be90271e092c8bcf813a8ae02d335952f9": {
    "soal": "In this example, we will just check what a bag of words model looks like. Load book-excerpts.tab using the ................. widget and connect it to the Bag of Words widget. Here, we deliberately use the default parameters - the simplest count is term frequency. Check the output of the Bag of Words widget using the Data Table widget. The last column represents the term frequency of each document.\n\n\n",
    "jawaban": "The correct answer is: Corpus"
  },
  "ca4b11149b92a57a588199b968dfe4cad355784bac8512cd615ef04442f7e6fd": {
    "soal": "Why are LLMs important in language-based technology?",
    "jawaban": "The correct answer is: They enhance understanding and communication"
  },
  "d0c08cf9d6ac6584887a50a3427fde42e91024bf38caef2739b70ce5b87a74e3": {
    "soal": "Data Mining is essential for supporting the success of modern organizations driven by data. An IDG survey of 70 IT and business leaders found that 2% of respondents want to implement advanced analytics more widely across their organizations. The same survey found that the benefits of data mining are profound and vast.",
    "jawaban": "The correct answer is 'False'."
  },
  "35127631f5a944b6b0368e6c48f623f1ac21b163aaf4b62ae70d5ec87a073c35": {
    "soal": "The Import Documents widget takes text files from a folder and creates a corpus. The Import Documents widget can read .txt, .docx, .odt, .pdf, and .xml. If there are ..........................., they can be used to label the class.\n",
    "jawaban": "The correct answer is: subfolder"
  },
  "fc97018bc49f8e06541b61335bc52a981542d3f72ad88bbe46a4e855d38ca624": {
    "soal": "In the Correspondence Analysis Widget - Correspondence Analysis (CA) computes the linear CA transformation of input data. Although similar to PCA, CA computes a linear transformation on ............. data, not on continuous data.\n",
    "jawaban": "The correct answer is: discrete"
  },
  "36dbc87bf96c311bebbd951c4c9bbacf5c13f0a91973d25979e61c893a02681f": {
    "soal": "Approximately what percentage of the total data is typically allocated to the Validation Set?",
    "jawaban": "The correct answer is: 10-15%"
  },
  "5d733882d9bc84c1823d27081a143f80bd1b5a295968ded732eb7464af723778": {
    "soal": "Which service's container name is set to `openwebui-postgres` in the `docker-compose.yml` file?",
    "jawaban": "The correct answer is: Postgres"
  },
  "76d9aaef0db99a6ae1b23055e116bc4cb26e5bc4567e628f6552a73b406a8b91": {
    "soal": "What is TensorFlow primarily used for?",
    "jawaban": "The correct answer is: Machine Learning and Deep Learning"
  },
  "40df9d3b383964a0c177ecce60508763843ce538bdd232bc8bb32a2bfb30bcdc": {
    "soal": "CN2 Rule Induction can only be used for classification tasks.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "1db2897e6d8e1bc70dc2f555a7b678128ed3c961fe5383068084259c4917d2b7": {
    "soal": "The kNN widget predicts based on the furthest training instances.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "a84d645790c4fd28e1b9c4280a805b349b1d994f3602d7ec222913741b7949bd": {
    "soal": "The Data Info widget is a simple widget that presents information about the dataset size, ........................, target, meta attributes, and location.\n",
    "jawaban": "The correct answer is: feature"
  },
  "5ac39d073a1fab41e3d200af8d1791a41436f9f6368b83d4b3ed6705e0dfbe1b": {
    "soal": "Widget Calibrated Learner membungkus / melanjutkan kerja dari learner lain dengan statistically calibration dan decision threshold optimization.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "d64eae1565d635079f30fe2776f5ecd5c2240d1e19781fbedf78f5010a42a25e": {
    "soal": "import numpy as np\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n# Contoh data: nilai aktual dan prediksi\ny_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0]) # Nilai aktual\ny_pred = np.array([1, 0, 1, 0, 0, 1, 0, 1, 1, 0]) # Prediksi model\n# Menghitung confusion matrix\ncm = confusion_matrix(y_true, y_pred)\n# Menampilkan confusion matrix dalam bentuk teks\nprint(\"Confusion Matrix:\n\", cm)\n# Menampilkan confusion matrix dalam bentuk grafik\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.show()\nMengapa pustaka matplotlib digunakan dalam contoh kode untuk confusion matrix?",
    "jawaban": "The correct answer is: Untuk membuat visualisasi confusion matrix."
  },
  "d07de7b2616ec643f5e2b711ccba35fc87029e0efd3a9cdb5814dea8f8b880eb": {
    "soal": "In the following example, we will see how to properly use Preprocess with the Predictions widget or the Test & Score widget.\n\nThis time we are using the heart disease.tab data from the File widget. We can access the data through the dropdown menu. This dataset consists of 303 patients who visited a doctor with chest pain. After the test was conducted, some patients were found to have narrowed arteries while others did not (this is our class variable).\n\nThe heart disease data has some missing values, and we want to handle that. First, we will split the dataset into train and test data using the Data Sampler.\n\nNext, we will send the Data Sample to Preprocess. We will use the Impute widget to handle the missing values, but we can try combinations of preprocessors on our data. We will send the processed data to the Logistic Regression widget and the model built to the Predictions widget.\n\nFinally, the Predictions widget also needs data to predict. We will use the output from the Data Sampler widget for prediction, but this time not the Data Sample, but the Remaining Data (remaining data), which is the data that was not used to train the model.\n\nNotice how we send the remaining data directly to the Predictions widget without applying any preprocessing. This is because Orange handles preprocessing on new data internally to avoid errors in model construction. The same preprocessor used on the training data will be used for prediction. The same process applies to the Linear Regression widget.\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "df00b9c2afafa5d50264016ce90cdcf10bf5256e5bf48fc81c08891d81d18990": {
    "soal": "Similar to R-programming, Apache Hadoop is open source. It is a framework tool created by Google and Apache. The Hadoop framework allows for the processing of larger amounts of data, storing heterogeneous data, and accelerating the processing time.\nAccording to AWS, Hadoop is an \nAnswer Question 35\n source framework that is highly effective for storing extremely large datasets. In addition to storing, this framework can also efficiently process data from gigabytes to petabytes.",
    "jawaban": "The correct answer is: open"
  },
  "eb1fc0d81dbb61e7c83be4fd6ddccd547df79ae6d6a61a3f25122e6a3b98d2c7": {
    "soal": "Which of the following is an example of a loss function used in regression tasks?",
    "jawaban": "The correct answer is: Mean Squared Error"
  },
  "695ee26ddbda6614d0cf4c1c1d3f782d6adff4d8730bbbc04d62706f58e56301": {
    "soal": "This workflow example shows how Network Analysis can enrich the workflow. We used ..........(1)........... lastfm.net data as the input network from the ...........(2).......... Widget and sent it to the Network Analysis Widget. We can decide to compute the degree, degree centrality, and closeness centrality at the node-level.\n\nWe can then visualize the network in the ............(3).............. widget. In the Network Explorer widget, it colors with the best tags, like the default for this dataset. But now we can also adjust the node size to fit the degree centrality results. This is a good way to visualize network properties.\n\n\n\n",
    "jawaban": "The correct answer is: (3) \u2192 Network Explorer, (1) \u2192 data, (2) \u2192 Network File"
  },
  "50ee789ed13c32f5665b7602678d10dc0441264013ee811c229c1549cfa7f29b": {
    "soal": "The ....................... widget can visualize cycle, seasonality, periodicity, and important periods in time series.\n",
    "jawaban": "The correct answer is: Periodogram"
  },
  "2ff17548fd0a5ca5522f922f5dc6088a8cda21b43c4a56cb246aabb9c552ee7e": {
    "soal": "Instalasi Orange3 yang\u00a0 baik di Ubuntu 18.04 dapat menggunakan perintah\n\n\u00a0pip3 install orange3\n",
    "jawaban": "The correct answer is 'True'."
  },
  "411269067466cb2b548ff3c7a4b327292ecc91091e4969cef1d542e366848ca1": {
    "soal": "The ........................ widget takes the selected dataset from the server and sends it to the output. The file is downloaded to local memory and is therefore immediately available even without an internet connection. Each dataset comes with a description and information about the size of the data, number of instances, number of variables, target, and tags.\n",
    "jawaban": "The correct answer is: Dataset"
  },
  "04fa9f4739f4027be8008fe8e57cdb6e0f86a57aa80f280c9a1143fa02d6338e": {
    "soal": "Another alternative to the hash notation format is Orange's native data format with three (3) header rows: the first with attribute names, the second specifying the type (continuous, discrete, date, or string), and the third row providing information about the attribute roles (class, meta, weight, or ignore).\n",
    "jawaban": "The correct answer is 'False'."
  },
  "50f0eaa20fd0c38a0af2622064a56cc9a9fc1b36a7e754079414c5bbe1a44757": {
    "soal": "The following ticker code used in the Yahoo Finance Stock Data Widget (ORANGE)\n\n^DJI\n\n\nBelongs to the company,\n\n\n",
    "jawaban": "The correct answer is: Dow Jones Industrial Average"
  },
  "6aca00ad60a2c84fcbb08941ea4a4c4ff535d8e88b2d29c02cdffe38fd30c807": {
    "soal": "The visualization in the Network Explorer Widget functions similarly to the one used in the Scatter Plot widget. To select a subset of nodes, draw a rectangle around the subset. Shift will add ..................... new group. Ctrl-Shift (Cmd-Shift) will add to the existing group. Alt (Option) will remove from the group. Clicking outside the network will deselect the selection.\n\n\n\n",
    "jawaban": "The correct answer is: group"
  },
  "b3eb3c5542eb184e2a0cec7ecf6fde9025de4d9ebc4412822a63d6637b42532d": {
    "soal": "import pm4py\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\n# Import XES log\nlog = xes_importer.apply(xes_filename)\n# Cek jumlah kasus dan aktivitas\nprint(f\"Jumlah kasus: \")\nactivities = set()\nfor trace in log:\nfor event in trace:\nactivities.add(event['concept:name'])\nprint(f\"Jumlah aktivitas unik: {len(activities)}\")\nfrom pm4py.algo.discovery.dfg import algorithm as dfg_discovery\nfrom pm4py.visualization.dfg import visualizer as dfg_visualization\n# DFG berdasarkan frekuensi\ndfg = dfg_discovery.apply(log,\nvariant=dfg_discovery.Variants.FREQUENCY)\n# Tampilkan visualisasi\ndfg_vis = dfg_visualization.apply(dfg, log=log,\nvariant=dfg_visualization.Variants.FREQUENCY)\ndfg_visualization.view(dfg_vis)\nWhich example file name is NOT provided in the original PM4Py example?\n{\n=Example_Process.xes\n-BPIC_2012_A.xes\n-BPI_Challenge_2019.xes\n-PM4PY-running-example.xes\n-training_log_8.xes\n-Cross_Hospital.xes\n}",
    "jawaban": "The correct answer is: len(log)"
  },
  "fc5c177f118e4dd036c544226b38e2fadc29114e41203f3b7a95f468e2169814": {
    "soal": "What is the purpose of batch normalization in a neural network?",
    "jawaban": "The correct answer is: To normalize the inputs of each layer to improve training stability and performance."
  },
  "69015906ea9f5fabad8d3784477f6e4475dc8cf64a0f824fb2a8e7ed26e75d7c": {
    "soal": "How does the LLM adapt to follow-up prompts?",
    "jawaban": "The correct answer is: By using updated context"
  },
  "4d9fd11bcbcedf2ce77f4d06722bcbb94f21ff2759e090f10715374ada547db0": {
    "soal": "A pivot table is a statistical table that summarizes data from a larger table. This summary might include associations, averages, or other statistics, with the pivot table grouping them together in a meaningful way. Pivot tables are a technique in data processing.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "dc9f05a7d3c4afcb2299513188456e1f5c6f03459e49b6e63d5026ec54485b79": {
    "soal": "Column attribute names can be preceded by a label followed by a hash. Use c for class and m for meta attributes, i for ignoring columns, w for weight (of a column), and C , D , T , S for continuous attribute types ............ , discrete, time, and string. Example: C#mph, mS#name, i#dummy.\n\nWhere ........ is,\n",
    "jawaban": "The correct answer is: discrete"
  },
  "5e6dcf77aeb48361b95162b8d94cca200a1cae97715486e89d5edaa1b6eebd8f": {
    "soal": "Which device is NOT checked using tf.config.list_physical_devices()?",
    "jawaban": "The correct answer is: RAM"
  },
  "26b946fd92a709e0c87d24c9b27792159c2b5b266a1eb37130591913d34a798a": {
    "soal": "For supervised problems, where data instances are described with class labels, we want to know which feature is the most informative. The Rank widget provides a table of features and their informativeness scores, and supports manual feature selection. In the workflow, we use it to find the top two features (out of 79 initial features from the selected dataset) and display their line plot, as shown in the image below.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "882ae14496804648c93830ad53b7c3ad1a3479cfb4485bb1f43261d5258beb02": {
    "soal": "\n\n\n\nIn this example, we are using the lastfm dataset, which can be loaded into the Network File widget under Browse documentation networks. The network node represents musicians, marked by the genre they play, the number of albums produced, and so on. ............... is the number of listeners on LastFm.\n\nThe entire dataset is visualized in the Network Explorer Widget. In this widget, we remove the coloring and adjust the node size to match the number of albums. Then, we select several nodes from the network. We can observe the selection in the Network Explorer Widget (1).\n\n\n\n",
    "jawaban": "The correct answer is: edge"
  },
  "46a70b26c87a593ae7c8d3e38e381efb1cf66ffd9de726f88708fb0b2ca29677": {
    "soal": "The Scatter Plot widget can perform visualizations using lines with exploratory analysis and intelligent data visualization enhancements.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "e5e7423ef5bcab029c924011123019cd22d10bc5dab2be0237bcb271c26b5079": {
    "soal": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n# Contoh: Memuat dataset dari file CSV\n# Gantilah 'data.csv' dengan path ke dataset Anda\ndf = pd.read_csv('data.csv')\n# Memisahkan fitur (X) dan target \n# Asumsikan kolom terakhir adalah target\nX = df.iloc[:, :-1]\ny = df.iloc[:, -1]\n# Membagi dataset menjadi data latih dan data uji (80% : 20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model Decision Tree\nmodel = DecisionTreeClassifier()\n# Melatih model dengan data latih\nmodel.fit(X_train, y_train)\n# Melakukan prediksi pada data uji\ny_pred = model.predict(X_test)\n# Menghitung akurasi\nakurasi = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model: ')\n# Menampilkan laporan klasifikasi yang mencakup presisi, recall, dan F1-score\nprint(classification_report(y_test, y_pred))\nWhich function is used to calculate the accuracy of the model's predictions?\n{\n~precision_score()\n~recall_score()\n=accuracy_score()\n~f1_score()\n~roc_auc_score()\n~mean_squared_error()\n}",
    "jawaban": "The correct answer is: akurasi:.2f"
  },
  "3436153010ce1057753efc86e1237f1b061d9ff023418097e6224f04c864f4c9": {
    "soal": "The Sentiment Analysis widget allows basic sentiment analysis of corpora. It currently works for English and uses two techniques supported by NLTK - Liu Hu and Vader. Both techniques are lexicon-based. Liu Hu calculates a single normal score for sentiment in text (negative score for negative sentiment, positive for positive sentiment, and 0 for neutral), while Vader generates scores for each category (positive, negative, neutral) and adds a total sentiment score called the total score.\n\n\n\n\nLiu Hu score\nVader score\n",
    "jawaban": "The correct answer is 'False'."
  },
  "282d4df24d86a009a1302f4d08f0e452d4d8408574688a93b1520b9d398bb2c0": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#_xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nmap = pm4py.discover_heuristics_net(log)\npm4py.view_heuristics_net(map)\nfile extension '.xes' is commonly used in:",
    "jawaban": "The correct answer is: Event logs for process mining -Spreadsheets -JSON databases -XML parsing -Image processing -Video streaming"
  },
  "04740cf1d0e7acb749e9812febce3c78d083a7861a2dc09aa64d12ab7460df35": {
    "soal": "If\n\n1 AND 1 = 1\n\n1 AND 0 = 0\n\n0 AND 1 = 0\n\n0 AND 0 = 0\n\nthen 1011 AND 1100 = 0000",
    "jawaban": "The correct answer is 'False'."
  },
  "e3f35a80347cf1f4bbafdc63dc6d0c816adf2f3a2e689eb2b4e6a1d487268434": {
    "soal": "Which loss function is typically used with a model for classification tasks?",
    "jawaban": "The correct answer is: SparseCategoricalCrossentropy"
  },
  "9094cb7a743ef2c2c6ee106516a073a9a4ba5e96f8eea31bf6f428de41daf214": {
    "soal": "The Distance Map widget can visualize the distances between objects. The visualization is the same as if we printed a table of numbers, but the numbers are multiplied with colored dots.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "2c506a584ea6ea82f163da5380bc7cd215cef30539bb8948fdd2ea920031e654": {
    "soal": "\n\n\n\n\n\n\n\n\nSupervised Learning techniques are generally divided into:\n\nAssociation.\nRegression.",
    "jawaban": "The correct answer is 'False'."
  },
  "2c8d6bfc3bb098bdb92a3c305d80c2a8146cee61759c2df8eeff55c439f4baf8": {
    "soal": "What does the LLM generate after processing the prompt?",
    "jawaban": "The correct answer is: Output/Answer"
  },
  "4d00aa1efdf321c3cd2d8ea4d45334dfa8280ebf55b1ca488e1141428b5e649f": {
    "soal": "This workflow example shows how Network Analysis can enrich the workflow. We used lastfm.net data as the input network from the Network File Widget and sent it to the Network Analysis Widget. We can decide to compute the degree, degree centrality, and ...............(1)................ closeness centrality at the node-level.\n\nWe can then visualize the network in the Network Explorer widget. In the ..............(2)............... widget, it colors with the best tags, like the default for this dataset. But now we can also adjust the node size to fit the degree centrality results. This is a good way to visualize network properties.\n\n\n\n\n",
    "jawaban": "The correct answer is: (2) \u2192 Network Explorer, (1) \u2192 centrality, (3) \u2192 node"
  },
  "e36fa6f9bbac0af9724c81c0403782d500d0fe7c1ab64ebee6fe5fd157e1e9ae": {
    "soal": "In the Text Preprocessing Widget, Information on preprocessed data, document count calculates and reports the number of documents in the input. The total token count calculates all tokens in the corpus. Unique tokens exclude duplicate tokens and report only unique tokens in the corpus.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "6a337e6bec7f72776c2ec4be7984cd180e2ba2db38c18d35d778a06368d5f383": {
    "soal": "The Calibrated Learner widget generates a model that calibrates the distribution of class probabilities and optimizes the decision threshold. This widget only works for binary classification tasks.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "22d1747d7aaa38e6f9e7447dbc6cf7fb3257d05db82b60885a6aa300c129b789": {
    "soal": "from google.colab import drive\ndrive.mount('/content/drive')\nCan you access shared drives (formerly Team Drives) using drive.mount() in Colab?",
    "jawaban": "The correct answer is: Yes, shared drives are accessible under '/content/drive/Shared drives/'."
  },
  "90c4c02c9c5874615084a2f2c221771553dfc960a85487e9715c75d194122f09": {
    "soal": "import pm4py\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\n# Import XES log\nlog = xes_importer.apply(xes_filename)\n# Cek jumlah kasus dan aktivitas\nprint(f\"Jumlah kasus: \")\nactivities = set()\nfor trace in log:\nfor event in trace:\nactivities.add(event['concept:name'])\nprint(f\"Jumlah aktivitas unik: {len(activities)}\")\nfrom pm4py.algo.discovery.dfg import algorithm as dfg_discovery\nfrom pm4py.visualization.dfg import visualizer as dfg_visualization\n# DFG berdasarkan frekuensi\ndfg = dfg_discovery.apply(log,\nvariant=dfg_discovery.Variants.FREQUENCY)\n# Tampilkan visualisasi\ndfg_vis = dfg_visualization.apply(dfg, log=log,\nvariant=dfg_visualization.Variants.FREQUENCY)\ndfg_visualization.view(dfg_vis)\nThe `log` variable in PM4Py stores:\n{\n=Imported event log data\n-Only performance results\n-Graph visualizations\n-Processed CSV data\n-Exported XML data\n-Python installation logs\n}",
    "jawaban": "The correct answer is: len(log)"
  },
  "a67b265644e065374145c7cacb8f1c188decfa37c83e7b838be9661a5df1a5f7": {
    "soal": "Orange uses its own data format, so it cannot handle files in Excel, comma- or tab-delimited data file formats.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "7034634dc7f44ba14b4e0852992d2f1995b2f692af58a5168c50e432b36b6e03": {
    "soal": "If\n\n1 AND 1 = 1\n\n1 AND 0 = 0\n\n0 AND 1 = 0\n\n0 AND 0 = 0\n\nThen 111 AND 100 = ......... (binary)",
    "jawaban": "The correct answer is: 100"
  },
  "f77b7f0a71a9e0699a49aab26208ad129cb33931bc4c62a23c2e144ab939d77f": {
    "soal": "If\n\n1 AND 1 = 1\n\n1 AND 0 = 0\n\n0 AND 1 = 0\n\n0 AND 0 = 0\n\nThen 111 AND 010 = ......... (binary)",
    "jawaban": "The correct answer is: 010"
  },
  "14663895964ff3572bc46b803a50339ebe988248feb92cc524a1e8b534b97bb4": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nWhat is the primary purpose of the Dropout layer in the model?",
    "jawaban": "The correct answer is: To reduce overfitting by randomly setting 20% of the input units to 0 during training."
  },
  "2ab5c32071c07d3b5f4034583a36e28afa7598a6074fdfa5dd2125f0ada62965": {
    "soal": "import tensorflow as tf\nfrom tensorflow.keras import layers, models\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nmodel = models.Sequential([\nlayers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(64, (3,3), activation='relu'),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(128, (3,3), activation='relu'),\nlayers.Flatten(),\nlayers.Dense(128, activation='relu'),\nlayers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5, batch_size=64)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nWhat optimizer is used for compiling the model?",
    "jawaban": "The correct answer is: Adam"
  },
  "92df67b59aec46163c928199ba7e061031b14cf5c8a3f553474b46e0750e2b72": {
    "soal": "If\n\n1 AND 1 = 1 and 1 OR 1 = 1\n\n1 AND 0 = 0 and 1 OR 0 = 1\n\n0 AND 1 = 0 and 0 OR 1 = 1\n\n0 AND 0 = 0 and 0 OR 0 = 0\n\nThen in binary\n\n( 001 AND 100 ) OR 111 = ..... (binary)",
    "jawaban": "The correct answer is: 111"
  },
  "08fcc96c267ad808868fab34f1045ba51ca1a65391ce1d133ab4e1b054d6775f": {
    "soal": "The Import Documents widget takes text files from a folder and creates a corpus. The Import Documents widget can read .txt, .docx, .odt, .pdf, and .xml. If there are subfolders in the folder, they can be used to label classes.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "6b6340e3efa3c54b4c38b681209dc286ec53325296f1fae6545046b10e1bed0c": {
    "soal": "Instalasi Orange3 yang\u00a0 baik di Ubuntu 18.04 dapat menggunakan perintah\n\n\u00a0pip install orange3\n",
    "jawaban": "The correct answer is 'False'."
  },
  "cd583b6009f5b0c9cde3143b60a6a0792e9539f827687488e029ce921c6edc60": {
    "soal": "The Tree algorithm is a simple algorithm that can separate data into nodes based on class purity (category/class purity). The Tree algorithm is the predecessor of the Random Forest algorithm. The Tree widget in Orange is designed in-house and can handle both discrete and ........................\n",
    "jawaban": "The correct answer is: continuous"
  },
  "43ffe6a8803435abf1d0a2094d1fb258dbf09eef9e377e2d64ac1bfa12bdf4ee": {
    "soal": "The main programming language used to create Orange3 data mining is\n",
    "jawaban": "The correct answer is: python"
  },
  "3443b2912dd5e61fee790b25466c13e6d73ac921e9cce646383cd909a7aaaa90": {
    "soal": "In K-Fold Cross Validation, what does 'K' represent?",
    "jawaban": "The correct answer is: The number of subsets the dataset is divided into"
  },
  "4f8450ea1870fd8b18f89316010f16a9b743ccb9ccc74873d667b7f682967367": {
    "soal": "As an example of using kNN for classification, we use the iris dataset. We compare the results of k-Nearest Neighbors with the default Constant model, which will predict the majority class. It appears that kNN is worse.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "71d30ed9cb75e391b287f37bca3e6c679810c3575185d82767aaa90ee1163134": {
    "soal": "When there is no data in the input, the Corpus widget reads the text corpus from a file and sends the corpus instance to the output channel. The history of the last opened file is saved in the Corpus widget. The Corpus widget also contains a directory with sample corpora that were previously installed with the add-on. The widget can read data from Excel files (.xlsx), comma-separated (.csv), and tab-delimited (.tab).\n",
    "jawaban": "The correct answer is 'True'."
  },
  "f0b66971b70e9a67271bb7f8cf94eaf6677fe7c64d0374ddd26f267b9d80cba9": {
    "soal": "Widget Import Images allows us to import documents from a directory.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "5a4c7e1db52c75e25d3bb5e78f4a8513b2758d0b9b55a09f6051042a581a7de1": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nx_train = np.random.rand(1000, 10)\ny_train = np.random.randint(2, size=(1000, 1))\nmodel = keras.Sequential([\nkeras.layers.Dense(64, activation='relu', input_shape=(10,)),\nkeras.layers.Dense(32, activation='relu'),\nkeras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer='adam',\nloss='binary_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=10, batch_size=32)\nWhat is the primary advantage of using TensorFlow in Google Colab?",
    "jawaban": "The correct answer is: It comes pre-installed"
  },
  "a3241f56cdaeef99bf19575eaf485a9e8d7832ed642a389b9070f5faba33f623": {
    "soal": "The easiest way to use the Venn Diagram Widget is to select a data subset and find matching examples in the visualization. We use the breast-cancer dataset to select two subsets with the Select Rows widget \u2013 the first subset is breast cancer patients aged between 40 and 49 years, and the second subset is patients with tumor sizes between 20 and 29. The Venn Diagram helps us find examples that meet all three criteria, which can be found at the intersection of the three circles.\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "f62697030be669acf07bbcf8274281a9a3674de407af360bdd95a43d57f2f195": {
    "soal": "The Distances Widget cannot interact with other Orange add-ons. The Distance Matrix Widget can be fed into the Network from Distances Widget (Network add-on) to turn the matrix into a graph and into the Duplicate Detection Widget (Text add-on) to find document duplicates in the corpus.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "9e5d7dffc554b37ca0a1406e06245d29c548519b24f37464f7710d6f481674ed": {
    "soal": "In TensorFlow, what is the default data type for a tensor?",
    "jawaban": "The correct answer is: float32"
  },
  "4e2cd4c16bc0ce6990c300943dd514cefe8fd2efb7d3dd02fe12e087ac29aa68": {
    "soal": "After installing 'nvidia-docker2', which command is used to restart the Docker service?",
    "jawaban": "The correct answer is: sudo systemctl restart docker"
  },
  "97605fcc1639ccc6a7602fa71c4ea45611111330ac425647b5ea04e2a7ec6c76": {
    "soal": "\n\n\nDalam contoh di atas, kita menggunakan dataset zoo dan mengirimkannya ke CN2 Rule Induction. Kita bisa me-review dan meng-interpretasi model yang dibuat dengan CN2 .....................\u00a0 widget.\n\n\n\n",
    "jawaban": "The correct answer is: Rule Viewer"
  },
  "2497270a90f9e71241978923ffb22cd9384be3efea3ddb5cc049fbd7d9f55907": {
    "soal": "In 2012, when Harvard Business Review called it \"The Sexiest Job of the 21st Century\", the term \"data science\" became a buzzword. Data Science cannot be interchanged with previous concepts such as business analytics, business intelligence, predictive modeling, and statistics.",
    "jawaban": "The correct answer is 'False'."
  },
  "8d6ca79b7cb713ed767883d0f522a4ae24201c9ec9fee884f95319858ed61b84": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nWhich function is used to load the MNIST dataset in TensorFlow?",
    "jawaban": "The correct answer is: tf.keras.datasets.mnist.load_data()"
  },
  "de1084870f287ca43a90f0700809078880eb04a6e274f5bae966af42c2d37566": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#-xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\ndfg, start_activities, end_activities = pm4py.discover_dfg(log)\npm4py.view_dfg(dfg, start_activities, end_activities)\nWhat variable stores the start activities?",
    "jawaban": "The correct answer is: start_activities -end_activities -log -dfg -xes_filename -pm4py"
  },
  "ceea75e7102ddce63c9f44f67c0367ed4390e953abc12e0bb2f5e76ac9779e94": {
    "soal": "What does Precision measure in a classification model?",
    "jawaban": "The correct answer is: The proportion of predicted positives that are truly positive."
  },
  "b99a316d237c20f7321e55c947949c17b35ecdaaf56528a51896d79ec6937258": {
    "soal": "The selected data in the first Data Table widget is passed to the second .............. widget. Observe that we can choose which dataset to view (iris or glass). Switching from one dataset to another changes the data instance selection that is communicated if Commit on any change is selected.\n\n\n",
    "jawaban": "The correct answer is: Data Table"
  },
  "99e791e02ffdacd3dcc70144e72617aa880951da0dbd5644a3928738a1b343b4": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#-xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\ndfg, start_activities, end_activities = pm4py.discover_dfg(log)\npm4py.view_dfg(dfg, start_activities, end_activities)\nWhich function is used to read XES files?",
    "jawaban": "The correct answer is: pm4py.read_xes() -pm4py.read_csv() -pm4py.import_xes() -pm4py.load_file() -pm4py.open_file() -pm4py.read_json()"
  },
  "1dbbdc3c9e04ec06a6930aa9834143acf4d4aeca36fc844bf155fca674af906b": {
    "soal": "The Distances Widget also works well with other Orange add-ons. The Distance Matrix Widget can be fed into the Network from Distances Widget (Network add-on) to turn the matrix into a graph and into the Duplicate Detection Widget (Text add-on) to find document duplicates in the corpus.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "d3cf4eaca9223be40836c5fddcd2ca5631e993c13baaf1fca65f31f1e314a168": {
    "soal": "The workflow clusters data items in the iris dataset by first checking the distances between data instances using the Distances widget. The Distance Matrix is passed to the Hierarchical Clustering widget, which creates the ........... We can select different parts of the .......... to further analyze the related data.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: dendrogram"
  },
  "23afa0a3f01c07f77e23b0e0358d3a2c5cdd9b3c29aa3742fcae276b4bfe9cc4": {
    "soal": "Why is it important to ensure data quality before training a machine learning model?",
    "jawaban": "The correct answer is: To ensure the model's accuracy and reliability"
  },
  "3048bb4739ac53edabed6a60d459da359664da25175920577e0b971453ad68c5": {
    "soal": "Inductive Visual Miner ensures the generated models are:",
    "jawaban": "The correct answer is: Deadlock-free"
  },
  "fcf787cb3ad2e14158ea1f7355f9ce984593a8b2067f6c1873085efd07af07a5": {
    "soal": "RapidMiner, formerly known as YALE (Yet Another Learning Environment), is closed-source software. This software serves as a solution for performing data mining, text mining, and predictive analysis.\nRapidMiner uses various descriptive and predictive techniques to provide insights to users, enabling them to make the best decisions. RapidMiner is written in Java, allowing it to work on all operating systems.",
    "jawaban": "The correct answer is 'False'."
  },
  "1cbdf2f22eb9f55ac5d1583c987bc6c3ff00e829aff224c1ecee1f40a123a7bf": {
    "soal": "What will the output of `print(tf.__version__)` likely display?",
    "jawaban": "The correct answer is: The installed TensorFlow version"
  },
  "8188f6e28a817245adad050c4f06d79e72e932fba231802126125585de3fc243": {
    "soal": "Widget .............. can present information about dataset size, features, target, meta attributes, and location.\n",
    "jawaban": "The correct answer is: Data Info"
  },
  "a6d6fb0099aeb45702f302cc454fe59ce27acb9db2aa9f4e1613cb935b7fd67b": {
    "soal": "In the ORANGE example below, we can predict the classification of a text. The classification model is obtained using the logistic regression learner with training data from the Corpus Grimms. The text prediction from Corpus Andersen is performed by the ............... widget.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Prediction"
  },
  "21aa1bc11cfb13c18d348ec0ee2353ab81d8045e8a737dcdc13501e06c1817c6": {
    "soal": "\n\n\nExamples of professions in traditional methods in data science include,\n\n\n\n\n\n\n",
    "jawaban": "The correct answers are: Data Scientist, Data Analyst"
  },
  "076e6a00de85c5244496f8d0865434ad3c69f392edb650fcd5f75b85b8daf548": {
    "soal": "If\n\n1 AND 1 = 1 and 1 OR 1 = 1\n\n1 AND 0 = 0 and 1 OR 0 = 1\n\n0 AND 1 = 0 and 0 OR 1 = 1\n\n0 AND 0 = 0 and 0 OR 0 = 0\n\nThen in binary\n\n( 100 AND 110 ) OR 111 = ..... (binary)",
    "jawaban": "The correct answer is: 111"
  },
  "58c909ddd3910713531bab65018749fcadafaf798f30cc398ad3f33f8d3f30fe": {
    "soal": "Word Cloud can be built from documents that we have. In the text mining toolbox, there is an Import Documents widget to read document files. Before displaying it as a word cloud, it is not good to pass it through the Preprocess Text widget first, to reduce unnecessary words, such as connecting words, etc. Next, we can pass it through the Bag of Words widget first, or go directly to the Word Cloud widget to display it.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "0cd69c4b24b894eeeafa5c848a32e153d3f7f366205f444af34298bf588d512f": {
    "soal": "According to the official Apache website, Apache Spark is a framework used to analyze big data. Data processing through the Apache Spark framework is considered faster than other frameworks like MapReduce because the data is processed in-memory. The growth of data at the terabyte level produced daily requires solutions that provide real-time analysis with high speed, one of which is using Apache Spark.\nThe advantages of Apache Spark include:\n- Faster performance compared to traditional data processing frameworks.\n- Easy to use, data processing applications built with Spark can be written in programming languages like Python, R, Java, and Scala.\n- Equipped with SQL Library, Streaming, and Graph Analysis, which makes it easier for data processing and analysis.",
    "jawaban": "The correct answer is 'True'."
  },
  "b0f3399106f227225e0fb5977d589182ec31a874ea95719bd3005b8c827fb7a0": {
    "soal": "What is the primary function of the `ollama serve` command?",
    "jawaban": "The correct answer is: Starts the Ollama server on your local system."
  },
  "5e37fb69b702103472dcc4e453021600d064fc8cd5ced6a11a988f3d9c0e7d4e": {
    "soal": "In the Distance Map widget, usually, a color palette is used to visualize the entire range of distances that do not appear in the matrix. This can be changed by setting low and high thresholds. In this way, we ignore the distance differences outside this range and visualize the interesting part of the distribution.\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "ac0d331e67486567877e198fb8aa1a2515b263aac398f9b0f1517927b62e176c": {
    "soal": "In TensorFlow, what is the purpose of the 'Variable' class?",
    "jawaban": "The correct answer is: To maintain state across executions of a graph"
  },
  "5209c95f28863a4b93addf93def3f3ee4866d8b00d27029de67e185f8dc7a6fe": {
    "soal": "According to the official Zoho website, Zoho Analytics is a complete, reliable, and scalable analytics platform. Developers and system integrators (\nAnswer Question 15\n) can use this platform to develop and implement custom analytics applications and integrations.\nAnother advantage of Zoho Analytics is its user-friendliness, making it easy for users to upload and control data. By using Zoho Analytics, data practitioners can create multifaceted and custom dashboards. This platform is easy to use and implement.",
    "jawaban": "The correct answer is: SI"
  },
  "5f9bc03efa47b150f8a2117986a9f907f41fa0e02d7683da75a61129cbe7977b": {
    "soal": "Which of the following fields can use LLMs?",
    "jawaban": "The correct answer is: Sentiment analysis"
  },
  "812e53b67f9d06794a1b8b7d94c05f1594e1fd911143d57bafbe159d7290a79e": {
    "soal": "Proper Orange3 installation on Ubuntu 18.04 can use the command\n\n\u00a0pip install orange3\n",
    "jawaban": "The correct answer is 'False'."
  },
  "74af375d1f4768f07c18a37de4a29600fb16c763c6c80cdf83141d32c6bdad12": {
    "soal": "Contoh penggunaan Widget......................\u00a0 untuk task regresi di perlihatkan pada workflow di bawah ini. Workflow ini menunjukkan cara menggunakan Learner output. Untuk tujuan contoh ini, kita menggunakan dataset housing. Kita memasukkan model prediksi\u00a0 ........................ ke dalam Predictions dan mengamati nilai yang diprediksi.\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: kNN"
  },
  "794201c367e6766bc75b7ca100fa4963d03326a18b1d73d0d1411440ca0cf078": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\n# xes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\nxes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nprocess_tree = pm4py.discover_bpmn_inductive(log)\nbpmn_model = pm4py.convert_to_bpmn(process_tree)\npm4py.view_bpmn(bpmn_model)\nWhich object is created immediately after loading the XES file?",
    "jawaban": "The correct answer is: process_tree"
  },
  "ee7a4d90a5ff45e97d0e57b7da8b320fa2c613f266e3df9f31ed4c0fae155cbd": {
    "soal": "The basic unit (smallest) that forms binary data inside a computer is called:",
    "jawaban": "The correct answer is: Bit"
  },
  "5ce7339e1fb14c7b11c6e976b32bdde927cee9f1a37a2d642ab8c4dc91d74c00": {
    "soal": "\n\n\nThe conclusion of classification errors (number of errors) in the above workflow can be read as\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: confusion matrix"
  },
  "8434cd52d76d00a58989a516b245ccae2693b003f826e79a5acaf9729a23eeda": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#-xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\ndfg, start_activities, end_activities = pm4py.discover_dfg(log)\npm4py.view_dfg(dfg, start_activities, end_activities)\nWhich filename is active by default in the given code?",
    "jawaban": "The correct answer is: BPIC_2012_A.xes -BPIC_2012_W.xes -BPIC_2012_O.xes -PM4PY-running-example.xes -excercise.xes -Cross_Hospital.xes"
  },
  "dddebefd2f1d4c3a63cc8703042de41e9c942f651aaa5c67bbe12d578ded414a": {
    "soal": "In the following workflow example, using the zoo dataset and creating a clustering workflow with Distances and Hierarchical Clustering. Now set the threshold for cluster selection (click on the ruler above). Connect the Box Plot to Hierarchical Clustering, check Order by relevance, and select Cluster as an uppergroup. This will sort the attributes by how well they define the selected subgroup, in our case, a cluster.\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "ced7ff69c4b5365b64d8bae29ca4f0f5947142163f76b85868d77637702be8cf": {
    "soal": "Widget Network Analysis calculates statistical summaries from the node-level and graph-level for the network. Widget Network Analysis will output the network with the computed statistics and an extended item data table (only node-level index).\n",
    "jawaban": "The correct answer is: statistics"
  },
  "ab36d4fa49aa3480e9d2d370bd2694450c4ad2255e9a1d654c830650f954ac8b": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#-xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\ndfg, start_activities, end_activities = pm4py.discover_dfg(log)\npm4py.view_dfg(dfg, start_activities, end_activities)\nWhich library is used in this code?",
    "jawaban": "The correct answer is: pm4py -pandas -sklearn -numpy -matplotlib -seaborn"
  },
  "538eee7e008df80a618c119718795b8af640ee343a884bacdb7549374af804cd": {
    "soal": "Widget Import Images scans a directory and returns one row per image retrieved. The columns contain the image name, directory path to the image, width, height, and image size. The column with the directory path to the image is then used as an attribute for visualization and Hierarchical Clustering.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "685fa2f2a1355cfa574f54d92efb753d8258fc4bd291d0840ad7386d34696824": {
    "soal": "Widget Save Data does not save data every time a new signal is received in the input because it would continuously (and mostly unintentionally) overwrite the file. Instead, data will not be saved after a new file name is set or the user presses the Save button.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "7d103ba932fc05f808fc22f54c5fca2f62052d6ae44193b353c411fba6cf52ed": {
    "soal": "The Image Embedding widget in Orange is very easy to use. Technically, Orange will send the image to the server, where the server will push the image through a pre-trained ................(1)................., such as Google Inception v3. Deep networks are often trained for a specific purpose. Inception v3, for example, can classify images into one of 1,000 ............(2)................ image classes. We can ignore the suggested classification and instead consider the penultimate layer of the network with 2048 nodes (numbers) and use them for the image's vector-based representation.\n",
    "jawaban": "The correct answer is: (1) \u2192 deep neural network, (2) \u2192 class"
  },
  "766e8efce53a0f7d9c15deac9d71a0896a3f7a8f957e8bf8e7f8339d0711c56b": {
    "soal": "What is the main function of LLMs?",
    "jawaban": "The correct answer is: Understanding and generating human-like text"
  },
  "5f7f889f6a930b478bcdb8939492d86a42dbd88f9dfec0c3f385dabfc3dee88b": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#-xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\ndfg, start_activities, end_activities = pm4py.discover_dfg(log)\npm4py.view_dfg(dfg, start_activities, end_activities)\nHow is the Directly-Follows Graph (DFG) generated?",
    "jawaban": "The correct answer is: pm4py.discover_dfg(log) -pm4py.create_dfg(log) -pm4py.visualize_dfg(log) -pm4py.read_dfg(log) -pm4py.generate_graph(log) -pm4py.export_dfg(log)"
  },
  "cdd905d347e0b2453feb3c6f5f77a1f6a66ee043c61860a8c88e60b3e8c3aa5a": {
    "soal": "\n\n\nThe workflow above tests/evaluates CN2 Rule Induction and Tree in the Widget Test & Score. It appears that Tree is better.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "66985dbcbd49a7055487a56390e68a0d46267eb6652117d0f03332c3f44107db": {
    "soal": "In a dataset with 90% positive and 10% negative samples, what issue might arise during model training?",
    "jawaban": "The correct answer is: Model bias towards the majority class"
  },
  "b08f0ec20188b4a7fea6fd4746ede76d1b9e759b86a3b338f76f298646e059e3": {
    "soal": "What is a training set in machine learning?",
    "jawaban": "The correct answer is: A subset of data used to train a model."
  },
  "da8b0d78afbf9f9216a49f915140cc7f77914a700fc381cb11a155d6cc74a7af": {
    "soal": "Now, each core Orange widget can function/interact with Text smoothly, so you can mix and match widgets at will. Previously, it was not possible to pass output from Select Columns (data table) to Preprocess Text (corpus), but this is no longer an issue.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "bf3d34b979f1a372161f70ed9a546492dde9769066fd2fefa6af940ff7b19de9": {
    "soal": "Most visualizations in Orange are interactive. In the workflow below, the Scatter Plot widget, for example. Double-click the icon to open it, and click-and-drag to select multiple data points from the plot. The selected data will automatically enter the Data Table widget. Double-click to inspect which data was selected. Change the selection and observe the changes in the Distance Map widget.. This works well if both widgets are opened.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "fee92bd6116bd6afa703f7e5de08c5e5e26404d7e5300bc34605dd2a4161b9f1": {
    "soal": "import numpy as np\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n# Contoh data: nilai aktual dan prediksi\ny_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0]) # Nilai aktual\ny_pred = np.array([1, 0, 1, 0, 0, 1, 0, 1, 1, 0]) # Prediksi model\n# Menghitung confusion matrix\ncm = confusion_matrix(y_true, y_pred)\n# Menampilkan confusion matrix dalam bentuk teks\nprint(\"Confusion Matrix:\n\", cm)\n# Menampilkan confusion matrix dalam bentuk grafik\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.show()\nFungsi mana yang digunakan untuk menampilkan confusion matrix dalam bentuk grafik dalam pustaka scikit-learn?",
    "jawaban": "The correct answer is: ConfusionMatrixDisplay"
  },
  "2e290f551fead26b278bf437bb19d1f71f5f863bd46131ee7f279adac64809e0": {
    "soal": "The Venn Diagram widget can also be used to explore different prediction models. In the following example, we analyze 3 prediction methods, namely Naive Bayes, SVM, and Random Forest, based on their classification errors on data instances.\n\nBy selecting misclassification in the three Confusion Matrix widgets and sending them to the ..............., we can see all misclassification examples visualized by the method used. We then open the Venn Diagram widget and select, for example, misclassified instances identified by all three methods. This is represented as the intersection of the three circles. Click on the intersection to view these two examples highlighted in the Scatter Plot widget. Try selecting different parts of the diagram to see how the scatter plot visualization changes.\n\n\n\n\n",
    "jawaban": "The correct answer is: Venn Diagram"
  },
  "a32a3a1636a650d23f8e82e52c8d17b7a33b556265d851ea1b636e16303b0f5b": {
    "soal": "A heat map is a graphical method for visualizing attribute values based on class in a two-way matrix. Heat maps can only be used on datasets that contain continuous variables. The values are represented by colors: the higher the value, the darker the color. By combining class and attributes on the x and y axes, we see where which attribute values are strongest and weakest, allowing us to find typical features (discrete) or value ranges (...........................) for each class.\n\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: continuous"
  },
  "2cc27048bfb7469ceb45f93c718b9a38fc5b6dfaafe773ee1d42ee23e3bc8da6": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nWhen is the Dropout layer active during the model's lifecycle?",
    "jawaban": "The correct answer is: During training only"
  },
  "f5fc320b46d7382373941e4b0339767ff5e50e134f06e862733b9fa8a8b4c037": {
    "soal": "In the ...................(1)..............., to fetch data, select the folder icon on the right side of the widget. Choose the folder that we want to use as the corpus. After loading is complete, we will see how many documents were successfully retrieved by the Import Documents Widget. To observe the obtained corpus, connect the Import Documents widget to the ...............(2)................ Widget. In this workflow, we are using a collection of President Kennedy's speeches in plain text format.\n\n",
    "jawaban": "The correct answer is: (2) \u2192 Corpus Viewer, (1) \u2192 Import Documents"
  },
  "8739e108e74ecc28f42ec582ead0239a24268e353be116ee7edde1c0d316a7f4": {
    "soal": "What is a key advantage of using K-Fold Cross Validation?",
    "jawaban": "The correct answer is: It provides a more accurate evaluation by ensuring each data point is used for both training and validation"
  },
  "b4aae8fcfc656b96cfa5ccefe3d5b132e0dc7d04b1da3f80fdced2d6a32f51f7": {
    "soal": "The result of 8 (decimal) AND 3 (decimal) is",
    "jawaban": "The correct answer is: 0"
  },
  "e66b88e535ae0a65af4945dbf8b763a414c70c822f1b439352082c33f5092ee3": {
    "soal": "The Tree algorithm is a simple algorithm that can separate data into nodes based on .................(class purity). The Tree algorithm is the predecessor of the Random Forest algorithm. The Tree widget in Orange is designed in-house and can handle both discrete and continuous datasets.\n",
    "jawaban": "The correct answer is: class purity"
  },
  "ff1fecfbc91a750ebf25420edfd9be7e8d0413193d7588c90d0a0ecd46a5df3b": {
    "soal": "The Dataset widget takes the selected dataset from ...........(1)............. and sends it to the output. The file is downloaded to local memory and is immediately available even without an internet connection ............(2).............. Each dataset is accompanied by a description and information about the data size, the number of instances, the number ...........(3).............., target, and tags.\n",
    "jawaban": "The correct answer is: (2) \u2192 internet, (3) \u2192 variables, (1) \u2192 server"
  },
  "f5c3bcd9c367278da2f2f29d3b80967639e8ec091437811bb288e70c9ac2ee71": {
    "soal": "To group based on similarity, we can measure the distance between images. The numerical representation of the images obtained from the Image Embedding widget can be used to measure the distance using the Distances widget. Typically, cosine distance works best for images, but you can experiment on your own. We then send the distance vector to Hierarchical Clustering to visualize the similar pair in the dendrogram.\n\nThe result is that all animals are correctly grouped together. In the dendrogram of the clustering results, we do not see the animal images. We can use the Image Viewer widget to view the images.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "8cc85a4ae44a5010523622db92e0f534f88c2bc30294bad0d7aedd57a854e825": {
    "soal": "import pandas as pd\nfile_path = \"/content/drive/MyDrive/your_file.csv\"\ndf = pd.read_csv(file_path)\nprint(df.head())\nWhich function provides a concise summary of a DataFrame, including the number of non-null entries in each column?",
    "jawaban": "The correct answer is: df.info()"
  },
  "a0c255a55dd730fdee34eb0f29d856660177c6dc0ce52b99fa1c6351a74dc85b": {
    "soal": "Which command initiates the Directly-Follows Graph visualization in PM4Py?",
    "jawaban": "The correct answer is: dfg_visualization.apply()"
  },
  "a5436c5abceae1f64bdcc644526cd514ea2e902e53f1aca6507637097f1256fe": {
    "soal": "Alpha Miner is used primarily with:",
    "jawaban": "The correct answer is: Petri Net"
  },
  "dfb98825222ba890a93e6d838149366e8593ab027d20bc1c1ab09dfd5ee9524f": {
    "soal": "In the Line Chart widget, we can input forecast output signals from the forecast model. The forecast is drawn with a dashed line and confidence intervals as the range area.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "904193f135bb6198d08b8a5e5991791095f86a75bedfa3a95d5198d75478a6f5": {
    "soal": "\n\n\nIn the machine learning model creation process, data is usually split, for example, 80%:20%, where 80% is used for the training dataset to obtain the model, while the remaining 20% is used for the test dataset to evaluate the model's performance.\n\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "1abd9a0a972cba1e9db47db95aecbeecc082737cd7e621658546b8b80c3bd5f0": {
    "soal": "The workflow clusters data items in the iris dataset by first checking the distances between data instances using the Distances widget. The distance matrix is passed to the Hierarchical Clustering widget, which creates a radiogram. We can select different parts of the radiogram to analyze related data further.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "7cb986c16d29ca456493ccc34be638a335c32911e5e442b726c1be2e94481e77": {
    "soal": "Each core Orange widget cannot function/interact with Text, so you cannot mix and match widgets at will.\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "d6aec80b1e075c8a67b5d0453c25d80fe6b7303e6b3b4f5b70e76a69032c22a0": {
    "soal": "The .............(1) ................. widget in Orange is very easy to use. Technically, Orange will send the image to the server, where the server will push the image through a pre-trained deep neural network, such as ..............(2).............. Inception v3. Deep networks are often trained for a specific purpose. Inception v3, for example, can classify images into one of 1,000 image classes. We can ignore the classification suggestions and instead consider the penultimate layer of the network with 2048 nodes (numbers) and use them for the image's vector-based representation.\n",
    "jawaban": "The correct answer is: (1) \u2192 Image Embedding, (2) \u2192 Google"
  },
  "a1f34e4a54146ff22268078ce18c7bd783b2c14a73a658ee441160e7d13f71c5": {
    "soal": "Data Science is an interdisciplinary field that uses methods, processes, algorithms, and scientific systems to extract knowledge and insights from data in various forms, both structured and unstructured, similar to data ........................... .\n",
    "jawaban": "The correct answer is: mining"
  },
  "b3a77286bf2701f230e5489dde55c696f5846838b701a6cf87a9113cbd694565": {
    "soal": "The Calibration Plot widget can show the match between predicted .................. from the classifier and ................... actual class probability.\n",
    "jawaban": "The correct answer is: probability"
  },
  "6cf3c09906d54df516f9b52fa222e1acea8b05e7d8956a06848067e5cb962b9f": {
    "soal": "What is one of the main benefits of using TensorFlow in Google Colab?",
    "jawaban": "The correct answer is: It comes pre-installed"
  },
  "328eea122b99340657a80a73bd5b0ee95cbf1c4b0958ffff2e95354d1589127b": {
    "soal": "................. Table can help us to collect and transform data. This workflow takes Kickstarter projects and aggregates them by month. We can check the frequency of projects published per month and observe the differences between funded and unfunded projects.\n\n",
    "jawaban": "The correct answer is: Pivot"
  },
  "9a3b38b516288b7bbdf046cfb286f64db0500bc6ecd3edb7948cce40b7b75ae7": {
    "soal": "perform the AND operation on the following IP address\n\n192.168.123.245 AND 255.255.0.0\n\nthe result is\n\n\n",
    "jawaban": "The correct answer is: 192.168.0.0"
  },
  "c5e3c652478301d3f523e1205ada8e90f86e2ee3302890a98b8cc6ee640a41a8": {
    "soal": "Some formats supported by ORANGE3 (based on its add-ons):\n\n\n\n\n............................... : Widget File\n",
    "jawaban": "The correct answers are: Excel (.xlsx), tab-delimited (.txt) plain, comma-separated file (.csv), URL"
  },
  "cbeb767ca789826b5267c9c052487d3859d75a5350ddc0b3938bcbe1405d35de": {
    "soal": "Line plot, a type of plot that displays data as a series of .................., connected by straight line segments. Line Plot is used for numeric data. Meanwhile, for categorical data, Line Plot can be used for grouping data points.\n",
    "jawaban": "The correct answer is: points"
  },
  "155a0bd23fc4b7667e16c5f4d7a79979334ee45096d9342e179672e1a28753f7": {
    "soal": "Match the ticker code used in the Yahoo Finance Widget (ORANGE) with the actual company name.\n",
    "jawaban": "The correct answer is: AMZN \u2192 Amazon.com, GOOG \u2192 Alphabet Inc., FB \u2192 Facebook, Inc., AAPL \u2192 Apple, ^DJI \u2192 Dow Jones Industrial Average"
  },
  "28a29b753443a8aced956cea1f1153797c8dc57dca2dddc2a359285370273007": {
    "soal": "One example of using the Tree widget is shown in the workflow below. Using the ................. widget, we can induce a model and check it using a view similar to the Tree Viewer widget.\n\n\n",
    "jawaban": "The correct answer is: Tree"
  },
  "86bd50e03c47b7ceaf807a94a1e29e22337c505b974d258aac1eb82584bda286": {
    "soal": "import pandas as pd\n# Contoh: Membuat DataFrame dengan missing values\ndata =\ndf = pd.DataFrame(data)\n# Menampilkan jumlah missing values di setiap kolom\nprint(\"Jumlah missing values per kolom:\")\nprint(df.isnull().sum())\n# Menghapus baris yang memiliki missing values\ndf_cleaned = df.dropna()\nprint(df_cleaned)\n# Mengisi missing values pada kolom 'Usia' dengan mean\ndf['Usia'].fillna(df['Usia'].mean(), inplace=True)\n# Mengisi missing values pada kolom 'Usia' dengan median\ndf['Usia'].fillna(df['Usia'].median(), inplace=True)\n# Mengisi missing values pada kolom 'Kota' dengan modus\ndf['Kota'].fillna(df['Kota'].mode()[0], inplace=True)\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n# Inisialisasi imputer untuk mengisi missing values dengan mean\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n# Mengimputasi kolom 'Usia'\ndf[['Usia']] = imputer.fit_transform(df[['Usia']])\nWhich pandas function is used to remove rows containing missing values?\n{\n=dropna()\n~fillna()\n~isnull()\n~notnull()\n~replace()\n~interpolate()\n}",
    "jawaban": "The correct answer is: 'Nama': ['Andi', 'Budi', 'Citra', 'Dewi'], 'Usia': [25, 30, None, 22], 'Kota': ['Jakarta', None, 'Bandung', 'Surabaya']"
  },
  "f2e629a519735aa22804a6a9667acca204c74153405d42c043976a4a1983d22f": {
    "soal": "The ......(1)........... widget can visualize ..........(2)......... between objects. The visualization is similar to printing a table of numbers, except the numbers are replaced by colored dots.\n",
    "jawaban": "The correct answer is: (1) \u2192 Distance Map, (2) \u2192 Distance"
  },
  "f0225014aea57cc357e940d20306bc6e4d7add7ca19d0c83fd08582db5dcf083": {
    "soal": "After updating the package lists, which command installs Docker on the system?",
    "jawaban": "The correct answer is: sudo apt install -y docker.io"
  },
  "b0e48c7db8246a60eba8ecd31834522d3ff175ac025b7204b39235151f640533": {
    "soal": "The Distributions widget displays the distribution of discrete attribute values or ........................ If the data contains a class variable, the distribution can be conditioned on the class.\n",
    "jawaban": "The correct answer is: continuous"
  },
  "a100ddeebb9b355ea5af5a132a813cb955281fc9cca636af4de6514882765366": {
    "soal": "import pm4py\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\n# Import XES log\nlog = xes_importer.apply(xes_filename)\n# Cek jumlah kasus dan aktivitas\nprint(f\"Jumlah kasus: \")\nactivities = set()\nfor trace in log:\nfor event in trace:\nactivities.add(event['concept:name'])\nprint(f\"Jumlah aktivitas unik: {len(activities)}\")\nfrom pm4py.algo.discovery.dfg import algorithm as dfg_discovery\nfrom pm4py.visualization.dfg import visualizer as dfg_visualization\n# DFG berdasarkan frekuensi\ndfg = dfg_discovery.apply(log,\nvariant=dfg_discovery.Variants.FREQUENCY)\n# Tampilkan visualisasi\ndfg_vis = dfg_visualization.apply(dfg, log=log,\nvariant=dfg_visualization.Variants.FREQUENCY)\ndfg_visualization.view(dfg_vis)\nBottleneck analysis with PM4Py helps businesses to:\n{\n=Improve process efficiency\n-Develop software faster\n-Find financial errors\n-Increase sales\n-Design user interfaces\n-Enhance graphic design\n}",
    "jawaban": "The correct answer is: len(log)"
  },
  "58983c904e585c41c284dc8d6536742e020234e9502f7ce061fafdc391294c45": {
    "soal": "The workflow below demonstrates the use of the Confusion Matrix widget\n\nTest & Score obtains data from File and two learning algorithms from Naive Bayes and Tree. Test & Score performs cross-validation or other train-and-test procedures to obtain class prediction by both algorithms for all (or some) data instances. The test results are sent to Confusion Matrix, where we can observe how many instances were misclassified and why.\n\n\nIn the output, we use the Data Table to show examples selected in the confusion matrix. If we, for example, click Misclassified, the table will contain all instances that were correctly classified by the selected method.\n\n\nThe Scatter Plot receives two sets of data. From the File widget, it gets the full data, while the confusion matrix only sends selected data, such as misclassified data. The Scatter Plot will display all data, with bold symbols representing selected data.\n\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "b2616d27e9953aa0cc2df4cd01705871be5dc4ebd077c10be020828876e293c7": {
    "soal": "Principal Component Analysis (PCA) transforms data into a dataset with uncorrelated variables, also called principal components. The PCA widget displays a graph (scree diagram) that shows the amount of variance explained by the best ................ components and allows you to interactively adjust the number of components to include in the output dataset. In this workflow, we can observe the transformation in the Data Table and in the Scatter Plot.\n\n\n",
    "jawaban": "The correct answer is: principal"
  },
  "072c0a007bea134a4dda0a2a1bc9b6a354259517cd1ba7dd14f85dad5a95a18a": {
    "soal": "\n\n\nThe classification method used in the workflow above is\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: logistic regression"
  },
  "236c437f6b9242da073a4679bf1165f5067fa8920059d8b6fb6e2bc1654bdee2": {
    "soal": "The following workflow demonstrates the use of the Distances Widget. We use the iris.tab data from the File widget. We calculate the distance between data instances (rows) and pass the result to Hierarchical Clustering. This workflow is useful for finding groups/clusters in the data instances.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "21676e1b402a84a122b89008916c2aab630bb19bbefbaf3060ca80784c51e13c": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\n# xes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\nxes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nprocess_tree = pm4py.discover_bpmn_inductive(log)\nbpmn_model = pm4py.convert_to_bpmn(process_tree)\npm4py.view_bpmn(bpmn_model)\nWhich function is responsible for discovering a BPMN model inductively?",
    "jawaban": "The correct answer is: discover_bpmn_inductive"
  },
  "333d68cb031bf1889aba9ab8177039c6d9ddb801739511c0ed3f6772b2ced352": {
    "soal": "Which Python library is mentioned for static visualization of processes?",
    "jawaban": "The correct answer is: PM4Py"
  },
  "85d14a90d9525d65f62e1a3745fd65787f76ee39e1b0639be96ef53a6d903f47": {
    "soal": "!pip install googlesearch-python nltk matplotlib wordcloud\nfrom googlesearch import search\nimport csv\ndef google_scrape(keyword, num_results=40,\noutput_file='results.csv'):\nprint(f\"Searching Google for: \")\n# Lakukan pencarian dan ubah hasil ke dalam list\nresults = list(search(keyword, num_results=num_results))\n# Simpan ke file CSV\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'URL'])\nfor url in results:\nwriter.writerow([keyword, url])\nprint(f\"Saved {len(results)} results to '{output_file}'\")\n# Contoh penggunaan\nkeyword = \"berita terbaru teknologi Indonesia\"\ngoogle_scrape(keyword, num_results=40)\nWhat does the 'num_results' parameter in the 'search' function specify?\n{\n=The number of search results to retrieve.\n~The number of pages to scrape.\n~The timeout duration for each request.\n~The number of retries upon failure.\n~The maximum length of each result URL.\n~The delay between consecutive requests.\n}",
    "jawaban": "The correct answer is: keyword"
  },
  "39aaccc03193f4acef9a3c81ca9544c8f702ac65f02d7ae0e99c21a4e5403c54": {
    "soal": "In which fields are datasets primarily utilized?",
    "jawaban": "The correct answer is: Data science, machine learning, and statistics."
  },
  "f13af7a846d9823e3d25321b40a6e6c16e484d5d59bb1848707365f81d5fd6d9": {
    "soal": "Which TensorFlow component is used to visualize data flow graphs and learning metrics?",
    "jawaban": "The correct answer is: TensorBoard"
  },
  "533b3e862cb3e23807f281356c4b08809e24e0764f5bf4d5e28c6a8ac0a8e01b": {
    "soal": "In the snapshot workflow ORANGE below, we will observe the effects of preprocessing on text. For example, we are working with book-excerpts.tab, which we load with the Corpus widget. We have connected Preprocess Text to the Corpus and maintained the default preprocessing method (lowercase, word tokenization, and stopword removal). The only additional parameter we added as output is the first 100 most frequent tokens. Then we connect Preprocess Text with Sea Word to observe the most frequent words in the text.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "0c9c31e8a3038d47deade95da7a4f2efa5622383fa8e5769410775b7f1d0560f": {
    "soal": "The following workflow is an example where we can compare three (3) classifiers (i.e., Naive Bayes, Tree, and Constant) and input them into the Predictions widget. We are using the Titanic dataset. The Test & Score widget will display the evaluation results for each classifier. Then, we can plot through the Calibration Plot widget and the ROC Analysis widget from the output of the Distributions widget for further analysis of the performance of each classifier. The Calibration Plot widget allows us to see the accuracy predictions of the class probability in the form of a plot/image.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "d5fa2ea1245bdb94574aeb78d09ac2191aa9a2f8cdb04c1b87cf9a807723fa00": {
    "soal": "\n\n\nIn the process of building a machine learning model, model performance evaluation using classification is usually measured by accuracy, sensitivity, specificity, and MCC\n",
    "jawaban": "The correct answer is 'True'."
  },
  "b500cd4b5af611c929a12e67cc4f870240a705b3f3eab08736f4db05010355f3": {
    "soal": "One URL that contains many examples of Orange workflows for performing data analysis is\n\nhttps://www.youtube.com/c/OrangeDataMining\n",
    "jawaban": "The correct answer is 'True'."
  },
  "3a69483e580f5b23617b380c97b636e7abb0bc4cac807d80f9d8250121d34f50": {
    "soal": "The Sentiment Analysis widget enables basic sentiment analysis of corpora. It works for English and uses two techniques supported by ................. - Liu Hu and Vader. Both techniques are lexicon-based. Liu Hu calculates a single normal score for sentiment in text (negative score for negative sentiment, positive for positive, 0 for neutral), while Vader produces scores for each category (positive, negative, neutral) and adds a total sentiment score called compound.\n\n\n\n\nLiu Hu score\nVader score\n\n\nAnswer: \nQuestion 89",
    "jawaban": "The correct answer is: NLTK"
  },
  "b5f8f1cb106031ad4af90afe7f29ee45cbac48f0d6e8a5b9d1d9739bb2f0a0dd": {
    "soal": "The Calibrated Learner widget wraps/continues the work of another learner with probability calibration and decision threshold optimization.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "af3923de6617eb52efffa2aed1f625bbd1c7f5ecf63d93e8dbb80c6bc5d8a770": {
    "soal": "In the ................................. widget, we will visualize the autocorrelation coefficient for the time series we selected.\n",
    "jawaban": "The correct answer is: Correlogram"
  },
  "102df1a7dca3b336f80b56dd2a9fbb979ddf10ef70752ad9b80114c9fdffd9f3": {
    "soal": "In the Line Chart widget, we can input forecast input signals from the forecast model. The forecast is drawn with a straight line and confidence intervals as the range area.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "2ffb079c440780824fabe812091c448659f4b54481e50274bb50be3bcd61aa77": {
    "soal": "We use the zoo dataset in combination with the ..................... widget to find animal groups. Now we have the clusters we want to identify and what is significant for each cluster! Provide the cluster to the Box Plot widget and use \u2018Order by relevance\u2019 to find out what defines the cluster. It appears they are separated by their type, even though the clustering was done without class labels! This is an example of unsupervised learning.\n\n\n\n",
    "jawaban": "The correct answer is: Hierarchical Clustering"
  },
  "41be958105200755abd5f59d161c968cc5ddfb84c2e0c69407985d5e1314fb34": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nx_train = np.random.rand(1000, 10)\ny_train = np.random.randint(2, size=(1000, 1))\nmodel = keras.Sequential([\nkeras.layers.Dense(64, activation='relu', input_shape=(10,)),\nkeras.layers.Dense(32, activation='relu'),\nkeras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer='adam',\nloss='binary_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=10, batch_size=32)\nWhat is the optimizer used in this example?",
    "jawaban": "The correct answer is: adam"
  },
  "09ddc3619ebfe679e1b20930d894802b71dbe9aa717b3f17a5abedb0511ea25f": {
    "soal": "model.save('my_model')\nfrom google.colab import files\nfiles.download('my_model')\nnew_model = tf.keras.models.load_model('my_model')\nIn the context of TensorFlow, what does \"model.save('my_model')\" do?",
    "jawaban": "The correct answer is: It saves the entire model including architecture and weights"
  },
  "a7378572a53b70f2a2052df2de4be9919371e47a4940f6e6bc63599d839e26ca": {
    "soal": "000000 (binary) + 101 (binary) = ..... (binary)",
    "jawaban": "The correct answer is: 000101"
  },
  "0f56290ca09ef4ef8299eb2d0cdeee5a09ef5e66782c0d40f47cccabb72fac1b": {
    "soal": "We can measure the distance between embedded images and see which ones are the most similar. We can use the Distances widget to measure the distance. Typically, cosine distance works best for images. We send the distance matrix to ...................... Hierarchical Clustering to visualize similar pairs in the dendrogram.\n\n\n",
    "jawaban": "The correct answer is: Hierarchical"
  },
  "9e5082eb9600346326f0aad4eef0435e4de04f2a0a701fe681bdf9bfe0a42124": {
    "soal": "The Distance Matrix widget creates a distance matrix, which is a two-dimensional array containing distances, taken pairwise, between elements of the dataset. The number of features in the dataset determines the size of the matrix. Data matrices are very important for the Hierarchical Clustering Widget.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "0c17fb57d6dd563b98abb617fe5affc2586b95e7a0250327af32692541d7df39": {
    "soal": "Widget Save Data will save data every time a new signal is received in the input because it would continuously (and mostly unintentionally) overwrite the file. Instead, data is saved only after a new file name is set or the user presses the Save button.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "7f87723e82c02e63081ae7b4af0b997af86a03c55d87ed5410385b8c7d2d068d": {
    "soal": "The Distance Matrix widget creates a distance matrix, which is a ............. array containing distances, taken pairwise, between elements of the dataset. The number of elements in the dataset determines the size of the matrix. Data matrix is essential for the Hierarchical Clustering Widget.\n\n\n",
    "jawaban": "The correct answer is: two"
  },
  "71b39a94e742841bed0691c6b36d38419892d1b56feddfded5adc71690f44793": {
    "soal": "The workflow clusters data items in the iris dataset by first checking the Distances between data instances using the Distances widget. The Distance Matrix is then passed to the ....................... widget, which creates a dendrogram. We can select different parts of the dendrogram for further analysis of the related data.\n\n\n",
    "jawaban": "The correct answer is: Hierarchical Clustering"
  },
  "2c4c0249b479b7b2195bbc53ae9bf3839c0b9cdc1b89fc4c45b2c751ec4702da": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#_xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nmap = pm4py.discover_heuristics_net(log)\npm4py.view_heuristics_net(map)\nThe variable `log` is the result of which operation?",
    "jawaban": "The correct answer is: Reading an event log file -Loading a CSV file -Parsing a JSON object -Analyzing an XML file -Visualizing a graph -Performing an API request"
  },
  "32036f6d0b2184e754017dba936ea6fd580c134c480447139fe288430b01e058": {
    "soal": "The Predictions widget shows the model's calculations for the data.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "b6cf4c6434ad2ed5c5eed292022c2dc053c8cc71b1a663e7fa9b33257d6a6ef0": {
    "soal": "Which command removes a specified model from your system?",
    "jawaban": "The correct answer is: ollama rm model"
  },
  "42cfbe939ddde090dcae44758352962d34cadfdf84bd8c6423c85f1b04db99ce": {
    "soal": "\n\n\nDalam contoh di atas, kita menggunakan dataset zoo dan mengirimkannya ke CN2 .................. . Kita bisa me-review dan meng-interpretasi model yang dibuat dengan CN2 Rule Viewer widget.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Rule Induction"
  },
  "cc2cf90acc92cbee78cda444a76b75ab960565cb4ef7475b3a5f259fba25571e": {
    "soal": "How can you view the configuration details of a specific model?",
    "jawaban": "The correct answer is: ollama show model"
  },
  "ebc4ec261ebf097b1070fa0b1652fd5988b86ce117c224808ba62c07dfbd6b26": {
    "soal": "What is necessary to visualize processes using PM4Py?",
    "jawaban": "The correct answer is: Python installation and dependencies"
  },
  "4c9de234f90cd186004f06051456b8b4b7bedd10b948f7fb712b43954c3b904a": {
    "soal": "In the Moving Transform widget, to integrate the time series\u2019 difference from the ..................., use Cumulative sum aggregation on a wide enough window to capture the entire series.\n\n\n\n\n",
    "jawaban": "The correct answer is: Difference"
  },
  "80923a1b8d6ea4dcc64e7ebda7486ee170b00afcee1cb9da4c10c10c94915d10": {
    "soal": "import pandas as pd\nfrom pm4py.objects.conversion.log import converter as log_converter\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom pm4py.visualization.petrinet import visualizer as pn_visualizer\nfrom pm4py.objects.log.util import dataframe_utils\n# csv_filename='Insurance_claims_event_log.csv'\n# csv_filename='Business Travel Permits.csv'\n# csv_filename='Parent-Child Union Output.csv'\n# csv_filename='PM4PY_Dummy_Event_Log_100_Cases_pm4py.csv'\n# csv_filename='PM4Py_Dummy_Event_Log-congested-business-process.csv'\n# 1. Load CSV event log\ndf = pd.read_csv(csv_filename)\n# df = pd.read_csv(csv_filename,sep=';')\n# print(df.head())\n# 2. Format timestamps properly\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n# 3. Ensure column names match PM4Py expectations\ndf = dataframe_utils.convert_timestamp_columns_in_df(df)\n# 4. Convert to event log object\nevent_log = log_converter.apply(df, parameters=)\n# 5. Discover process model using Alpha Miner\nnet, initial_marking, final_marking = alpha_miner.apply(event_log)\n# 6. Visualize Petri Net\ngviz = pn_visualizer.apply(net, initial_marking, final_marking)\npn_visualizer.view(gviz)\nWhich command correctly converts timestamps to datetime objects?\n{\n~df[\"timestamp\"] = pd.datetime(df[\"timestamp\"])\n=df['timestamp'] = pd.to_datetime(df['timestamp'])\n~df[\"timestamp\"] = datetime(df[\"timestamp\"])\n~df.convert_timestamp()\n~df.datetime.timestamp()\n~timestamp = datetime(df)\n}",
    "jawaban": "The correct answer is: log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case_id'"
  },
  "f3b2330c6e4803571ddb61cba6123df6dd5ea2595618a20cfb56d5469adad94b": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nHow many images are designated for testing in the MNIST dataset?",
    "jawaban": "The correct answer is: 10,000"
  },
  "2d5ef0511fc4dd1c910af977e80ee63db57f819ec938c1765477ad1f8bea7a38": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nx_train = np.random.rand(1000, 10)\ny_train = np.random.randint(2, size=(1000, 1))\nmodel = keras.Sequential([\nkeras.layers.Dense(64, activation='relu', input_shape=(10,)),\nkeras.layers.Dense(32, activation='relu'),\nkeras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer='adam',\nloss='binary_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=10, batch_size=32)\nHow many layers are defined in the model?",
    "jawaban": "The correct answer is: 3"
  },
  "29f9c988170dcc98ea80e731482f1844b6a28b26ec2ebe924439c75af691996c": {
    "soal": "The following regular expression (regex)\n\n\nmatches words ending with the letter y\nis\n",
    "jawaban": "The correct answer is: \\b\\w+(Y|y)\\b"
  },
  "93237bcf1ad3e2be0f8a9941d2763e5ccb0ec8e5de0165b6a50565c2c8c2658e": {
    "soal": "Salah satu URL tempat lokasi contoh2 pengembangan / penggunaan ORANGE ada di\n\n\u00a0https://orange.biolab.si/workflows/\n",
    "jawaban": "The correct answer is 'True'."
  },
  "16427bace1548ff2481769fd8044021376b01d535c44be05f996ef41c8eb7faf": {
    "soal": "What is transfer learning in the context of deep learning?",
    "jawaban": "The correct answer is: Utilizing a pre-trained model on a new, related task."
  },
  "af3c3bd243142e7ad90538ec455462e0c29b8e0aae5f553339c43f2f9a532068": {
    "soal": "Which library is used to create and manipulate DataFrames in Python?",
    "jawaban": "The correct answer is: Pandas"
  },
  "6c200a989b7adc9fc39de7f9c4e0eacd0d176464fcfa9375e2d5bafefab64448": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nplt.imshow(x_train[0], cmap='gray')\nplt.title(f\"Label: \")\nplt.show()\nmodel = keras.Sequential([\nkeras.layers.Flatten(input_shape=(28, 28)),\nkeras.layers.Dense(128, activation='relu'),\nkeras.layers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nWhich function is used to load the MNIST dataset?\n{\n= keras.datasets.mnist.load_data()\n~ keras.datasets.load_mnist()\n~ tf.load_mnist_data()\n~ mnist.load()\n~ keras.load_dataset('MNIST')\n~ tf.keras.datasets.get_mnist()\n}",
    "jawaban": "The correct answer is: y_train[0]"
  },
  "e80683828462508873c7956924e85508588ec5b452d695e41007e9a22cf29d28": {
    "soal": "Which of the following is an example of process mining usage?",
    "jawaban": "The correct answer is: Analyzing event logs to improve business process efficiency"
  },
  "a36911a22e92f8010b60ab0a679999122c5908e0bd0d82553a662f890bea1de9": {
    "soal": "The Periodogram widget can visualize cycle, seasonality, periodicity, cosinusity, and important periods in time series.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "368a0d082f73e849583f74965faff46fdb201c406679c5bcc7b6f606ede7eda3": {
    "soal": "\n\n\n\n\n\nIn this example, we are using the lastfm dataset, which can be loaded into the Network File widget under Browse documentation networks. The network node represents musicians, marked by the genre they play, the number of albums produced, and so on. Edge is the number of listeners on LastFm.\n\nThe entire dataset is visualized in the Network Explorer Widget. In this widget, we remove the coloring and adjust the node size to match the number of albums. Then, we select several nodes from the network. We can observe the selection in the Network Explorer Widget (1).\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "3dafbc302c57275dc26526f355086f74dbcf9f1b10c64c1f9a9bae0b7695e0b1": {
    "soal": "Widget Calibrated Learner menghasilkan sebuah model yang mengkalibrasi distribusi dari class probabilities dan meng-optimasi decision threshold. Widget ini hanya bekerja untuk classification task.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "583bad582d61df0c284b1b468b77c4f8fc52715f661efdb199b30aba02c2e769": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nWhat is the primary purpose of the `Dense` layer in a neural network?",
    "jawaban": "The correct answer is: To apply a fully connected operation to the input data"
  },
  "c93164be871dbc1befc5ff97598338a546afdd3b4bac79983631a616e2e1e03c": {
    "soal": "In the workflow below, we compare the statistics of two Data Info widgets - one with information from the entire dataset and the other with information from a selected subset (manually) from the ............................. Here, we are using the Iris dataset.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Scatter Plot"
  },
  "d1ee4c98e397c30d58fc22c0a543addbdbf40b958c51354f10e2c46afa82d085": {
    "soal": "The CSV File Import widget can import a data table from a ......................... formatted file.\n",
    "jawaban": "The correct answer is: csv"
  },
  "0d494eac8dc58fc35cae54d462f4536514206f10683da3960494f739e9906859": {
    "soal": "The Image Viewer widget can be used to compare ................... , for example, when looking at the similarities or differences between selected data instances (for example, bacterial growth or bitmap representation of handwriting).",
    "jawaban": "The correct answer is: image"
  },
  "50fd578eab29f292943eceb42cf155eb5c7be57b218a3c555c0c9ebe1ae5ce57": {
    "soal": "Which command adds the path '/home/onno/.local/bin' to the current session's PATH variable?",
    "jawaban": "The correct answers are: export PATH, \"$PATH:/home/onno/.local/bin\", \"$PATH:/home/onno/.local/bin\", \"$PATH:/home/onno/.local/bin\", \"/home/onno/.local/bin:$PATH\", \"/home/onno/.local/bin\", \"/home/onno/.local/bin\""
  },
  "574754e515ea1f2a5359ee9e6f58fd996d198d82dd7623cb3bb3b29ef5a276e0": {
    "soal": "The Calibrated Learner widget generates a model that calibrates the distribution of class probabilities and optimizes the decision threshold. This widget only works for ......................... tasks.\n",
    "jawaban": "The correct answer is: binary classification"
  },
  "6a85753c6e149a257b2abefbba926ed4f0a665d266aeec8514f1d6faa0fb4836": {
    "soal": "Some visualization widgets, such as Scatter Plot and several data projection widgets, can expose data instances in a data subset. In this workflow, ..................... visualizes data from the input data file, but also highlights the data points selected in the Data Table (selected rows).\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Scatter Plot"
  },
  "3bcf0031abe26d2951782698b6fe71499408021f6a70ee2de4ff9d1441f2b053": {
    "soal": "What volume is mounted to store Ollama's data persistently?",
    "jawaban": "The correct answer is: ollama:/root/.ollama"
  },
  "30b9a6e6ee339777d1f3603707d5a8c5642d98451debf9d959717045ed80702a": {
    "soal": "The CN2 Rule Induction widget will induce (induce) rules from the data using the ................... algorithm.\n\n\n\n",
    "jawaban": "The correct answer is: CN2"
  },
  "de81a7aad37a738c2313bc3f9be924b50cb0f8f1fa83e585eaee106b7001ddbb": {
    "soal": "Principal Component Analysis (PCA) transforms data into a dataset with uncorrelated variables, also called principal components. The PCA widget displays a graph (scree plot) showing the level of variance explained by the best principal components and allows interactive adjustment of the number of components to include in the output dataset. In this workflow, we can observe the transformation in the Data Table and the Scatter Plot.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "df847bf3324cca6e68c29b342b5a94c6aa76785994929385b48ce402d1b2a9ad": {
    "soal": "Network Explorer Widget is the main widget for visualizing the network in text. It displays the graph using Fruchterman-Reingold layout optimization and allows for the adjustment of node color, size, and labels. One can also highlight nodes based on certain properties and output them.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "0db2ddc3cd4732d66d55e59047d1f4ef9bd139d1474280ebb33c876a314e2f3a": {
    "soal": "What allows LLMs to generate relevant predictions?",
    "jawaban": "The correct answer is: Recognizing patterns and structure"
  },
  "b169a4b2d5f1f3ca15b4a73e184e2dd718d9856118bc3c24987d59fa4caf0b01": {
    "soal": "The workflow example below shows the use of the Distance Matrix widget, which is quite standard. We calculate the distance between rows in a sample of the Iris dataset and output it in ................... Unsurprisingly, Iris Virginica and Iris Setosa are the farthest apart.\n\n\n\n\n",
    "jawaban": "The correct answer is: Distance Matrix"
  },
  "6653189f51b74517339d06b01f5c33e2a7765835dbdd2e9714d369c3070366af": {
    "soal": "Which of the following is a common loss function used for multi-class classification problems?",
    "jawaban": "The correct answer is: Categorical Cross-Entropy"
  },
  "e6becb861b8adf46aea89a7ab3fa0d99902df28e2daa984d5bae36a931645225": {
    "soal": "The core of Apache Hadoop consists of the storage part (Hadoop Distributed Disk System (HDDS)) and the processing part (MapReduce). Hadoop splits files into large blocks and distributes them among the nodes in the cluster. To process the data, Hadoop MapReduce transfers code to the nodes for parallel processing, based on the data that needs to be processed at each node. This approach takes advantage of the data-node locality manipulating the data it holds in hand\u2014to allow data to be processed faster and more efficiently than conventional supercomputer architectures that rely on parallel file systems where computations and data are connected through high-speed networks.",
    "jawaban": "The correct answer is 'False'."
  },
  "746709163d48f22d58c5b4efadb1e1252141e0c411e364ee5b260dc3c9237d90": {
    "soal": "Which command initiates an interactive session with a specified model?",
    "jawaban": "The correct answer is: ollama run model"
  },
  "9e8fc26e8831bf2b47331df732768d9cc73e18e31c49f4563fc54c3049fa7416": {
    "soal": "How can you list running Docker containers?",
    "jawaban": "The correct answer is: docker ps"
  },
  "547618cd538c8af71cdf4e93a0da25d26c9e135916e7de465d3f7d890c42b3c9": {
    "soal": "Network Clustering Widget can detect clusters in statistics.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "758c32fea203a0dc7f4fc97e43942ee9f3ce904ab0cf6399f2b7d1fca4c33609": {
    "soal": "In the following workflow example, it shows how to quickly visualize the corpus with the Word Cloud widget. We can connect the ...................... widget directly to the Corpus widget, but instead, we decided to apply some preprocessing with the Text Preprocess widget. We are working with the book-excerpts.tab dataset. We can change all text to lowercase, tokenize (split) the text into words, filter out English stopwords, and select the 100 most frequently occurring tokens.\n\n\n\n\n",
    "jawaban": "The correct answer is: Word Cloud"
  },
  "80cba59bd7baf25e9eafeffb5f035f207280fe0ced0aac1914827fcae2c70e4b": {
    "soal": "Column attribute names can be preceded by a label followed by a hash. Use c for class and m for meta attributes, i for ignoring columns, ...... for weight (of a column), and C, D, T, S for continuous, discrete, time, and string attribute types. Example: C#mph, mS#name, i#dummy.\n\nWhere ........ is,\n",
    "jawaban": "The correct answer is: w"
  },
  "3e8c9c69a17f20ccf1f4cab653700214a578fd78e0e3d0a2a017b984e4d43fc9": {
    "soal": "In this example, we will just check what a bag of words model looks like. Load book-excerpts.tab using the Corpus widget and connect it to the Bag of Words widget. Here we intentionally use the default parameters - the simplest count is term frequency. Check what the Bag of Words widget outputs using the Data Table widget. The last column represents the term frequency for each document.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "f72be1dc1f5a4146c580718bbe4daaf6bf56a75c2125366a35080af4c7e05e02": {
    "soal": "The Import Documents widget takes text files from a folder and creates a corpus. The Import Documents widget can read .txt, .docx, .odt, .pdf, and ................................. If there are subfolders in the folder, they can be used to label the class.\n",
    "jawaban": "The correct answer is: xml"
  },
  "e5872daf7bc35f59b0c7452b46d56c10a00812c695f8e52590da534e83f13609": {
    "soal": "\n\n\n\n\n\nThe following Workflow can be used to analyze classification errors made by the program/algorithm used.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "3e1e9e67705824f372129fd947bd1d04c4d439c23ab792ad2171f1cf7fda4789": {
    "soal": "What does the 'activity' attribute in an event log represent?",
    "jawaban": "The correct answer is: Description of the event that is occurring"
  },
  "1881a41908281cb61dd8a651b2ba3a9e1aec3dcd20aa7b480c0623ba0a5de755": {
    "soal": "Supported data formats supported by ORANGE3 (based on its add-ons):\n\n\ndistance matrix: Distance File\npredictive model: Load Model\nnetwork: Network File from Network add-on\nimages: Import Images from Image Analytics add-on\ntext/corpus: Corpus or Import Documents from Docs add-on\nsingle cell data: Load Data from Single Cell add-on\nseveral spectroscopy files: Multifile from Spectroscopy add-on",
    "jawaban": "The correct answer is 'False'."
  },
  "98105c7f42615fab292d9ac332219ec15cd5f905ff42ffb672f686632843e2b8": {
    "soal": "Which plugin can be used for process discovery in ProM?",
    "jawaban": "The correct answer is: Inductive Visual Miner"
  },
  "d54414738cb7577742600ec81b171e7761dd1f894c9ca827cdf42d336437077f": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#_xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nmap = pm4py.discover_heuristics_net(log)\npm4py.view_heuristics_net(map)\nWhat function is used to visualize the discovered heuristic net?",
    "jawaban": "The correct answer is: pm4py.view_heuristics_net -pm4py.plot_heuristics -pm4py.visualize_net -pm4py.show_net -pm4py.display_net -pm4py.render_net"
  },
  "e383dce18cb434fa621d1e5a2eb16b1e9937d70ed3a7c82f96e2b114adac3a0d": {
    "soal": "The Moving Transform widget can apply the ....................window function to the time series. Use the Moving Transform widget to get the average value of the series.\n",
    "jawaban": "The correct answer is: rolling"
  },
  "4b447d8527973bcee13e431c8c2657b80b72c843894bcc20562aaa1d7cd19dbf": {
    "soal": "The Bag of Words model creates a corpus with word counts for each data instance (document). The count can be absolute, binary (present or not), or sublinear (logarithmic of term frequency). The Bag of Words model is needed in combination with the Word Enrichment widget and can be used for predictive modeling.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "ad99db8694aa972a8ac864a710c097f5361b6ee960625799a7ca685447ddfd93": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nIf a `Dense` layer has 10 neurons, what will be the dimension of its output?",
    "jawaban": "The correct answer is: 10"
  },
  "f359b67c65ddb5f7eb637d97ddfc9b54b76b03f3987bb945ad14b7eac8d39ece": {
    "soal": "from sklearn.model_selection import train_test_split\nimport pandas as pd\n# Contoh: Membuat DataFrame dengan fitur 'X' dan target 'y'\ndata =\ndf = pd.DataFrame(data)\n# Memisahkan fitur dan target\nX = df[['X1', 'X2']]\ny = df['y']\n# Membagi dataset menjadi data pelatihan dan pengujian dengan stratifikasi\nX_train, X_test, y_train, y_test = train_test_split(\nX, y, test_size=0.3, stratify=y, random_state=42\n)\n# Menampilkan proporsi kelas di setiap subset\nprint(\"Proporsi kelas di y_train:\n\", y_train.value_counts(normalize=True))\nprint(\"\nProporsi kelas di y_test:\n\", y_test.value_counts(normalize=True))\nWhat happens if you do not set the `stratify` parameter in `train_test_split`?\n{\n~The function will automatically stratify based on the target variable.\n~An error will be raised due to missing parameters.\n=The data will be split randomly without considering class distributions.\n~The split will default to a 50-50 train-test ratio.\n~The function will prompt for user input to proceed.\n~The minority class will be oversampled in the training set.\n}",
    "jawaban": "The correct answer is: 'X1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'X2': [11, 12, 13, 14, 15, 16, 17, 18, 19, 20], 'y': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]"
  },
  "4ad459349311259ac0a94c4ec309ecbe03a1f7e461fe5faa2921ee046b5bdcb5": {
    "soal": "The Distances Widget also works well with other Orange add-ons. The Distance Matrix Widget can be fed into the Network from Distances Widget (Network add-on) to turn the matrix into a GUI and into the Duplicate Detection Widget (Text add-on) to find document duplicates in the corpus.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "e002de5fa270cb4b383251040b8a59a355f456541c1e99da92c57503868aadde": {
    "soal": "Where does the output go after being generated?",
    "jawaban": "The correct answer is: Back to the context window"
  },
  "d24c7ebf2875c10bda3531d2035adb6739ed6536e52148e1ac5b0a7b853bc2a3": {
    "soal": "The ..................... widget in Orange is very easy to use. Technically, Orange will send the image to the server, where the server will push the image through a pre-trained deep neural network, such as Google Inception v3. Deep networks are often trained for a specific purpose. Inception v3, for example, can classify images into one of 1,000 image classes. We can ignore the suggested classification and instead consider the penultimate layer of the network with 2048 nodes (numbers) and use them for the image\u2019s vector-based representation.\n",
    "jawaban": "The correct answer is: Image Embedding"
  },
  "8c5c3c90bfe3dc9871d20b84723341cc26c66e14f8f5f634a82296ce142e6ec5": {
    "soal": "Widget Data Info is a simple widget that presents information about dataset size, feature, target, meta attributes, and URL.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "cf254b211e66f4cbcfc269147172497e436f39d4bcef991497d47dd641c6a228": {
    "soal": "Network Analysis Widget can be used for statistical analysis of network load.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "5d7c9d89fac6063a5e86bc37eb687834d4a1035dc0d8e84360a6b57add86dec5": {
    "soal": "Which of the following is NOT a main class of process mining techniques?",
    "jawaban": "The correct answer is: Data warehousing"
  },
  "77eef93fc39d21c555a11b53417d3c6e601e31ae0c832fb03371eac2f2a34b84": {
    "soal": "The Import Images widget is probably the first widget we will use in image analysis. The Import Images widget loads images and creates a class value from the folder. In this example, we use the Import Images widget to load 26 paintings by Monet or Manet.\n\nWe can observe the results in the Data Table widget. It is clear how Orange adds an additional class attribute with values Monet and Manet.\n\nNext, we can continue with ...................... standard methods. We send the images to the Image Embedding widget, where we will use the Painters embedder to get the image vector.\n\nThen we will use the Test & Score widget and the Logistic Regression widget to create a model to predict the artist of a painting (in this case Monet or Manet). We get a very good score. How could that be? It turns out these images were already trained on the Painters embedder, so the accuracy achieved will be high.\n\u00a0\n\n\n\n",
    "jawaban": "The correct answer is: machine learning"
  },
  "13915c5db2c928f6715894d27536d9980128b16ed9d0337930129aec1482cd84": {
    "soal": "Algoritma CN2 adalah teknik klasifikasi yang di rancang agar secara effisien melakukan induksi dari rules yang simple & komprehesif dalam bentuk \u201cif cond then predict class\u201d, meskipun dalam domain dimana mungkin ada data.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "361e6b3050f2cb5addc2bbeeea8dabfc60104ba433ed80b210f44a9191a8f095": {
    "soal": "What is web scraping primarily used for?",
    "jawaban": "The correct answer is: Extracting data from websites"
  },
  "acc614db9be371cb902047615d85588f93ffa2a4c77816f5ec5a307bb3474321": {
    "soal": "What is data augmentation in the context of deep learning?",
    "jawaban": "The correct answer is: Applying random transformations to the training data to increase its diversity."
  },
  "c4d6815e8ed49d31753f0b13b0ab3e7493a9f7ca79d6824afae957eed2a3de5a": {
    "soal": "How can you access the help documentation for Ollama commands?",
    "jawaban": "The correct answer is: ollama --help"
  },
  "d2a9c7893afaecb851467cb27882aba84180db2aa16f28e0c8429bbc881f4d60": {
    "soal": "\n\n\n\n\n\nThe image above shows the modeling choices depending on the data we have.",
    "jawaban": "The correct answer is 'False'."
  },
  "3a893d4dffaabdd4386da8ad8dba563270ad6d248113d73c080993e6b3d93ac3": {
    "soal": "One URL that contains many examples of Orange workflows for performing data analysis is\n\nhttps://orange.biolab.si/workflows/\n",
    "jawaban": "The correct answer is 'True'."
  },
  "c885c7e767a0c58ba3486788dcf2cdfdad80403171dd84ea82c119ccf387def8": {
    "soal": "result 8 (decimal) OR 0 (decimal) is",
    "jawaban": "The correct answer is: 8"
  },
  "7f75894ec35691e73f0abe5bae8a9d33276a444ca8bb6ff7ef6f80f3426f44cc": {
    "soal": "Heuristic Net differs from DFG primarily because it:",
    "jawaban": "The correct answer is: Includes probabilities and strength of connections"
  },
  "6029fe79eb5ba849e6a22f9e0356415cd3f599410488048711102ccd4cea13fe": {
    "soal": "Proper Orange3 installation can use the following command\n\npip3 install numpy scipy mkl nose sphinx pydot-ng pandas-datareader\npip3 install parameterized Theano cntk matplotlib sklearn seaborn h5py\npip3 install matplotlib pandas scipy sklearn seaborn features\npip3 install orange3\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "b573841bd73297ea6efcb9511eec37a1b6d753774d7cde9782eb1d8ab9c66c85": {
    "soal": "The following ticker code used in the Yahoo Finance Stock Data Widget (ORANGE)\n\nAAPL\n\n\nBelongs to the company,\n\n\n",
    "jawaban": "The correct answer is: Apple"
  },
  "acc172abe7d84864dcb42387dd2e7f55393ca6616344063b7bace935b409ee57": {
    "soal": "Now, each core widget in Orange can function/interact smoothly with Text so you can mix and match widgets as you like. Previously, one could not forward output from Select Columns (data table) to .................... (corpus), but this is no longer an issue.",
    "jawaban": "The correct answer is: Preprocess Text"
  },
  "43cbfd3f0805a32708eb8d41fdb3a935479435ff20369994f8491c5666b0adeb": {
    "soal": "The Line Plot widget is a standard visualization widget, which displays the ............ profile, usually in the form of ordered numeric data. In this simple example, we will display the iris data in a line plot, grouped by iris attributes. The plot shows how petal length separates the .............. values well.\n\nIf we observe this in the Scatter Plot Widget, we can confirm this is indeed true. Petal length is an interesting attribute for class separation, especially when enhanced with petal width, which is also well separated in the line plot.\n\n\n\n\n",
    "jawaban": "The correct answer is: class"
  },
  "eaaf8a178842557f2668c9f3829aabe7d7cf71353842a56002542ce993af5f39": {
    "soal": "\n\n\nExamples of traditional methods in data science applications include,\n\n\n\n",
    "jawaban": "The correct answers are: User Experience (UX), Sales Forecasting"
  },
  "e3e7c0cf93e96cf47f7ea4b09790350dfb7550bbbe458a7f103ac1a2f2d42823": {
    "soal": "Most visualizations in Orange are interactive. In the workflow below, the Widget .................. for example. Double-click the icon to open it and click-and-drag to select several data points from the plot. The selected data will automatically enter the Data Table Widget. Double-click to check which data was selected. Change the selection and observe the changes in the Data Table Widget.\n\n\n",
    "jawaban": "The correct answer is: Scatter Plot"
  },
  "72ff69fcff0c21763122ceb8453a6290cf8aa3edd1a6d4ad5b9749998f3fae12": {
    "soal": "In TensorFlow, what is the purpose of the 'clip_by_value()' function?",
    "jawaban": "The correct answer is: To clip tensor values to a specified min and max"
  },
  "f37c9cf60561fde72686f3e7a6d9451c8d00aa7633f4d75da3f029f949589b47": {
    "soal": "In the example below, we will use the Attrition - Train data from the Datasets widget. This data is about employee attrition. In other words, we want to know whether a particular employee will resign from their job or not. We will create a prediction model with the ................. widget and observe the probabilities in Predictions.\n\nFor predictions, we need training data, which we have loaded into the first Datasets widget, and data to be predicted, which we will load into another Datasets widget. We will now use the Attrition - Predict data. Connect the second dataset to the Predictions Widget. Now we can see the predictions for three data instances from the second dataset.\n\nThe ............... model predicts no employees will leave the company. We can try another model and see if the predictions change. Or test the predictive scores first in the Test & Score widget.\n\n\n\n",
    "jawaban": "The correct answer is: Tree"
  },
  "7304e9052b0932754dcfab5e83bd098221d68e795d519592b6a3ff0d4cec6313": {
    "soal": "from googlesearch import search\nimport csv\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef google_scrape_bulk(keywords, num_results=10, output_file='results.csv'):\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'Rank', 'URL'])\nfor keyword in keywords:\nprint(f\"\ud83d\udd0d Searching for: \")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nwriter.writerow([keyword, i+1, url])\ntime.sleep(1) # Delay agar tidak dianggap bot\nexcept Exception as e:\nprint(f\"\u274c Error while searching '{keyword}': {e}\")\nprint(f\"\n\u2705 All results saved to '{output_file}'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\ngoogle_scrape_bulk(keywords, num_results=10)\nWhich exception handling construct is used in the script to manage errors during the search process?\n{\n=try-except\n~if-else\n~while-try\n~do-catch\n~begin-rescue\n~attempt-handle\n}",
    "jawaban": "The correct answer is: keyword"
  },
  "ece1235794f4e13fbd6fa2255d28393c6c18e3ffa187c69f1dc2c85fdfc5f8a3": {
    "soal": "1000 (binary) AND 1111 (binary) = ............................. (binary)",
    "jawaban": "The correct answer is: 1000"
  },
  "a490bc08a140e7411f2e74fa94d1c4cae998c2262fc8b270e3f98ba5e084b030": {
    "soal": "Data science is \"a concept to unify statistics, data analysis, machine learning, and related methods\" to \"understand and analyze real-world phenomena\" with data. It uses techniques and theories drawn from a single field in the context of mathematics, statistics, information science, and computer science.",
    "jawaban": "The correct answer is 'False'."
  },
  "91083c1aa0d923bde808dd4150e3cd03116960a18ab7bd39374996777823255c": {
    "soal": "Widget kNN menggunakan algoritma kNN yang akan mencari n instance training terdekat di ruang feature dan menggunakan rata-rata feature terdekat tersebut untuk mem-prediksi\n",
    "jawaban": "The correct answer is 'False'."
  },
  "7fd094e777f9fab893b65b7dc336ed521df2b850e6c9d38949114654422783f2": {
    "soal": "The Correspondence Analysis Widget is used to perform Correspondence Analysis only for categorical single-variable data.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "1469bdf92e03d39edf8c523e07ec3fdc8b2c417f526dc6e720e0ee0fb78d94e2": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=10000)\nIn the IMDB dataset, how are the reviews represented?",
    "jawaban": "The correct answer is: As sequences of word index integers"
  },
  "2c6ee2efe8230a4d1baa909037f0a9a8201a60f1a96ef2e044be9c6badba73fc": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nx_train = np.random.rand(1000, 10)\ny_train = np.random.randint(2, size=(1000, 1))\nmodel = keras.Sequential([\nkeras.layers.Dense(64, activation='relu', input_shape=(10,)),\nkeras.layers.Dense(32, activation='relu'),\nkeras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer='adam',\nloss='binary_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=10, batch_size=32)\nWhat is the role of `Dense` in Keras?",
    "jawaban": "The correct answer is: Add a fully connected layer"
  },
  "ccf5e06db910e6148b5c89d8142341f86f099d63a4679f7d91669a72897876a7": {
    "soal": "The PCA widget in ORANGE3 provides two outputs: transformed data and principal component. Transformed Data represents the weights for individual instances in a new coordinate system, while component is the descriptor of the system (weights for the principal components). When inserted into the Data Table, we can see both outputs as numbers. We use two data tables to provide a cleaner workflow visualization, but we can also choose to edit the links so that we display the data in just one data table. We only need to create two links and connect the input data and the transformed components to the output data.\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "26a691ee994f6ac90302a8ef87ad0edc6499d745dfc50cb12dc7a67cfac7ca3c": {
    "soal": "The Scatter Plot widget, like other Orange widgets, supports zooming in and out of the plot area and automatic selection of data instances. These functions are available at the bottom left corner of the widget.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "ab0c9d2495a80efa38529c06f98faac2312e37ebfe85f708b463cebccecbb631": {
    "soal": "Which programming language is primarily used to develop TensorFlow?",
    "jawaban": "The correct answer is: Python"
  },
  "f1836cb619c293e7628dafed3ef5aec516a2fa64c43ded1f5918f774a0a63dc5": {
    "soal": "Orange uses its own data format, but it can also handle Excel, binary, comma- or tab-delimited data files.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "b151c4be5ade850b4cc57efc6b779c9a22d54194372464be608c1f64f732c9b8": {
    "soal": "perform AND operation on the following IP address\n\n192.168.123.245 AND 255.255.0.0\n\nthe result is",
    "jawaban": "The correct answer is: 192.168.0.0"
  },
  "07e4c8f0183191565c16e846622289e21fdbaea86cc5851859aed9723816edb9": {
    "soal": "This workflow combines the classification tree interface and visualization with a scatter plot. When both the tree viewer and scatter plot are open, selecting any node from the tree will send the related data example to the line plot. In the workflow, the selected data is treated as part of the entire dataset and displayed in the scatter plot. With a simple combination of widgets, we have built an interactive classification tree browser.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "0ac553cba65a9939e7c3ac90eb31331111f0c52385e54f47ce22cc24c36d2ba9": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.rand(100) # 100 nilai target\n# Inisialisasi model regresi linier\nmodel = LinearRegression()\n# Tentukan jumlah fold (misalnya, 5-fold cross-validation)\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n# Evaluasi model menggunakan cross-validation\nscores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n# Menampilkan skor R^2 untuk setiap fold dan rata-ratanya\nprint(\"Skor R^2 untuk setiap fold:\", scores)\nprint(\"Rata-rata skor R^2:\", np.mean(scores))\nWhy is the `random_state=42` parameter used in the `KFold` function?",
    "jawaban": "The correct answer is: To ensure reproducibility of the data split."
  },
  "8aefffb4f4144e5d88a05a08e4dd216aff793845e4da5d85f5b8c38f08ae476b": {
    "soal": "The Bag of Words widget can create a bag of words from the input corpus.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "69f0a735732977fab0b1192d854f1c97e8ae1cdfbe7f5201f14bf0aed762a066": {
    "soal": "Which of the following technologies is commonly used for distributed storage and processing of Big Data?",
    "jawaban": "The correct answer is: Hadoop"
  },
  "7bddce1251c1f5eabe630d0fe17244a6b47c4786582255e978de18fa45b8a5e5": {
    "soal": "The Box Plot widget is most commonly used immediately after the File widget to observe statistical properties of a dataset. In this example, we have used heart-disease data to examine our variables.\n\n\n\n\nBox Plot is also useful for finding values from a specific dataset, for example, a set of instances manually defined in another widget (e.g., Scatter Plot, or instances from several clusters, or nodes in a classification tree).\n",
    "jawaban": "The correct answer is 'False'."
  },
  "6743304e3bd537e26b2927dd4d3d862e74fbba2a698f30c4655c118a5716c2f1": {
    "soal": "In the snapshot below, we can see how transformation affects the distance matrix. We load the Iris dataset and calculate the distance between rows with the help of the Distances widget. To show how .......................... affects the Distance Matrix, we create the workflow below and compare the transformed distance matrix with the \"original\".\n\n\n",
    "jawaban": "The correct answer is: Distance Transformation"
  },
  "4586758778402a932f127f69380a235361428cd18064c5942322ad665dde4519": {
    "soal": "\n\n\nData visualization with a single variable that is unordered can use ..............................\n\n\n\n",
    "jawaban": "The correct answers are: histogram, density plot, box plot"
  },
  "e6c9a89186bdb004345f12e7f179f0c2c4b8ea43a308dc7b4bca7e0a75c211c1": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#_xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nmap = pm4py.discover_heuristics_net(log)\npm4py.view_heuristics_net(map)\nWhat is the extension of the output visualization produced by pm4py.view_heuristics_net?",
    "jawaban": "The correct answer is: Graphical visualization (interactive) -CSV -JSON -TXT -XLS -PDF"
  },
  "ae525c895f3f49f026d76557e88be909f752ca6c958929e0db502f9d3ca5bd0c": {
    "soal": "Which package is installed to enable NVIDIA GPU support in Docker?",
    "jawaban": "The correct answer is: nvidia-docker2"
  },
  "45f4751057e47da38f7c33920efa55b218a69e01c897ba8a323b70e153ae0095": {
    "soal": "CN2 Rule Induction can be used for both classification and regression tasks.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "2af07ca305cf58eeefe183194da600a17eeafb0bfbfc5d80c14bb9d4930ade1e": {
    "soal": "\n\n\n\n\n\nThe above Workflow in the world of machine learning is part of supervised learning.\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "5ffc4c076333cf9709fdb335b416eb281d3dcb814222cc7d4d3b83f67985a2b1": {
    "soal": "The workflow example below shows the use of the ................. widget, which is quite standard. We calculate the distance between rows in a sample of the Iris dataset and output it in the Distance Matrix. Unsurprisingly, Iris Virginica and Iris Setosa are the farthest apart.\n\n\n\n",
    "jawaban": "The correct answer is: Distance Matrix"
  },
  "0f666f1dcba79fd2f3f36463c5a74de9fe53e86512fbf49c1bec709da0e7fa5c": {
    "soal": "The Import Images widget is probably the first widget we will use in ..................... The Import Images widget loads images and creates a class value from the folder. In this example, we use the Import Images widget to load 26 paintings by Monet or Manet.\n\nWe can observe the results in the Data Table widget. It is clear how Orange adds an additional class attribute with values Monet and Manet.\n\nNext, we can continue with standard machine learning methods. We send the images to the Image Embedding widget, where we will use the Painters embedder to get the image vector.\n\nThen we will use the Test & Score widget and the Logistic Regression widget to create a model to predict the artist of a painting (in this case Monet or Manet). We get a very good score. How could that be? It turns out these images were already trained on the Painters embedder, so the accuracy achieved will be high.\n\n\n",
    "jawaban": "The correct answer is: image analysis"
  },
  "9f56abb9f1defa569c21dd11840577c6a12945461a277afe19d054f9b3bdad9a": {
    "soal": "In the workflow below, the Import Documents widget can load subfolders. We store President Kennedy's speeches in two (2) subfolders, pre-1962 and post-1962. If we load the parent folder, both subfolders will be used as class labels. Check the output of the Import Documents widget in the .......................\n\n\n\n\n",
    "jawaban": "The correct answer is: Data Table"
  },
  "8ee9741ebb2e98f0ef3a342cfa6ff82d0a2f83d4f800974d456254cbb3ff0908": {
    "soal": "The ............................. widget can visualize distances between objects. The visualization is the same as printing a table of numbers, except the numbers are replaced with colored dots.\n",
    "jawaban": "The correct answer is: Distance Map"
  },
  "cb4f1d1be22906ea525235736b21bb559a53aaac0352932738babb9b3627aa35": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nx_train = np.random.rand(1000, 10)\ny_train = np.random.randint(2, size=(1000, 1))\nmodel = keras.Sequential([\nkeras.layers.Dense(64, activation='relu', input_shape=(10,)),\nkeras.layers.Dense(32, activation='relu'),\nkeras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer='adam',\nloss='binary_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=10, batch_size=32)\nWhat is the shape of the input layer?",
    "jawaban": "The correct answer is: (10,)"
  },
  "79346fd5d29f2c706fb8fe9d668d6f441f592b53fee339cb6743173625a8ab19": {
    "soal": "In the workflow below, the usage of the .................... widget is shown to obtain a 5-day moving average, we can use a rolling window with mean aggregation.\n\n",
    "jawaban": "The correct answer is: Moving Transform"
  },
  "128702406a81cf0e2ab00bdd7f9c7d19a2106daa676ad29578b891c00570f49d": {
    "soal": "import tensorflow as tf\nfrom tensorflow.keras import layers, models\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nmodel = models.Sequential([\nlayers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(64, (3,3), activation='relu'),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(128, (3,3), activation='relu'),\nlayers.Flatten(),\nlayers.Dense(128, activation='relu'),\nlayers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5, batch_size=64)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nWhat dataset is used in the example?",
    "jawaban": "The correct answer is: CIFAR-10"
  },
  "540f3bb86d1b5d58fe6ed33c24826df67b893ab39474676093e5f987baac71c9": {
    "soal": "PCA can be used to simplify the visualization of large datasets. Below, we use the Iris dataset to show how we can improve the visualization of the dataset with PCA. The transformed data in the Scatter Plot does not show much clearer differences between classes than the default settings.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "73d207d52c5e3ac68a8ffd1d21cef7c1ea79ee12276bca01c8a49b85c870626f": {
    "soal": "If the Import Documents widget fails to read a specific file for any reason, that file will be skipped. Files that are successfully read will be sent to the input.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "4df0f690c9fdfa5a83e50b004eb3d4c12495740aae628a05431428b5226de13f": {
    "soal": "Which port is exposed by the PostgreSQL service for database connections?",
    "jawaban": "The correct answer is: 5432"
  },
  "35e530307376920a194a33494981f0a682bbdbf01eee104f126795827c50a68d": {
    "soal": "\n\n\n\n\n\n\n\n\nSupervised Learning techniques are generally divided into:\n\nClassification.\nClustering.",
    "jawaban": "The correct answer is 'False'."
  },
  "b92d35326a55a2bcf952ff6d11bed1137fbf639ff634e2f69eb4853535e2c7ef": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\n# xes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\nxes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nprocess_tree = pm4py.discover_bpmn_inductive(log)\nbpmn_model = pm4py.convert_to_bpmn(process_tree)\npm4py.view_bpmn(bpmn_model)\nWhat type of file is \"BPI_Challenge_2019.xes\"?",
    "jawaban": "The correct answer is: Event log file"
  },
  "7bb7998851a9ba197a0bae1c7c70dca6bd37469766d5e216b3858fdcd694c155": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#_xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nmap = pm4py.discover_heuristics_net(log)\npm4py.view_heuristics_net(map)\nWhich function discovers the heuristic net?",
    "jawaban": "The correct answer is: pm4py.discover_heuristics_net -pm4py.view_heuristics_net -pm4py.create_heuristics -pm4py.find_heuristics -pm4py.analyze_heuristics -pm4py.generate_heuristics"
  },
  "d9b1195702930e003f313b76b461f77987d8dd5f2215ce3becafdda9ebc113c0": {
    "soal": "What is a Tensor in TensorFlow?",
    "jawaban": "The correct answer is: A multi-dimensional array"
  },
  "0f59d296ccd6778d374d8413ec0414b01269c21b6844321daed4b8fc3493c7b0": {
    "soal": "The Distributions widget displays the distribution of values for ..................... attributes\n\n\n\n",
    "jawaban": "The correct answer is: <p>one<br></p>"
  },
  "66403f8d8e70e93dd1c3a688433a2760278aef4160212d11d9eb96842d6a138b": {
    "soal": "The Image Viewer widget will display .................\u00a0 from the dataset.\n",
    "jawaban": "The correct answer is: image"
  },
  "307114b33c7b42ab3d6b963f95970dfeed4f4e42309b0af60300ddb1c0d30f81": {
    "soal": "In the workflow below, we use two File widgets to read the Iris and Glass datasets (provided in the Orange distribution), and send them to the Data Save widget.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "5a0fe57d5c7d05209ff92a9649ede14c4f122684ff639c8c1147da12976b74c6": {
    "soal": "model.save('my_model')\nfrom google.colab import files\nfiles.download('my_model')\nnew_model = tf.keras.models.load_model('my_model')\nWhat type of file does 'model.save' generate by default?",
    "jawaban": "The correct answer is: A directory or HDF5 file depending on format"
  },
  "9339f97a9d17ac0bd54aff736fa17e4d85581bcc7ae414df2f6d24d288a6caf2": {
    "soal": "Widget Save Data can consider the dataset provided in the input channel and save it to a data file with the specified name. It can save data as a stream with tab-separated or comma-separated data.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "8b12cdeaf89b973a4cf493b096998681bfe6c1ce5e734dcf9c6c88a43df6059a": {
    "soal": "The ................(1)............ widget can apply the .............(2)............... function to the ................(3)............. Use the Moving Transform widget to get the average value from the series.\n",
    "jawaban": "The correct answer is: (2) \u2192 rolling, (1) \u2192 Moving Transform, (3) \u2192 time series"
  },
  "592395dcd19f4fbfb3cae764701e9ff89fed8aec125ffc6f4eb15a81dec1f22f": {
    "soal": "The ............................ widget displays information from the selected dataset.\n\n\n\n",
    "jawaban": "The correct answer is: Data Info"
  },
  "ceb291453a502db7739d27a93fe9b3ddf021b3e978564c4cc89f473560d8d484": {
    "soal": "To obtain numerical representations of the images from Image Import in Orange, we will send the images to the Image Embedding widget in Orange. As a result, we will have the numbers we want. There are 2048 of them (columns n0 to n2047). Therefore, we can apply all standard machine learning techniques, such as clustering, etc.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "2b13eaa4918b957758e0fe5793c78ef46af2737ee4024b93f12f9c8be12fdc36": {
    "soal": "Handling Sparse data in the Distances Widget can only be used with Euclidean, ........................... and Cosine metrics.\n",
    "jawaban": "The correct answer is: Manhattan"
  },
  "af73e8050abacdea4acd30403b26bd34f8a8c963db8e7e1fdd3a79c0c6f03f53": {
    "soal": "The Moving Transform widget uses the Aggregation function to aggregate values in time series windows. The available options are: mean, sum, max, min, median, mode, standard deviation, variance, product, linearly-weighted moving average, exponential moving average, harmonic mean, geometric mean, non-zero count, integrated, and cumulative product.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "2a6b5019d96d673035f9d0fe03f8b54c896ccbd20c10bab0639adb7e8e0ef931": {
    "soal": "The Constant Widget will predict the most frequent class or max value from a training set.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "9346946da63be459a406b613ee8400af9528b16304945a3eec766c5c1fa8f15f": {
    "soal": "The Line Plot widget is a standard visualization widget that displays data profiles, typically numeric data arranged in order. In this simple example, we will display iris data in a line plot, grouped by iris attributes. The plot shows how petal length separates class values well.\n\n\nIf we observe this in the Scatter Plot widget, we can confirm this is true. Petal length is an interesting attribute for class separation, especially when enhanced with petal length, which is also well separated in the line plot.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "60c22860454511010e531faed38d8ee0fd1fb0bd2f0b88b3e0beea4b9f6b6ff0": {
    "soal": "The ................................. widget can manually select data attributes and the composition of the data domain.\n",
    "jawaban": "The correct answer is: Select Columns"
  },
  "4046660c8c781c2c5f65e68327d2d40c1d2607b6dafb199f52ff5653d071270b": {
    "soal": "The winner of the Turing award, Jim Gray, envisioned data science as the \"fourth paradigm\" of science:\n\nempirical\ntheoretical\ncomputational\ndata-mining\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "e0b003414df7fef018d7cc5a3cfdbafad45e64ca57660c1107d258215e2456da": {
    "soal": "The Image Viewer widget will display the perceptions in the dataset.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "a67372303e3b144bf1ae29d9e24a6ddf22d8eb96d28a5fec1f4902aaa3cbc14f": {
    "soal": "Which of the following is NOT a typical method of accessing the web for scraping?",
    "jawaban": "The correct answer is: Sending emails to website owners"
  },
  "0d28a8e7d6fadf67928a0ef94caefec5b5a6cbacf4b694d2878886a185d0f953": {
    "soal": "What percentage of the dataset is typically allocated to the Testing Set?",
    "jawaban": "The correct answer is: 10-15%"
  },
  "74894ac0f504053bc9f8ae91b4888fa6877ddf7e52dc6bd68faeff8f230c1405": {
    "soal": "The Text Preprocessing widget can break text into smaller units (tokens), filter them, perform normalization (stemming, lemmatization), create n-grams, and tag tokens with part-of-speech labels. The steps in the analysis are applied randomly and can be turned on or off.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "246e13f5906980864f2ba0b8f1a74244d39f3b8327cf0d472528a70a8def2322": {
    "soal": "Using the workflow above, if we input purchase data from an e-commerce site, we can find customer segmentation and discover which groups of items are frequently bought together.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "59ba06246ef12839c00aded8df31cddf78f7afaa1d607454f47675a14b76c4fb": {
    "soal": "The basic framework of Apache Hadoop consists of the following modules:\n- Hadoop \nAnswer Question 47\n - contains libraries and utilities required by other Hadoop modules;\n- Hadoop Distributed File System (HDFS) - a distributed file system that stores data on commodity machines, providing very high aggregate bandwidth in clusters;\n- Hadoop YARN - a resource management platform responsible for managing computing resources in the cluster and using them for user application scheduling; and\n- Hadoop MapReduce - a programming model for large-scale data processing.",
    "jawaban": "The correct answer is: Common"
  },
  "89ac5589d2cebd614223488004ba56baf82c26aff204acf8c446803ab958595f": {
    "soal": "Widget Calibrated Learner membungkus / melanjutkan kerja dari learner lain dengan probability calibration dan decision threshold maximation.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "eaaa18ec9d0af52e3bb264c30b4e3b9f3dbd999d7338534f8764361f9f344185": {
    "soal": "import pm4py\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\n# Import XES log\nlog = xes_importer.apply(xes_filename)\n# Cek jumlah kasus dan aktivitas\nprint(f\"Jumlah kasus: \")\nactivities = set()\nfor trace in log:\nfor event in trace:\nactivities.add(event['concept:name'])\nprint(f\"Jumlah aktivitas unik: {len(activities)}\")\nfrom pm4py.algo.discovery.dfg import algorithm as dfg_discovery\nfrom pm4py.visualization.dfg import visualizer as dfg_visualization\n# DFG berdasarkan frekuensi\ndfg = dfg_discovery.apply(log,\nvariant=dfg_discovery.Variants.FREQUENCY)\n# Tampilkan visualisasi\ndfg_vis = dfg_visualization.apply(dfg, log=log,\nvariant=dfg_visualization.Variants.FREQUENCY)\ndfg_visualization.view(dfg_vis)\nHow do you check the number of unique activities in the event log using Python?\n{\n=By iterating over events and adding to a set\n-Using pandas' describe() method\n-Counting rows in Excel\n-Checking log properties directly\n-Using matplotlib to plot\n-Using numpy unique()\n}",
    "jawaban": "The correct answer is: len(log)"
  },
  "347a5e9eaf793d52f67eff0cb4c47dcab1e41e60bade27163bb691a2366cf70a": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.reuters.load_data(num_words=10000)\nIn the Reuters Newswire dataset, what do 'y_train' and 'y_test' represent?",
    "jawaban": "The correct answer is: Topic labels (0-45)"
  },
  "ee9f3290d38ad2002cf1f7c93b7a1cdf35758d3c872f516f6591b38ca4163c67": {
    "soal": "\n\n\n............................ was introduced/found in 1970.\n",
    "jawaban": "The correct answer is: Relational Databases"
  },
  "b8229662f1220bdc0cdbb3dc4f52d2ff8cc1fdd1b7e3f49c9cbe99ae8db2f9d9": {
    "soal": "Which command provides an interactive shell inside a running Docker container?",
    "jawaban": "The correct answer is: docker exec -it /bin/bash"
  },
  "5c518871039d55978218f1247d547881c69e0b569f9df06b5333879da9257774": {
    "soal": "The Distances widget calculates the distance between rows or columns in the dataset. By default, the data is normalized to ensure equal treatment of individual features. Normalization is always done column-wise (using columns as the reference).\n",
    "jawaban": "The correct answer is 'True'."
  },
  "877633fb12e466db1c9f3939d563da7b5cdbcdd8b8d20a108a87a646cf5f659e": {
    "soal": "The workflow below demonstrates the use of the Confusion Matrix widget\n\nTest & Score obtains data from File and two learning algorithms from Naive Bayes and Tree. Test & Score performs cross-validation or other train-and-test procedures to obtain class prediction by both algorithms for all (or some) data instances. The test results are sent to the Confusion Matrix, where we can observe how many instances were misclassified and why.\n\n\nIn the output, we use the Data Table to show examples selected in the confusion matrix. If we, for example, click Misclassified, the table will contain all instances that were correctly classified by the selected method.\n\n\nThe Scatter Plot receives two sets of data. From the File widget, it gets the full data, while the confusion matrix only sends selected data, such as misclassified data. The Scatter Plot will display some data, with bold symbols representing selected data.\n\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "f4b8ee859558c605caebeef72c16d71c521eebf12182425a8e0892aae47fcec4": {
    "soal": "In the workflow below, the use of the Moving Transform widget to obtain a 5-day moving average is shown, we can use a max transmission window with mean aggregation.\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "6d0fdbc3519489366470027f4d98006b1b909b4d23ce5a2415bac3d0dc239808": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.rand(100) # 100 nilai target\n# Inisialisasi model regresi linier\nmodel = LinearRegression()\n# Tentukan jumlah fold (misalnya, 5-fold cross-validation)\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n# Evaluasi model menggunakan cross-validation\nscores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n# Menampilkan skor R^2 untuk setiap fold dan rata-ratanya\nprint(\"Skor R^2 untuk setiap fold:\", scores)\nprint(\"Rata-rata skor R^2:\", np.mean(scores))\nWhat is the purpose of the `np.random.seed(42)` statement in the code?",
    "jawaban": "The correct answer is: To ensure reproducibility of random number generation."
  },
  "a38c7ed09cb6d8f30bd5656daa739d1490ca25310fbcf32b1bf0859a19e131b3": {
    "soal": "The Predictions widget receives a dataset and only one predictor (predictive model, not learner algorithm). The Predictions widget generates data and predictions.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "3f63506a1a73b642d1bf145cd79241bc43f7090ff44a444f8527eb9cb2ecdc5a": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nHow are the MNIST images normalized in the provided example?",
    "jawaban": "The correct answer is: Dividing by 255.0"
  },
  "da2d47fb656531e8e5920af92a5ba18bf6ee11ef9b35110d4af249cb2de7e6dc": {
    "soal": "In Orange, the input dataset is usually in the form of a table, with data instances (samples) in rows and data attributes in columns. Attributes can consist of various types (numeric, categorical, datetime, and text) and have defined roles (input feature, meta attribute, and class). The type and role of data attributes can be provided in the data table header. They can also be modified in the File widget, while data roles can also be modified with the Select Rows widget.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "c53df4d48252d7b1ea3362a23a2be7c91d0806c0464bd6616875cc95efd20e4f": {
    "soal": "What environment variable sets the API base URL for Ollama in the Open WebUI service?",
    "jawaban": "The correct answer is: OLLAMA_API_BASE_URL"
  },
  "8a114ba7e53afacbb38c81c3488c867170fda54cb0780e9b11dc7835773bcee9": {
    "soal": "import tensorflow as tf\nfrom tensorflow.keras import layers, models\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nmodel = models.Sequential([\nlayers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(64, (3,3), activation='relu'),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(128, (3,3), activation='relu'),\nlayers.Flatten(),\nlayers.Dense(128, activation='relu'),\nlayers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5, batch_size=64)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nHow many epochs is the model trained for?",
    "jawaban": "The correct answer is: 5"
  },
  "e4b9f012d8aaceabfc81440890f256804168423c29427d3c47be5e3555c03717": {
    "soal": "The CN2 algorithm is a classification technique designed to efficiently perform induction of simple and comprehensive rules in the form of \u201cif .................. then predict class\u201d, even in domains where there may be noise.\n",
    "jawaban": "The correct answer is: condition"
  },
  "c123ae603ef5591d957c416377e4bd64198052e632e5e83ab301234250a1aa67": {
    "soal": "The CN2 algorithm is a classification technique designed to efficiently induce simple and comprehensive rules in the form of \u201cif cond then predict class,\u201d even in domains where there may be noise.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "e9fe3b6104847e599c5c38ce6a7065c4be931df817b0e10ffc1bf2c7085cddb6": {
    "soal": "Simple example with the Calibrated Learner. We use the Titanic dataset because this widget requires a binary class value (in this case, they are 'survived' or 'not survived').\n\nWe use Logistic Regression as the base learner, which will be calibrated with default settings, i.e., with sigmoid optimization of the distribution of values and optimized with CA.\n\nComparing the results of the uncalibrated Logistic Regression model, we will clearly see that the calibrated model is ..................\n\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: better"
  },
  "dc7b35ee46f5613c4dcf753e3f11beda37e56dd673edf13464e276ce561b1c6d": {
    "soal": "How can you print the version of TensorFlow in Google Colab?",
    "jawaban": "The correct answer is: print(tf.__version__)"
  },
  "a82a8c7814280297edba1ab466b21d1af16b95d7aa357c8d37e4ec278a71cf92": {
    "soal": "Salah satu kerugian Orange adalah Workflow analisis data interaktif dengan toolbox yang banyak.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "fb0740500868e89abac8b39a18bc3f841e494d1b53418fd9fa050a29f20c2141": {
    "soal": "When there is no data in the input, the Corpus widget reads text corpus from a file and sends the corpus instance to the output channel. The history of the last opened file is saved in the Corpus widget. In the Corpus widget, there is also a directory with ............(1)............ corpora that have been pre-installed with the add-on. The widget can read data from Excel files (.xlsx), comma-separated (.csv) and ...........(2).............-delimited (.tab).\n",
    "jawaban": "The correct answer is: (2) \u2192 tab, (1) \u2192 sample, \u2192 corpus"
  },
  "1e027d5050dd61c10d50d1557c7f0159d7eda87ff6703ea26f7533a5832c4a51": {
    "soal": "Orange uses its own data format, but can also handle Excel, comma- or tab-delimited data files. Input datasets are usually in the form of tables, with data instances (samples) in rows and data attributes in columns. Attributes can consist of various types except ....... and have designated roles (input features, meta attributes, and class). The type and role of data attributes can be specified in the data table header.\n\nSelect ..... below,\n",
    "jawaban": "The correct answer is: binary"
  },
  "995fe66ca6c57e574d2edbe5f618c416faa41a5e09e98edd4237b1413282743b": {
    "soal": "Which command stops all running Docker containers?",
    "jawaban": "The correct answer is: docker stop $(docker ps -q)"
  },
  "cc139d350ff9132f2f4380452b2e0bb7cf664cf0e92296f362bd29f7885e3f12": {
    "soal": "What enables continued conversation with an LLM?",
    "jawaban": "The correct answer is: Reinserting outputs into the context"
  },
  "f7438b64f822ebd0cdaf0b87b3ab7de1774ff715218ad6407e3bdf94e32688af": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nplt.imshow(x_train[0], cmap='gray')\nplt.title(f\"Label: \")\nplt.show()\nmodel = keras.Sequential([\nkeras.layers.Flatten(input_shape=(28, 28)),\nkeras.layers.Dense(128, activation='relu'),\nkeras.layers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nWhich function is used to display the image?\n{\n= plt.imshow()\n~ plt.plot()\n~ plt.display()\n~ plt.draw()\n~ plt.map()\n~ show.image()\n}",
    "jawaban": "The correct answer is: y_train[0]"
  },
  "2ddc245bc3576477cd4b230517145f06beb1e2141b740b5f9b109a16c7fdc059": {
    "soal": "How do LLMs relate to human language?",
    "jawaban": "The correct answer is: They imitate human language structure and use"
  },
  "f01ac4b97d14642132caa3e2abfa521e52fdec79a2226b38afa83b096845e5dd": {
    "soal": "Proper Orange3 installation can use the following command\n\npip3 install numpy scipy mkl nose sphinx pydot-ng pandas-datareader\npip3 install parameterized Theano dodol matplotlib sklearn seaborn h5py\npip3 install matplotlib pandas scipy sklearn seaborn features\npip3 install orange3\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "60f83cb63a259bc9a145f297884313771146344b7ec609f414e0571bbc96d326": {
    "soal": "Another alternative to the hash notation format is Orange's native data format with three (3) header rows: the first with attribute names, the second specifying the type (continuous, discrete, time, or string), and the third row providing information about the attribute roles (class, meta, weight, or ignore).\n",
    "jawaban": "The correct answer is 'True'."
  },
  "c68f888479902e3be12c8869fb4374204927cab45c5ea6e3f514df7980c03dcc": {
    "soal": "In a production environment or real-world application, what is the ideal dataset size?",
    "jawaban": "The correct answer is: The larger, the better"
  },
  "d90b664180edac4f85a9f132f2d96bd416553620f1e1eadb91fa33042b9e8c6c": {
    "soal": "Which of these tools is built on Scrapy and provides a web interface?",
    "jawaban": "The correct answer is: portia"
  },
  "e4ebeadc84a4608aab7312a454b2a082e9d96c38a5d4b30bfdb0a0038052e35c": {
    "soal": "In the Interpolate widget, which is part of the Time Series add-on, the interpolation type - ...........(1)............ interpolation ............(2)............ missing values with previously defined values.\n\n",
    "jawaban": "The correct answer is: (1) \u2192 Nearest, (2) \u2192 replaces"
  },
  "1b0a5754b7bd18a7bde56f3559456d829a711f2a7e17d22dcf3e13477ccfc692": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nWhat type of color channels do the Fashion-MNIST images have?",
    "jawaban": "The correct answer is: Grayscale"
  },
  "673f83b339a03136a52e3aedaee01481e668c9ee8003515246ab89a9f279c352": {
    "soal": "In Orange, the input dataset is usually in the form of a table, with data instances (samples) in rows and data attributes in columns. Attributes can consist of various types (numeric, categorical, datetime, and text) and have defined roles (input feature, meta attribute, and class). The type and role of data attributes can be provided in the data table header. They can also be modified in the File widget, while data roles can also be modified with the Select Columns widget.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "7277a90c0d2200e35fc39ffc64feef0035acc91f86f8ecfc2f7446227eeb4d5a": {
    "soal": "Which command checks for any dependencies required by a specific model?",
    "jawaban": "The correct answer is: ollama check model"
  },
  "b6dc9e24318953f2ccf9066bc1156a6367766fec1745e3c35c782b0ee55c28a9": {
    "soal": "How do you import a previously exported model into Ollama?",
    "jawaban": "The correct answer is: ollama import model"
  },
  "415aa91d36fe5e58754198b0df7151d64700bc4e92bdb32790bed3e64960dafc": {
    "soal": "The Box Plot widget is most commonly used immediately after the File widget to observe regression properties of a dataset. In this example, we have used heart-disease data to examine our variables.\n\n\n\n\nBox Plot is also useful for finding properties of a specific dataset, for example, a set of instances manually defined in another widget (e.g., Scatter Plot, or instances from several clusters, or nodes in a classification tree).\n",
    "jawaban": "The correct answer is 'False'."
  },
  "3886719ce6fc9091b6c93642483ec46945d0c013dd703f78fb918233905edddc": {
    "soal": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n# Contoh: Memuat dataset dari file CSV\n# Gantilah 'data.csv' dengan path ke dataset Anda\ndf = pd.read_csv('data.csv')\n# Memisahkan fitur (X) dan target \n# Asumsikan kolom terakhir adalah target\nX = df.iloc[:, :-1]\ny = df.iloc[:, -1]\n# Membagi dataset menjadi data latih dan data uji (80% : 20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model Decision Tree\nmodel = DecisionTreeClassifier()\n# Melatih model dengan data latih\nmodel.fit(X_train, y_train)\n# Melakukan prediksi pada data uji\ny_pred = model.predict(X_test)\n# Menghitung akurasi\nakurasi = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model: ')\n# Menampilkan laporan klasifikasi yang mencakup presisi, recall, dan F1-score\nprint(classification_report(y_test, y_pred))\nWhat is the purpose of the `train_test_split` function?\n{\n~To train the machine learning model.\n~To test the accuracy of the model.\n=To split the dataset into training and testing subsets.\n~To visualize the data distribution.\n~To normalize the dataset features.\n~To encode categorical variables.\n}",
    "jawaban": "The correct answer is: akurasi:.2f"
  },
  "937e3e02651eeee14f95167b70e90662f6a9bac2689669f8a0c13874cb0fd7ea": {
    "soal": "\n\n\nIn preprocess text, we can do several things, such as\n\n\n\nConvert all letters to lowercase.\nRemove (stop words), words that are not useful, such as conjunctions like and, in, to, from, etc.\nRemove HTML tags\nRemove URLs\nTranslate all text/words to Indonesian.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "73e3cdc0b9ebed3e657a4811286f65da83f68abf93aa675250f94c80f041ef63": {
    "soal": "In the Interpolate widget, which is part of the Time Series add-on, the interpolation type - Nearest interpolation adds missing values with previously defined values.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "541cbf839c7de8a7928f1128e09f4b5058f9e880bcde2d827758b009f087ce7f": {
    "soal": "In the following example, we will look at how to properly use Preprocess with the .............. widget or the Test & Score widget.\n\nThis time, we are using the heart disease.tab data from the File widget. We can access the data through the dropdown menu. This is a dataset with 303 patients who visited the doctor suffering from chest pain. After tests were conducted, some patients were found to have narrowed arteries, while others did not (this is our class variable).\n\nThe heart disease data has some missing values, and we want to handle that. First, we will split the dataset into training and test data using the Data Sampler.\n\nNext, we will send the Data Sample into Preprocess. We will use the Impute Widget to address the missing values, but we can try combinations of preprocessors on our data. We will send the processed data to the Logistic Regression Widget and the built model to the Predictions Widget.\n\nFinally, the Predictions Widget also needs data to predict. We will use the output of the Data Sampler Widget for predictions, but this time not the Data Sample, but the Remaining Data (the remaining data), which is data not used for training the model.\n\nNotice how we send the remaining data directly to the Predictions Widget without applying any preprocessing. This is because Orange handles preprocessing on new data internally to avoid errors in model construction. The exact same preprocessor used on training data will be used for predictions. The same process applies to the Test & Score Widget.\n\n\n",
    "jawaban": "The correct answer is: Predictions"
  },
  "abb1ad06f210fd35a02cdd8f6bed6b05d4f400e4c0ccd29e442018e0d72df4e4": {
    "soal": "The Test & Score widget will test the learning algorithm on the results.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "e69f5a657cf8e801eebf3841b01a73644eb0d11ef131c083641f1b302b6fcc15": {
    "soal": "What does the `try-except` block do in TPU detection?",
    "jawaban": "The correct answer is: It handles the case when TPU is not available"
  },
  "1ad72c0258500d60ce85829be86ff4ee549158163d11f02ca1885d210f16f065": {
    "soal": "The ........................... widget reads the input data file (data table with data instances) and sends the dataset to its output channel. The history of the last opened file is saved in the widget. This widget also includes a directory with sample pre-installed datasets in Orange.\n",
    "jawaban": "The correct answer is: File"
  },
  "17f509280f389664bae949fd6ae773fa8467a2aab5bd2a3ade6cc074ba9d50cf": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\n# xes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\nxes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nprocess_tree = pm4py.discover_bpmn_inductive(log)\nbpmn_model = pm4py.convert_to_bpmn(process_tree)\npm4py.view_bpmn(bpmn_model)\nWhich component of the script is explicitly visualized at the end?",
    "jawaban": "The correct answer is: BPMN model"
  },
  "dfa8ebb0ebd9be7612c7c995c8f0d7c3b8ae443bc3b6cb6b7a6682b3516ee88f": {
    "soal": "What is the primary function of the Flatten layer in the TensorFlow model?",
    "jawaban": "The correct answer is: To convert a 2D matrix into a 1D vector"
  },
  "6d75bc7156c8ee0b25cb61f04f39d9793b5c63233ef16c98504e6318710383c3": {
    "soal": "Line plot, a type of plot that displays data as a series of points connected by straight line segments. Line Plot works for image data. For categorical data, Line Plot can be used for grouping data points.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "d30985c73e892921a6c01ae7f96f551395f81f18d57a077f84f77e99659c7831": {
    "soal": "import pandas as pd\nfrom pm4py.objects.conversion.log import converter as log_converter\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom pm4py.visualization.petrinet import visualizer as pn_visualizer\nfrom pm4py.objects.log.util import dataframe_utils\n# csv_filename='Insurance_claims_event_log.csv'\n# csv_filename='Business Travel Permits.csv'\n# csv_filename='Parent-Child Union Output.csv'\n# csv_filename='PM4PY_Dummy_Event_Log_100_Cases_pm4py.csv'\n# csv_filename='PM4Py_Dummy_Event_Log-congested-business-process.csv'\n# 1. Load CSV event log\ndf = pd.read_csv(csv_filename)\n# df = pd.read_csv(csv_filename,sep=';')\n# print(df.head())\n# 2. Format timestamps properly\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n# 3. Ensure column names match PM4Py expectations\ndf = dataframe_utils.convert_timestamp_columns_in_df(df)\n# 4. Convert to event log object\nevent_log = log_converter.apply(df, parameters=)\n# 5. Discover process model using Alpha Miner\nnet, initial_marking, final_marking = alpha_miner.apply(event_log)\n# 6. Visualize Petri Net\ngviz = pn_visualizer.apply(net, initial_marking, final_marking)\npn_visualizer.view(gviz)\nWhich column name is essential for activities in PM4Py event logs?\n{\n~id\n~case\n=activity\n~log\n~process\n~event\n}",
    "jawaban": "The correct answer is: log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case_id'"
  },
  "15260bcd890a36ca8478fed473dd256a20517b3a1f773851a7dd57b414f4bafc": {
    "soal": "Which architecture underlies LLMs?",
    "jawaban": "The correct answer is: Transformer"
  },
  "a2576e0c54dfd4171bfa92a149b4a6c1b7c7e624f9e7c641b9285445755f8bb3": {
    "soal": "The CN2 algorithm is a classification technique designed to efficiently perform .............. of simple and comprehensive rules in the form of \u201cif cond then predict class\u201d, even in domains where there may be noise.\n",
    "jawaban": "The correct answer is: induction"
  },
  "a2d983ca9b853296fa477e972e34b8268bac2c8c8c4920f1b3a7d36dfffea16a": {
    "soal": "Handling Sparse data in the Distances Widget can only be used with Euclidean, Manhattan and ......................... metrics.\n",
    "jawaban": "The correct answer is: Cosine"
  },
  "b9f24f5cb7adbf2ee24063cf42f511294d9c10ae124d996afb593e5d7819db57": {
    "soal": "The Import Images widget allows us to import image (..........................) from a directory.\n",
    "jawaban": "The correct answer is: gambar"
  },
  "e9b19e60d9be135f9338499e516e6d5872ee174455ba5f869134ca767ba55d5b": {
    "soal": "How can you ensure that your data split is reproducible across different runs?",
    "jawaban": "The correct answer is: By setting the `random_state` parameter to a fixed integer value."
  },
  "18a08dd8a5a20f1955c22a227eeb4a3ead668d2834fb28316d63720c81827b69": {
    "soal": "In the workflow below, we use the Zoo dataset. We load data into the Scatter Plot widget, which allows us to select a subset of data instances. Then, we can send the selected data instances to the Load Data widget to save them into a file.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "9c9edc0a70318f033f4dad23c286fdd3b3dfe126620068b4b619da7ff6b47b1a": {
    "soal": "Currently, the only widget that provides the correct signal required by the Calibration Plot widget is the ......................... widget. Therefore, the Calibration Plot widget always follows the ............ widget and, since it has no output, no other widgets follow it.\n",
    "jawaban": "The correct answer is: Test & Score"
  },
  "6242a2c6660c93164f4318757744b23f01880e75d1961aecde3dfbb4769ef17f": {
    "soal": "The File widget reads the input data file (data table with data instances) and sends the dataset to its output channel. The history of the last opened file is saved in ....................... This widget also includes a directory with sample pre-installed datasets in Orange.\n",
    "jawaban": "The correct answer is: widget"
  },
  "c7fac416775eca84573351152953c66491538867c991ab6622035863928eecae": {
    "soal": "The following workflow is an example where we compare three (3) classifiers (Naive Bayes, Tree, and Constant) and input them into the Test & Score widget. We are using the Titanic dataset. The Test & Score widget will display the evaluation results for each classifier. Then, we can plot through the Calibration Plot widget and the ROC Analysis widget from the Test & Score output for further analysis of the performance of each classifier. The .................. widget allows us to see the predicted accuracy of class probabilities in a plot/image form.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Calibration Plot"
  },
  "f950796462eaa5c5860dac790f424a256812cee626e3812652fab3626b0b1bbb": {
    "soal": "\n\n\nIn general, machine learning techniques are divided into three (3), which are:\n\nSupervised Learning.\nUnsupervised Learning.\n................................ Learning\n\n\n",
    "jawaban": "The correct answer is: Reinforced / Reinforcement"
  },
  "e6867fdca051def71c0ba7ec4aae4f4279aed828a4dec8a5feb58554270ac98e": {
    "soal": "In the Import Documents Widget, to retrieve data, select (click) the folder icon on the right side of the widget. Select (choose) the folder that we want to make a corpus. After loading is complete, we will see how many documents were successfully retrieved by the Import Documents Widget. To observe the obtained corpus, connect the Import Documents widget to the Corpus Viewer Widget. In this workflow, we are using a set of speeches by President Soekarno in plain text format.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "210b3c66d39c6dc2de29788a256f1438535a957a8dd15ec923e854cd3346b783": {
    "soal": "One URL that contains many examples of Orange workflows for performing data analysis is\n\nhttps://www.youtube.com/channel/UClKKWBe2SCAEyv7ZNGhIe4g\n",
    "jawaban": "The correct answer is 'True'."
  },
  "b656fd474bca1bfdf1aec7666f17424e9e5fdb31063a00fe2bd6287a160e62f9": {
    "soal": "Data attributes in Orange have types discrete, .................... or character string. The attribute type is marked by a symbol that appears before the attribute name (D, C, S).\n\n\n\nAnswer: \nQuestion 10",
    "jawaban": "The correct answer is: continuous"
  },
  "b5cff59381cea14d729baa785e5bdc89b3af20883b347974110a5b3fefcfa0db": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.rand(100) # 100 nilai target\n# Inisialisasi model regresi linier\nmodel = LinearRegression()\n# Tentukan jumlah fold (misalnya, 5-fold cross-validation)\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n# Evaluasi model menggunakan cross-validation\nscores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n# Menampilkan skor R^2 untuk setiap fold dan rata-ratanya\nprint(\"Skor R^2 untuk setiap fold:\", scores)\nprint(\"Rata-rata skor R^2:\", np.mean(scores))\nWhat is the purpose of the `cross_val_score` function in the code?",
    "jawaban": "The correct answer is: To evaluate the model using cross-validation."
  },
  "a77340ab8c63e009281d4d49684daf357ee516a4e88b1b5d6e29e1123e8d8c5a": {
    "soal": "The Select Columns widget is used to manually organize the data domain. Users can decide which attributes to use and how to use them. Orange differentiates between ordinary attributes, class attributes (optional), and ...............(1).................... attributes. For example, to build a classification model, the domain will consist of a set of attributes and a discrete class attribute. Meta attributes are not used in modeling, but some widgets can use them to ...............(2)............. instances.\n",
    "jawaban": "The correct answer is: (1) \u2192 meta, (2) \u2192 label"
  },
  "9d2a76b0d9868b71f242b7f5553523f5c08eaa9c31800637093aab793b3f14b0": {
    "soal": "If\n\n1 AND 1 = 1 and 1 OR 1 = 1\n\n1 AND 0 = 0 and 1 OR 0 = 1\n\n0 AND 1 = 0 and 0 OR 1 = 1\n\n0 AND 0 = 0 and 0 OR 0 = 0\n\nThen in binary\n\n( 100 AND 000 ) OR 111 = ..... (binary)",
    "jawaban": "The correct answer is: 111"
  },
  "ada0436743c794d2bec88b07e8fda6bf7625088c81715c8d49cca544ac42c54c": {
    "soal": "What is the purpose of 'callbacks' in model training?",
    "jawaban": "The correct answer is: To perform specific actions at different stages of model training"
  },
  "c8ea0c1921cf2fe186fc297bf6bbede5d08a32af134f1aad25d51a0a129ba478": {
    "soal": "Which volume is mounted for the Ollama service to persist data?",
    "jawaban": "The correct answer is: ollama_data:/root/.ollama"
  },
  "1993dc6c29ae5c6b219d42939bb193ca34dfcf89d9b5a1d07180f7e18d23bae2": {
    "soal": "Which of the following best describes 'Data Lake'?",
    "jawaban": "The correct answer is: A centralized repository that allows storage of structured and unstructured data at any scale"
  },
  "1f2500e36151a9fb7fab6643a5679b52ede793331a4c25cb3ebc634f637cc82a": {
    "soal": "\n\n\nThe image above shows the sequence of processes in data science, which are:\n\nASK - Ask interesting questions (such as, What do we want to achieve / scientific goal? What would we do if we had all the data? What do we want to predict / estimate?\nGET - Obtain data.\nEXPLORE - Explore data, looking for anomalies, patterns, etc.\nMODEL - Create, fit, and validate models.\nVISUALIZATION - Communicate and visualize the data.",
    "jawaban": "The correct answer is 'True'."
  },
  "749ab0401128272400a2ecfa917716e381b84f42078fbd5c2585e5cbda257752": {
    "soal": "import pm4py\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\n# Import XES log\nlog = xes_importer.apply(xes_filename)\n# Cek jumlah kasus dan aktivitas\nprint(f\"Jumlah kasus: \")\nactivities = set()\nfor trace in log:\nfor event in trace:\nactivities.add(event['concept:name'])\nprint(f\"Jumlah aktivitas unik: {len(activities)}\")\nfrom pm4py.algo.discovery.dfg import algorithm as dfg_discovery\nfrom pm4py.visualization.dfg import visualizer as dfg_visualization\n# DFG berdasarkan frekuensi\ndfg = dfg_discovery.apply(log,\nvariant=dfg_discovery.Variants.FREQUENCY)\n# Tampilkan visualisasi\ndfg_vis = dfg_visualization.apply(dfg, log=log,\nvariant=dfg_visualization.Variants.FREQUENCY)\ndfg_visualization.view(dfg_vis)\nWhich Python command correctly imports the XES importer from PM4Py?\n{\n=from pm4py.objects.log.importer.xes import importer as xes_importer\n-import pm4py.xes_importer\n-from pm4py.importer import xes\n-from pm4py.xes import load\n-import pm4py.log_importer\n-from pm4py.visualization import xes_importer\n}",
    "jawaban": "The correct answer is: len(log)"
  },
  "cf3feeee5c956b48d675ecf74ee70e112bdaead6bff649d4ff2ed2c3c8557769": {
    "soal": "The Moving Transform widget can apply the rolling window function to time series. Use the Moving Transform widget to get the average value from the series.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "257656dce25c7936d3c9af59a81c6988688dd9f5f77bc6bef1c4839b4f2e16f4": {
    "soal": "from sklearn.model_selection import train_test_split\nimport pandas as pd\n# Contoh: Membuat DataFrame dengan fitur 'X' dan target 'y'\ndata =\ndf = pd.DataFrame(data)\n# Memisahkan fitur dan target\nX = df[['X1', 'X2']]\ny = df['y']\n# Membagi dataset menjadi data pelatihan dan pengujian dengan stratifikasi\nX_train, X_test, y_train, y_test = train_test_split(\nX, y, test_size=0.3, stratify=y, random_state=42\n)\n# Menampilkan proporsi kelas di setiap subset\nprint(\"Proporsi kelas di y_train:\n\", y_train.value_counts(normalize=True))\nprint(\"\nProporsi kelas di y_test:\n\", y_test.value_counts(normalize=True))\nIn the `train_test_split` function, what does the `stratify` parameter do?\n{\n~It randomizes the data before splitting.\n~It determines the size of the test set.\n=It ensures that the class distribution in the training and test sets matches the original dataset.\n~It specifies the random seed for reproducibility.\n~It normalizes the data before splitting.\n~It selects features for model training.\n}",
    "jawaban": "The correct answer is: 'X1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'X2': [11, 12, 13, 14, 15, 16, 17, 18, 19, 20], 'y': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]"
  },
  "158da2b307965e2322e5f04e88cb6bd44afbbf48278667a6074627bfb99c39eb": {
    "soal": "To group based on similarity, we can measure the distance between images. The numeric representation of images obtained from the Image Embedding widget can be measured for distance using the Distances widget. Usually, cosine distance works best for images, but you can experiment on your own. Then we will send the distance matrix to Hierarchical Clustering to visualize similar pairs in ..............(1)............\n\nAs a result, all animals are correctly grouped. In the dendrogram of the clustering results, we do not see images of animals. We can use the widget .............(2)................... to view the images.\n\n\n\n\n",
    "jawaban": "The correct answer is: (2) \u2192 Image Viewer, (1) \u2192 dendrogram"
  },
  "5bcf4eaba6aaca3f8ff83246eedb518ebf17620a47f994a00cf6f5f38cb7472f": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#-xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\ndfg, start_activities, end_activities = pm4py.discover_dfg(log)\npm4py.view_dfg(dfg, start_activities, end_activities)\nWhat is the main purpose of the code?",
    "jawaban": "The correct answer is: To discover and visualize a Directly-Follows Graph (DFG) -To perform data cleaning -To train a machine learning model -To perform sentiment analysis -To visualize data distribution -To calculate performance metrics"
  },
  "d7c725c9ca87e27af97c7cb86462f5610ee0c859ec77363c44d840d104c5524f": {
    "soal": "What is the recommended dataset size for a simple binary classification problem (e.g., positive/negative sentiment analysis)?",
    "jawaban": "The correct answer is: Smaller datasets are sufficient"
  },
  "dd8c91bd166a58af6a624caab6eba115ae59fe18f8ba28767297b9aac38133b2": {
    "soal": "import tensorflow as tf\nfrom tensorflow.keras import layers, models\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nmodel = models.Sequential([\nlayers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(64, (3,3), activation='relu'),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(128, (3,3), activation='relu'),\nlayers.Flatten(),\nlayers.Dense(128, activation='relu'),\nlayers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5, batch_size=64)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nHow many Conv2D layers are used in the CNN model?",
    "jawaban": "The correct answer is: 3"
  },
  "14d12dc2e66e99228873e73aaa00b8336c907a013915515a12f384d5c066d0fe": {
    "soal": "One disadvantage of Orange is the interactive data analysis Workflow with a large toolbox.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "e73c30d57018ba40a744e305eb33ff56da29c63459495f8bf3093495a190293a": {
    "soal": "1000 (binary) OR 1111 (binary) = ............................. (binary)",
    "jawaban": "The correct answer is: 1111"
  },
  "4b56d5902eaadf9cb4261551c55efd7df0f3a33be2b41a663df745fabba1ef04": {
    "soal": "Which best describes the role of context in LLM?",
    "jawaban": "The correct answer is: Maintains memory for understanding"
  },
  "fb820151cddd5b5c78c746a39681b6b873b4a70fcfaeed59cb8b7f2b6aa4c739": {
    "soal": "from sklearn.model_selection import train_test_split\nimport pandas as pd\n# Contoh: Membuat DataFrame dengan fitur 'X' dan target 'y'\ndata =\ndf = pd.DataFrame(data)\n# Memisahkan fitur dan target\nX = df[['X1', 'X2']]\ny = df['y']\n# Membagi dataset menjadi data pelatihan dan pengujian dengan stratifikasi\nX_train, X_test, y_train, y_test = train_test_split(\nX, y, test_size=0.3, stratify=y, random_state=42\n)\n# Menampilkan proporsi kelas di setiap subset\nprint(\"Proporsi kelas di y_train:\n\", y_train.value_counts(normalize=True))\nprint(\"\nProporsi kelas di y_test:\n\", y_test.value_counts(normalize=True))\nIn the provided code, how are the features and target variable separated from the DataFrame?\n{\n~Using the `split` function.\n~By applying the `separate` method.\n=By selecting columns for `X` (features) and `y` (target) using DataFrame indexing.\n~Through the `partition` function.\n~By utilizing the `divide` method.\n~With the `subset` function.\n}",
    "jawaban": "The correct answer is: 'X1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'X2': [11, 12, 13, 14, 15, 16, 17, 18, 19, 20], 'y': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]"
  },
  "26d3e49935b03585bc8873f6032f5035040edb612e9889be78164f186c0ceb4a": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nWhat does setting the dropout rate to 0.2 achieve during training?",
    "jawaban": "The correct answer is: It randomly sets 20% of the input units to 0 at each update during training."
  },
  "4bc168be895db4382752228acb3cd7e311d891bca6f7ae3ac6dd33e407e06cec": {
    "soal": "Widget Network Analysis can be used for ......................... of network data.\n",
    "jawaban": "The correct answer is: statistical analysis"
  },
  "3e4140383a66ab705dcd67e7f5077f394936038f04006d6ce2fb499e76aab56b": {
    "soal": "In the Interpolate widget, which is part of the Time Series add-on, the interpolation type - .............(1)............ interpolation replaces missing values with ............(2).............. values between two nearest and defined data points.\n",
    "jawaban": "The correct answer is: (2) \u2192 linearly-spaced, (1) \u2192 Linear"
  },
  "e2185ba892ea420643c28b771cbdd41e80a036c659ca6d95fe2cd0d70078d00f": {
    "soal": "Which of the following is an example of a real-time Big Data processing tool?",
    "jawaban": "The correct answer is: Apache Storm"
  },
  "7fb072d1c637b0f17d037862afedae8ed71cf62987443f9ff1ef03fecd139584": {
    "soal": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n# Contoh: Memuat dataset dari file CSV\n# Gantilah 'data.csv' dengan path ke dataset Anda\ndf = pd.read_csv('data.csv')\n# Memisahkan fitur (X) dan target \n# Asumsikan kolom terakhir adalah target\nX = df.iloc[:, :-1]\ny = df.iloc[:, -1]\n# Membagi dataset menjadi data latih dan data uji (80% : 20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model Decision Tree\nmodel = DecisionTreeClassifier()\n# Melatih model dengan data latih\nmodel.fit(X_train, y_train)\n# Melakukan prediksi pada data uji\ny_pred = model.predict(X_test)\n# Menghitung akurasi\nakurasi = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model: ')\n# Menampilkan laporan klasifikasi yang mencakup presisi, recall, dan F1-score\nprint(classification_report(y_test, y_pred))\nWhy is the `pandas` library imported in the code?\n{\n~To perform numerical computations.\n~To create visualizations of the data.\n=To handle and manipulate the dataset in DataFrame format.\n~To implement machine learning algorithms.\n~To split the dataset into training and testing sets.\n~To evaluate the performance of the model.\n}",
    "jawaban": "The correct answer is: akurasi:.2f"
  },
  "e6b75081b66ea9bfa5c1cb650c178bd754c989cdca090a98d1404f6e4ba826ee": {
    "soal": "Pada Widget Constant ORANGE - untuk classification, ketika memprediksi nilai class dengan Prediction, widget ini akan menghasilkan frekuensi max dari class yang ada di training set. Jika ada dua atau lebih majority class, classifier akan memilih secara random dari predicted class, tapi akan selalu menghasilkan class yang sama untuk contoh tersebut.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "8a768290084af463246e28460cb2355836f787453ed2f6ee695d8f4b71558488": {
    "soal": "Widget ..........(1)............ can load a collection of text documents, (optional) tagged with categories, or convert the input ............(2)............ into a corpus.\n",
    "jawaban": "The correct answer is: (1) \u2192 Corpus, (2) \u2192 data"
  },
  "09b648891ee69ee43fd6e71c14dcbe9ba4d83c53c9d4c579d79055df76704f0d": {
    "soal": "import os\nimport string\nfrom collections import Counter\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\n# Buat daftar stopwords Bahasa Indonesia\nstop_words = set(stopwords.words('indonesian'))\ndef bersihkan_teks(teks):\n# Lowercase + hilangkan tanda baca\nteks = teks.lower()\nteks = teks.translate(str.maketrans('', '', string.punctuation))\nreturn teks\ndef analisis_folder(folder='outputs', top_n=20):\nsemua_konten = ''\nfor filename in os.listdir(folder):\nif filename.endswith('.txt'):\nfilepath = os.path.join(folder, filename)\nwith open(filepath, 'r', encoding='utf-8') as f:\nisi = f.read()\n# Ambil hanya bagian isi, setelah header\nif '\n\n' in isi:\nisi = isi.split('\n\n', 1)[1]\nsemua_konten += isi + ' '\n# Bersihkan teks dan tokenisasi\nteks_bersih = bersihkan_teks(semua_konten)\nkata_kata = teks_bersih.split()\n# Hapus stopwords\nkata_kata = [kata for kata in kata_kata if kata not in stop_words and kata.isalpha()]\n# Hitung frekuensi\ncounter = Counter(kata_kata)\ntop_kata = counter.most_common(top_n)\nprint(f\"\n\ud83d\udd0d Top Kata Paling Sering Muncul:\")\nfor kata, jumlah in top_kata:\nprint(f\"{kata}: {jumlah}\")\n# Buat Word Cloud\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(counter)\n# Tampilkan\nplt.figure(figsize=(12, 6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud dari Semua Konten')\nplt.tight_layout()\nplt.savefig('wordcloud.png')\nplt.show()\nprint(\"\n\u2705 Word Cloud disimpan sebagai 'wordcloud.png'\")\n# Jalankan analisis\nif __name__ == '__main__':\nanalisis_folder('outputs', top_n=30)\nWhat is the role of the 'bersihkan_teks' function in the script?\n{\n=To convert text to lowercase and remove punctuation.\n~To perform sentiment analysis on the text.\n~To translate text from Indonesian to English.\n~To extract keywords from the text.\n~To tokenize the text into individual words.\n~To count the frequency of each word in the text.\n}",
    "jawaban": "The correct answer is: top_n"
  },
  "91e76e2b8e6270234724e5a6a581bc283ae6a22468460a1515e833ef4fabd81a": {
    "soal": "Widget Import Documents takes text files from a folder and creates a ............(1).............. Widget Import Documents can read .txt, ..............(2)..............., .odt, .pdf, and .xml. If there are subfolders in the folder, they can be used to label classes.\n",
    "jawaban": "The correct answer is: (2) \u2192 docx, (1) \u2192 corpus."
  },
  "4bd0f01d3ca4b9f67244518f5bbf150826ce96ad9911eecc9bb78048e82e16cf": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nWhat is the primary purpose of using the Dropout layer in a neural network?",
    "jawaban": "The correct answer is: To prevent overfitting by regularizing the model"
  },
  "8de08ca09e16c12c96fbc6498754ac06da28320a3d50c99ca4075d795b77f9a5": {
    "soal": "Attribute names in the column header can be preceded by a label followed by a hash. Use c for class, m for meta attribute, i for ignoring the column, w for weight (importance) of the column, and C, D, T, S for continuous, discrete, time, and string attribute types. Example: C#mph, mS#name, i#dummy.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "fc0e1df9d9959f67eece7533102fd9761c4911d5bcb1a9c11e7c812f9272e054": {
    "soal": "The Image Embedding widget in Orange is very easy to use. Technically, Orange will send the image to the server, where the server will push the image through a pre-trained deep neural network, such as Google ................(1)................. v3. Deep networks are often trained for a specific purpose. Inception v3, for example, can classify images into one of 1,000 image classes. We can ignore the suggested classification and instead consider the penultimate layer of the network with 2048 nodes (numbers) and use them for the image's .................(2)...................representation.\n",
    "jawaban": "The correct answer is: (2) \u2192 vector-based, (1) \u2192 Inception"
  },
  "a8a9074e6cbdab0c502a522f61beea2315f596fec272eaa180baea6ebc9e7271": {
    "soal": "Data attributes in Orange have types discrete, continuous or character string. The attribute type is marked by a symbol that appears before the attribute name (D, .............................., S).\n\n\n\nAnswer: \nQuestion 44",
    "jawaban": "The correct answer is: C"
  },
  "50c21ea5d8df6c57374cacb27a2617cd324cdd6f3c8ed99d27ede4195ddd8324": {
    "soal": "When the user provides data to the input, the Import Text widget will convert the data into a corpus. The user can choose which feature to use as the text feature.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "b30911430d107b930cb40934c34090315be4ed2ad9337b5a68b5cfaaa8c2d3f0": {
    "soal": "Widget Calibrated Learner menghasilkan sebuah model yang mengkalibrasi distribusi dari class probabilities dan meng-optimasi decision threshold. Widget ini hanya bekerja untuk regression task.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "b25d1f318e6b5e3497e5c10035614a829c736a8f7c18892a021522b3e5dc9268": {
    "soal": "Most Time Series algorithms assume that we do not have missing values in our data. In this widget, we can select the interpolation method to estimate the missing values. By default, the Interpolate widget will use ................. interpolation (quick and reasonable).\n",
    "jawaban": "The correct answer is: linear"
  },
  "80d7654f1c84fc326899d7d4cd2d2895383fd39ddffb443522451dc5d2082c10": {
    "soal": "The Distance Map widget can optimize distances between items.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "d851426c8c34fb481a6af9a81a55820d40e077b7918a27480144ef0615e23895": {
    "soal": "As an example of using kNN for .........................., we use the iris dataset. We compare the results of k-Nearest Neighbors with the default Constant model, which will predict the majority class. It appears that kNN is better.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: classification"
  },
  "a58e7e1b942cbeadae195cc16b0033b5236cdee2b5767b7423ae3e7314ced2b8": {
    "soal": "The Distances widget also works well with other Orange add-ons. The Distance Matrix widget can be fed into the Network from Distances Widget (Network add-on) to convert the matrix into ................... and into the Duplicate Detection Widget (Text add-on) to find duplicate documents in the corpus.\n",
    "jawaban": "The correct answer is: graph"
  },
  "897225030ae465c2998fcbe2796b679b467b2c50dbebdfeeddc4072ec541b6b6": {
    "soal": "The core of the Apache Hadoop framework consists of the following modules:\n- Hadoop Common - contains libraries and utilities needed by other Hadoop modules;\n- Hadoop Distributed File System (\nAnswer Question 52\n) - a distributed file system that stores data on commodity machines, providing very high aggregate bandwidth in the cluster;\n- Hadoop YARN - a resource management platform responsible for managing computational resources in the cluster and using them for scheduling user applications;\n- Hadoop MapReduce - a programming model for processing large-scale data.",
    "jawaban": "The correct answer is: HDFS"
  },
  "efb7facda26836a87e75d39a5d0731ced9d15e4d60734fc602228b9e1614d767": {
    "soal": "The Tree widget can be used for classification tasks and regression tasks.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "44272ab04746a8bad2b2b7c45fc707dc74703d6cbaccdfbdf0d43b1fa82ca07f": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nWhat is a benefit of applying the softmax activation within the loss function rather than in the `Dense` layer?",
    "jawaban": "The correct answer is: It simplifies the model architecture by reducing the need for an explicit activation layer"
  },
  "5f333c5aaea36c6889e0194fd19b758be0c80628d0fc443c524a14bd24857c2d": {
    "soal": "from googlesearch import search\nimport csv\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef google_scrape_bulk(keywords, num_results=10, output_file='results.csv'):\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'Rank', 'URL'])\nfor keyword in keywords:\nprint(f\"\ud83d\udd0d Searching for: \")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nwriter.writerow([keyword, i+1, url])\ntime.sleep(1) # Delay agar tidak dianggap bot\nexcept Exception as e:\nprint(f\"\u274c Error while searching '{keyword}': {e}\")\nprint(f\"\n\u2705 All results saved to '{output_file}'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\ngoogle_scrape_bulk(keywords, num_results=10)\nIn which format does the script save the search results?\n{\n=CSV\n~JSON\n~XML\n~TXT\n~XLSX\n~HTML\n}",
    "jawaban": "The correct answer is: keyword"
  },
  "a3caada3bb0b42a728bdb246309afeb61566e11ce76435174085013145098d4b": {
    "soal": "The Select Columns widget can manually select data .................... and the composition of the data domain.\n",
    "jawaban": "The correct answer is: attribute"
  },
  "1210dad022b870808932c3d5b81c7cfd22ddaf8bd504c4899a46feaebcdd83af": {
    "soal": "The Distributions widget displays the distribution of values for all attributes\n",
    "jawaban": "The correct answer is 'False'."
  },
  "5a2e15987ab9ca420f4223bedb2c871b823fc101220b144799e7af0f7785fd5c": {
    "soal": "Most Time Series algorithms assume that we do not have missing values in our data. In this widget, we can select an interpolation method to estimate missing values. By default, the Interpolate widget will use nearest interpolation (fast and reasonable).\n",
    "jawaban": "The correct answer is 'False'."
  },
  "81db9056b45812b0f72d01d4f40e1b423a50ad085233127921c7cacb4e79f57b": {
    "soal": "\n\n\nData visualization with a single variable that is ordered can use ..............................\n\n\n\n",
    "jawaban": "The correct answers are: box plot, line chart, area chart"
  },
  "8d48d583a038f04939e7f265c319da940021484286349568d93e23eda98fb44d": {
    "soal": "\n\n\nData visualization with many variables (multivariate), same, ordered can use ..............................\n\n\n\n",
    "jawaban": "The correct answers are: stacked area chart, stacked line chart"
  },
  "bddf8b2c68cc25943e7b6f8a9ac8c245ef97045c682fbcd4956894c345d6b35e": {
    "soal": "If\n\n1 AND 1 = 1\u00a0\u00a0\u00a0\u00a0\u00a0 and\u00a0\u00a0 1 OR 1 = 1\n\n\n1 AND 0 = 0\u00a0\u00a0\u00a0\u00a0\u00a0 and\u00a0\u00a0 1 OR 0 = 1\n\n0 AND 1 = 0\u00a0\u00a0\u00a0\u00a0\u00a0 and\u00a0\u00a0 0 OR 1 = 1\n\n\n0 AND 0 = 0\u00a0\u00a0\u00a0\u00a0\u00a0 and\u00a0\u00a0 0 OR 0 = 0\n\n\u00a0\n\nthen in binary\n\n( 101 AND 010 )\u00a0 OR 101 = ..... (binary)\n",
    "jawaban": "The correct answer is: 101"
  },
  "f98972493f74f09816a0d684f55de356edb37a9945a6e7e458d9bcdb18cd673c": {
    "soal": "\nThe Box Plot widget is most commonly used immediately after the File widget to observe ....................... properties of a dataset. In this example, we have used the heart-disease data to inspect our variables.\n\n\n\n\nThe Box Plot is also useful for finding properties of a specific dataset, such as a set of instances manually defined in another widget (e.g., Scatter Plot, or instances from multiple clusters, or classification tree nodes).\n",
    "jawaban": "The correct answer is: statistics"
  },
  "b4604cdd88fa5c3184727194f9f5fedb20a40f5af3f4939c4dcddd5124230528": {
    "soal": "Network Clustering Widget can help us reveal clusters and sparsely connected groups in a network. First, we will use the Network File Widget to load the lastfm.net dataset. Then, we will send the network to the Network Clustering Widget. The Network Clustering Widget finds 79 clusters in the network. To visualize the results, we use the Network Explorer Widget and set the Color attribute to Cluster. This will color the network nodes with the corresponding cluster color - this is a good way to visualize highly connected groups in a dense network.\n\nNote that the Network Explorer Widget here will color the 10 largest clusters and color the rest as 'Other'.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "c0a555c4508e724cc1b7b2b8494a7bb95647a9779448501147ed8b1a4032a9d8": {
    "soal": "In the next example, we will try to predict the category of documents. We use the book-excerpts.tab dataset, which we send through the ............... widget with default parameters. Then we connect the Preprocess Text widget to the Bag of Words widget to obtain term frequency. With the term frequency information, we will calculate the model.\n\nConnect the Bag of Words widget to the Test & Score widget for predictive modeling. Connect the SVM widget or another classifier widget to the Test & Score widget (all on the left). The Test & Score widget will compute performance scores for each learner input. Here, we get very good results with the SVM widget.\n\nNext, we need to check where the model made errors. Add the Confusion Matrix widget to the Test & Score widget. The Confusion Matrix widget will display the documents that were correctly classified and misclassified. Select \"misclassified\" to output the misclassified documents, which we will further examine using the Corpus Viewer.\n\n\n",
    "jawaban": "The correct answer is: Preprocess Text"
  },
  "e2e9a6f8469e1a710d1456e6fb93148053f616308a1451cb2c6ea832d9f534b4": {
    "soal": "The Line Plot widget is a standard visualization widget, which displays the ............ profile, usually in the form of ordered numeric data. In this simple example, we will display the iris data in a line plot, grouped by iris attributes. The plot shows how ............... petal length separates the class values well.\n\nIf we observe this in the Scatter Plot Widget, we can confirm this is indeed true. Petal length is an interesting attribute for class separation, especially when enhanced with petal width, which is also well separated in the line plot.\n\n\n\n\n",
    "jawaban": "The correct answer is: petal"
  },
  "14922c74840e489b3bf0844003a76d37b0acb79a55be1e273927ef87ab4de0a6": {
    "soal": "The following ticker code used in the Yahoo Finance Stock Data Widget (ORANGE)\n\nFB\n\n\nBelongs to the company,\n\n\n",
    "jawaban": "The correct answer is: Facebook, Inc."
  },
  "04a569d41214f12e96a89faa318c466df01b75f96bcd5e3fb051c2998b7de11a": {
    "soal": "\n\n\n............................ was introduced/found by Yahoo! in 2006.\n",
    "jawaban": "The correct answer is: Hadoop"
  },
  "7b5edce444eb0b230da8caf1813515de6256a3177844889f7a2afcbb0ec0d034": {
    "soal": "The Box Plot widget shows the maximum value of attributes. It is good practice to check any new data with this widget to quickly find anomalies such as duplicate values (e.g., gray or grey), outliers, and the like.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "774a7813db84d2191f4cce775d43f1a5004d2b25c33bde269a111415ceea2be4": {
    "soal": "import numpy as np\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n# Contoh data: nilai aktual dan prediksi\ny_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0]) # Nilai aktual\ny_pred = np.array([1, 0, 1, 0, 0, 1, 0, 1, 1, 0]) # Prediksi model\n# Menghitung confusion matrix\ncm = confusion_matrix(y_true, y_pred)\n# Menampilkan confusion matrix dalam bentuk teks\nprint(\"Confusion Matrix:\n\", cm)\n# Menampilkan confusion matrix dalam bentuk grafik\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.show()\nBagaimana cara menampilkan confusion matrix dalam bentuk grafik menggunakan scikit-learn?",
    "jawaban": "The correct answers are: ConfusionMatrixDisplay(confusion_matrix, cm, display_labels, [0, 1]).plot(cmap, plt.cm.Blues)"
  },
  "565789130bda92d8dd26a35a7bb146b2dc221ac07be73d0b35efbc4cca0b7406": {
    "soal": "101 (binary) = ...... decimal",
    "jawaban": "The correct answer is: 5"
  },
  "8ae26368622555430aaf30deb7ee9dea5a5ae1960867ece540e99954ef3d46f5": {
    "soal": "In the Open-WebUI service configuration, what does the environment variable `GLOBAL_LOG_LEVEL=DEBUG` do?",
    "jawaban": "The correct answer is: Sets the logging level to debug for detailed logs"
  },
  "1589f474be339478a698caf11a6880407ed6fc20037ac35a5ca415c3f16abccd": {
    "soal": "\n\n\nClassification error (misclassification) is displayed in the form of a scatter plot by comparing two (2) inputs at once, namely total data and the data we select in the confusion matrix.\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "554456617239c5094d2359e22210fd297638d680656c1a72fcfb4e0c3769f819": {
    "soal": "In the workflow below, the Import Documents widget can load subfolders. We store President Kennedy's speeches in ................. subfolders, pre-1962 and post-1962. If we load the parent folder, both subfolders will be used as class labels. Check the output of the Import Documents widget in the Data Table widget.\n\n\n\n\n",
    "jawaban": "The correct answer is: dua"
  },
  "65fddb0d224250fcb508d3c0b014cb355ebab76dfd8dc8bcc5b79bddf43fb049": {
    "soal": "The Venn Diagram widget can create a Scatter Plot for two or more data subsets.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "69540da5d1a60881bf0f15125ad48de29fb74af7547b49fb8cb3d43db22b62ba": {
    "soal": "The Test & Score widget shows the model's predictions against the data.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "969f7c423176eb22eac0fe20748789c320fc32c2950c7d7df133107d3623b636": {
    "soal": "Which tool is used to manage multi-container Docker applications?",
    "jawaban": "The correct answer is: docker-compose"
  },
  "8488f6ca60929aff598eb4c1027fee08014538504b95b1a65219d64ef930efb0": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\n# xes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\nxes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nprocess_tree = pm4py.discover_bpmn_inductive(log)\nbpmn_model = pm4py.convert_to_bpmn(process_tree)\npm4py.view_bpmn(bpmn_model)\nPM4PY is mainly associated with which language?",
    "jawaban": "The correct answer is: Python"
  },
  "48f3814a0f0f65c8cb6a5d66e403e781833365bded74e7ef16806e906deb293a": {
    "soal": "The ................................ widget calculates the distance between rows/columns in the dataset.\n",
    "jawaban": "The correct answer is: Distances"
  },
  "0a946f01aff5723d742d04690d952cb81e0bfd8da8f5a90d2bd3132631ebe582": {
    "soal": "If\n\n1 AND 1 = 1\n\n1 AND 0 = 0\n\n0 AND 1 = 0\n\n0 AND 0 = 0\n\n\u00a0\n\nThen 111 AND 000 = ......... (binary)",
    "jawaban": "The correct answer is: 000"
  },
  "638e376873d7ce816b3d2aa007e9dac0650cbe38c84577c0cdd15ad51f96af7c": {
    "soal": "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target biner\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(0, 2, 100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi data latih dan data uji (80% latih, 20% uji)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi dan pelatihan model klasifikasi (misalnya RandomForest)\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n# Prediksi pada data uji\ny_pred = model.predict(X_test)\n# Menghitung Precision, Recall, dan F1-Score\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nprint(f'Precision: ')\nprint(f'Recall: {recall:.2f}')\nprint(f'F1-Score: {f1:.2f}')\n# Alternatif: Menggunakan classification_report untuk ringkasan metrik\nprint('\nClassification Report:')\nprint(classification_report(y_test, y_pred))\nWhat is the default test size if not specified in the `train_test_split` function?\n{\n~0.1\n~0.2\n~0.25\n=0.25\n~0.3\n~0.5\n}",
    "jawaban": "The correct answer is: precision:.2f"
  },
  "71530903cec150ff305533f51c4564aa4cad530008c98d7ab3711618776dc1fe": {
    "soal": "A heat map is a graphical method for visualizing attribute values based on class in a two-way matrix. Heat maps can only be used on datasets that contain continuous variables. The values are represented by colors: the higher the value, the darker the color. By combining class and attributes on the x and y axes, we see where which attribute values are strongest and weakest, allowing us to find typical features (discrete) or value ranges (.................) for each class.\n\n\n\n",
    "jawaban": "The correct answer is: continuous"
  },
  "46c9d18040ff78db95a8c790eba0d8df46515d898fee152625db97b4e07e70e7": {
    "soal": "The ..............(1)............... widget can also be used to explore various prediction models. In the following example, we analyze 3 prediction methods, namely Naive Bayes, SVM, and Random Forest, based on their classification errors on data instances.\n\nBy selecting misclassifications in the three .............(2)............... widgets and sending them to the Venn Diagram widget, we can see all misclassification examples visualized by the method used. Then we open the Venn Diagram widget and select, for example, the misclassified instances identified by all three methods. This is represented as the intersection of the three circles. Click on the intersection to see these two examples marked in the ........(3).............. widget. Try selecting different parts of the diagram to see how the scatter plot visualization changes.\n\n\n\n",
    "jawaban": "The correct answer is: (3) \u2192 Scatter Plot, (2) \u2192 Confusion Matrix, (1) \u2192 Venn Diagram"
  },
  "1e26ab5759900c5f57e5b6002013915cdae083aa657aeec424d69388862fa360": {
    "soal": "What is the role of a 'case ID' in an event log?",
    "jawaban": "The correct answer is: Uniquely identify a specific instance of the process"
  },
  "437dc5140c49001e149c54df7934b451524d4757a455ba8ed1d3347a2bc3e88f": {
    "soal": "PM4PY primarily stands for:",
    "jawaban": "The correct answer is: Process Mining for Python -Python Mining for Processes -Python Module for Analysis -Process Modeling for Python -Python Management for Process -Python Methods for Prediction"
  },
  "1bc034870502d034c6f9abe4d8b5684056c3655ee499de5e8411960b233cc1f2": {
    "soal": "The Import Documents widget takes text files from a folder and creates a corpus. The Import Documents widget can read .txt, .docx, .odt, .ppt, and .xml. If there are subfolders in the folder, they can be used to label classes.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "cf4b413f1dd35dbd4737797dc1138e92b5f2d519d8b71fce45f195ca20eedee8": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nWhat does the ReLU (Rectified Linear Unit) activation function compute?",
    "jawaban": "The correct answers are: max(0, x), ReLU(x), max(0, x), min(0, x), x^2, 1 / (1 + e^-x)"
  },
  "358acf4322e5669076f08cd1cf12d4d5f100e84527e0ea208d5b87ba099dff79": {
    "soal": "The PCA widget in ORANGE3 provides three outputs: transformed data and principal component. Transformed Data represents the weights for individual instances in a new coordinate system, while component is the descriptor of the system (weights for the principal components). When inserted into the Data Table, we can see both outputs as numbers. We use two data tables to provide a cleaner workflow visualization, but we can also choose to edit the links so that we display the data in just one data table. We only need to create two links and connect the input data and the transformed components to the output data.\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "3511959254f1163218966e41a0dfd6f77c5e6c9643d0927843130637efb257d3": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nplt.imshow(x_train[0], cmap='gray')\nplt.title(f\"Label: \")\nplt.show()\nmodel = keras.Sequential([\nkeras.layers.Flatten(input_shape=(28, 28)),\nkeras.layers.Dense(128, activation='relu'),\nkeras.layers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nWhat loss function is used in model.compile()?\n{\n= sparse_categorical_crossentropy\n~ mean_squared_error\n~ binary_crossentropy\n~ hinge\n~ log_loss\n~ categorical_hinge\n}",
    "jawaban": "The correct answer is: y_train[0]"
  },
  "2f68764e9972956302cc8887a19a181c2ccb9dd70d71b89880cfc1b7c990d542": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nx_train = np.random.rand(1000, 10)\ny_train = np.random.randint(2, size=(1000, 1))\nmodel = keras.Sequential([\nkeras.layers.Dense(64, activation='relu', input_shape=(10,)),\nkeras.layers.Dense(32, activation='relu'),\nkeras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer='adam',\nloss='binary_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=10, batch_size=32)\nHow many input features are used in this model?",
    "jawaban": "The correct answer is: 10"
  },
  "ec0d18ba4d3e7ed754f1ace53fdb68b8c8282f1bef5527b9f56b96ca3dc8e99e": {
    "soal": "Which model type enables LLMs to understand context better?",
    "jawaban": "The correct answer is: Transformer"
  },
  "6808c513004cd8afecc1e3abadabb0cbd0166642f722fb5ad0a0090cae050b4b": {
    "soal": "The Distributions widget displays the distribution of values for one attribute\n",
    "jawaban": "The correct answer is 'True'."
  },
  "503a7977c890a296cadd737fbb05dc74fc2287f50e0bd9c2f47e6659b3d80fd9": {
    "soal": "100 (binary) = ...... (decimal)",
    "jawaban": "The correct answer is: 4"
  },
  "e83c7a392c544726a5c041c8c88b61a27e9694250c07976bdee52225973b2a09": {
    "soal": "Why is stratification important when dealing with imbalanced datasets?",
    "jawaban": "The correct answer is: It ensures each subset maintains the same class proportion as the original dataset."
  },
  "4ae518c9fcd481fad7f848a7f2f3ebcdfe059f59e90e4e743660b6b12f2939f8": {
    "soal": "In using the Constant Widget for regression, we use the Constant Widget to create a predictor in Prediction. We use the housing dataset. In Prediction, we can see that the Mean Learner returns two (average) values for all instances.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "cae9872211cba2e5c118e8e6f07e1b536e5e05451affaea2e8b415e9d65f5c60": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nplt.imshow(x_train[0], cmap='gray')\nplt.title(f\"Label: \")\nplt.show()\nmodel = keras.Sequential([\nkeras.layers.Flatten(input_shape=(28, 28)),\nkeras.layers.Dense(128, activation='relu'),\nkeras.layers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nWhich optimizer is used?\n{\n= adam\n~ sgd\n~ rmsprop\n~ adagrad\n~ adadelta\n~ ftrl\n}",
    "jawaban": "The correct answer is: y_train[0]"
  },
  "32f476dc4d26a8bb513d53c386dd55d766dc6660a43822bcab23d53e9c5438bd": {
    "soal": "In the Moving Transform widget, we can determine which neural function to run on the time series and the window size.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "810be1e7de867c7dd5a6a3e97ab450a52ad968a3218fa259ef2f16d94f1228b2": {
    "soal": "In the Periodogram widget, the periodogram for non-equispaced series is calculated using the .......................... method.\n",
    "jawaban": "The correct answer is: Lomb-Scargle"
  },
  "898b6f96c7808a5e3e4297ecfb44ddf5e52a27e91ae55095bab28e2ba7528654": {
    "soal": "The core of Apache \nAnswer Question 22\n consists of a storage part (Hadoop Distributed File System (HDFS)) and a processing part (MapReduce). Hadoop divides files into large blocks and distributes them among nodes in the cluster. To process data, Hadoop MapReduce transfers code to nodes for parallel processing, based on the data that needs to be processed at each node. This approach takes advantage of data locality\u2014nodes manipulate the data they hold\u2014to allow faster and more efficient data processing compared to more conventional supercomputer architectures relying on parallel file systems where computation and data are connected via high-speed networks.",
    "jawaban": "The correct answer is: Hadoop"
  },
  "a5347d43e5808a5f1024f94c04e01b0ee05991ba277c74377c9ddf073a11c1d3": {
    "soal": "In the Distance Map widget, usually, a color palette is used to visualize the entire range of distances that appear in the matrix. This can be changed by setting low and high thresholds. In this way, we ignore the distance differences outside this range and visualize the interesting part of the distribution.\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "d04867d89ae7c04dd713c085bdc46dc72189ef9e779b65c25d9c32921b184db9": {
    "soal": "What visual output does PM4Py produce by default from XES files?",
    "jawaban": "The correct answer is: Static visualizations"
  },
  "91a520feff113c42ad0947823515584e624f8010d07e0e88476964045072438d": {
    "soal": "...........(1).............. can be built from documents we have. In the text mining toolbox, there is a widget ..............(2)........... to read document files. Before displaying it as a word cloud, it is advisable to pass it through the Preprocess Text widget first, to reduce unnecessary words such as conjunctions, etc. Then, we can pass it to the Bag of Words widget first, or directly to the Word Cloud widget to display it.\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 Word Cloud, (2) \u2192 Import Documents"
  },
  "7f8149ae45dfcf8805fc60f13dd3996b61f7aba5fc6e6e0b05c3e58349808bcc": {
    "soal": "Contoh sederhana dengan Calibrated Learner. Kita menggunakan dataset titanic karena widget ini membutuhkan nilai binary class (dalam hal ini mereka adalah 'survived\u2019 atau \u2018not survived\u2019).\n\nKita menggunakan .................... sebagai base learner yang akan dikalibrasi dengan nilai setting default, yaitu dengan sigmoid optimization dari distribusi nilai dan di optimasi dengan CA.\n\nMembandingkan hasil dari uncalibrated Logistic Regression model kita akan melihat dengan jelas bahwa calibrated model lebih baik.\n\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Logistic Regression"
  },
  "26d21bcef5636f9d49f110eef16ff194db87e8e10605967012e4f0547261d1bb": {
    "soal": "The following regular expression (regex) function:\n\nmatches exact word\nis\n",
    "jawaban": "The correct answer is:\n\\bword\\b\n\n\\bword\\b"
  },
  "73042a0dbb11fa257d710a61541004fcac1f0f623b944e35b4d3563a21024734": {
    "soal": "The Test & Score widget tests the learning algorithm. Different sampling schemes are available, including using separate test data. The widget does two things. First, it shows a table with performance measures of different classifiers, such as classification accuracy and area under the curve. Second, the output of the evaluation, which can be used by other widgets to analyze the performance of classifiers, such as ROC Analysis or ...................... Matrix.\n",
    "jawaban": "The correct answer is: Confusion"
  },
  "61c1f12e8367881215e4d978a20c0424f138bf226a07bc510b2b5aaf90e11599": {
    "soal": "The .................. widget receives a dataset and one or more predictors (predictive models, not algorithm learners). The .................... widget generates data and predictions.\n",
    "jawaban": "The correct answer is: Predictions"
  },
  "208f1d65925b9d5cc996973e54cc0359d66298c3730bdca4b3ae256a97e57d0d": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.rand(100) # 100 nilai target\n# Inisialisasi model regresi linier\nmodel = LinearRegression()\n# Tentukan jumlah fold (misalnya, 5-fold cross-validation)\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n# Evaluasi model menggunakan cross-validation\nscores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n# Menampilkan skor R^2 untuk setiap fold dan rata-ratanya\nprint(\"Skor R^2 untuk setiap fold:\", scores)\nprint(\"Rata-rata skor R^2:\", np.mean(scores))\nWhat does the variable `scores` represent in the code?",
    "jawaban": "The correct answer is: The R\u00b2 scores for each fold in the cross-validation."
  },
  "eb99e2650c0232a896f422690bde730e5e2ecb3b79543d6d756c8d5df74acd40": {
    "soal": "If\n\n1 OR 1 = 1\n\n1 OR 0 = 1\n\n0 OR 1 = 1\n\n0 OR 0 = 0\n\nthen\n\n11101 (binary) OR 11011(binary) = 11111 (binary)",
    "jawaban": "The correct answer is 'True'."
  },
  "d7ee3889d4e6ce98a8d52ff3db405d737c0898a215f224c8545cd82563fa8e8b": {
    "soal": "Data does not always come in a nice table format. Data can also be in the form of text, audio recordings, video materials, or even images. However, computers can only work with numbers, so for any data mining, we need to transform the unstructured data into a matrix representation.\n\n\nTo extract numbers from unstructured data, Orange can use a deep network embedder. Orange has just started integrating various embedders in the Image Embedding widget, and for now, they are available for text and images.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "f5f85d43a31c022677f4535598512dbf2b0f3e4967aa42e702723abe5f4fa1eb": {
    "soal": "\n\n\n\n\n\nIn this example, we are using the lastfm dataset, which can be loaded into the Network File widget under Browse documentation networks. The network node represents musicians, marked by the genre they play, the number of albums produced, and so on. Edge is the number of listeners on LastFm.\n\nThe entire dataset is visualized in Network Visualization. In this widget, we remove the coloring and adjust the node size to match the number of albums. Then, we select several nodes from the network. We can observe the selection in the Network Explorer Widget (1).\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "6fea0747f85d6132b051e0a176fca65073b4415dd4a15ba6b59b67526a0582ab": {
    "soal": "Which command checks for GPU availability in TensorFlow?",
    "jawaban": "The correct answer is: print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"
  },
  "430e07b94e08bc45bdc4f350882c8902849be3d7ec6eb38884a54a937d89effb": {
    "soal": "Which miner tends to produce complex models?",
    "jawaban": "The correct answer is: Heuristic Miner"
  },
  "ae58f070e4b93400830203b9b9c5d6f30118889711b73b91f816f88c53804c3a": {
    "soal": "\n\n\n\n\n\nClassification is performed using the data regression algorithm/module, and the results are immediately assessed for accuracy by the Test & Score module.\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "128f66d1077187c07c425a1ccc7bf66b1f9fd980704052dbcb86880774e08fb3": {
    "soal": "Widget Save Data can consider the dataset provided in the input channel and save it to a data file with the specified name. Widget Save Data can save data as a file with tab-separated or question-separated data.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "47f84de80b61e5ec805559533a3ce2854319630130f2d8c9a108660b9af33b93": {
    "soal": "In the next example, we will try to predict the category of documents. We use the book-excerpts.tab dataset, which we send through the Preprocess Text widget with default parameters. Then we connect the Preprocess Text widget to the .............. widget to obtain term frequency. With the term frequency information, we will calculate the model.\n\nConnect the Bag of Words widget to the Test & Score widget for predictive modeling. Connect the SVM widget or another classifier widget to the Test & Score widget (all on the left). The Test & Score widget will compute performance scores for each learner input. Here, we get very good results with the SVM widget.\n\nNext, we need to check where the model made errors. Add the Confusion Matrix widget to the Test & Score widget. The Confusion Matrix widget will display the documents that were correctly classified and misclassified. Select \"misclassified\" to output the misclassified documents, which we will further examine using the Corpus Viewer.\n\n\n",
    "jawaban": "The correct answer is: Bag of Words"
  },
  "a30fbed87450357b5b47003620b8c1750798c383c984c4dff829596c0b39f4b1": {
    "soal": "When there is no data in the input, the ................... widget reads the text corpus from a file and sends the corpus instance to the output channel. The history of the last opened file is stored in the Corpus widget. In the Corpus widget, there is also a directory with sample corpora pre-installed with the add-on. The widget can read data from Excel (.xlsx), comma-separated (.csv), and tab-delimited (.tab).\n",
    "jawaban": "The correct answer is: Corpus"
  },
  "c0786a6f169af9a876d277e50706238eb6e8a73750f4e2807786ec9dc1b5c5bd": {
    "soal": "In the Import Documents Widget, to retrieve data, select (click) the folder icon on the right side of the widget. Select (choose) the folder that we want to make a corpus. After loading is complete, we will see how many documents were successfully retrieved by the Import Documents Widget. To observe the images obtained, connect the Import Documents widget to the Corpus Viewer Widget. In this workflow, we are using a set of speeches by President Kennedy in plain text format.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "e430943e2116efe16c02c46e0ddb3220c19a2273ff135e4d2c45dddc9dd3ad61": {
    "soal": "The basic framework of Apache Hadoop consists of the following modules:\n- Hadoop Common - contains libraries and utilities required by other Hadoop modules;\n- Hadoop Distributed File System (HDFS) - a distributed file system that stores data on commodity machines, providing very high aggregate bandwidth in clusters;\n- Hadoop YARN - a resource management platform responsible for managing computing resources in the cluster and using them for user application scheduling; and\n- Hadoop \nAnswer Question 85\n - a programming model for large-scale data processing.",
    "jawaban": "The correct answer is: MapReduce"
  },
  "25e39d9f5f2f253f2f5a1151b4f71c1329d20a5c605575d8993eafdf9427a9fa": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(2, size=100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi training (80%) dan testing (20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model, misalnya RandomForestClassifier\nmodel = RandomForestClassifier(random_state=42)\n# Melatih model pada data training\nmodel.fit(X_train, y_train)\n# Memprediksi data testing\ny_pred = model.predict(X_test)\n# Mengevaluasi akurasi model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model pada data testing: ')\nHow many samples are created in the dummy dataset?\n{\n~50\n~75\n=100\n~150\n~200\n~250\n}",
    "jawaban": "The correct answer is: accuracy:.2f"
  },
  "207621944f6c9b737e53041329632253f24b57fa2d068aa6c1513047c2ba485d": {
    "soal": "\n\n\n\n\n\nThe above Workflow in the world of machine learning is part of unsupervised learning.\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "a54022767284e8a677f8c59f82c71982475efa4fb6de20e684758a0fdb48cfcd": {
    "soal": "The Box Plot widget is most commonly used immediately after the ..............(1).............. widget to observe statistical properties of a dataset. In this example, we have used heart-disease data to examine our variables.\n\n\n\n\nBox Plot is also useful for finding properties of specific datasets, such as a set of instances manually selected in another widget (e.g., Scatter Plot, or instances from some clusters, or nodes in a classification tree).\n",
    "jawaban": "The correct answer is: file"
  },
  "268dbb1ed8c1d1aed1e17eb301324bbf5f748ed1c6266aa660e1b0a8301d5863": {
    "soal": "Heat maps are a graphical method to visualize attribute values based on class in a two-way matrix. Heat maps can only be used on datasets containing continuous variables. The values are represented by colors: the higher the value, the darker the color represented. By combining class and attributes on the x and y axes, we can see where the strongest and weakest attribute values are, allowing us to find typical features (discrete) or value ranges (...........................) for each class.\n\n\n\n",
    "jawaban": "The correct answers are: continuous, continuous"
  },
  "6c7ef3baeaa544ff0b574f3b75230ef46e1f54eeeb749243120b9ebcf320f8a3": {
    "soal": "This workflow example shows how Network Analysis can enrich the workflow. We have used the lastfm.net data as input network from Network File and sent it to Network Analysis. We can decide to calculate the degree, degree centrality, and closeness centrality at node-level.\n\nWe can then visualize the network in Network Explorer. In the Network Explorer widget, we color it with the best tags, such as the default for this dataset. But now we can also set the node size to match the degree centrality calculation results. This is a good way to visualize network properties.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "20dbb4ef835b0f654a41b183452211a98fbcee427651ddeca448f402f91fda22": {
    "soal": "Which of the following is an advantage of using mini-batch gradient descent over stochastic gradient descent?",
    "jawaban": "The correct answer is: It provides a balance between the robustness of stochastic gradient descent and the efficiency of batch gradient descent."
  },
  "f6042bf81ecb452465f4ecf9b74195290e17243f8127d6b0b9690b2b2ae9b356": {
    "soal": "011 (binary) = ...... decimal",
    "jawaban": "The correct answer is: 3"
  },
  "35bb67403be0f21952902c3bb5c5ead43e7ad0ae8357312c6016b2264626d696": {
    "soal": "In the Correlogram widget, we will visualize the ......................... coefficient for the time series we selected.\n",
    "jawaban": "The correct answer is: autocorrelation"
  },
  "9f5bf42d0101e5718596a07b949c1c5b452841a582f489a62c39a554f25dad69": {
    "soal": "The Tree widget uses the Tree algorithm with the ability to perform forward pruning (backward pruning).\n",
    "jawaban": "The correct answer is 'False'."
  },
  "22ac85ef8db0abd0ecef689bbbbfb36f28f4715e69d8f770a895ec2bfa2f49eb": {
    "soal": "What is a prompt?",
    "jawaban": "The correct answer is: A question or instruction from the user"
  },
  "6e537a6ed4b4a1047bb80a418846db648853b276e6c60d9c3bbd8e8de41cb56e": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(2, size=100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi training (80%) dan testing (20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model, misalnya RandomForestClassifier\nmodel = RandomForestClassifier(random_state=42)\n# Melatih model pada data training\nmodel.fit(X_train, y_train)\n# Memprediksi data testing\ny_pred = model.predict(X_test)\n# Mengevaluasi akurasi model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model pada data testing: ')\nWhat does the variable `y_pred` represent in the code?\n{\n~The actual target values of the test set.\n~The features of the training data.\n=The predicted target values for the test set.\n~The accuracy score of the model.\n~The residual errors of the model.\n~The feature importances determined by the model.\n}",
    "jawaban": "The correct answer is: accuracy:.2f"
  },
  "5f78c8537a5930428bc6b1c61d5036fd2bbb4fa50808b75fb36f5caff5f3a993": {
    "soal": "import os\nimport string\nfrom collections import Counter\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\n# Buat daftar stopwords Bahasa Indonesia\nstop_words = set(stopwords.words('indonesian'))\ndef bersihkan_teks(teks):\n# Lowercase + hilangkan tanda baca\nteks = teks.lower()\nteks = teks.translate(str.maketrans('', '', string.punctuation))\nreturn teks\ndef analisis_folder(folder='outputs', top_n=20):\nsemua_konten = ''\nfor filename in os.listdir(folder):\nif filename.endswith('.txt'):\nfilepath = os.path.join(folder, filename)\nwith open(filepath, 'r', encoding='utf-8') as f:\nisi = f.read()\n# Ambil hanya bagian isi, setelah header\nif '\n\n' in isi:\nisi = isi.split('\n\n', 1)[1]\nsemua_konten += isi + ' '\n# Bersihkan teks dan tokenisasi\nteks_bersih = bersihkan_teks(semua_konten)\nkata_kata = teks_bersih.split()\n# Hapus stopwords\nkata_kata = [kata for kata in kata_kata if kata not in stop_words and kata.isalpha()]\n# Hitung frekuensi\ncounter = Counter(kata_kata)\ntop_kata = counter.most_common(top_n)\nprint(f\"\n\ud83d\udd0d Top Kata Paling Sering Muncul:\")\nfor kata, jumlah in top_kata:\nprint(f\"{kata}: {jumlah}\")\n# Buat Word Cloud\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(counter)\n# Tampilkan\nplt.figure(figsize=(12, 6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud dari Semua Konten')\nplt.tight_layout()\nplt.savefig('wordcloud.png')\nplt.show()\nprint(\"\n\u2705 Word Cloud disimpan sebagai 'wordcloud.png'\")\n# Jalankan analisis\nif __name__ == '__main__':\nanalisis_folder('outputs', top_n=30)\nWhat is the purpose of the 'stop_words' set in the script?\n{\n=To store a list of common words in Indonesian that should be excluded from analysis.\n~To keep track of words that have been analyzed.\n~To store the most frequently occurring words in the text.\n~To translate Indonesian words to English.\n~To highlight important words in the text.\n~To store words that should be emphasized in the word cloud.\n}",
    "jawaban": "The correct answer is: top_n"
  },
  "c664b29d61c213779158a4387e39a070f257c7b75b33d21f26b9f28bf9bca080": {
    "soal": "The Box Plot widget can compute the distribution of attribute values.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "398382ea4ce1582c9af17ebca1457f7181e4e0666871fb90078843e373df8413": {
    "soal": "The Heat Map widget can plot a heat map for all attributes.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "c61e9ce1951f0e9290ef1a91ca9209af466c9969593a97e4bbad97c9e25a531e": {
    "soal": "In the snapshot below, we can see how the transformation affects ..............(1)............. We load the Iris dataset and calculate the distance between the rows with the help of the .............(2)........... widget. To show how ..............(3)............. affects the Distance Matrix, we create the workflow below and compare the transformed distance matrix with the \"original\".\n\n\n",
    "jawaban": "The correct answer is: (3) \u2192 Distance Transformation, (1) \u2192 distance matrix, (2) \u2192 Distances"
  },
  "853655aca97c8716d7ced998f63fad850ec6609d405ff95b9ef9839831a0d3b9": {
    "soal": "Heuristic Miner is effective for:",
    "jawaban": "The correct answer is: Data with high noise and variability"
  },
  "4b10d076783bbb97f41bd1cd38bd02f8ef6eb8687f0e70304c1b1b63d6ea417e": {
    "soal": "Widget Calibrated Learner membungkus / melanjutkan kerja dari learner lain dengan probability calibration dan decision threshold optimization.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "c1c1bf5289149ac966c7b19080c38febcfe13fae1557197828f8537104e99fd4": {
    "soal": "In the workflow below, Iris data from the File widget is passed to the Select Columns widget, where we choose to display only two attributes (i.e., petal width and ...................)). Then, we can view the original dataset and the dataset with the selected columns in the Data Table widget.\n\n\n",
    "jawaban": "The correct answer is: petal length"
  },
  "fa2d6a586500bc8b771c5bf4cb967acd607b2bda99b57684c98088309a4eb14a": {
    "soal": "The following workflow demonstrates the use of the Distances Widget. We use the iris.tab data from the File widget. We calculate the distance between data instances (rows) and pass the result to Hierarchical Clustering. This workflow is useful for finding errors in the data instances.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "677ac4417c0fb99ff77e014af32b3d520837678473e0fde7a7dbf33216808360": {
    "soal": "Data attributes in Orange have types discrete, continuous or character string. The attribute type is marked by a symbol that appears before the attribute name (D, .............................., S).\n\n\n\nAnswer: \nQuestion 16",
    "jawaban": "The correct answer is: C"
  },
  "d93cbe6e07e0113f126826bf3051fa3666515070a1b5fdd7b5537cb62fc76316": {
    "soal": "In the image below, we use the Network Generator Widget to create a .................. graph with a height of 4 and a width of 5 and send it to Network Analysis. We can calculate node degrees and send the data to Network Explorer. Finally, we observe the resulting graph in the visualization and set the node size and color to Degree. Network Explorer is a good tool for observing and explaining network properties.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Grid"
  },
  "6ea27bc8c9a615f9d564ad2153e77cc795277952975417cdf03249e6f55a956c": {
    "soal": "What is the UCI Machine Learning Repository?",
    "jawaban": "The correct answer is: A collection of databases for machine learning research."
  },
  "3d8d1197af8b57cdad49a7ba017cbc2e4a1baab3062bcdcf65539c206efbf50c": {
    "soal": "The Distance Matrix widget creates a distance matrix, which is a two-dimensional array containing distances, taken pairwise, between elements of the dataset. The number of elements in the dataset determines the .................... matrix size. Data matrix is essential for the Hierarchical Clustering Widget.\n\n\n",
    "jawaban": "The correct answer is: size"
  },
  "4671a354cce92dca172dc5d71811cabb5774041381065d3e688c07de34e283ca": {
    "soal": ".................. is a statistical table summarizing data from a larger table. This summary may include totals, averages, or other statistics, with the pivot table grouping them in a meaningful way. Pivot tables are a technique in data processing.\n",
    "jawaban": "The correct answer is: Pivot table"
  },
  "3951da99fcd27aef17f63b514873b8cd86a7e91e58916acaa8ed5c2a917d1fc4": {
    "soal": "Which of the following is a method to prevent overfitting in a TensorFlow model?",
    "jawaban": "The correct answer is: Dropout"
  },
  "f9914548b7d012d6ed0e61e8f70e8970880c9956b69b037e16cf293ed5c22eaf": {
    "soal": "Widget Calibrated Learner membungkus / melanjutkan kerja dari learner lain dengan probability calibration dan .........................................\u00a0 optimization.\n",
    "jawaban": "The correct answer is: decision threshold"
  },
  "79ab4dc2482df8f9de47dbf18a2cd602851e2cdfa4676d59b95f0180ab34bcfa": {
    "soal": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n# Contoh: Memuat dataset dari file CSV\n# Gantilah 'data.csv' dengan path ke dataset Anda\ndf = pd.read_csv('data.csv')\n# Memisahkan fitur (X) dan target \n# Asumsikan kolom terakhir adalah target\nX = df.iloc[:, :-1]\ny = df.iloc[:, -1]\n# Membagi dataset menjadi data latih dan data uji (80% : 20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model Decision Tree\nmodel = DecisionTreeClassifier()\n# Melatih model dengan data latih\nmodel.fit(X_train, y_train)\n# Melakukan prediksi pada data uji\ny_pred = model.predict(X_test)\n# Menghitung akurasi\nakurasi = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model: ')\n# Menampilkan laporan klasifikasi yang mencakup presisi, recall, dan F1-score\nprint(classification_report(y_test, y_pred))\nAfter training the model, which method is used to make predictions on the test data?\n{\n~forecast()\n~guess()\n=predict()\n~determine()\n~infer()\n~estimate()\n}",
    "jawaban": "The correct answer is: akurasi:.2f"
  },
  "003aa2e86c02a199a9028a2bb1f66c9ca40daf45d0f7faba891cd11ddb46c514": {
    "soal": "What makes the LLM act like a \"brain\"?",
    "jawaban": "The correct answer is: Its ability to remember and respond based on context"
  },
  "400bbae9205a0372b2f58bfb0b1dc81ab042ca63ed0edd438c454f4aa16bdc6c": {
    "soal": "Machine learning is a branch of computer science that focuses on developing algorithms and techniques to create computer systems that can learn from data. In machine learning, computers are trained to recognize patterns and rules in data so that they can make intelligent and accurate predictions or decisions based on that data.\nThe learning process in machine learning can be done with three main methods: supervised learning, unsupervised learning, and reinforcement learning.\n- Supervised learning involves using labeled or annotated data to train the algorithm so that the computer can recognize patterns or relationships between data features and labels. Examples of supervised learning applications are image classification, stock price prediction, and spam email classification.\n- Unsupervised learning, on the other hand, involves using unlabeled data to identify hidden patterns or structures in the data. Examples of unsupervised learning applications are data clustering and dimensionality reduction.\n- Reinforcement learning involves computer systems learning through experience by taking actions and receiving feedback on whether the action was right or wrong. The goal is to find the optimal decision or action that results in maximum benefit.\nMachine learning has various applications, including in facial recognition, image analysis, speech recognition, forecasting, anomaly detection, and more.",
    "jawaban": "The correct answer is 'True'."
  },
  "34482f520037c3fa4f995c425e19fc7d547efbdb9c7737fc7483d604a96eceb8": {
    "soal": "The Image Embedding widget in Orange is very easy to use. Technically, Orange will send the image to the server, where the server will push the image through a pre-trained deep neural network, such as Google Inception v3. Deep networks are often trained for a specific purpose. Inception v3, for example, can classify images into one of 1,000 image classes. We can ignore the suggested classification and instead consider the penultimate layer of the network with 2048 nodes (numbers) and use them for the image\u2019s ............ representation.\n",
    "jawaban": "The correct answer is: vector-based"
  },
  "5c224224ca28db5838cb5582c8734cb37a0520ab3d4d1810be5eb9704d9b963e": {
    "soal": "Word Cloud can be created from the Documents we have. In the text mining toolbox, there is a ................. widget to read document files. Before displaying them as a word cloud, it is advisable to pass them through the Preprocess Text widget first, to reduce unnecessary words like conjunctions, etc. Then we can pass it through the Bag of Words widget first, or directly to the Word Cloud widget to display.\n\n\n\n\n",
    "jawaban": "The correct answer is: Import Documents"
  },
  "17ee239f6b246278f46b3b6b3fd0b67b3066e6ad3adc7247f98710b5757e3c9a": {
    "soal": "import pandas as pd\n# Contoh: Membuat DataFrame dengan missing values\ndata =\ndf = pd.DataFrame(data)\n# Menampilkan jumlah missing values di setiap kolom\nprint(\"Jumlah missing values per kolom:\")\nprint(df.isnull().sum())\n# Menghapus baris yang memiliki missing values\ndf_cleaned = df.dropna()\nprint(df_cleaned)\n# Mengisi missing values pada kolom 'Usia' dengan mean\ndf['Usia'].fillna(df['Usia'].mean(), inplace=True)\n# Mengisi missing values pada kolom 'Usia' dengan median\ndf['Usia'].fillna(df['Usia'].median(), inplace=True)\n# Mengisi missing values pada kolom 'Kota' dengan modus\ndf['Kota'].fillna(df['Kota'].mode()[0], inplace=True)\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n# Inisialisasi imputer untuk mengisi missing values dengan mean\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n# Mengimputasi kolom 'Usia'\ndf[['Usia']] = imputer.fit_transform(df[['Usia']])\nIn scikit-learn, which class is used to impute missing values using the mean strategy?\n{\n=SimpleImputer\n~Imputer\n~MissingValueHandler\n~DataCleaner\n~MeanImputer\n~Fillna\n}",
    "jawaban": "The correct answer is: 'Nama': ['Andi', 'Budi', 'Citra', 'Dewi'], 'Usia': [25, 30, None, 22], 'Kota': ['Jakarta', None, 'Bandung', 'Surabaya']"
  },
  "6032226ab519cff4e400d03e9d29dc09a03ee17d21269d61e67c682d8a2905a0": {
    "soal": "The Box Plot widget can show/describe the distribution of attribute values.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "f74c1e0d6dec665ae4f28b046f7103525218f7fc45d300b3dd1796fcdab21016": {
    "soal": "For machine learning, we need numbers. To obtain ...........(1)............. representations of images, we send images to the ...........(2).............. widget.\n\nThe output data from the Image Embedding widget can be inspected using the Data Table widget. Now we have a numeric/numerical presentation of the images. For each image, there are 2048 numeric representations (columns n0 to n2047). With these feature numeric representations, we can apply all standard machine learning techniques, such as clustering.\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 numeric, (2) \u2192 Image Embedding"
  },
  "a37e80afacf268ee03582b531fe1f792f6d1e3092f4f8e8ce97883d0c6414f4e": {
    "soal": "\n\n\n\n\n\nClassification is performed using the logistic regression algorithm/module, and the results are immediately assessed for accuracy by the Test & Score module.\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "e5c3df50edcbe5bf62254e826c99d2065fe215f6e55f04b7383162c57218615c": {
    "soal": "What is the default port that Open WebUI listens on inside the container?",
    "jawaban": "The correct answer is: 8080"
  },
  "f71a4be28b7980eb6779d5ce0ec0ede98a3c078f86e2090b5254b1f75e5c8ac9": {
    "soal": "Which environment variable in the Open-WebUI service configuration specifies the base URL for Ollama?",
    "jawaban": "The correct answer is: OLLAMA_BASE_URL"
  },
  "4ac23e0c89541bbfc32f17212bcd0aed811bc3bf50908e2dd24b2f79f8d4ead3": {
    "soal": "There are three main challenges faced by businesses with Big Data:\n- Protecting sensitive and personal information\n- Data rights and ownership of the throne\n- Lack of talent (such as data scientists) to analyze data",
    "jawaban": "The correct answer is 'False'."
  },
  "dcfc5cb5470a1bdf3631bf0a95cb936e0a482e36b76b6235a1111668206a6c0f": {
    "soal": "What is the correct command to import TensorFlow in Python?",
    "jawaban": "The correct answers are: import tensorflow as tf, import tensorflow"
  },
  "8bf8a559d675236c557d455a22ca1dbcb0135d9030c349a133322e18e2bdb8e4": {
    "soal": "In the Import Documents Widget, to retrieve data, select (click) the folder icon on the right side of the widget. Select (choose) the folder that we want to make a corpus. After loading is complete, we will see how many documents were successfully retrieved by the Import Documents Widget. To observe the obtained corpus, connect the Import Documents widget to the Corpus Viewer Widget. In this workflow, we are using a set of speeches by President Kennedy in plain text format.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "28d2493a82736cf7247c2f46cbc3c5cede0faea961fec295683c489b6841087c": {
    "soal": "Which search engine is mentioned as a fallback option for scraping?",
    "jawaban": "The correct answer is: Bing"
  },
  "17cf6645541fd6c602692239c357ccac8ce32b8ac277f466663008a97a7dbb4d": {
    "soal": ".......................... can be created from the Documents we have. In the text mining toolbox, there is an Import Documents widget to read document files. Before displaying them as a word cloud, it is advisable to pass them through the Preprocess Text widget first, to reduce unnecessary words like conjunctions, etc. Then we can pass it through the Bag of Words widget first, or directly to the Word Cloud widget to display.\n\n\n\n",
    "jawaban": "The correct answer is: Word Cloud"
  },
  "9a136b40e8d53fd37d57de859a385bc4584a6dfb6920b9eb1040d611768dd294": {
    "soal": "Network Clustering Widget can help us reveal clusters and highly connected groups in a network. First, we will use the Network File Widget to load the lastfm.net dataset. Then we will send the network to the Network Clustering Widget. The Network Clustering Widget finds 79 clusters in the network. To visualize the results, we use the Network Explorer Widget and set the Color attribute to Cluster. This will color the network nodes with the corresponding cluster color - this is a good way to visualize highly connected groups in a dense network.\n\nNote that the Network Explorer Widget here will color the 10 largest clusters and color the rest as 'Other'.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "86bef088afba0b17cc03e5dd9ca9bb02c68efe1b6f743b39b5173aa92e49c2d2": {
    "soal": "What is the typical percentage range allocated to the Training Set during dataset partitioning?",
    "jawaban": "The correct answer is: 70-80%"
  },
  "efbf9dbd3fbfdee3fba17d138efc363da41671bc02f0c3816b3bd932c48cf5b3": {
    "soal": "The Bag of Words model creates a corpus with word counts for each data instance (document). The count can be absolute, binary (present or not) or sublinear (logarithmic of term frequency). The Bag of Words model is required in combination with the Word Enrichment widget and can be used for ................modelling.\n\n\n",
    "jawaban": "The correct answer is: predictive"
  },
  "06a6510846baa966acad3184b77a1a96744d7846424dd448737a41c8b992181f": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=10000)\nWhich of the following code snippets correctly loads the IMDB dataset using TensorFlow's Keras API?",
    "jawaban": "The correct answers are: import tensorflow as tf\n(x_train, y_train), (x_test, y_test), tf.keras.datasets.imdb.load_data(num_words, 10000), tf.keras.datasets.imdb.load_data(), tf.keras.datasets.imdb.load_data(num_words, 5000), tf.keras.datasets.imdb.load_data(path, 'imdb.npz'), tf.keras.datasets.imdb.load_data(skip_top, 20), tf.keras.datasets.imdb.load_data(maxlen, 100)"
  },
  "c9fcd1b4bd63a4babbd367b0602b1d07a66c778eebed6855da935e552060bf17": {
    "soal": "from google.colab import drive\ndrive.mount('/content/drive')::\nWhere are Colab notebooks saved by default in Google Drive?",
    "jawaban": "The correct answer is: '/content/drive/MyDrive/Colab Notebooks/'"
  },
  "96827a47577ac5a06b18fe275eb5ef227e9bb631908602f8bf833f6d478a2bf2": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nx_train = np.random.rand(1000, 10)\ny_train = np.random.randint(2, size=(1000, 1))\nmodel = keras.Sequential([\nkeras.layers.Dense(64, activation='relu', input_shape=(10,)),\nkeras.layers.Dense(32, activation='relu'),\nkeras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer='adam',\nloss='binary_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=10, batch_size=32)\nWhat is the batch size used in training?",
    "jawaban": "The correct answer is: 32"
  },
  "963492a71d823d34d9e0368b48afa7af2011740c3f37a6768f1ff4733cdd669a": {
    "soal": "This workflow example shows how Network Analysis can enrich the workflow. We have used the lastfm.net data as the input network from the Widget Network File and sent it to the Widget Network Analysis. We can decide to compute the degree, degree centrality, and closeness centrality at the node-level.\n\nWe can then visualize the network in the Widget ......................... In the Network Explorer widget, it colors with the best tags, like the default for this dataset. But now we can also adjust the node size to fit the degree centrality results. This is a good way to visualize network properties.\n\n",
    "jawaban": "The correct answer is: Network Explorer"
  },
  "fa12e98887a62bfae971c199d8a95832e5b580e5482d4bd2db8b51e9f6cb17cd": {
    "soal": "The Distributions widget displays the distribution of the maximum value for one attribute\n",
    "jawaban": "The correct answer is 'False'."
  },
  "701d05c1a7788f941614ca2941749854e5c847ff4a40c082290535dc8d822bf8": {
    "soal": "The Image Embedding widget in Orange is very easy to use. Technically, Orange sends the image to the server, where the server pushes the image through a pre-trained deep neural network, such as Google Inception v3. Deep networks are often trained with a specific purpose. Inception v3, for example, can classify images into one of 1,000 image classes. We may ignore the suggested classifications and instead consider the penultimate layer of the network with 2048 nodes (numbers) and use it for the image\u2019s matrix-based representation.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "cbff955e44f636ef23176e38247f28211f3fa45fc868bfa3900e62ee8b24ff0b": {
    "soal": "The Select Columns widget is used to manually arrange the data domain. The user can decide which attributes to use and how to do it. Orange distinguishes between regular attributes, class attributes (optional), and meta attributes. For example, to build a classification model, the domain will consist of a set of attributes and ................... discrete class attributes. Meta attributes are not used in modeling, but some widgets can use them to label instances.\n",
    "jawaban": "The correct answer is: class"
  },
  "6f0a9d73b43994d80e3a6139e621dd8935eb71f821735c24eb79ee672f55f736": {
    "soal": "Algoritma CN2 adalah teknik klasifikasi yang di rancang agar secara effisien melakukan induksi dari rules yang simple & komprehesif dalam bentuk \u201cif ..................\u00a0 then predict class\u201d, meskipun dalam domain dimana mungkin ada noise.\n",
    "jawaban": "The correct answer is: cond"
  },
  "453a9813316ee1a59ce9602527e456f9e5e658fd1a60846ee34c11765c64acec": {
    "soal": "The Sentiment Analysis widget enables basic sentiment analysis of corpora. It works for English and uses two techniques supported by ................. - Liu Hu and Vader. Both techniques are lexicon-based. Liu Hu calculates a single normal score for sentiment in text (negative score for negative sentiment, positive for positive, 0 for neutral), while Vader produces scores for each category (positive, negative, neutral) and adds a total sentiment score called compound.\n\n\n\n\nLiu Hu score\nVader score\n\n\nAnswer: \nQuestion 31",
    "jawaban": "The correct answer is: NLTK"
  },
  "9548718022f84d3ba16faa9483966f512af64b4a1ec1f8ddf6da536a3b6436a1": {
    "soal": "Machine learning is a branch of computer science focused on the development of algorithms and techniques to create computer systems that can learn from data. In machine learning, computers are trained to recognize patterns and rules in data, allowing them to make intelligent and accurate predictions or decisions based on the data.\nThe learning process in machine learning can be carried out through three main methods: supervised learning, unsupervised learning, and reinforcement learning.\n- Supervised learning involves using labeled or annotated data to train algorithms, so computers can recognize patterns or relationships between data features and labels. Examples of supervised learning applications include image classification, stock price prediction, and email spam classification.\n- \nAnswer Question 49\n learning, on the other hand, involves using unlabeled data to identify hidden patterns or structures in the data. Examples of unsupervised learning applications include data clustering and dimensionality reduction.\n- Reinforcement learning involves computer systems learning through experience by performing actions and receiving feedback on whether the actions were right or wrong. The goal is to find the optimal decision or action that produces the maximum benefit.\nMachine learning has various applications, including facial recognition, image analysis, speech recognition, forecasting, anomaly detection, and more.",
    "jawaban": "The correct answer is: Unsupervised"
  },
  "defa1366ef9ba8728705b4e232307480efbbfc875f60f18e60040f927d1dccf2": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.rand(100) # 100 nilai target\n# Inisialisasi model regresi linier\nmodel = LinearRegression()\n# Tentukan jumlah fold (misalnya, 5-fold cross-validation)\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n# Evaluasi model menggunakan cross-validation\nscores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n# Menampilkan skor R^2 untuk setiap fold dan rata-ratanya\nprint(\"Skor R^2 untuk setiap fold:\", scores)\nprint(\"Rata-rata skor R^2:\", np.mean(scores))\nWhat does the `shuffle=True` parameter specify in the `KFold` function?",
    "jawaban": "The correct answer is: That the data is shuffled before splitting into folds."
  },
  "4107cb1452d628846339d6d02ec35fd3792bca29e07b4e47668b0260d067b543": {
    "soal": "Which best reflects the dynamic nature of LLM interaction?",
    "jawaban": "The correct answer is: Repeated updates of context"
  },
  "48066f91c38d89a651c95fa13ec5d767624cc9c06a6365280bd68bc68cbe66fe": {
    "soal": "\n\n\n\n\n\nThe image above shows the visualization choices depending on the data we have.",
    "jawaban": "The correct answer is 'True'."
  },
  "45b7ddeb1fde3088e45d2b2bc36c4a2cfedd9cfc0187a5d515d4679eb55a1b0e": {
    "soal": "What does TensorBoard help developers do?",
    "jawaban": "The correct answer is: Visualize and debug machine learning models"
  },
  "919deb833981d675ec1fc980e7a300143f6c819bf0e62a72ebf579f1dde04186": {
    "soal": "What is the correct command to save a TensorFlow model?",
    "jawaban": "The correct answer is: model.save('my_model')"
  },
  "b46cb3fefb2bdf3aa26cefa1b6d594370179e4bec15a71a24ae643a75826d27e": {
    "soal": "In the workflow below, we use two File widgets to read the Iris and Glass datasets (provided in the Orange distribution), and send them to the ...................... widget.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Data Table"
  },
  "111bebd5258ccb4275b7dc49b1c2b761a2c12e19575753359dbdfbf5ee8f8463": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nHow many output classes does the model predict?",
    "jawaban": "The correct answer is: 10"
  },
  "9af29aa3e49d4fbea3e6bdbf10378f41aca252e3ec965c3d9efa367d7c01aca7": {
    "soal": "Orange is an open-source Machine Learning and data visualization tool for beginners and experts.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "ec5910283ab6bff50c54bc58239118d232a87161eb6aa7b3fb4ce6c349ff48fd": {
    "soal": "import os\nimport string\nfrom collections import Counter\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\n# Buat daftar stopwords Bahasa Indonesia\nstop_words = set(stopwords.words('indonesian'))\ndef bersihkan_teks(teks):\n# Lowercase + hilangkan tanda baca\nteks = teks.lower()\nteks = teks.translate(str.maketrans('', '', string.punctuation))\nreturn teks\ndef analisis_folder(folder='outputs', top_n=20):\nsemua_konten = ''\nfor filename in os.listdir(folder):\nif filename.endswith('.txt'):\nfilepath = os.path.join(folder, filename)\nwith open(filepath, 'r', encoding='utf-8') as f:\nisi = f.read()\n# Ambil hanya bagian isi, setelah header\nif '\n\n' in isi:\nisi = isi.split('\n\n', 1)[1]\nsemua_konten += isi + ' '\n# Bersihkan teks dan tokenisasi\nteks_bersih = bersihkan_teks(semua_konten)\nkata_kata = teks_bersih.split()\n# Hapus stopwords\nkata_kata = [kata for kata in kata_kata if kata not in stop_words and kata.isalpha()]\n# Hitung frekuensi\ncounter = Counter(kata_kata)\ntop_kata = counter.most_common(top_n)\nprint(f\"\n\ud83d\udd0d Top Kata Paling Sering Muncul:\")\nfor kata, jumlah in top_kata:\nprint(f\"{kata}: {jumlah}\")\n# Buat Word Cloud\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(counter)\n# Tampilkan\nplt.figure(figsize=(12, 6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud dari Semua Konten')\nplt.tight_layout()\nplt.savefig('wordcloud.png')\nplt.show()\nprint(\"\n\u2705 Word Cloud disimpan sebagai 'wordcloud.png'\")\n# Jalankan analisis\nif __name__ == '__main__':\nanalisis_folder('outputs', top_n=30)\nWhat is the primary purpose of the provided Python script?\n{\n=To perform word frequency analysis and generate a word cloud visualization from text files.\n~To scrape web pages and extract article content.\n~To translate documents from one language to another.\n~To monitor changes in website content over time.\n~To perform sentiment analysis on social media posts.\n~To generate reports from structured database data.\n}",
    "jawaban": "The correct answer is: top_n"
  },
  "88b413039d8fa51106678cb91ad9e0bf5e7edc2fa2acc2b5ff6312af73bb6f1e": {
    "soal": "The CN2 Rule Induction widget will induce (induce) rules from the data using the CN2 algorithm.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "1afc0e8849d502300083e2c52f9d5d739fdb652b3c60eb7bcd80513cc2baa137": {
    "soal": "Data Science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from data in various forms, both structured and unstructured, similar to data mining.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "5a4450115777a083f7e8c96a202598f03cfff7ab2f1c22657d18f1cf6ad5d446": {
    "soal": "Big data is a term referring to large and complex datasets obtained from various sources, including business transactions, social data, census data, medical data, scientific data, and others. The main characteristics of big data are volume, velocity, and variety.\n- Volume refers to the very large amount of data generated constantly.\n- Velocity (Velocity) refers to the speed at which data is generated and updated, as well as the ability to process data in real-time.\n- Variety (\nAnswer Question 93\n in English) refers to the various sources of data and different types of data that can be collected.\nBig data typically requires specialized technology and techniques to process, analyze, and interpret it, including machine learning, data mining, and statistical analysis techniques. The goal of big data analysis is to gain valuable insights and information, such as trends and patterns useful for business, scientific, or social decision-making.",
    "jawaban": "The correct answer is: Variety"
  },
  "ff8870adec6ac780b30c0ce1ff1906a89bca27f2889287f4f05140e5181da02c": {
    "soal": "import pm4py\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\n# Import XES log\nlog = xes_importer.apply(xes_filename)\n# Cek jumlah kasus dan aktivitas\nprint(f\"Jumlah kasus: \")\nactivities = set()\nfor trace in log:\nfor event in trace:\nactivities.add(event['concept:name'])\nprint(f\"Jumlah aktivitas unik: {len(activities)}\")\nfrom pm4py.algo.discovery.dfg import algorithm as dfg_discovery\nfrom pm4py.visualization.dfg import visualizer as dfg_visualization\n# DFG berdasarkan frekuensi\ndfg = dfg_discovery.apply(log,\nvariant=dfg_discovery.Variants.FREQUENCY)\n# Tampilkan visualisasi\ndfg_vis = dfg_visualization.apply(dfg, log=log,\nvariant=dfg_visualization.Variants.FREQUENCY)\ndfg_visualization.view(dfg_vis)\nThe PM4Py function `dfg_discovery.apply(log, variant=dfg_discovery.Variants.FREQUENCY)` primarily provides:\n{\n=Frequency-based Directly-Follows Graph\n-Performance metrics only\n-List of active cases\n-File conversion to CSV\n-JSON data of all cases\n-Visualization of performance\n}",
    "jawaban": "The correct answer is: len(log)"
  },
  "2e933740ec859f181b0602b22799bca9250aa0b126db435669a7eb92c9863ff8": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nHow many images are designated for testing in the Fashion-MNIST dataset?",
    "jawaban": "The correct answer is: 10,000"
  },
  "24e2e1b627cde4a917eefce25a2b34071d93310204c366c5be1c8202425d6ac9": {
    "soal": "Some visualization widgets, such as Scatter Plot and several data projection widgets, can expose data instances within a data subset. In this workflow, the Line Plot visualizes data from the input data file, but also marks the data points selected in the Data Table (the selected rows).\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "fb84e963f7df1da2751f353da1e7b2a7b668f45cbf63f3f54935621793671c7e": {
    "soal": "To cluster based on similarity, we can measure the distance between images. The numerical representation of the images obtained from the Image Embedding widget, we measure the distance using the Distances widget. Typically, cosine distance works best for images, but you can experiment yourself. We then send the distance matrix to Hierarchical Clustering to visualize similar pairs in ......................\n\nThe result is that all the animals are correctly grouped. In the dendrogram, we do not see the images of the animals. We can use the Image Viewer widget to view the images.\n\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: dendogram"
  },
  "ff01f5b3636a2145c1203cfdbed2750a45f9a12a8a1265579eb9f64ae2f0cff0": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.rand(100) # 100 nilai target\n# Inisialisasi model regresi linier\nmodel = LinearRegression()\n# Tentukan jumlah fold (misalnya, 5-fold cross-validation)\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n# Evaluasi model menggunakan cross-validation\nscores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n# Menampilkan skor R^2 untuk setiap fold dan rata-ratanya\nprint(\"Skor R^2 untuk setiap fold:\", scores)\nprint(\"Rata-rata skor R^2:\", np.mean(scores))\nHow many samples are created in the dummy dataset?",
    "jawaban": "The correct answer is: 100"
  },
  "c9db3b8ccd508a0d7907cd85e9d4739322b620a2b8960ce367439c06ff1c8130": {
    "soal": "The Text Preprocessing widget can break text into smaller units (tokens), filter them, perform normalization (stemming, lemmatization), create n-grams, and tag tokens with part-of-speech labels. The steps in the analysis are applied sequentially and can be turned on or off.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "d48df4d5450b8b0ecc580167369c7588c11eda3ef3df17be680090dfe05fde80": {
    "soal": "Widget CSV File Import reads comma-separated files and sends the dataset to the output channel. The separator can be a comma, semicolon, space, tab, or manually defined delimiters. The history of the newly opened file is maintained in the widget.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "580b34dd1011f474c18c32158d1fd0c81fbab73a1cb5b861ea3082445db2085f": {
    "soal": "In the snapshot workflow below, we will observe the effects of preprocessing on text. For example, we are working with book-excerpts.tab, which we load with the Corpus widget. We have connected the Preprocess Text widget to the Corpus and kept the default preprocessing method (lowercase, tokenization by word, and stopword removal). The only additional parameter we added as output is the first 100 most frequent tokens. Then we connect the Data Table widget with the Word Cloud widget to observe the most frequent words in the text.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "80c958ea0afff5c07b95af7e1d6edc1363ff0e64e4894d2c7722d10734918b02": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#-xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\ndfg, start_activities, end_activities = pm4py.discover_dfg(log)\npm4py.view_dfg(dfg, start_activities, end_activities)\nWhat does the variable 'log' represent?",
    "jawaban": "The correct answer is: Event log data from XES file -List of filenames -Graph visualization -Model parameters -File metadata -Data transformation steps"
  },
  "3c2c2a1783e5d572ff60cabf17101330b0dd619f0ac195bd1411d3daeee63cb4": {
    "soal": "ProM supports animation based on:",
    "jawaban": "The correct answer is: Data from XES logs"
  },
  "0b223bbea5f350c9c4e891c9718c8b45727e0c022976ee9dfce9cd382b381944": {
    "soal": "import pandas as pd\nfile_path = \"/content/drive/My Drive/NamaFolder/NamaFile.xlsx\"\ndf = pd.read_excel(file_path)\nprint(df.head())\nWhich function is used to read the Excel file into a DataFrame?",
    "jawaban": "The correct answer is: pd.read_excel()"
  },
  "0ac61fa3234619a88766aae8cfe4fb34dc4c1f85cacd5042b4b397627a142a6a": {
    "soal": "import pandas as pd\nfrom pm4py.objects.conversion.log import converter as log_converter\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom pm4py.visualization.petrinet import visualizer as pn_visualizer\nfrom pm4py.objects.log.util import dataframe_utils\n# csv_filename='Insurance_claims_event_log.csv'\n# csv_filename='Business Travel Permits.csv'\n# csv_filename='Parent-Child Union Output.csv'\n# csv_filename='PM4PY_Dummy_Event_Log_100_Cases_pm4py.csv'\n# csv_filename='PM4Py_Dummy_Event_Log-congested-business-process.csv'\n# 1. Load CSV event log\ndf = pd.read_csv(csv_filename)\n# df = pd.read_csv(csv_filename,sep=';')\n# print(df.head())\n# 2. Format timestamps properly\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n# 3. Ensure column names match PM4Py expectations\ndf = dataframe_utils.convert_timestamp_columns_in_df(df)\n# 4. Convert to event log object\nevent_log = log_converter.apply(df, parameters=)\n# 5. Discover process model using Alpha Miner\nnet, initial_marking, final_marking = alpha_miner.apply(event_log)\n# 6. Visualize Petri Net\ngviz = pn_visualizer.apply(net, initial_marking, final_marking)\npn_visualizer.view(gviz)\nWhich file format would you use with 'xes_importer.apply()'?\n{\n~CSV\n~TXT\n=XES\n~Parquet\n~JSON\n~XML\n}",
    "jawaban": "The correct answer is: log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case_id'"
  },
  "8821bb3b502ea7e50518b62f9be4e59bb980431b0640603a01ade7c38b574b17": {
    "soal": "import pandas as pd\nfrom pm4py.objects.conversion.log import converter as log_converter\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom pm4py.visualization.petrinet import visualizer as pn_visualizer\nfrom pm4py.objects.log.util import dataframe_utils\n# csv_filename='Insurance_claims_event_log.csv'\n# csv_filename='Business Travel Permits.csv'\n# csv_filename='Parent-Child Union Output.csv'\n# csv_filename='PM4PY_Dummy_Event_Log_100_Cases_pm4py.csv'\n# csv_filename='PM4Py_Dummy_Event_Log-congested-business-process.csv'\n# 1. Load CSV event log\ndf = pd.read_csv(csv_filename)\n# df = pd.read_csv(csv_filename,sep=';')\n# print(df.head())\n# 2. Format timestamps properly\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n# 3. Ensure column names match PM4Py expectations\ndf = dataframe_utils.convert_timestamp_columns_in_df(df)\n# 4. Convert to event log object\nevent_log = log_converter.apply(df, parameters=)\n# 5. Discover process model using Alpha Miner\nnet, initial_marking, final_marking = alpha_miner.apply(event_log)\n# 6. Visualize Petri Net\ngviz = pn_visualizer.apply(net, initial_marking, final_marking)\npn_visualizer.view(gviz)\nWhich command is used to install PM4Py in a Jupyter notebook?\n{\n~pip install pandas\n=!pip install pm4py\n~conda install pm4py\n~install pm4py\n~!pip install graphviz\n~!install pm4py\n}",
    "jawaban": "The correct answer is: log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case_id'"
  },
  "82ab435908554644cf8e9cf4e7abd108adb1fefe6508838ef5c3bebb47fa8432": {
    "soal": "To perform machine learning, we need numbers. To obtain numerical representations of the images, we send the images to the Image Embedding widget.\n\nThe output data from the Image Embedding widget can be inspected using the Data Table widget. Now we have the numerical representation of the images. Each image has 2048 numerical representations (columns n1 to n2048). With this numerical feature representation, we can apply all standard machine learning techniques, such as clustering.\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "6f0314472a3bc9e02a836c61d91c0ecc22cf8d11d0ac3785898cb73fb19a3a8d": {
    "soal": "import pandas as pd\nfrom pm4py.objects.conversion.log import converter as log_converter\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom pm4py.visualization.petrinet import visualizer as pn_visualizer\nfrom pm4py.objects.log.util import dataframe_utils\n# csv_filename='Insurance_claims_event_log.csv'\n# csv_filename='Business Travel Permits.csv'\n# csv_filename='Parent-Child Union Output.csv'\n# csv_filename='PM4PY_Dummy_Event_Log_100_Cases_pm4py.csv'\n# csv_filename='PM4Py_Dummy_Event_Log-congested-business-process.csv'\n# 1. Load CSV event log\ndf = pd.read_csv(csv_filename)\n# df = pd.read_csv(csv_filename,sep=';')\n# print(df.head())\n# 2. Format timestamps properly\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n# 3. Ensure column names match PM4Py expectations\ndf = dataframe_utils.convert_timestamp_columns_in_df(df)\n# 4. Convert to event log object\nevent_log = log_converter.apply(df, parameters=)\n# 5. Discover process model using Alpha Miner\nnet, initial_marking, final_marking = alpha_miner.apply(event_log)\n# 6. Visualize Petri Net\ngviz = pn_visualizer.apply(net, initial_marking, final_marking)\npn_visualizer.view(gviz)\nIn the minimal code example, which variable holds the Petri net visualization?\n{\n~net\n~event_log\n=df\n~initial_marking\n~final_marking\n=gviz\n}",
    "jawaban": "The correct answer is: log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case_id'"
  },
  "500e1f8ab9a62cb5a8d9085062041c1dafaea93b7dc2f107810df3e222817c46": {
    "soal": "\n\n\nThe image above shows the difference between statistics vs. data scientist. It includes when, why, what, where, how, who. It also covers job fields starting from traditional statistics, business intelligence, to machine learning.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "731244557cc7006585eda81e278363b8747b1a24b2a314fa5cfa61019f5059f5": {
    "soal": "In a typical use of the Test & Score widget, we will input the dataset and several learning algorithms, and observe their performance in the table within the Test & Score widget and in ROC. Data is often preprocessed before testing; in this case, we perform some manual feature selection (Select Columns widget) on the Titanic dataset, where we only want to know the gender and survival status of the passengers, ignoring age.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "962d7f21277fd487a15cf5864f56c41402d0755fe40f58542d6c7c26c9159fe5": {
    "soal": "What is the purpose of dropout in neural networks?",
    "jawaban": "The correct answer is: To prevent overfitting by randomly dropping neurons during training"
  },
  "45c6342c6b759bc0a40ca771cb043b0596332e6efaa07216803dfc43a8bb9d79": {
    "soal": "Which article discusses 9 open-source datasets used for training Large Language Models and important data preprocessing steps?",
    "jawaban": "The correct answer is: Open-Sourced Training Datasets for LLMs"
  },
  "b3b5a5739317e962c0d9c3f8dd1b4cf6bae3d9a11f5d936f5ce84085503cd3ad": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\n# xes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\nxes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nprocess_tree = pm4py.discover_bpmn_inductive(log)\nbpmn_model = pm4py.convert_to_bpmn(process_tree)\npm4py.view_bpmn(bpmn_model)\nWhat type of mining does PM4PY primarily perform?",
    "jawaban": "The correct answer is: Process mining"
  },
  "53de93d015368977082cc50d533910af09bebac27ee22ce9ad279bd6c4897758": {
    "soal": "In the workflow below, the Iris data from the .............(1)................ widget is entered into the Select Columns widget, where we choose to display only two attributes (i.e. flower petal width and flower petal ...............(2)............). Then, we can see the original dataset and the dataset with the selected columns in the Data Table widget.\n\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 File, (2) \u2192 petal length"
  },
  "10dfcdb8867ac9eeaca1648a983ee04b9c7603fda308956c87b0d92c1b9b5343": {
    "soal": "Cassandra, or fully known as Apache Cassandra, is one of the open-source products for distributed database management by Apache. Cassandra is designed to manage large (\nAnswer Question 33\n data) structured data that is spread across many servers. This software is highly scalable, so it is no wonder that many large companies have trusted Cassandra as one of their work solutions, such as Facebook, Twitter, and Apple.",
    "jawaban": "The correct answer is: big"
  },
  "62198a7a17cdd520234d6d1a0f1b10168429b4b2e3e04b5134ad28ac579ab266": {
    "soal": "Activity relationship frequency is analyzed by:",
    "jawaban": "The correct answer is: Heuristic Miner"
  },
  "c340b5f9b048225e2237c4fec25d664a403bde829a81692f70db3b86af736b57": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(2, size=100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi training (80%) dan testing (20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model, misalnya RandomForestClassifier\nmodel = RandomForestClassifier(random_state=42)\n# Melatih model pada data training\nmodel.fit(X_train, y_train)\n# Memprediksi data testing\ny_pred = model.predict(X_test)\n# Mengevaluasi akurasi model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model pada data testing: ')\nWhich module from `sklearn` provides the `accuracy_score` function?\n{\n~sklearn.model_selection\n~sklearn.preprocessing\n=sklearn.metrics\n~sklearn.ensemble\n~sklearn.datasets\n~sklearn.pipeline\n}",
    "jawaban": "The correct answer is: accuracy:.2f"
  },
  "7d1cf1202677bcc238c737922c3a56dff8eee2c83c048bf8e294c485b20888c0": {
    "soal": "The Interpolate widget is part of the Time Series add-on. In the Interpolate Widget, we can choose between linear, cubic spline, quadrature, or mean interpolation.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "ba462641a95920aadfd09e2abfc976d20097a88699a392b780e23c4aaa436fda": {
    "soal": "Network Clustering Widget tries to find clusters in the network. Network Clustering works with two algorithms, one from Raghavan et al. (2007), which uses label propagation to find suitable clusters, and one from Leung et al. (2009), which builds on Raghavan's work and adds data attenuation as a parameter for cluster formation.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "5fe32ef405634e6320d872509b0879bc1eb0edc9fea89707b95e8c4445637e41": {
    "soal": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n# Contoh: Memuat dataset dari file CSV\n# Gantilah 'data.csv' dengan path ke dataset Anda\ndf = pd.read_csv('data.csv')\n# Memisahkan fitur (X) dan target \n# Asumsikan kolom terakhir adalah target\nX = df.iloc[:, :-1]\ny = df.iloc[:, -1]\n# Membagi dataset menjadi data latih dan data uji (80% : 20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model Decision Tree\nmodel = DecisionTreeClassifier()\n# Melatih model dengan data latih\nmodel.fit(X_train, y_train)\n# Melakukan prediksi pada data uji\ny_pred = model.predict(X_test)\n# Menghitung akurasi\nakurasi = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model: ')\n# Menampilkan laporan klasifikasi yang mencakup presisi, recall, dan F1-score\nprint(classification_report(y_test, y_pred))\nWhat is the primary purpose of using the `DecisionTreeClassifier` in the provided code?\n{\n~To perform regression analysis on the dataset.\n~To cluster the dataset into different groups.\n=To classify data based on learned decision rules.\n~To reduce the dimensionality of the dataset.\n~To perform association rule learning.\n~To visualize the data distribution.\n}",
    "jawaban": "The correct answer is: akurasi:.2f"
  },
  "2440eba9bc4515ec152a28a7c657d9eb8fa80a127976a84c5db9d2931e1801b8": {
    "soal": "The Save Data widget can consider the dataset provided in the input channel and save it to a data file with a specified name. The ........................ widget can save the data as a tab-delimited or comma-separated file.\n",
    "jawaban": "The correct answer is: Save Data"
  },
  "f4a69a93fb0654cac08d9004913fc30e10feae0487e4feb53af9d2e4ff2fcbab": {
    "soal": "The Predictions widget shows probabilities and the final decision of the prediction model. The output of the Predictions widget is another dataset, where the predictions are added as a new meta attribute. We can choose which features we want to remove (original data, ................ , probabilities). The results can be observed in the Data Table. If the predicted data includes the actual class value, the prediction results can also be observed in the Confusion Matrix Widget.\n",
    "jawaban": "The correct answer is: predictions"
  },
  "126e2bb843957e041c2d293e6d5e8b7beff7574066a382bd71642316f560d62c": {
    "soal": "For supervised problems, where data instances are described with class labels, we want to know which feature is the most informative. The Rank widget provides a table of features and their informativeness scores, and supports manual feature selection. In the workflow, we use it to not find the top two features (out of 79 initial features from the selected dataset) and display their scatter plot.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "acb66ad2e6d37f5782c0fd7a79c0c13cf7af30aa55170746b7ae1827134267f1": {
    "soal": "To cluster based on similarity, we can measure the distance between images. The numerical representation of the images obtained from the Image Embedding widget, we measure the distance using the Distances widget. Typically, ..................... distance works best for images, but you can experiment yourself. We then send the distance matrix to Hierarchical Clustering to visualize similar pairs in the dendrogram.\n\nThe result is that all the animals are correctly grouped. In the dendrogram, we do not see the images of the animals. We can use the Image Viewer widget to view the images.\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: cosine"
  },
  "40f4ef42f67749f58e89dc55bf8be8290523d65fbbbe895411c8ca1ec9a8d46b": {
    "soal": "The Distance Matrix widget creates ..............................., which is a two-dimensional array containing distances, taken pairwise, between elements of the dataset. The number of elements in the dataset determines the size of the matrix. Data matrix is essential for the Hierarchical Clustering Widget.\n\n\n",
    "jawaban": "The correct answer is: distance matrix"
  },
  "1b16e90ec689c654909a1919635a22b6fdfd9ad6def6ac9bec5a9ec5e69c1ac2": {
    "soal": "Orange3 installation on Ubuntu 18.04 can use the command\n\n\u00a0apt -y install orange3\n",
    "jawaban": "The correct answer is 'False'."
  },
  "6273bb669bea0ad84929af57592358e7800bc66be9a302d5d370d54ee5085423": {
    "soal": "Widget CSV File Import reads comma-separated files and sends the dataset to the output channel. The separator can be a comma, semicolon, png, space, tab, or manually defined delimiter. The history of the newly opened file is maintained in the widget.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "b243ee27a95057c3e773fa995a3454992172730421cab33881c4c0381df2220c": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#-xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\ndfg, start_activities, end_activities = pm4py.discover_dfg(log)\npm4py.view_dfg(dfg, start_activities, end_activities)\nIf the code 'xes_filename = 'BPIC_2012_W.xes'' is uncommented, what happens?",
    "jawaban": "The correct answer is: BPIC_2012_W.xes will be processed instead of BPIC_2012_A.xes -An error occurs -No action occurs -All files are processed -A random file is selected -The file will be deleted"
  },
  "c6866e2bd436c499ba4fd86b2eb5afe8f7502a037a952975e3905ed53e6c759b": {
    "soal": "Network Clustering Widget can help us reveal clusters and highly connected groups in a network. First, we will use the Network File Widget to load the lastfm.net dataset. Then we will send the network to the Network Clustering Widget. The Network Clustering Widget finds 79 clusters in the network. To visualize the results, we use the Network Explorer Widget and set the Color attribute to Cluster. This will color the network nodes with the corresponding cluster color - this is a good way to visualize highly connected groups in a dense network.\n\nNote that the Network Explorer Widget here will color the 100 largest clusters and color the rest as 'Other'.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "a925152158572d3caf681028fd06501a0de71562a266251ef887a1661f54a49a": {
    "soal": "In the snapshot below, we can see how transformation affects the distance matrix. We load the Iris dataset and calculate the distance between rows with the help of the ....................... widget. To show how Distance Transformation affects the Distance Matrix, we create the workflow below and compare the transformed distance matrix with the \"original\".\n\n\n",
    "jawaban": "The correct answer is: Distances"
  },
  "53e7283af549c9aab58b5c83f12f2439b6588c2b5292c46db697c834244e1c35": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\n# xes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\nxes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nprocess_tree = pm4py.discover_bpmn_inductive(log)\nbpmn_model = pm4py.convert_to_bpmn(process_tree)\npm4py.view_bpmn(bpmn_model)\nWhat is the library PM4PY commonly used for?",
    "jawaban": "The correct answer is: Process mining and process analysis"
  },
  "b5fa184868db2a2575b51e18c6df4e505351e3333e59d0dd09ff651fb733e610": {
    "soal": "The Distributions widget displays the distribution of discrete or continuous attribute values. If the data contains a class variable, the distribution can be conditioned on the class.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "35b1ff6b94e6794030220eda9e4336f26c7188b25d9b95d146838d65670f5508": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.rand(100) # 100 nilai target\n# Inisialisasi model regresi linier\nmodel = LinearRegression()\n# Tentukan jumlah fold (misalnya, 5-fold cross-validation)\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n# Evaluasi model menggunakan cross-validation\nscores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n# Menampilkan skor R^2 untuk setiap fold dan rata-ratanya\nprint(\"Skor R^2 untuk setiap fold:\", scores)\nprint(\"Rata-rata skor R^2:\", np.mean(scores))\nWhat is the primary purpose of importing the `numpy` library in the provided Python code?",
    "jawaban": "The correct answer is: To generate and manipulate arrays and matrices."
  },
  "85bc406cda7f30fdbcad5578b8e6a76c08697989be41761cd96c4e886764e5d8": {
    "soal": "from google.colab import drive\ndrive.mount('/content/drive')\nWhat happens to the files accessed from Google Drive after unmounting in Colab?",
    "jawaban": "The correct answer is: They are cached locally for future access."
  },
  "8eaa7ceeb8a5ba12666edf968410fad0c2f8148f8d56341249ce1425d7e40999": {
    "soal": "Heuristic Net visualization may become overly complex if:",
    "jawaban": "The correct answer is: Parameters aren't adjusted well"
  },
  "fc428bf4bb4692eb05e9e219c14bcb0feb34308878ca7d59c2e75520ef9b99c0": {
    "soal": "Data does not always come in the form of neat tables. Data can also be in the form of text, audio recordings, video content, or even images. However, computers can only work with numbers, so for any data mining, we need to transform that unstructured data into a vector representation.\n\nTo extract numbers from unstructured data, Orange can use deep network embedder. Orange has just started incorporating various .................(1)................. in Orange in the Image Embedding widget, and currently, they are available for text and ...................(2)................\n",
    "jawaban": "The correct answer is: (2) \u2192 image, (1) \u2192 embedder"
  },
  "5fb568873fed07de26870b1248e262703d49331fd77465b8b52d97174de08d81": {
    "soal": "import tensorflow as tf\nfrom tensorflow.keras import layers, models\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nmodel = models.Sequential([\nlayers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(64, (3,3), activation='relu'),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(128, (3,3), activation='relu'),\nlayers.Flatten(),\nlayers.Dense(128, activation='relu'),\nlayers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5, batch_size=64)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nHow many units are in the first Dense layer?",
    "jawaban": "The correct answer is: 128"
  },
  "051228e3bd3d6110030673c3d92a656916f0711c2b6983cce2bbaafc572b67cb": {
    "soal": "In the example workflow below, a very simple use of the Corpus widget is shown. Place the Corpus widget onto the canvas and connect it to the Corpus Viewer widget. We are using the book-excerpts .............. dataset, which is available in the add-on, and examining it in the Corpus Viewer widget.\n\n\n\n",
    "jawaban": "The correct answer is: tab"
  },
  "10b9ad6441e52afbbe1b34472d14683f618a7457998d495fc818dbe4d26b7e8b": {
    "soal": "Similar to R-programming, Apache Hadoop is \nAnswer Question 6\n source. It is a framework tool created by Google and Apache. The Hadoop framework allows for the processing of larger amounts of data, storing heterogeneous data, and accelerating the processing time.\nAccording to AWS, Hadoop is an open-source framework that is highly effective for storing extremely large datasets. In addition to storing, this framework can also efficiently process data from gigabytes to petabytes.",
    "jawaban": "The correct answer is: open"
  },
  "9ce0865d9bb7ed7f14e3d0426764e783515e91e378a933081b69e1155d2417a8": {
    "soal": "What is the purpose of an activation function in neural networks?",
    "jawaban": "The correct answer is: To introduce non-linearity into the network"
  },
  "d2418c09b973ab7a85bd437232581b719a68261d5af261aba4584b902b7cf185": {
    "soal": "When there is no data in the input, the widget ...........(1)........... reads the text corpus from a file and sends the corpus instance to the output channel. The history of the last opened file is saved in the Corpus widget. In the Corpus widget, there is also a directory with sample corpora that have been pre-installed with the add-on. The widget can read data from Excel files (.xlsx), ..............(2)...............-separated (.csv) and tab-delimited (.tab).\n",
    "jawaban": "The correct answer is: (1) \u2192 corpus, (2) \u2192 comma"
  },
  "d3a3c04723d5ce1d19658e188456fc460394f21e731cbd21b7f99fc6f109cb98": {
    "soal": "The Predictions widget displays .................... predictions on the data.\n",
    "jawaban": "The correct answer is: model"
  },
  "0bd3d07e606ee905cccc8066248bef6d0742eae937d5b2a0d15c962b011359e4": {
    "soal": "The Import Documents widget takes .................... files from a folder and creates a corpus. The Import Documents widget can read .txt, .docx, .odt, .pdf, and .xml. If there are subfolders in the folder, they can be used to label the class.\n",
    "jawaban": "The correct answer is: text"
  },
  "93f321e0699d93be390a053f170efffda2a8e109145b9267bcfe582f27e399a6": {
    "soal": "\n\n\n\n\n\nWorkFlow berikut dapat digunakan untuk menganalisa kesalahan klasifikasi yang dilakukan oleh program / algoritma yang digunakan.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "0355257c946ecf6bca0cdacb5c286414804a8692721a4348feb4d59162cccba9": {
    "soal": "model.save('my_model')\nfrom google.colab import files\nfiles.download('my_model')\nnew_model = tf.keras.models.load_model('my_model')\nWhich line of code initializes a new model from a saved one?",
    "jawaban": "The correct answers are: load_model('my_model'), new_model, tf.keras.models.load_model('my_model'), keras.load_model('my_model')"
  },
  "26cd931f391b00b135ee06c7bef8cde102346a27ea3b72e8acd40a6326d81774": {
    "soal": "Which command displays the logs for the Ollama server?",
    "jawaban": "The correct answer is: ollama logs"
  },
  "c734946428e0e49447a3a30844d093fb7aa6bb8a69b05bdf4f6b77f0cf9ad511": {
    "soal": "8 (decimal) + 11 (binary) - 0000 (binary) = ...... (binary)",
    "jawaban": "The correct answer is: 1011"
  },
  "a4f39f4bd7a925b40bb88a9712ae93d0a27a4131aeda4edb6d5f6402da5fff7f": {
    "soal": "\n\n\n\n\n\nIn this example, we are using the lastfm dataset, which can be loaded into the Data File widget under Browse documentation networks. The network node represents musicians, marked by the genre they play, the number of albums produced, and so on. Edge is the number of listeners on LastFm.\n\nThe entire dataset is visualized in the Network Explorer Widget. In this widget, we remove the coloring and adjust the node size to match the number of albums. Then, we select several nodes from the network. We can observe the selection in the Network Explorer Widget (1).\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "527e955c1c824c12ee38bebf341346c6c50e0064ff132d85da3abfd338e167ea": {
    "soal": "Which plugin is an alternative to Inductive Visual Miner in ProM?",
    "jawaban": "The correct answer is: Alpha Miner"
  },
  "fb4d93231599dae8970172f10934ca6b4ad631e3999c43163c1a71ef0e9670cf": {
    "soal": "Machine learning is a branch of computer science focused on the development of algorithms and techniques to create computer systems that can learn from data. In machine learning, computers are trained to recognize patterns and rules in data, allowing them to make intelligent and accurate predictions or decisions based on the data.\nThe learning process in machine learning can be carried out through three main methods: supervised learning, unsupervised learning, and reinforcement learning.\n- Supervised learning involves using labeled or annotated data to train algorithms, so computers can recognize patterns or relationships between data features and labels. Examples of supervised learning applications include image classification, stock price prediction, and email spam classification.\n- Unsupervised learning, on the other hand, involves using unlabeled data to identify hidden patterns or structures in the data. Examples of unsupervised learning applications include data clustering and dimensionality reduction.\n- Reinforcement \nAnswer Question 86\n involves computer systems learning through experience by performing actions and receiving feedback on whether the actions were right or wrong. The goal is to find the optimal decision or action that produces the maximum benefit.\nMachine learning has various applications, including facial recognition, image analysis, speech recognition, forecasting, anomaly detection, and more.",
    "jawaban": "The correct answer is: learning"
  },
  "1d8af9f8b3a819d97acde3dee3cf8be7632ee4a82699aa7a30410fb4d247d461": {
    "soal": "What does each row in a dataset represent?",
    "jawaban": "The correct answer is: A single entry or data instance."
  },
  "05131b9c13a41252ffdfbcd5393fcf4180ebdaa58c383952c118fe5241f45c30": {
    "soal": "How does stratification affect the class distribution in training and testing subsets?",
    "jawaban": "The correct answer is: It maintains the same class proportions as in the original dataset."
  },
  "c648c23a5e59d73b459790aea0585e9b43d5c1f377cc933634790f3da463db05": {
    "soal": "The Apache Hadoop framework consists of the following modules:\n- Hadoop Common - contains the libraries and utilities required by other Hadoop modules;\n- Hadoop Distributed File System (HDFS) - a distributed file system that stores data on commodity machines, providing very high aggregate bandwidth in the cluster;\n- Hadoop YARN - a resource management platform responsible for managing computational resources in the cluster and using them for scheduling user applications; and\n- Hadoop MapReduce - a programming model for large-scale data processing.",
    "jawaban": "The correct answer is 'True'."
  },
  "fbd9768957580498bbeafabb3fe8c5c66be5a52c60cec89c3f4b14522c566159": {
    "soal": "The Box Plot widget shows the distribution of attribute values. It is good practice to check any new data with this widget to quickly find anomalies such as duplicate values (e.g., gray or grey), outliers, and the like.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "e1950d347637a132e8d26c05fc03c944680a50bca7b181980f77b696df92fbed": {
    "soal": "Widget Network Analysis calculates statistical summaries from the node-level and graph-level for the network. Widget Network Analysis will output .................. with the computed statistics and an extended item data table (only node-level index).\n",
    "jawaban": "The correct answer is: network"
  },
  "3a990ab55617863ec83b1ec68419eec1eeea236062913a909c742d3f21f6a29e": {
    "soal": "from google.colab import drive\ndrive.mount('/content/drive')\nHow do you list the contents of a directory in your mounted Google Drive within Colab?",
    "jawaban": "The correct answer is: Both os.listdir() and ls command methods work."
  },
  "5b1331b49fc4504b23a4860abebaf2f43e9b01348b8afcda143d07609c4f443f": {
    "soal": "To visualize a Petri net using PM4Py, what must you first generate?",
    "jawaban": "The correct answer is: Process discovery model"
  },
  "3bfdbf4c489318e3c47bcdbed3b474255a53fd866a0b2719b2098d7c5044e8c1": {
    "soal": "In 2012, when Harvard Business Review called it \"The Sexiest Job of the 21st Century\", the term \"data science\" became a buzzword. Data Science is often interchanged with previous concepts such as business analytics, business intelligence, predictive modeling, and forensic.",
    "jawaban": "The correct answer is 'False'."
  },
  "5106a5fdc877ae89324a86cc6bc5f6c3224882acc5162cde57dce417d19d7806": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#_xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nmap = pm4py.discover_heuristics_net(log)\npm4py.view_heuristics_net(map)\nThe main focus of pm4py is on:",
    "jawaban": "The correct answer is: Process Mining -Web Scraping -Data Visualization -Machine Learning -Networking -Cloud Computing"
  },
  "84e111933b9c4191bf33457ba1b18e9a68c4b26617b2d92743c0f3e7dfa46ecb": {
    "soal": "Some formats supported by ORANGE3 (based on its add-ons):\n\n.............................. : Import Images from Image Analytics add-on\n",
    "jawaban": "The correct answer is: images"
  },
  "b749560ad16e1920827f9c55a83388976b249f23d881bbbdc5326d978728e531": {
    "soal": "The following regular expression (regex) function\n\nmatches words beginning with the letter b\nis\n",
    "jawaban": "The correct answer is:\n\\b(B|b)\\w+\\b\n"
  },
  "d4a8a2877b63985426fd06949bc1c75789d22d5ee38c7678bd562afc52bd8aa1": {
    "soal": "What is a gradient in machine learning?",
    "jawaban": "The correct answer is: The direction and rate of change of the loss function"
  },
  "ca30d5255b8e92d3a6c5d81ed618e3050b370d3b5b4a61d69f49d600d5d13dde": {
    "soal": "In TensorFlow, what is used to feed data into a model during training?",
    "jawaban": "The correct answer is: Placeholder"
  },
  "e104f513e60875528721d2e8e9e36830226dff8eb73f15b145c331fb503acc76": {
    "soal": "Widget kNN mem-prediksi berdasarkan instance training terdekat.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "370965f069ea9371bd9c72e18e9c64f18847f22cbfcc2f1cc0a023cffa21e860": {
    "soal": "Similar to R-programming, Apache Hadoop is open source. It is a framework tool created by \nAnswer Question 67\n and Apache. The Hadoop framework allows for the processing of larger amounts of data, storing heterogeneous data, and accelerating the processing time.\nAccording to AWS, Hadoop is an open-source framework that is highly effective for storing extremely large datasets. In addition to storing, this framework can also efficiently process data from gigabytes to petabytes.",
    "jawaban": "The correct answer is: Google"
  },
  "e162f69afa05e88b92c4ae9afb8aa2c5e74bd4d7ceb1c594ab2d015d14e8c38f": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(2, size=100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi training (80%) dan testing (20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model, misalnya RandomForestClassifier\nmodel = RandomForestClassifier(random_state=42)\n# Melatih model pada data training\nmodel.fit(X_train, y_train)\n# Memprediksi data testing\ny_pred = model.predict(X_test)\n# Mengevaluasi akurasi model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model pada data testing: ')\nWhy is `random_state=42` set when initializing the `RandomForestClassifier`?\n{\n~To ensure the model's predictions are random.\n~To limit the depth of the trees in the forest.\n=To ensure reproducibility of the model's results.\n~To specify the number of trees in the forest.\n~To increase the model's accuracy.\n~To set the maximum number of features considered.\n}",
    "jawaban": "The correct answer is: accuracy:.2f"
  },
  "d360cd3438e2aa7d5354727d4de157265f4e21b7f2a1ee7a150f79500d86232e": {
    "soal": "Data attributes in Orange have types discrete, continuous, or character string. Attribute types are marked with symbols that appear before the attribute name (D, C, E).\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "69b9eb4ea5f01bc11cc0efd808853357f30a041b8af9b7916dcc2897402d1d2b": {
    "soal": "The Predictions widget shows the classifier's predictions against the data.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "ba64fef81f591124d556f8ddeec582c8cc69d17eb5ffd9646d91847dc6744afd": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nWhat impact does applying Dropout have on a model's dependence on specific neurons?",
    "jawaban": "The correct answer is: It reduces dependence on specific neurons by promoting redundancy"
  },
  "63a31a35903d08f37d4bd9d79766651b886bc525cca88cdde8f34f2774dded2b": {
    "soal": "As an example of using kNN for classification, we use the iris dataset. We compare the results of k-Nearest Neighbors with the default Constant model, which will predict the majority class. It appears that kNN is ................\n\n\n\n\n\n",
    "jawaban": "The correct answer is: better"
  },
  "f1aa675862346157930a7180021ab24f3da65c8f9c9b93eba0db8689ceaf07a3": {
    "soal": "\n\n\nThe workflow above tests/evaluates CN2 Rule Induction and Random Tree in the Widget Test & Score.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "d4aa0c4f055e5b1001fa124963d9f7710167dbdb05f7281451d785558cdde4fd": {
    "soal": "Handling ..................... data in the Distances widget can only be used with the Euclidean, Manhattan, and Cosine metrics.\n\n\n\n",
    "jawaban": "The correct answer is: sparse"
  },
  "444df737974793a9c0f80fdc5996874735257ee6473310d935d3c7b74e949921": {
    "soal": "CN2 Rule Induction hanya bisa jalan untuk regresi saja.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "bc04ce14573069b9fe5128de2863aee65b0e934407e11c9b518bac95c8257056": {
    "soal": "The MapReduce and HDFS components of Apache Hadoop were inspired by Yahoo's papers on MapReduce and the Yahoo File System.",
    "jawaban": "The correct answer is 'False'."
  },
  "080f861a8aa6414895b1c48ed6ad301cf3f3b7c317d04e31d809f625d4f46a8d": {
    "soal": "The ORANGE Constant Widget is typically used as the main model for other learners.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "f8f61d54e7cf5c44b2a1119580881d636ca69c2d053006cb7e3780d7877f95de": {
    "soal": "Which element supports long conversations with LLM?",
    "jawaban": "The correct answer is: Context recycling"
  },
  "31f24241b151db7498ba400b092c529df61f45f3b037f27fa2254ee2ec35d0a6": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nHow many images per class are there in the CIFAR-10 dataset?",
    "jawaban": "The correct answer is: 6,000"
  },
  "311c44167c30a6cecb50ebeae89eb0578ff3acea5af4cf1fc265b043ba45d9c8": {
    "soal": "from googlesearch import search\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef get_page_content(url):\ntry:\nheaders =\nresponse = requests.get(url, headers=headers, timeout=10)\nsoup = BeautifulSoup(response.content, 'html.parser')\n# Ambil judul halaman\ntitle = soup.title.string if soup.title else 'No Title'\n# Ambil konten paragraf utama\nparagraphs = soup.find_all('p')\ntext_content = ' '.join([p.get_text() for p in paragraphs[:5]]) # Batasi 5 paragraf pertama\nreturn title.strip(), text_content.strip()\nexcept Exception as e:\nreturn 'Error', f\"Failed to fetch content: {e}\"\ndef google_scrape_with_content(keywords, num_results=5, output_file='scraped_results.csv'):\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'Rank', 'Title', 'URL', 'Content'])\nfor keyword in keywords:\nprint(f\"\n\ud83d\udd0d Searching for: {keyword}\")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nprint(f\" \u2192 Fetching: {url}\")\ntitle, content = get_page_content(url)\nwriter.writerow([keyword, i+1, title, url, content])\ntime.sleep(2) # Delay biar aman\nexcept Exception as e:\nprint(f\"\u274c Error while searching '{keyword}': {e}\")\nprint(f\"\n\u2705 All results + content saved to '{output_file}'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\ngoogle_scrape_with_content(keywords, num_results=5)\nWhat is the role of the `enumerate` function in the script?\n{\n=To keep track of the rank (position) of each search result.\n~To count the number of keywords processed.\n~To iterate over the list of keywords.\n~To reverse the order of search results.\n~To filter out duplicate URLs from the results.\n~To sort the search results alphabetically.\n}",
    "jawaban": "The correct answer is: \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\""
  },
  "c17b0bd74c9ba63e0c2efa80a3b7f90a1b7d2dd77668682d324445056c47f590": {
    "soal": "What is the vanishing gradient problem in deep learning?",
    "jawaban": "The correct answer is: When gradients become too small during training, slowing down learning."
  },
  "0ae84069b24b09037a267c1ab1dc7743de3f6f3dc32297d86361bc90c1a8d3f6": {
    "soal": "Widget Calibrated Learner menghasilkan sebuah model yang mengkalibrasi distribusi dari class probabilities dan meng-optimasi decision threshold. Widget ini hanya bekerja untuk binary classification task saja.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "7c230427dbea4e31bc2c277c56ffc3b801c61dcff0a2ebb8bb801e0a5fb79599": {
    "soal": "The distance matrix generated by the Distances Widget can be further fed into the Hierarchical Clustering Widget to uncover groups in the data, into the Distance Map Widget or the Distance Matrix Widget to calculate distances (the Distance Matrix Widget can be very slow for large datasets), into the MDS Widget to map data samples using the distance matrix, and finally, saved with the Save Distance Matrix Widget. The Distance File can be loaded with the Distance File Widget.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "b48397e8b60df3f6441ae617e6818733cfec8ff992d5094840ca4f7349889e23": {
    "soal": "Orange can import comma- or tab-delimited data files, or native Excel files or Google Sheets documents. Use the File widget to load data and, if necessary, specify class, info, and meta attributes.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "22eeb676d4a49032674506186f7456e748bf250f702ff7e7e0a44f728762b604": {
    "soal": "...................... can be built from the text files (ASCII) we have, as in the workflow below. First, data from the Text Files Widget must be segmented into words using the Segment Widget. Then, the output segmented data needs to be converted from segmented data into a corpus so that it can be processed by the Widget Interchange in the text mining toolbox. Before displaying it as a word cloud, it is recommended to do preprocessing first to reduce unnecessary words, such as conjunctions, etc., using the Preprocess Text Widget.\n\n\n\n\n",
    "jawaban": "The correct answer is: Word Cloud"
  },
  "6c5cfe7e9c58c36aba08cf0b07dd2e55df8c62b799d72042248651f044ce2aae": {
    "soal": "The correct installation of Orange3 on Ubuntu 18.04 can use the command\n",
    "jawaban": "The correct answer is: pip3 install orange3"
  },
  "4ca4caad85887817c21dba1df8b64ab90a7f0488e665fb8bb45bbc74a3701ccd": {
    "soal": "Widget Tree menggunakan algoritma Tree dengan kemampuan untuk melakukan forward pruning (pemangkasan ke depan).\n",
    "jawaban": "The correct answer is 'True'."
  },
  "fcb0336cdbbc26e0c257c32c51a33ead68b01af11987ff1404bf2bda2d532258": {
    "soal": "How do you verify the models available in Ollama after pulling new ones?",
    "jawaban": "The correct answer is: ollama list"
  },
  "c5879945c1e0631ccc4a447a0612311d89d06cf41680dc4bae519dd44502a2d8": {
    "soal": "\n\n\nExamples of business intelligence professions include,\n\n\n\n\n\n\n",
    "jawaban": "The correct answers are: BI Analyst, BI Consultant, BI Developer"
  },
  "c6f36fc19700327a59e7ab4f099dbfc0d8d92a959ac4afd57535a8edaf7e04e0": {
    "soal": "Big data is a term referring to large and complex datasets obtained from various sources, including business transactions, social data, census data, medical data, scientific data, and others. The main characteristics of big data are volume, velocity, and variety.\n- Volume refers to the very large amount of data generated constantly.\n- Velocity (\nAnswer Question 64\n in English) refers to the speed at which data is generated and updated, as well as the ability to process data in real-time.\n- Variety refers to the various sources of data and different types of data that can be collected.\nBig data typically requires specialized technology and techniques to process, analyze, and interpret it, including machine learning, data mining, and statistical analysis techniques. The goal of big data analysis is to gain valuable insights and information, such as trends and patterns useful for business, scientific, or social decision-making.",
    "jawaban": "The correct answer is: Velocity"
  },
  "9f3f69612678e9ef90b240ce1554bfe0f3e2407defe638aad78e33cbef44b49e": {
    "soal": "For problems that ....................... , where data instances are explained by class labels, we want to know which feature is the most informative. The Rank Widget provides a table of features and their informativeness scores, and supports manual feature selection. In this workflow, we use it to find the two best features (from the 79 initial ones from the selected dataset) and display their scatter plot.\n",
    "jawaban": "The correct answer is: supervised"
  },
  "db12666276f4c2f23246044fa41d132b7f81baa95fa8ef2c7e9b60c969daf25d": {
    "soal": "The Sentiment Analysis widget enables basic sentiment analysis of corpora. It works for English and uses two techniques supported by NLTK - ....................... and Vader. Both techniques are lexicon-based. Liu Hu calculates a single normal score for sentiment in text (negative score for negative sentiment, positive for positive, 0 for neutral), while Vader produces scores for each category (positive, negative, neutral) and adds a total sentiment score called compound.\n\n\n\n\nLiu Hu score\nVader score\n\n\nAnswer: \nQuestion 70",
    "jawaban": "The correct answer is: Liu Hu"
  },
  "0a0328a504f23eb6722ab06c7a74a2ecd05ea99e26866b216f5cef0a70ea0631": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nHow is the input shape specified in the model?",
    "jawaban": "The correct answers are: (784,), 28, (28, 28, 1), input_shape, (28, 28), 784, 784"
  },
  "5aa1fd41afe905b43d520149a8e76f17adee5e064f8c433fd008f7a27c6fe653": {
    "soal": "Dalam penggunakan Widget Constant untuk regresi, kita menggunakan Widget Constant untuk membuat prediktor dalam Prediction. Kita menggunakan dataset housing. Dalam Prediction, kita dapat melihat bahwa Mean Learner mengembalikan ....\u00a0 (rata-rata) nilai untuk semua instance.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: satu"
  },
  "4ba045d639dbe9cbbd1d3c83df4090be148451f1e4273ffc7aaecd8ca27faaf5": {
    "soal": "Just like R-programming, Apache Hadoop is open source. It is a tool framework created by Google and Apache. The Hadoop framework enables more data processing, stores heterogeneous data, and accelerates its processing.\nAccording to AWS, Hadoop is an open-source framework that is ineffective for storing very large datasets. In addition to storing, it is also capable of processing data ranging from gigabytes to petabytes efficiently.",
    "jawaban": "The correct answer is 'False'."
  },
  "b2137874e85535f05b6357b79afe913a66ba6fa6db575e96f46cfa932e2c7b2b": {
    "soal": "What role does the first Dense layer play in a model?",
    "jawaban": "The correct answer is: Serves as a hidden layer with 128 neurons to learn features from the flattened input"
  },
  "33e46ccd93931d6db1aef3966e27009f7fb6f3a4400b68747ec6c6fdd0f2b0bd": {
    "soal": "An example of using the kNN widget for classification task is shown in the workflow below. This workflow demonstrates how to use the Learner output. For this example, we use the housing dataset. We input the kNN prediction model into Predictions and observe the predicted values.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "84dbea50b464d011b05b70fc7ac950ed95f7f5f4a0ea426f1e4963e304622ed2": {
    "soal": "import pandas as pd\nfrom pm4py.objects.conversion.log import converter as log_converter\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom pm4py.visualization.petrinet import visualizer as pn_visualizer\nfrom pm4py.objects.log.util import dataframe_utils\n# csv_filename='Insurance_claims_event_log.csv'\n# csv_filename='Business Travel Permits.csv'\n# csv_filename='Parent-Child Union Output.csv'\n# csv_filename='PM4PY_Dummy_Event_Log_100_Cases_pm4py.csv'\n# csv_filename='PM4Py_Dummy_Event_Log-congested-business-process.csv'\n# 1. Load CSV event log\ndf = pd.read_csv(csv_filename)\n# df = pd.read_csv(csv_filename,sep=';')\n# print(df.head())\n# 2. Format timestamps properly\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n# 3. Ensure column names match PM4Py expectations\ndf = dataframe_utils.convert_timestamp_columns_in_df(df)\n# 4. Convert to event log object\nevent_log = log_converter.apply(df, parameters=)\n# 5. Discover process model using Alpha Miner\nnet, initial_marking, final_marking = alpha_miner.apply(event_log)\n# 6. Visualize Petri Net\ngviz = pn_visualizer.apply(net, initial_marking, final_marking)\npn_visualizer.view(gviz)\nWhat Python function converts CSV dataframes into event log objects in PM4Py?\n{\n=log_converter.apply\n~pd.read_csv\n~pm4py.convert\n~dataframe_utils.format\n~alpha_miner.apply\n~pn_visualizer.apply\n}",
    "jawaban": "The correct answer is: log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case_id'"
  },
  "3bf098337c72d84bb1c3537abb04f77683208edc98922da53e4888209a8b8875": {
    "soal": "In a typical use of the Test & Score widget, we will input the dataset and only one learning algorithm, and observe its performance in the table within the Test & Score widget and in ROC. Data is often preprocessed before testing; in this case, we perform some manual feature selection (Select Columns widget) on the Titanic dataset, where we only want to know the gender and survival status of the passengers, ignoring age.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "2b8cb81072229682911aba80a01440f32c6aea19b84ac6f34184d0e5beb3de8d": {
    "soal": "The Import Documents widget takes text files from a folder and creates a corpus. The Import Documents widget can read ...................., .docx, .odt, .pdf, and .xml. If there are subfolders in the folder, they can be used to label the class.\n",
    "jawaban": "The correct answer is: txt"
  },
  "7423c6dceb23349fb462ff9dabe3652a9138be7db796629d1ac513982e782d08": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#-xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\ndfg, start_activities, end_activities = pm4py.discover_dfg(log)\npm4py.view_dfg(dfg, start_activities, end_activities)\nWhich line correctly assigns the filename to be processed?",
    "jawaban": "The correct answer is: xes_filename"
  },
  "fac6964ba704e81aace9bce7cd836e7b1487d8afc677715bca5f345788c79537": {
    "soal": "RapidMiner, formerly known as YALE (Yet Another Learning Environment), is open-source software. This software serves as a solution for performing data mining, text mining, and predictive analysis.\nRapidMiner uses various descriptive and predictive techniques to provide insights to users, enabling them to make the best decisions. RapidMiner is written in Java, allowing it to work only on Linux.",
    "jawaban": "The correct answer is 'False'."
  },
  "b4ff172b83254c68b36f287c6a83dc32eb61b571d305e8bcc0dc83a41e945a53": {
    "soal": "The Tree widget can work for regression tasks. In the workflow below, we use the housing dataset and give the dataset to the Tree widget. The selected Tree node in .............. will be displayed in the Scatter Plot, and we can see that the selected example has the same feature.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Tree Viewer"
  },
  "77e74921ea08556dd106a86bfb1558367b19a1b3c5d8288a20d4fe14210ee9e7": {
    "soal": "Widget Import Images scans a directory and returns one row per image retrieved. The columns contain the image name, directory path to the image, width, height, and image size. The column with the directory path to the image is then used as an attribute for visualization and Image Embedding.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "a4801038d166ae6d8bc131a4718d8c477396dfe97eea41f6f1d22a58b8b27889": {
    "soal": "What kind of technology is used to train LLMs?",
    "jawaban": "The correct answer is: Deep learning"
  },
  "03547c07b82979ee23d25d879d64c8c38b8dd487c3660d9d9feb789ea4d945ac": {
    "soal": "What is Big Data?",
    "jawaban": "The correct answer is: Extremely large datasets that require specialized tools and techniques to process and analyze"
  },
  "939c278cd25d6a228cfa91b5acbfcb05df4c5b9ecfb7a2361d1d83a0dbb78b30": {
    "soal": "Which component in TensorFlow is responsible for running computational graphs?",
    "jawaban": "The correct answer is: Session"
  },
  "798e5caf205d1252453a5ffaa80ce19bed7d700381da1b70c6a2b595d2d18e05": {
    "soal": "What does the Alpha Miner primarily reconstruct from event logs?",
    "jawaban": "The correct answer is: Causality between events"
  },
  "090e7337a8386d2657fa5a7db6f1ae057cbd586e7c4fb62482cdf04326192e92": {
    "soal": "\n\n\nExamples of big data statistics usage include,\n\n\n\n",
    "jawaban": "The correct answers are: Social Media, Financial Trading Data"
  },
  "7c290994014f5e4589760fbb74388bc45e69cc0c5e0ba7cd2ef2eb0b8d1abf60": {
    "soal": "The Sentiment Analysis widget enables basic sentiment analysis of corpora. It works for English and uses two techniques supported by NLTK - ....................... and Vader. Both techniques are lexicon-based. Liu Hu calculates a single normal score for sentiment in text (negative score for negative sentiment, positive for positive, 0 for neutral), while Vader produces scores for each category (positive, negative, neutral) and adds a total sentiment score called compound.\n\n\n\n\nLiu Hu score\nVader score\n\n\nAnswer: \nQuestion 53",
    "jawaban": "The correct answer is: Liu Hu"
  },
  "15cf68072ccf964c630393fa5bf8cfafc01d5d07a2d3c62d4972d67b4370a954": {
    "soal": "To obtain numerical representations of the images from Image Import, we need to send the images to the Image Embedding widget as shown in the workflow below.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "54cd08206b4d33bd7b5753217315075a69462acd376bcf99dc504c5e65eecd51": {
    "soal": "Word Cloud can be built from documents we have. In the text mining toolbox, there is a widget Import Documents to read document files. Before displaying it as a word cloud, it is advisable to pass it through the ...............(1)............... widget first, to reduce unnecessary words such as conjunctions, etc. Then we can pass it to the ................(2).................. widget first, or directly to the Word Cloud widget to display it.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: (2) \u2192 Bag of Words, \u2192 Word Cloud"
  },
  "ec1eaf78898df4348cb9a42c8ff33e0e6e6dad6835060787f3a6e9ec0455431c": {
    "soal": "The easiest way to use the .............(1)............ widget is to select a subset of data and find matching examples in the visualization. We use the breast-cancer dataset to select two subsets with ................(2)............... widget - the first subset is breast cancer patients aged between 40 and 49, and the second is patients with tumor sizes between 20 and 29. The Venn Diagram helps us find examples that match both criteria, which can be found at the intersection of the two circles.\n\n\n",
    "jawaban": "The correct answer is: (2) \u2192 Select Rows, (1) \u2192 Venn Diagram"
  },
  "279d7b06b4c3b72155314a7bfa11a87c8277e15176d056181bf253c45ff5dc93": {
    "soal": "Widget File can read attribute-value data to input files.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "3b4cb2144309bf1a7ad53d05272be40e6fc219f7231650d4b70b83a946a63980": {
    "soal": "Currently, the only widget that provides the correct signal needed by the Calibration Plot widget is the Confusion Matrix widget. Therefore, the Calibration Plot widget always follows the Confusion Matrix widget, and since it has no output, no other widget follows it.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "21a1331e0fbb91fafe14f90b3f2dc5f091577924e3b1557163363cf399bc79c5": {
    "soal": "Similar to R-programming, Apache Hadoop is open source. It is a framework tool created by Google and Apache. The Hadoop framework allows for the processing of larger amounts of data, storing heterogeneous data, and accelerating the processing time.\nAccording to AWS, Hadoop is an \nAnswer Question 100\n source framework that is highly effective for storing extremely large datasets. In addition to storing, this framework can also efficiently process data from gigabytes to petabytes.",
    "jawaban": "The correct answer is: open"
  },
  "c3fdc11864b8ec0ea10ad8c09703691a835a4be45cc6dac6c59d4472454c3238": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#_xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nmap = pm4py.discover_heuristics_net(log)\npm4py.view_heuristics_net(map)\nWhich of the following is a valid action of the script?",
    "jawaban": "The correct answer is: Discovering a heuristics net -Sorting CSV files -Running SQL queries -Analyzing stock data -Performing sentiment analysis -Generating HTML pages"
  },
  "c3a7f4a1f5209732ff75a46ce4422785354d2251263f2cef17aae9da67b6800d": {
    "soal": "Contoh penggunaan Widget kNN untuk task klasifikasi di perlihatkan pada workflow di bawah ini. Workflow ini menunjukkan cara menggunakan Learner output. Untuk tujuan contoh ini, kita menggunakan dataset housing. Kita memasukkan model prediksi kNN ke dalam Predictions dan mengamati nilai yang diprediksi.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "177e0a56136c72d452014ceb0da7bc9322d2aecfd19c65602f693950bcd54329": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nWhat is the total number of images in the CIFAR-10 dataset?",
    "jawaban": "The correct answer is: 100,000"
  },
  "98e38a2e1dfc72291c7fc2af381b4480cdb6e515565c0196915431a053b9aa14": {
    "soal": "Which command installs the 'docker.io' package on Ubuntu?",
    "jawaban": "The correct answer is: sudo apt install -y docker.io"
  },
  "6bc867b5d2e70510b2c54f96c26d7e86bc95770080054a6edf61195ac2e3d926": {
    "soal": "The Apache Hadoop framework consists of the following modules:\n- Hadoop Common - contains libraries and utilities needed by other Hadoop modules;\n- Hadoop Distributed \nAnswer Question 74\n System (HDFS) - a distributed file system that stores data on commodity machines, providing very high aggregate bandwidth in the cluster;\n- Hadoop YARN - a resource management platform responsible for managing computational resources in the cluster and using them for scheduling user applications;\n- Hadoop MapReduce - a programming model for processing large-scale data.",
    "jawaban": "The correct answer is: File"
  },
  "feef7d3f5bc2f7a7ee6a004b156f4ca3505217f719a5f5a0f05b4423ab8cb7e8": {
    "soal": "Which algorithm is commonly used for classification tasks?",
    "jawaban": "The correct answer is: Support Vector Machines"
  },
  "5fd196c42250624a7253f944ad684f99a4c798eec9828342759232accd1fd46f": {
    "soal": "The Tree widget can work for regression tasks. In the workflow below, we use the housing dataset and give the dataset to the Tree Map widget. The selected Tree node in the Tree Viewer will be displayed in the Scatter Plot, and we can see that the selected example has the same feature.\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "3641920d62feb609e4fe2e18ef95fa623784bf19a73ef8eaa738c88c63857518": {
    "soal": "Machine learning is a branch of computer science focused on developing algorithms and techniques to create computer systems that can learn from data. In machine learning, computers are trained to recognize patterns and rules in the data, allowing them to make intelligent and accurate predictions or decisions based on that data.\nThe learning process in machine learning can be done through three main methods: supervised learning, unsupervised learning, and reinforcement learning.\n- \nAnswer Question 89\n learning involves using labeled or annotated data to train the algorithm, allowing the computer to recognize patterns or relationships between data features and labels. Examples of supervised learning applications include image classification, stock price prediction, and spam email classification.\n- Unsupervised learning, on the other hand, involves using unlabeled data to identify hidden patterns or structures within the data. Examples of unsupervised learning applications include data clustering and dimensionality reduction.\n- Reinforcement learning involves computer systems learning through experience by taking actions and receiving feedback about whether those actions are correct or incorrect. The goal is to find the optimal decision or action that produces the maximum benefit.\nMachine learning has various applications, including facial recognition, image analysis, speech recognition, forecasting, anomaly detection, and more.",
    "jawaban": "The correct answer is: Supervised"
  },
  "7256125663b67055be4879627f8745c640f8b3f419b1ca370155f573be225b0c": {
    "soal": "import pm4py\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\n# Import XES log\nlog = xes_importer.apply(xes_filename)\n# Cek jumlah kasus dan aktivitas\nprint(f\"Jumlah kasus: \")\nactivities = set()\nfor trace in log:\nfor event in trace:\nactivities.add(event['concept:name'])\nprint(f\"Jumlah aktivitas unik: {len(activities)}\")\nfrom pm4py.algo.discovery.dfg import algorithm as dfg_discovery\nfrom pm4py.visualization.dfg import visualizer as dfg_visualization\n# DFG berdasarkan frekuensi\ndfg = dfg_discovery.apply(log,\nvariant=dfg_discovery.Variants.FREQUENCY)\n# Tampilkan visualisasi\ndfg_vis = dfg_visualization.apply(dfg, log=log,\nvariant=dfg_visualization.Variants.FREQUENCY)\ndfg_visualization.view(dfg_vis)\nWhich PM4Py module is used for discovering Directly-Follows Graphs (DFG)?\n{\n=dfg_discovery\n-graphviz\n-visualizer\n-xes_importer\n-event_log\n-performance_spectrum\n}",
    "jawaban": "The correct answer is: len(log)"
  },
  "03d058c6b6e2303b43703fbf9bd9ff947a5edbf0a2e5cfca2d90d81796583bf9": {
    "soal": "What would happen if LLM didn\u2019t use a context window?",
    "jawaban": "The correct answer is: Responses would lack continuity"
  },
  "461adc17e8c8ee4c31202ad2b5902eea9659e813856027dd78a71392b2c2312f": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nWhat is the role of the Dense layer in the provided model?",
    "jawaban": "The correct answer is: Applies a fully connected operation to the input data"
  },
  "e26d68e978bd52b14b9614f66dded3802b615046427525b5dbf19641041b46c0": {
    "soal": "Which part of the LLM remembers previous interactions?",
    "jawaban": "The correct answer is: Context window"
  },
  "9c3f018645a6baa9dc455d85767b06cd73cd5ebfb342542ba8dcfa9a30974723": {
    "soal": "Data attributes in Orange have types discrete, continuous or character ............................ The attribute type is marked by a symbol that appears before the attribute name (D, C, S).\n\n\n\nAnswer: \nQuestion 6",
    "jawaban": "The correct answer is: string"
  },
  "f057e3732e04c234d4c2df08f0009289fa80249c8b737c0298eff97c6e883848": {
    "soal": "Which command installs Docker Compose on Ubuntu 24.04?",
    "jawaban": "The correct answer is: curl -L \"https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose"
  },
  "e73b383f7aa8440af374644cd7c3731ca6062fc5e8e9e3b6b4eed4d79344e899": {
    "soal": "If the dataset has few attributes, it is not feasible to manually scan all pairs to find interesting or useful scatter plots. Orange implements intelligent data visualization with the Search Informative Projections option in the widget.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "8b94e47b2dfed77087acfa3677ef34120220e43108c950071f3be19c00ebb09a": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#-xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\ndfg, start_activities, end_activities = pm4py.discover_dfg(log)\npm4py.view_dfg(dfg, start_activities, end_activities)\nWhich of these filenames is NOT commented?",
    "jawaban": "The correct answer is: BPIC_2012_A.xes -BPIC_2012_W.xes -BPIC_2012_O.xes -PM4PY-running-example.xes -excercise.xes -BPI_Challenge_2019.xes"
  },
  "919e90ce458c98c0310219f623138fe2b4be079d627e3530fe9a4789675f0d41": {
    "soal": "In the Distributions widget, for discrete attributes, the graph displayed by the widget shows how many times (e.g., how many instances) each attribute value appears in the data. If the data contains a class variable, the average class value for each attribute value will also be displayed (as shown in the snapshot below). To create this graph, we used the Zoo dataset.\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "b36d6d4bd69308dcedc28bfb7e8edf4f3a428ba3fed21ef490eccf3a0b747f27": {
    "soal": "In the image below, we use the Network Generator Widget to create a Grid graph with a height of 4 and a width of 5 and send it to Network Analysis. We can calculate node degrees and send the data to Network Explorer. Finally, we observe the resulting graph in the visualization and set the node size and color to Degree. Network Explorer is a good tool for observing and explaining the properties of ................... .\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Network"
  },
  "aece30c225b82da19e6ae7a0f44d76b05a4549e2b7e6a1cd136931d688a9b968": {
    "soal": "import tensorflow as tf\nfrom tensorflow.keras import layers, models\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nmodel = models.Sequential([\nlayers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(64, (3,3), activation='relu'),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(128, (3,3), activation='relu'),\nlayers.Flatten(),\nlayers.Dense(128, activation='relu'),\nlayers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5, batch_size=64)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nWhich function is used to load the CIFAR-10 dataset?",
    "jawaban": "The correct answer is: tf.keras.datasets.cifar10.load_data()"
  },
  "e7607a558e1a98666b26321a5fe39e68c2af50a6bea7aca5b5b0abe216c2cc69": {
    "soal": "Which of these tools currently does NOT support direct process animation?",
    "jawaban": "The correct answer is: PM4Py"
  },
  "23674329472da0484cb0ddf62065977ab70674c72f2b28a9ad190c62b01a39f9": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(2, size=100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi training (80%) dan testing (20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model, misalnya RandomForestClassifier\nmodel = RandomForestClassifier(random_state=42)\n# Melatih model pada data training\nmodel.fit(X_train, y_train)\n# Memprediksi data testing\ny_pred = model.predict(X_test)\n# Mengevaluasi akurasi model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model pada data testing: ')\nWhich function is used to evaluate the accuracy of the model's predictions?\n{\n~precision_score\n~recall_score\n=accuracy_score\n~f1_score\n~roc_auc_score\n~mean_squared_error\n}",
    "jawaban": "The correct answer is: accuracy:.2f"
  },
  "b472f4adc3cd388eb8c04dd236c36b180245342821496222bb4e784ba71e9428": {
    "soal": "The ..................... widget scans the directory and returns one row per image taken. The columns include image name, directory path to the image, width, height, and image size. The column with the directory path of the image is then used as an attribute for visualization and Image Embedding.\n",
    "jawaban": "The correct answer is: Import Images"
  },
  "98be5788ead9f7caa37ca05ab4b5f460c15a1a5f01fc0102847d7f80fcaf5802": {
    "soal": "The Tree widget uses the Tree algorithm with the ability to perform forward pruning.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "cb3ecf942971721a6c283ba5ec28557dc6b5278d84cb8cc8445ba8b4ed01ea13": {
    "soal": "The core of Apache Hadoop consists of the storage part (Hadoop \nAnswer Question 59\n File System (HDFS)) and the processing part (MapReduce). Hadoop splits files into large blocks and distributes them across nodes in the cluster. To process the data, Hadoop MapReduce transfers code to the nodes to process in parallel, based on the data that needs to be processed at each node. This approach takes advantage of data-node locality, manipulating the data they hold in hand to enable faster and more efficient processing than more conventional supercomputer architectures relying on parallel file systems where computation and data are linked through high-speed networks.",
    "jawaban": "The correct answer is: Distributed"
  },
  "d9a1507714ded303f9a3fac022f923fbc97e19ed128a11fd7def94dab30a3ea3": {
    "soal": "To obtain numerical representations of the images from Image Import in Orange, we will send the images to the Image Embedding widget in Orange. As a result, we will have the numbers we want. There are 2048 of them (columns n0 to n2047). Therefore, it is not possible to apply all standard machine learning techniques, such as clustering, etc.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "1138b648d90dc630221902adaa6cc0fe30de52fed52f926eb547c5745a89f548": {
    "soal": "\n\n\n\n\n\nWorkFlow di atas dalam dunia machine learning merupakan bagian dari unsupervised learning.\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "c9f4b09ddc2283bebd2d1c10d6dccfdec0e3245a636b76cd20fe5bc1e30e041a": {
    "soal": "How can you set specific parameters for a model before running it?",
    "jawaban": "The correct answer is: ollama run model --param value"
  },
  "800cffc50fc95a35ebfcb8616e3b4d3d7315fee5eaf2d72739bfee77bd7bd41e": {
    "soal": "!pip install googlesearch-python nltk matplotlib wordcloud\nfrom googlesearch import search\nimport csv\ndef google_scrape(keyword, num_results=40,\noutput_file='results.csv'):\nprint(f\"Searching Google for: \")\n# Lakukan pencarian dan ubah hasil ke dalam list\nresults = list(search(keyword, num_results=num_results))\n# Simpan ke file CSV\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'URL'])\nfor url in results:\nwriter.writerow([keyword, url])\nprint(f\"Saved {len(results)} results to '{output_file}'\")\n# Contoh penggunaan\nkeyword = \"berita terbaru teknologi Indonesia\"\ngoogle_scrape(keyword, num_results=40)\nWhich encoding is specified when opening the output file in the script?\n{\n=utf-8\n~ascii\n~iso-8859-1\n~utf-16\n~cp1252\n~latin1\n}",
    "jawaban": "The correct answer is: keyword"
  },
  "eada090e542b0d2117dc81b3ebd472e4ef7ef8569013f1be26de52271de2eae7": {
    "soal": "In the workflow below, Iris data from the ..................... widget is passed to the Select Columns widget, where we choose to display only two attributes (i.e., petal width and petal length). Then, we can view the original dataset and the dataset with the selected columns in the Data Table widget.\n\n\n",
    "jawaban": "The correct answer is: File"
  },
  "2888ef1f1c47b1de87c4abe8476db4b4324c9c3d2bb4b8cc3530483db92e9a61": {
    "soal": "The result of 8 (decimal) OR 1 (decimal) is",
    "jawaban": "The correct answer is: 9"
  },
  "378ba458061c4b0986799457c079f0cc0c34af0b5045052cd98ee598735a9567": {
    "soal": "Widget Network Clustering can detect clusters in a ..........................\n",
    "jawaban": "The correct answer is: network"
  },
  "d8b6489fcf79f07dc840daa12bf3ff942f85113576acdde98c94105de55a55a2": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#_xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nmap = pm4py.discover_heuristics_net(log)\npm4py.view_heuristics_net(map)\nWhat does the script primarily accomplish?",
    "jawaban": "The correct answer is: Discovers and visualizes a heuristic net -Generates a decision tree -Performs a clustering analysis -Trains a neural network -Analyzes numerical data -Conducts sentiment analysis"
  },
  "7ab0a21f142ff5f4098329924de8b31af3dfaa892c9788e8b6cac117900a6bc5": {
    "soal": "The ...................... widget displays the predictions of the model on the data.\n",
    "jawaban": "The correct answer is: Predictions"
  },
  "324eef4ce35727c712a06796b1b2e621d1015760f2eac2ebd434055f16c6d367": {
    "soal": "import pandas as pd\nfile_path = \"/content/drive/MyDrive/your_file.csv\"\ndf = pd.read_csv(file_path)\nprint(df.head())\nIn the code snippet, what does the 'file_path' variable represent?",
    "jawaban": "The correct answer is: The path to the CSV file in Google Drive."
  },
  "b80aaa7cf5c98a46259589c18e67aa8bf69343a55c7537d187267e682afd602e": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nx_train = np.random.rand(1000, 10)\ny_train = np.random.randint(2, size=(1000, 1))\nmodel = keras.Sequential([\nkeras.layers.Dense(64, activation='relu', input_shape=(10,)),\nkeras.layers.Dense(32, activation='relu'),\nkeras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer='adam',\nloss='binary_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=10, batch_size=32)\nWhat does the final model output?",
    "jawaban": "The correct answer is: Probability between 0 and 1"
  },
  "9142a3ca78abfd7cd19a418523e31ae9ffbce489389f3d4907d5074ea739d1ef": {
    "soal": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n# Contoh: Memuat dataset dari file CSV\n# Gantilah 'data.csv' dengan path ke dataset Anda\ndf = pd.read_csv('data.csv')\n# Memisahkan fitur (X) dan target \n# Asumsikan kolom terakhir adalah target\nX = df.iloc[:, :-1]\ny = df.iloc[:, -1]\n# Membagi dataset menjadi data latih dan data uji (80% : 20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model Decision Tree\nmodel = DecisionTreeClassifier()\n# Melatih model dengan data latih\nmodel.fit(X_train, y_train)\n# Melakukan prediksi pada data uji\ny_pred = model.predict(X_test)\n# Menghitung akurasi\nakurasi = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model: ')\n# Menampilkan laporan klasifikasi yang mencakup presisi, recall, dan F1-score\nprint(classification_report(y_test, y_pred))\nWhy is the `random_state=42` parameter used in the `train_test_split` function?\n{\n~To increase the randomness of data splitting.\n~To ensure the data is split differently each time.\n=To ensure reproducibility of the data split.\n~To limit the size of the dataset.\n~To set the seed for the model's random number generator.\n~To specify the number of splits.\n}",
    "jawaban": "The correct answer is: akurasi:.2f"
  },
  "bad5eb41f9f9a996963f3d7ad9c97e0662e38444bc389a9bdf0945720960bda0": {
    "soal": "Widget Tree menggunakan algoritma Tree dengan kemampuan untuk melakukan inward pruning (pemangkasan ke depan).\n",
    "jawaban": "The correct answer is 'False'."
  },
  "3178d8cf5eb8aa28d31b77ecd244a6966fe5fd351c7dbd39273cdb5c395ecc1b": {
    "soal": "Data attributes in Orange have types discrete, continuous or character string. The attribute type is marked by a symbol that appears before the attribute name (..................., C, S).\n\n\n\nAnswer: \nQuestion 33",
    "jawaban": "The correct answer is: D"
  },
  "ef64f1dc73785384df120d13140bdfccb6f8d80ce5a998d454bd5635f912fded": {
    "soal": "In the Interpolate widget, which is part of the Time Series add-on, the interpolation type - .......... interpolation fits with a cubic polynomial to the values around the missing value. Therefore, this technique will be very slow but will give the best results.\n",
    "jawaban": "The correct answer is: Spline"
  },
  "6e1b043704f2de66342e62658636c95578454519babebbff2c92c1031aa184f9": {
    "soal": "The basic framework of Apache Hadoop consists of the following modules:\n- Hadoop \nAnswer Question 39\n - contains libraries and utilities required by other Hadoop modules;\n- Hadoop Distributed File System (HDFS) - a distributed file system that stores data on commodity machines, providing very high aggregate bandwidth in clusters;\n- Hadoop YARN - a resource management platform responsible for managing computing resources in the cluster and using them for user application scheduling; and\n- Hadoop MapReduce - a programming model for large-scale data processing.",
    "jawaban": "The correct answer is: Common"
  },
  "c90c2695874e3157aa58ca052e064fcd58482254781ac439fd271529603e62c0": {
    "soal": "Below, we will use the Attrition - Train data from the ....(1)..... widget. This is data about employee attrition. In other words, we want to know if a certain employee will quit their job or not. We will create a prediction model with the ....(2).... widget and observe the probability in Predictions.\n\nFor prediction, we need training data, which we loaded in the first Datasets widget, and data to predict, which we will load in another ....(1)..... widget. We will use the Attrition - Predict data now. Connect the second dataset to the Predictions widget. Now we can see the predictions for three data instances from the second dataset.\n\nThe ....(2).... model predicts no employees will leave the company. We can try another model and see if the predictions change. Or test predictive scores first in the ....(3)..... widget.\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 Datasets, (3) \u2192 Test & Score, (2) \u2192 Tree"
  },
  "d7f7b4eb7f9919baa5ce11930ce2a7f618e729294920dc16674fecb8ddcd3bcc": {
    "soal": "\n\n\n\n\n\nProses klasifikasi dilakukan menggunakan algoritma / modul linear regression yang hasilnya langsung dinilai keakuratannya oleh modul Test & Score\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "5741bc96e02e15389c4bccc138a8cfa6194bc2424373737a0e9f640a060f8ef4": {
    "soal": "Salah satu keuntungan Orange adalah Workflow analisis data interaktif dengan toolbox yang banyak.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "c73c1f62bc57ff818d491470a7e1ad52ca5a4c246fbdabeb5f2784078e017270": {
    "soal": "In the Scatter Plot Widget - Selection can be used to define a manually defined subgroup in the data. Use the Shift modifier when selecting plot instances to place them into a new group. Shift + Ctrl (or Shift + Cmd in macOS) adds instances to the last group.\n\nSignal data outputs a data table with an additional column containing the group index.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "af4acca010d62e8ab26fed79cbbf7c171e67371d630d576ff6618b7c09093920": {
    "soal": "Data Science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from coding in various forms, both structured and unstructured, similar to data mining.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "141ee6e19993d91a18531b1f0daef16f94737f965fad5a4f15e87c12d87ff033": {
    "soal": "What is the purpose of dropout in a neural network?",
    "jawaban": "The correct answer is: To prevent overfitting by randomly deactivating neurons during training."
  },
  "b065f48f744989ef56ff131a56d24eee7533238e9b79fd4857b40343bed5667a": {
    "soal": "Algoritma CN2 adalah teknik klasifikasi yang di rancang agar secara effisien melakukan induksi dari rules yang simple & komprehesif dalam bentuk \u201cif true then predict class\u201d, meskipun dalam domain dimana mungkin ada noise.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "ed8a76a8fbac9d6f6592a93606251175ed6630ff20020e6cab16de0ac9ef20b6": {
    "soal": "In the following workflow example, it shows how to quickly visualize the corpus with the Word Cloud widget. We can connect the Word Cloud widget directly to the .................... widget, but instead, we decided to apply some preprocessing with the Text Preprocess widget. We are working with the book-excerpts.tab dataset. We can change all text to lowercase, tokenize (split) the text into words, filter out English stopwords, and select the 100 most frequently occurring tokens.\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Corpus"
  },
  "130e8da5a7b72e1b463c0d1dc98e390259c414faa27cd6525dc11c23e1dceceb": {
    "soal": "To cluster based on similarity, we can measure the distance between images. The numerical representation of the images obtained from the ...................... widget, we measure the distance using the Distances widget. Typically, cosine distance works best for images, but you can experiment yourself. We then send the distance matrix to Hierarchical Clustering to visualize similar pairs in the dendrogram.\n\nThe result is that all the animals are correctly grouped. In the dendrogram, we do not see the images of the animals. We can use the Image Viewer widget to view the images.\n\n\n\n\n",
    "jawaban": "The correct answer is: Image Embedding"
  },
  "f5b4a087d9945ebbf852235197920b614046f0eb1b85d28a2568223f8c4bd72f": {
    "soal": "Apache Hadoop is a proprietary software framework written in Java for distributed storage and processing of very large data sets on clusters of computers built from commodity hardware. All modules in Hadoop are designed with the fundamental assumption that hardware failures (individual machines or racks of machines) are common and should thus be handled automatically in software by the framework.",
    "jawaban": "The correct answer is 'False'."
  },
  "f1b1706a178778af1448fb8f487424a4fc3a17b180a13b51e72d69114e2f93cf": {
    "soal": "Widget Select Columns is used to manually organize data domains. Users can decide which attributes to use and how. Orange differentiates between regular attributes, class attributes (optional), and meta attributes. For example, to build a classification model, the domain will consist of a set of attributes and discrete class attributes. Meta attributes are not used in modeling, but some widgets can use them to label instances.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "cb805bf8d8cd581b3268ac022b665f71c418cd5e295e61d52dc2920756e70bca": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(2, size=100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi training (80%) dan testing (20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model, misalnya RandomForestClassifier\nmodel = RandomForestClassifier(random_state=42)\n# Melatih model pada data training\nmodel.fit(X_train, y_train)\n# Memprediksi data testing\ny_pred = model.predict(X_test)\n# Mengevaluasi akurasi model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model pada data testing: ')\nWhat is the purpose of the `np.random.seed(42)` statement in the code?\n{\n~To shuffle the dataset randomly.\n~To split the dataset into training and testing sets.\n=To ensure reproducibility of random number generation.\n~To initialize the RandomForestClassifier.\n~To normalize the dataset.\n~To set the number of iterations for training.\n}",
    "jawaban": "The correct answer is: accuracy:.2f"
  },
  "6f12ea6970dd764a9dce302cd8a5142af4d1e25ebb31673479def02800fc2cf5": {
    "soal": "\n\n\n\n\n\nUnsupervised Learning techniques are divided into:\n\nClassification.\nAssociation Analysis.\nDimensionality Reduction.",
    "jawaban": "The correct answer is 'False'."
  },
  "ab6ca4ed0e6799c896ac796715077e976bd0f89a50e41a80b7f06cd93868e44b": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.rand(100) # 100 nilai target\n# Inisialisasi model regresi linier\nmodel = LinearRegression()\n# Tentukan jumlah fold (misalnya, 5-fold cross-validation)\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n# Evaluasi model menggunakan cross-validation\nscores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n# Menampilkan skor R^2 untuk setiap fold dan rata-ratanya\nprint(\"Skor R^2 untuk setiap fold:\", scores)\nprint(\"Rata-rata skor R^2:\", np.mean(scores))\nHow many features does each sample in the dummy dataset have?",
    "jawaban": "The correct answer is: 10"
  },
  "0676a13b25f4d3f1ea4e4e66be64d2df7ff916071a5269737d3b5b68212f9c1e": {
    "soal": "In the Distributions widget, for ................. attributes, the graph displayed by the widget shows how many times (e.g., how many instances) each attribute value appears in the data. If the data contains a class variable, the class distribution for each attribute value will also be displayed (as shown in the snapshot below). To create this graph, we used the Zoo dataset.\n\n\n\n\n",
    "jawaban": "The correct answer is: discrete"
  },
  "75b20c6621bd628c157e0574960d4684081adbb3122391866cb0ae4cd87c870e": {
    "soal": "Data attributes in Orange have types discrete, continuous or character string. The attribute type is marked by a symbol that appears before the attribute name (D, C, ...............................).\n\n\n\nAnswer: \nQuestion 29",
    "jawaban": "The correct answer is: S"
  },
  "219f0adc1475c7142e188e2d88d65fd4d4a4239f4b8cd24dbaca394b00d7faf2": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#-xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\ndfg, start_activities, end_activities = pm4py.discover_dfg(log)\npm4py.view_dfg(dfg, start_activities, end_activities)\nWhat action will occur if the file specified in xes_filename does not exist?",
    "jawaban": "The correct answer is: An error will be raised when running pm4py.read_xes -A new empty file will be created -The file will be downloaded -No action occurs -A default file will be loaded -The file will be ignored"
  },
  "2f4d1404c0ec6a6ebec395f4a505cfeeca51545176538d0d5fe7dd0358e63693": {
    "soal": "Which of the following is a web scraping tool?",
    "jawaban": "The correct answer is: pyspider"
  },
  "e9cfda201cb87faaf75d675343880a974464d62514b5201e3cf762f87cd6027d": {
    "soal": "What is the primary flow of information in an LLM?",
    "jawaban": "The correct answer is: Prompt \u2192 Context \u2192 Output \u2192 Context"
  },
  "15083c417a36c4966b2e1d22cac90ac69bc1d067e85aa3f959b003512497e78a": {
    "soal": "Data does not always come in a nice table format. Data can also be in the form of text, audio recordings, video materials, or even images. However, computers can only work with numbers, so for any data mining, we need to transform the unstructured data into a vector representation.\n\n\nTo extract numbers from unstructured data, Orange can use a deep network embedder. Orange has just started integrating various embedders in the Image Embedding widget, and for now, they are available for text, images, and video.\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "1f2b8ab3d209aa588dc0bcee2d9b0c6ddd234e8e07f8dfc3e5349e7b7c0bdb8b": {
    "soal": "In the Import Documents Widget, to retrieve data, select (click) the folder icon on the right side of the widget. Select (choose) the folder that we want to make corpus. After loading is complete, we will see how many documents were successfully retrieved by the Import Documents Widget. To observe the obtained corpus, connect the Import Documents widget to the Corpus Viewer Widget. In this workflow, we are using a set of speeches by President ................................ in plain text format.\n\n\n\n",
    "jawaban": "The correct answer is: Kennedy"
  },
  "c11ddf91e44a520b038c9e470f599492d911ab8705feaec97e26f2666d24559f": {
    "soal": "The following workflow demonstrates the use of the Confusion ................ widget\n\n\nTest & Score obtains data from File and two learning algorithms from Naive Bayes and Tree. Test & Score performs cross-validation or other train-and-test procedures to obtain class predictions by both algorithms for all (or some) data instances. The test results are sent to the Confusion Matrix, where we can observe how many instances were misclassified and why.\n\nIn the output, we use the Data Table to show the examples we selected in the confusion matrix. If we, for example, click on Misclassified, the table will contain all the instances that were misclassified by the selected method.\n\nScatter Plot receives two sets of data. From the File widget, it gets the complete data, while the confusion matrix only sends selected data, for instance, misclassified data. The Scatter Plot will display all data, with bold symbols representing the selected data.\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: matrix"
  },
  "07ef96d8de4d4265b43cdb58b73a33ffd4c898aab93f5fc9d09b57fa44831641": {
    "soal": "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target biner\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(0, 2, 100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi data latih dan data uji (80% latih, 20% uji)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi dan pelatihan model klasifikasi (misalnya RandomForest)\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n# Prediksi pada data uji\ny_pred = model.predict(X_test)\n# Menghitung Precision, Recall, dan F1-Score\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nprint(f'Precision: ')\nprint(f'Recall: {recall:.2f}')\nprint(f'F1-Score: {f1:.2f}')\n# Alternatif: Menggunakan classification_report untuk ringkasan metrik\nprint('\nClassification Report:')\nprint(classification_report(y_test, y_pred))\nWhich function is used to generate random numbers in NumPy?\n{\n~np.random.randint\n~np.random.random\n=np.random.rand\n~np.random.randn\n~np.random.uniform\n~np.random.sample\n}",
    "jawaban": "The correct answer is: precision:.2f"
  },
  "feac72cc212c5352f23c01a401901573aeb5970eef799f86f821c4f742df5e52": {
    "soal": "The Venn Diagram widget can also be used to explore different prediction models. In the following example, we analyze 3 prediction methods, namely Naive Bayes, .................., and Random Forest, based on their classification errors on data instances.\n\nBy selecting misclassification in the three Confusion Matrix widgets and sending them to the Venn Diagram widget, we can see all misclassification examples visualized by the method used. We then open the Venn Diagram widget and select, for example, misclassified instances identified by all three methods. This is represented as the intersection of the three circles. Click on the intersection to view these two examples highlighted in the Scatter Plot widget. Try selecting different parts of the diagram to see how the scatter plot visualization changes.\n\n\n\n",
    "jawaban": "The correct answer is: SVM"
  },
  "27fb397dae762dd823066ceb54abe6a7a635321b7fdbad4969c6bfc4de3f1b2e": {
    "soal": "The ........................... widget is used for normalization and inversion of the distance matrix. Normalizing the data is required to bring all variables into proportion with one another.\n",
    "jawaban": "The correct answer is: Distances Transformation"
  },
  "0317fe59726a689d261d7bc8bc36fc883d72da66648fdf944f56a9de360157eb": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.rand(100) # 100 nilai target\n# Inisialisasi model regresi linier\nmodel = LinearRegression()\n# Tentukan jumlah fold (misalnya, 5-fold cross-validation)\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n# Evaluasi model menggunakan cross-validation\nscores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n# Menampilkan skor R^2 untuk setiap fold dan rata-ratanya\nprint(\"Skor R^2 untuk setiap fold:\", scores)\nprint(\"Rata-rata skor R^2:\", np.mean(scores))\nWhat type of values are contained in the target array `y`?",
    "jawaban": "The correct answer is: Continuous values between 0 and 1."
  },
  "d354d64952995899b51844c6e587a9e1e41cad764d0bd0fb81bfbc055fbf0677": {
    "soal": "How many trainable parameters does the Flatten layer have?",
    "jawaban": "The correct answer is: 0"
  },
  "56899ca9058bf0c04f77556308b65a968fa4bf03b10ea7b9dfb9175184331889": {
    "soal": "Widget kNN mem-prediksi berdasarkan instance training terbanyak.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "94d57f786467446d5ce018e9d8b69db53120d7599f1a34e277febb5bccca3744": {
    "soal": "Data doesn't always come in a nice table format. Data can also be in the form of text, audio recordings, video material, or even images. However, computers can only work with ................., so for every data mining, we need to convert this unstructured data into a vector representation.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "e254454d499d458f1248c7f4b0d235f9d4e34042b24909e835b6a24a983d4b69": {
    "soal": "perform the following operation\n\n192 (decimal) AND 255 (decimal)",
    "jawaban": "The correct answer is: 192"
  },
  "d0e58347c80d9e0dd63772c5800a9ba9882bf9931d2aaa560cab1961e3090853": {
    "soal": "import tensorflow as tf\nfrom tensorflow.keras import layers, models\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nmodel = models.Sequential([\nlayers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(64, (3,3), activation='relu'),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(128, (3,3), activation='relu'),\nlayers.Flatten(),\nlayers.Dense(128, activation='relu'),\nlayers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5, batch_size=64)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nWhat does MaxPooling2D do in CNN?",
    "jawaban": "The correct answer is: Reduces the spatial dimensions"
  },
  "1df764bcacf8ac508de9e3e3bb13f7168709b83a226017cd52e25269dfa67f83": {
    "soal": "For a more complex use of the Select Columns widget, we create a workflow to redefine the classification problem in the heart-disease dataset. Initially, the task was to predict whether the patient had coronary artery diameter narrowing. We changed the problem to gender classification, based on age, chest pain, and cholesterol levels, and informally kept diameter narrowing as a class attribute.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "145d8882e2a10818fe4247f7aede6c033b8f98c0503e1de6ebcaf0497e4137e4": {
    "soal": "Word Cloud data can be built from the text files (ASCII) we have, as in the workflow below. First, data from the Text Files Widget must be segmented into words using the Segment Widget. Then, the output segmented data needs to be converted from segmented data into a corpus so that it can be processed by the text mining toolbox using ................................ Before displaying it as a word cloud, it is recommended to do preprocessing first to reduce unnecessary words, such as conjunctions, etc., using the Preprocess Text Widget.\n\n\n\n",
    "jawaban": "The correct answer is: Widget Interchange"
  },
  "fd7fd953e16f8c4dad2911085261118b5a739bf4a98da52e3947a0aac1471c2d": {
    "soal": "Widget ........................ calculates statistical summaries at the node-level and graph-level for the network. Widget Network Analysis will output the network with the computed statistics and an extended item data table (only node-level index).\n",
    "jawaban": "The correct answer is: Network Analysis"
  },
  "75968b04b664488a97337b44fa388f0b9c30e677267a0de20a52e31da8696245": {
    "soal": "R-programming is a programming language used in big data processing. The nature of this programming language is open-source, meaning it can be used for free and modified by anyone. Its open-source nature allows many active users to contribute to the development of R-programming.\nSome advantages of R-programming include:\n- R programming can integrate with other programming languages, such as SQL.\n- It is used for data cleansing and manipulation, spatial analysis, data analysis and model building, data visualization, and even text analysis with natural \nAnswer Question 54\n processing.\n- It has many functions and packages that make it easier for data practitioners.",
    "jawaban": "The correct answer is: language"
  },
  "4f6aa23ed47f40c14cf0ad0a747ec4054d85daa767e2232579dd62dfdb8b89b6": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nIn the example usage, how are the training and testing images normalized?",
    "jawaban": "The correct answer is: Divided by 255.0"
  },
  "116020144dc48a20e00327278ac964fc931c045f256dcbd55974472562a2f6ef": {
    "soal": "In the next example, we will try to predict the category of documents. We use the book-excerpts.tab dataset, which we send through the Preprocess Text widget with default parameters. Then we connect the Preprocess Text widget to the Bag of Words widget to obtain term frequency. With the term frequency information, we will calculate the model.\n\nConnect the Bag of Words widget to the Test & Score widget for predictive modeling. Connect the SVM widget or another classifier widget to the Test & Score widget (all on the left). The Test & Score widget will calculate performance scores for each learner input. Here, we obtain excellent results for the SVM widget.\n\nNext, we need to check where the model made errors. Add the Confusion Matrix widget to the Test & Score widget. The Confusion Matrix widget will display the documents that were classified correctly and misclassified. Select \"misclassified\" to output the misclassified documents, which we will further examine using .........................\n\n\n",
    "jawaban": "The correct answer is: Corpus Viewer"
  },
  "92b356b4aadc4b67162fc3b184a41c3325d9f50297cc6a1f8ba39c1f284b8958": {
    "soal": "This workflow example shows how Network Analysis can enrich the workflow. We have used the lastfm.net data as input network from Network File and sent it to Network Analysis. We can decide to calculate the degree, degree centrality, and closeness centrality at node-level.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "a6bed579a9a084c4490f4944498bf49519316920414bb913f8c10e291f1f2f31": {
    "soal": "\n\n\nThe most popular programming language widely used to support data science is Java.",
    "jawaban": "The correct answer is 'False'."
  },
  "b01d69f774d47b6f739bd01a7bd1b064f47736bd4185e9a3c7fa556b95a7b822": {
    "soal": "Simple example with the Calibrated Learner. We use the Titanic dataset because this widget requires a binary class value (in this case, they are 'survived' or 'not survived').\n\nWe use Logistic Regression as the base learner, which will be calibrated with default settings, i.e., with sigmoid optimization of the distribution of values and optimized with CA.\n\nComparing the results of the uncalibrated Logistic Regression model, we will clearly see that the Logistic Regression model is ..................\n\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: worse"
  },
  "b0358280e1b34abe966c4ea3e3bab6dc1583c33e4bcec7b209a78ccb50ea0703": {
    "soal": "Contoh sederhana dengan Calibrated Learner. Kita menggunakan dataset titanic karena widget ini membutuhkan nilai binary class (dalam hal ini mereka adalah 'survived\u2019 atau \u2018not survived\u2019).\n\nKita menggunakan Logistic Regression sebagai base learner yang akan dikalibrasi dengan nilai setting default, yaitu dengan sigmoid optimization dari distribusi nilai dan di optimasi dengan CA.\n\nMembandingkan hasil dari uncalibrated Logistic Regression model kita akan melihat dengan jelas bahwa calibrated model lebih buruk.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "b00da1e978a43bac653dcc6f72a6e4e8fa3f29d73db24a1822774a744ce8712f": {
    "soal": "The Sentiment Analysis widget enables basic sentiment analysis of corpora. It works for English and uses two techniques supported by NLTK - Liu Hu and Vader. Both techniques are lexicon-based. Liu Hu calculates a single normal score for sentiment in text (negative score for negative sentiment, positive for positive, 0 for neutral), while .................... produces scores for each category (positive, negative, neutral) and adds a total sentiment score called compound.\n\n\n\n\nLiu Hu score\nVader score\n\n\nAnswer: \nQuestion 91",
    "jawaban": "The correct answer is: Vader"
  },
  "689b66d91b5c778137a644eeb77248270af6ec4c19ca31974b0e5da800e941f9": {
    "soal": "What is the purpose of the Open-WebUI service in the Docker Compose setup?",
    "jawaban": "The correct answer is: To provide a web interface connected to Ollama and PostgreSQL"
  },
  "f24e567de2b1ddcc75a30bbee6cc3566ebcd39ef533ef5deb3821cf8b38098ff": {
    "soal": "We can measure the distance between embedded images and see which ones are the most similar. We can use the Distances widget to measure the distance. Typically, cosine distance works best for images. We then send the distance matrix to Hierarchical Clustering to visualize similar pairs in .................. .\n\n\n",
    "jawaban": "The correct answer is: dendrogram"
  },
  "f67be1e2151d63f61e568556cf27f41d85b06a5d257b91112d162c543d13a728": {
    "soal": "Widget CN2 Rule Induction akan meng-induce (menginduksi) rule dari data menggunakan algoritma CN2.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "93c1360fa9844af39e4aa69a03c828fabb81d5487c755cd35acd1a35496dd00e": {
    "soal": "For machine learning, we need numbers. To obtain numerical representations of images, we send images to the Image Embedding widget.\n\nThe output data from the Image Embedding widget can be inspected using the Data Table widget. Now we have a numeric/numerical presentation of the images. For each image, there are ..............(1)............. numerical representations (columns n..........(2)........... to n...........(3).............). With these feature numeric representations, we can apply all standard machine learning techniques, such as, clustering.\n\n\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 2048, (2) \u2192 0, (3) \u2192 2047"
  },
  "628127e833c1377acbabb92e3e646130869f49f4c8c2891c76f768d4df87c44b": {
    "soal": "import pm4py\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\n# Import XES log\nlog = xes_importer.apply(xes_filename)\n# Cek jumlah kasus dan aktivitas\nprint(f\"Jumlah kasus: \")\nactivities = set()\nfor trace in log:\nfor event in trace:\nactivities.add(event['concept:name'])\nprint(f\"Jumlah aktivitas unik: {len(activities)}\")\nfrom pm4py.algo.discovery.dfg import algorithm as dfg_discovery\nfrom pm4py.visualization.dfg import visualizer as dfg_visualization\n# DFG berdasarkan frekuensi\ndfg = dfg_discovery.apply(log,\nvariant=dfg_discovery.Variants.FREQUENCY)\n# Tampilkan visualisasi\ndfg_vis = dfg_visualization.apply(dfg, log=log,\nvariant=dfg_visualization.Variants.FREQUENCY)\ndfg_visualization.view(dfg_vis)\nWhy is the Performance Spectrum important?\n{\n=It visually identifies bottlenecks\n-It performs machine learning\n-It exports data to CSV\n-It creates an XES file\n-It deletes unnecessary data\n-It corrects data errors\n}",
    "jawaban": "The correct answer is: len(log)"
  },
  "7730c85540b0b890f4abad7784dafbda1731a3fd3de9564594ee443961298c68": {
    "soal": "from google.colab import drive\ndrive.mount('/content/drive')\nIs it possible to upload files to Colab without mounting Google Drive?",
    "jawaban": "The correct answer is: Yes, using the files.upload() method from google.colab."
  },
  "ffcf3e6324cbfe0c5d49c85d5e72b623b9ed872f6567440bef5c7590db0e1012": {
    "soal": "import os\nimport string\nfrom collections import Counter\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\n# Buat daftar stopwords Bahasa Indonesia\nstop_words = set(stopwords.words('indonesian'))\ndef bersihkan_teks(teks):\n# Lowercase + hilangkan tanda baca\nteks = teks.lower()\nteks = teks.translate(str.maketrans('', '', string.punctuation))\nreturn teks\ndef analisis_folder(folder='outputs', top_n=20):\nsemua_konten = ''\nfor filename in os.listdir(folder):\nif filename.endswith('.txt'):\nfilepath = os.path.join(folder, filename)\nwith open(filepath, 'r', encoding='utf-8') as f:\nisi = f.read()\n# Ambil hanya bagian isi, setelah header\nif '\n\n' in isi:\nisi = isi.split('\n\n', 1)[1]\nsemua_konten += isi + ' '\n# Bersihkan teks dan tokenisasi\nteks_bersih = bersihkan_teks(semua_konten)\nkata_kata = teks_bersih.split()\n# Hapus stopwords\nkata_kata = [kata for kata in kata_kata if kata not in stop_words and kata.isalpha()]\n# Hitung frekuensi\ncounter = Counter(kata_kata)\ntop_kata = counter.most_common(top_n)\nprint(f\"\n\ud83d\udd0d Top Kata Paling Sering Muncul:\")\nfor kata, jumlah in top_kata:\nprint(f\"{kata}: {jumlah}\")\n# Buat Word Cloud\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(counter)\n# Tampilkan\nplt.figure(figsize=(12, 6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud dari Semua Konten')\nplt.tight_layout()\nplt.savefig('wordcloud.png')\nplt.show()\nprint(\"\n\u2705 Word Cloud disimpan sebagai 'wordcloud.png'\")\n# Jalankan analisis\nif __name__ == '__main__':\nanalisis_folder('outputs', top_n=30)\nWhat information does the 'top_kata' variable hold in the script?\n{\n=A list of the top N most frequently occurring words and their counts.\n~A list of all unique words found in the text files.\n~The total number of words analyzed across all text files.\n~A list of stopwords removed during the analysis.\n~The content of the most recently processed text file.\n~The filenames of the text files analyzed.\n}",
    "jawaban": "The correct answer is: top_n"
  },
  "42e67970e703f622ab99c11e1fb257e1b877222507323b05a86f44dfedb48725": {
    "soal": "The Line Plot widget is a standard visualization widget, which displays the .............. profile, usually in the form of ordered numeric data. In this simple example, we will display the iris data in a line plot, grouped by iris attributes. The plot shows how petal length separates the class values well.\n\nIf we observe this in the Scatter Plot Widget, we can confirm this is indeed true. Petal length is an interesting attribute for class separation, especially when enhanced with petal width, which is also well separated in the line plot.\n\n\n\n\n",
    "jawaban": "The correct answer is: data"
  },
  "b3a7c697badd27f9e548c4f09cdc933ac06682c3d35ba9fc9c190f0b20b4f8db": {
    "soal": "The Test & Score widget will test the learning algorithm on the data.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "106e90ab276032dcee09e8b7b182b5497ad5414291b6dff9d6603495f96284a3": {
    "soal": "This workflow example shows how Network Analysis can enrich the workflow. We have used the lastfm.net data as the input network from the Widget Network File and sent it to the Widget Network Analysis. We can decide to compute the degree, degree centrality, and closeness centrality at the node-level.\n\nWe can then visualize the network in the Widget Network Explorer. In the Network Explorer widget, it colors with the best tags, like the default for this dataset. But now we can also adjust the node size to fit the ..................... degree centrality results. This is a good way to visualize network properties.\n\n\n",
    "jawaban": "The correct answer is: centrality"
  },
  "03cb9c7a20fe30f23b4128317c3da0ef7c3f44ffde1e89e8c2c68c274c91c16f": {
    "soal": "If\n\n1 AND 1 = 1\u00a0\u00a0\u00a0\u00a0\u00a0 and\u00a0\u00a0 1 OR 1 = 1\n\n1 AND 0 = 0\u00a0\u00a0\u00a0\u00a0\u00a0 and\u00a0\u00a0 1 OR 0 = 1\n\n0 AND 1 = 0\u00a0\u00a0\u00a0\u00a0\u00a0 and\u00a0\u00a0 0 OR 1 = 1\n\n0 AND 0 = 0\u00a0\u00a0\u00a0\u00a0\u00a0 and\u00a0\u00a0 0 OR 0 = 0\n\n\u00a0\n\nthen in binary\n\n( 100 AND 110 )\u00a0 OR 111 = ..... (binary)\n\n\u00a0\n\n\u00a0",
    "jawaban": "The correct answer is: 111"
  },
  "d05e74276243e358c7d2594e6bdce553595809eeac3755f4aa1ea82daa8a0860": {
    "soal": "Which function is used to create a tensor with all elements set to zero?",
    "jawaban": "The correct answer is: zeros()"
  },
  "14523bf33e5e4a9e2b4c9555ecd7cb00e8a4c64a4d3410269090e7bd4a5e2b24": {
    "soal": "\n\n\n\n\n\nUnsupervised Learning uses unlabeled data.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "b7ff921414bf463df66b4484ce5263918c785ffbfe8c7ce922d5e7200f3d1526": {
    "soal": "What is the purpose of cross-validation in machine learning?",
    "jawaban": "The correct answer is: To assess how well a model generalizes to unseen data"
  },
  "e252e15c05563a0b8b990256951ef70e26f8cf4962e030ea62803e53d477f16c": {
    "soal": "Big data is a term referring to large and complex datasets obtained from various sources, including business transactions, social data, census data, medical data, scientific data, and others. The main characteristics of big data are \nAnswer Question 2\n, velocity, and variety.\n- Volume refers to the very large amount of data generated constantly.\n- Velocity (Velocity) refers to the speed at which data is generated and updated, as well as the ability to process data in real-time.\n- Variety (Variety) refers to the various sources of data and different types of data that can be collected.\nBig data typically requires specialized technology and techniques to process, analyze, and interpret it, including machine learning, data mining, and statistical analysis techniques. The goal of big data analysis is to gain valuable insights and information, such as trends and patterns useful for business, scientific, or social decision-making.",
    "jawaban": "The correct answer is: volume"
  },
  "6c64f3b0aab6218f6f470dd8d3cee37dd921ab860a6f1c49fb3858a6ae8f732e": {
    "soal": "The Calibrated Learner widget wraps / continues the work of another learner with ................... calibration and decision threshold optimization.\n",
    "jawaban": "The correct answer is: probability"
  },
  "79c81e4eb67fcd620cbd1e2e01fca6572b16bcd1b4cf210da9f106e24846aead": {
    "soal": "In the following workflow example, a very simple use of the Corpus widget will be shown. Place the Corpus widget onto the ....................... and connect it to the Corpus Viewer widget. We are using the book-excerpts.tab dataset, which is available in the add-on, and examining it in the Corpus Viewer widget.\n\n\n\n",
    "jawaban": "The correct answer is: kanvas"
  },
  "32c45932e5a894cba7e2c00da13d48e3e48d17cb2443af5c33f081d6395fbf6a": {
    "soal": "PCA can be used to simplify the visualization of small datasets. Below, we use the Iris dataset to show how we can improve the visualization of the dataset with PCA. The transformed data in the Scatter Plot shows much clearer differences between classes than the default settings.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "3e1d53e50665cb7419d81f35755c21478b6f7573227aa2fae393794ca1cfcdf9": {
    "soal": "In the workflow below, the Import Documents widget can load subfolders. We store President Soeharto's speeches in two (2) subfolders, pre-1962 and post-1962. If we load the parent folder, both subfolders will be used as class labels. Check the output of the Import Documents widget in the Data Table widget.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "9d8b3fa5578af3279a726aa9f840207fe6833d7544846aee68f14a991d874004": {
    "soal": "Only two inputs are suitable for the Distance Matrix widget, which are the .................... widget and the Distance Transformation widget. The output from the Distance Matrix widget is a data table containing the distance matrix. Users can decide how to label the table and distance matrix (or examples in the distance matrix), which can then be visualized or displayed in a separate data table.\n",
    "jawaban": "The correct answer is: Distances"
  },
  "928e181383bc4a1ee7a4befd104f4321af3e89d6a36e31e84e63816d29c1e49c": {
    "soal": "In the Scatter Plot widget, the default tool is Select, which selects data instances within the selected rectangular area. Pan allows us to move the scatter plot around. With Zoom, we can zoom in and out by moving the mouse, while Reset zoom resets the visualization to the optimal size.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "823675716429165ba2afcacde1f6466d86fcf343a8c676fa84a4e3670ba57833": {
    "soal": "Data Mining has the power to transform businesses; however, implementing processes that meet the needs of all stakeholders often hinders successful data mining investments \u2014 78% of respondents say they struggle to find the right data mining strategies or solutions.\nDespite these obstacles, businesses that are able to effectively mine widespread data have several key similarities. Successful companies:\n- Know the core needs of their business, both tactical and strategic, that can be met by data mining;\n- Identify and evaluate data sources to be used by data mining tools for accuracy and relevance;\n- Define applications, including Business Intelligence (BI) systems, where data mining tools cannot be interoperable;\n- Identify available data mining solutions that meet the entire scope of the organization's requirements, from budget to end-user technical capabilities;\n- Use one standard data mining tool that meets the needs of IT, data scientists, and analysts, while also meeting the consumption and visualization needs of business users.",
    "jawaban": "The correct answer is 'False'."
  },
  "9c94eb92321d5fc24b5f9a703a39f9179248d8a493ef31b455e999b03b8d66c8": {
    "soal": "The Periodogram widget can visualize cycle, seasonality, periodicity, and important periods in ...........................\n",
    "jawaban": "The correct answer is: time series"
  },
  "f756dae9cb3beb748fd461f340ce50c36d7d082207ad9fa2c35e5ae293de66de": {
    "soal": "In the snapshot below, we can see how transformation affects ......................... We load the Iris dataset and calculate the distance between rows with the help of the Distances widget. To show how Distance Transformation affects the Distance Matrix, we create the workflow below and compare the transformed distance matrix with the \"original\"\n\n\n",
    "jawaban": "The correct answer is: distance matrix"
  },
  "b7281de5ad8c69532ac8a4d19a2b1ca7210dfeebb920f9e1e71f96a0725f5103": {
    "soal": "What is the primary difference between eager execution and graph mode in TensorFlow?",
    "jawaban": "The correct answer is: Eager execution runs code immediately, while graph mode builds a computational graph first"
  },
  "afc9d44a3af67f2f866a3466e621d411f7bec76d4d2b9de4715ba84c44e95a25": {
    "soal": "The basic framework of Apache Hadoop consists of the following modules:\n- Hadoop Common - contains libraries and utilities required by other Hadoop modules;\n- Hadoop Distributed File System (HDFS) - a distributed file system that stores data on commodity machines, providing very high aggregate bandwidth in the cluster;\n- Hadoop YARN - a resource management platform responsible for managing computational resources in the cluster and using them for scheduling user applications; and\n- Hadoop MapReduce - a programming model for small-scale data processing.\nThe term \"Hadoop\" has come to refer not only to the basic modules above but also to the \"ecosystem,\" or collection of additional software packages that can be installed above or alongside Hadoop, such as Apache Pig, Apache Hive, Apache HBase, Apache Phoenix, Apache Spark, Apache Zookeeper, Impala, Apache Flume, Apache Sqoop, Apache Oozie, Apache Storm, and others.",
    "jawaban": "The correct answer is 'False'."
  },
  "6a0f7f6c49087bb2f4aa363bf2fb63e6cfed4ad7ae5257a8cc00016d550474af": {
    "soal": "In the Interpolate widget, which is part of the Time Series add-on, the interpolation type - ..............(1)............. interpolation fits with a ................(2)................. polynomial to the values around the missing value. Therefore, this technique will be very .............(3)............. but will give the best results.\n",
    "jawaban": "The correct answer is: (2) \u2192 cubic, (3) \u2192 slow, (1) \u2192 Spline"
  },
  "9575cba5ef9ae9bf55895f80526323870eb197468c00642bbbca64b075c1569b": {
    "soal": "The Distance Matrix widget creates a distance matrix, which is a two-dimensional array containing .............., taken pairwise, between elements of the dataset. The number of elements in the dataset determines the size of the matrix. Data matrix is essential for the Hierarchical Clustering Widget.\n\n\n",
    "jawaban": "The correct answer is: distance"
  },
  "b73340d929e56a3ff173f71cfd189e649b9855d7f6f0e095af27f0ca65e63a5b": {
    "soal": "In the workflow below, we collect data using the Twitter widget. We collect tweets from users @HillaryClinton and @realDonaldTrump over two weeks, totaling 242 tweets.\n\n\n\n\n\n\n\nIn the Preprocess Text widget, tweet tokenization is available, which stores hashtags, emojis, mentions, etc. However, this tokenizer does not remove punctuation, so we expand the Regexp filtering with symbols we want to remove. We will obtain tokens with only words, which we display in the Word Cloud. Then we can create a scheme to predict the tweet author based on tweet content, as described in more detail in the documentation for the Twitter widget.\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "5a6dbea6b324c03d129085ecd2e32c5900f831224a6a1f21453835269f46855b": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#-xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\ndfg, start_activities, end_activities = pm4py.discover_dfg(log)\npm4py.view_dfg(dfg, start_activities, end_activities)\nThe XES file format primarily relates to which field?",
    "jawaban": "The correct answer is: Process mining -Financial accounting -Bioinformatics -Machine translation -Image recognition -Network administration"
  },
  "d9d164aece7dd621a535973380ea9fb0ebcfac03e2aef6399a9d0a2c235dfa79": {
    "soal": "from google.colab import drive\ndrive.mount('/content/drive')\nDoes Google Drive remain mounted between different Colab sessions?",
    "jawaban": "The correct answer is: No, you need to mount it in each new session."
  },
  "f5d1714765f12aa6e1a51ddfc361d6072f72131db7b4fcf48ccaa17b0af5daa5": {
    "soal": "To perform machine learning, we need numbers. To obtain numerical representations of the images, we send the images to the Image Embedding widget.\n\nThe output data from the widget ......................... can be examined using the Data Table widget. Now we have numerical/pixel data for the images. Each image has 2048 numerical representations (columns n0 to n2047). With this numeric feature representation, we can apply all standard machine learning techniques, such as clustering.\n\n\n\n",
    "jawaban": "The correct answer is: Image Embedding"
  },
  "64e3c016aa115b3e5ecae3feec3d0bae817919c8b43f842e365e01cd65fdf193": {
    "soal": "In the Distributions widget, for continuous attributes, attribute values are displayed as a function graph. Class probabilities for continuous attributes are obtained using Gaussian kernel density estimation, while the curve display is adjusted with the Precision bar (smooth or precise). As an example, below we use the Iris dataset.\n\n\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "b2eca2dce8a509bc6ca5e752b875931f784978c851371f37bb52491d6b408e18": {
    "soal": "In the next example, we will try to predict the category of a document. We will use the dataset book-excerpts.tab, which we will send through the Preprocess Text widget with default parameters. Then we will connect the Preprocess Text widget to the Bag of Words widget to get term frequency. With the term frequency information, we will calculate the model.\n\nConnect the Bag of Words widget to the Test & Score widget for predictive modeling. Connect the SVM widget or another classifier widget to the Test & Score widget (all on the left side). The Test & Score widget will calculate the performance score for each learner in the input. Here we get excellent results with the SVM widget.\n\nNext, we need to check where the model made mistakes. Add the Network Map widget to the Test & Score widget. The Confusion Matrix widget will display documents that were correctly classified and misclassified. Select misclassified to output misclassified documents, which we will review further using the Corpus Viewer.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "ba8d918696de648f170970e5b092aa5400b066a053defce2301d1f7242995335": {
    "soal": "In the context of convolutional neural networks (CNNs), what does a pooling layer do?",
    "jawaban": "The correct answer is: It reduces the spatial dimensions of the feature maps."
  },
  "1511bcd2fa50e2be2c71ab97f4c60744b796e60289d5428f4f476635a7307cc3": {
    "soal": "In the workflow below, the Tree widget will train a model and evaluate it against Logistic Regression. The accuracy comparison between the Tree widget and Logistic Regression widget is done through the Test & Score widget. The ................. results can be viewed through the Confusion Matrix widget to compare the classification errors that occurred.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Test & Score"
  },
  "c2c83146fffa1f65fb7338a64f0caa0d0e51d12acf196b46d4811430ce0a3366": {
    "soal": "Why might accuracy not be a sufficient metric for evaluating models on imbalanced datasets?",
    "jawaban": "The correct answer is: It can be misleading, as high accuracy might be due to the model favoring the majority class."
  },
  "b44da068efeb0dce5a3b6df0b70649b2e7b8c60db31f7f4a1a1911e72dff02c4": {
    "soal": "The workflow clusters data items in the iris dataset by first checking the distances between data instances using the Distances widget. The sparse matrix is passed to the Hierarchical Clustering widget, which creates a dendrogram. We can select different parts of the dendrogram to analyze related data further.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "7a00c7ec3f20109273f68530edfe9e9cbcce7d3c44db95f4f0ebfd3da2393d84": {
    "soal": "Widget Network Analysis calculates ................. statistics at the node-level and graph-level for the network. Widget Network Analysis will output the network with the computed statistics and an extended item data table (only node-level index).\n",
    "jawaban": "The correct answer is: summary"
  },
  "e035e5c54462abd0b82b8209fe3d53af0c04e5defd839cf2d09e905060325369": {
    "soal": "import pandas as pd\n# Contoh: Membuat DataFrame dengan missing values\ndata =\ndf = pd.DataFrame(data)\n# Menampilkan jumlah missing values di setiap kolom\nprint(\"Jumlah missing values per kolom:\")\nprint(df.isnull().sum())\n# Menghapus baris yang memiliki missing values\ndf_cleaned = df.dropna()\nprint(df_cleaned)\n# Mengisi missing values pada kolom 'Usia' dengan mean\ndf['Usia'].fillna(df['Usia'].mean(), inplace=True)\n# Mengisi missing values pada kolom 'Usia' dengan median\ndf['Usia'].fillna(df['Usia'].median(), inplace=True)\n# Mengisi missing values pada kolom 'Kota' dengan modus\ndf['Kota'].fillna(df['Kota'].mode()[0], inplace=True)\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n# Inisialisasi imputer untuk mengisi missing values dengan mean\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n# Mengimputasi kolom 'Usia'\ndf[['Usia']] = imputer.fit_transform(df[['Usia']])\nHow can you calculate the mean of the 'Usia' column in pandas?\n{\n=df['Usia'].mean()\n~df['Usia'].median()\n~df['Usia'].mode()\n~df['Usia'].sum()\n~df['Usia'].min()\n~df['Usia'].max()\n}",
    "jawaban": "The correct answer is: 'Nama': ['Andi', 'Budi', 'Citra', 'Dewi'], 'Usia': [25, 30, None, 22], 'Kota': ['Jakarta', None, 'Bandung', 'Surabaya']"
  },
  "dbf6ab6f3517f2800628059d44b05399211c12b3b7aefcf0301ef9ba36561fc5": {
    "soal": "Cassandra, or fully known as Apache Cassandra, is one of the \nAnswer Question 66\n source products for distributed database management by Apache. Cassandra is designed to manage large (big data) structured data that is spread across many servers. This software is highly scalable, so it is no wonder that many large companies have trusted Cassandra as one of their work solutions, such as Facebook, Twitter, and Apple.",
    "jawaban": "The correct answer is: open"
  },
  "20589f7ab4694fcbf67bdbbdf9e1c2c192472e0de677b75946e7345a5b51ef0c": {
    "soal": "\n\n\nThe workflow above performs testing/evaluation of CN2 Rule Induction and Tree in the Test & Score Widget. It appears that CN2 Rule Induction is more ......................\n\n\n\n",
    "jawaban": "The correct answer is: better"
  },
  "e0c6d6ba4be43bf2f6db65715ca95b02d527cd551d23ee424c1109a89700e728": {
    "soal": "Big data is a term that refers to the very large and complex amount of data collected from various sources, including business transactions, social data, census data, medical data, scientific data, and more. The main characteristics of big data are volume, velocity, and variety.\n- \nAnswer Question 77\n refers to the very large amount of data created every moment.\n- Velocity refers to the speed at which this data is created and updated, as well as the ability to process data in real-time.\n- Variety refers to the different data sources and types of data that can be collected.\nBig data typically requires special technologies and techniques to process, analyze, and interpret the data, including machine learning, data mining, and statistical analysis techniques. The goal of big data analysis is to gain valuable insights and information, such as trends and patterns useful for business, scientific, or social decision-making.",
    "jawaban": "The correct answer is: Volume"
  },
  "1804bd44c7470a7d9f4237c7206f08f2a57769d12401ec5d0b15d53c2c688880": {
    "soal": "The ................. Widget uses the Tree algorithm with the ability to perform forward pruning.\n",
    "jawaban": "The correct answer is: Tree"
  },
  "e7db715e97bd4e2ed102f3548f318d621613258470010c0d5a13cd6c80f0ee64": {
    "soal": "Which TensorFlow function is used to compute the gradient of a tensor?",
    "jawaban": "The correct answer is: gradients()"
  },
  "b420e7530e8c4f8ac0ea969c1e12ffcf7216cb1d26bd37aedc2265bca05b3f79": {
    "soal": "model.save('my_model')\nfrom google.colab import files\nfiles.download('my_model')\nnew_model = tf.keras.models.load_model('my_model')\nAfter saving the model, what must be done before using files.download in Google Colab?",
    "jawaban": "The correct answer is: Import the files module from google.colab"
  },
  "c49a6d307e351c44f6f4ce807764146de2c52016ed059a4e97232611904bd795": {
    "soal": "Hadoop was introduced/found by ..................... in 2006\n",
    "jawaban": "The correct answer is: Yahoo"
  },
  "2b62c8c84b8a49ccdfa274646f68d265aa1f71e483e230133b6f78f5716a1708": {
    "soal": "Widget File can read attribute-value data from input files.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "3732d6e72df2904b545c66eb64ce8bb4bbd7d050ae111010348257f1e6cf63fc": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nWhat do the `y_train` and `y_test` arrays contain in the CIFAR-10 dataset?",
    "jawaban": "The correct answer is: Class labels (0-9)"
  },
  "8b7d21fc3d57ece381167f06311ac59b87a09b41b480717c9c89b22b8d661efa": {
    "soal": "In the snapshot workflow ORANGE below, we will observe the effects of preprocessing on text. For example, we are working with book-excerpts.tab, which we load with the Corpus widget. We have connected Preprocess Text to the Corpus and maintained the default preprocessing method (uppercase, word tokenization, and stopword removal). The only additional parameter we added as output is the first 100 most frequent tokens. Then we connect Preprocess Text with Word Cloud to observe the most frequent words in the text.\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "8bbc3784f935f3721316b8977ecc636d6164f1245784eb5de9bacfe774a01444": {
    "soal": "Bahasa pemrogramman utama yang digunakan untuk membuat Oragne3 data mining adalah bahasa\n",
    "jawaban": "The correct answer is: python"
  },
  "f65523fc09626b527c9bd44d3c876d9fb46666e6e28afcff0b581ba5c4e0e0bf": {
    "soal": "What is the purpose of the PostgreSQL service in this Docker Compose setup?",
    "jawaban": "The correct answer is: To serve as the vector database for Open-WebUI"
  },
  "725047ea963f94f7aa42169f0325dd4e94feca9304da6e9f9179a46dc9540641": {
    "soal": "Sebagai contoh penggunaan kNN untuk klasifikasi kita mengggunakan dataset iris. Kita bandingkan hasil dari k-Nearest neighbors dengan default model Constant, yang akan memprediksi class minoritas. Tampak kNN lebih baik.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "3b1cd3b385746d8f3b87f36c402acdfc795d58043975825452e711ecf5ed3d55": {
    "soal": "import tensorflow as tf\nfrom tensorflow.keras import layers, models\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nmodel = models.Sequential([\nlayers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(64, (3,3), activation='relu'),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(128, (3,3), activation='relu'),\nlayers.Flatten(),\nlayers.Dense(128, activation='relu'),\nlayers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5, batch_size=64)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nWhat is the shape of the input image used in the CNN model?",
    "jawaban": "The correct answer is: (32, 32, 3)"
  },
  "684703d6a0d86a4ce6d37b264a31f4235be9bec95673b1cd29af73e1c987fae2": {
    "soal": "How do you resume a model that has been paused?",
    "jawaban": "The correct answer is: ollama resume model"
  },
  "670b2f6e1fd6acd2f1ce7992d2cadbfe7b4dbd2e8a733f76d20329eaf7c38208": {
    "soal": "The Venn Diagram widget can also be used to explore various prediction models. In the following example, we analyze 3 prediction methods, namely ...............(1).............., SVM, and Random Forest, based on their classification errors on data instances.\n\nBy selecting ..............(2)................ in the three Confusion Matrix widgets and sending them to the Venn Diagram widget, we can see all misclassification examples visualized by the method used. Then we open the ................(3)............... widget and select, for example, the misclassified instances identified by all three methods. This is represented as the intersection of the three circles. Click on the intersection to see these two examples marked in the Scatter Plot widget. Try selecting different parts of the diagram to see how the scatter plot visualization changes.\n\n\n\n\n",
    "jawaban": "The correct answer is: (1) \u2192 Naive Bayes, (2) \u2192 misclassification, (3) \u2192 Venn Diagram"
  },
  "8f0eb57f6a6ec741136be49ce8a1419d01dbc8741f2eaded71a3ec373f7380e4": {
    "soal": "import tensorflow as tf\nprint(\"TensorFlow version:\", tf.__version__)\nTensorFlow version: 2.18.0\nWhich version of TensorFlow was shown in the sample output?",
    "jawaban": "The correct answer is: 2.18.0"
  },
  "843243985d676db0ea19955aedf96e0a9bb1595a016beaa2c0eab7cf0cfc18fd": {
    "soal": "Network Generator Widget creates an example network. It is intended for teaching/learning about the internet.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "025b694395d671a203e28d66d225e23beea7770b4ed552ff105b3da4f6d1c537": {
    "soal": "Petri Nets are particularly effective at representing:",
    "jawaban": "The correct answer is: Complex structures with parallelism and choices"
  },
  "1246f9d8e88c954edd34a18baf75a15dd129cbf9480c3300f86e6a610dc8b653": {
    "soal": "The Calibration Plot widget plots ........................ probability against the probability predicted by the classifier.\n",
    "jawaban": "The correct answer is: class"
  },
  "c2f59d4f4f6e19e7ca71e50ca9bc8402194a0b5a505a91099d71c0d6ba265631": {
    "soal": "In the Interpolate widget, which is part of the Time Series add-on, the interpolation type - ..................... interpolation replaces missing values with the average values of the series.\n",
    "jawaban": "The correct answer is: Mean"
  },
  "b113aa82f1aaf4d1d087293a29509ee4d1d2c74d45df65409431b4d76f13b324": {
    "soal": "In the image below, we use the Network Generator Widget to create a Grid graph with a height of 4 and a width of 5 and send it to Network Analysis. We can calculate node degrees and send the data to Network ..................... . Finally, we observe the resulting graph in the visualization and set the node size and color to Degree. Network Explorer is a good tool for observing and explaining network properties.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Explorer"
  },
  "fb8b86d4cd18931c23e0f5f84bd094a9bd788dd45674ae7223d8bffec0413c19": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\n# xes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\nxes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n# xes_filename = 'Production.xes'\n# xes_filename = 'training_log_1.xes'\n# xes_filename = 'training_log_3.xes'\n# xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nprocess_tree = pm4py.discover_bpmn_inductive(log)\nbpmn_model = pm4py.convert_to_bpmn(process_tree)\npm4py.view_bpmn(bpmn_model)\nWhich library is imported in the given Python script?",
    "jawaban": "The correct answer is: pm4py"
  },
  "a6021d0286dba90cc0ea2b23370291f33d3b1d014db55f460bdf65121100fa45": {
    "soal": "What does GPU acceleration help with in TensorFlow?",
    "jawaban": "The correct answer is: Speeding up matrix computations and model training"
  },
  "76c5c486d049aee636f39ba7c6bd831064e1c2a2243cfbfeaa6daa0763385db8": {
    "soal": "Apache Hadoop is an open-source software framework written in C++ for distributed storage and processing of very large data sets on clusters of computers built from commodity hardware. All modules in Hadoop are designed with the fundamental assumption that hardware failures (individual machines or racks of machines) are common and should thus be handled automatically in software by the framework.",
    "jawaban": "The correct answer is 'False'."
  },
  "0bb6acd2c136a371efd30bbec5fd9b4988b0f6d8246c52eec4350845eea80b0e": {
    "soal": "The Text Preprocessing widget can break text into smaller units (tokens), filter them, perform normalization (stemming, lemmatization), create x-grams, and tag tokens with part-of-speech labels. The steps in the analysis are applied sequentially and can be turned on or off.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "046b73710f9945e9867353967ebc6add31964f0b7375c4f60d0ff72607fcbac0": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nx_train = np.random.rand(1000, 10)\ny_train = np.random.randint(2, size=(1000, 1))\nmodel = keras.Sequential([\nkeras.layers.Dense(64, activation='relu', input_shape=(10,)),\nkeras.layers.Dense(32, activation='relu'),\nkeras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer='adam',\nloss='binary_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=10, batch_size=32)\nHow many epochs is the model trained for?",
    "jawaban": "The correct answer is: 10"
  },
  "98724f8c10d5b8e17993f0c934d57380f0e7b0462f7fb9e2dd15dff7cd4c1f91": {
    "soal": "The Tree algorithm is a simple algorithm that can split data into nodes based on class purity (category/class purity). The Tree algorithm is the predecessor of the ...............(1)........... algorithm. The Tree widget in Orange was designed in-house and can handle both discrete and ...............(2)............. datasets.\n",
    "jawaban": "The correct answer is: (2) \u2192 continuous, (1) \u2192 Random Forest"
  },
  "dbce6892e67ad9def0b3df7065c33cf3b5261a7716ab3b66f8d3f5654c18d93d": {
    "soal": "Apache Hadoop is an open-source software framework written in \nAnswer Question 61\n for distributed storage and processing of massive data sets on clusters of commodity hardware. All modules in Hadoop are designed under the assumption that hardware failures (individual machines or machine racks) are common and must be automatically handled by the software in the framework.",
    "jawaban": "The correct answer is: Java"
  },
  "a1a6493efaa16e28d351cebab8ec7806c0821f13c90038ad50ff2d4a77731e3b": {
    "soal": "The basic framework of Apache Hadoop consists of the following modules:\n- Hadoop Common - contains libraries and utilities required by other Hadoop modules;\n- Hadoop Distributed File System (HDFS) - a distributed file system that stores data on commodity machines, providing very high aggregate bandwidth in clusters;\n- Hadoop \nAnswer Question 64\n - a resource management platform responsible for managing computing resources in the cluster and using them for user application scheduling; and\n- Hadoop MapReduce - a programming model for large-scale data processing.",
    "jawaban": "The correct answer is: YARN"
  },
  "5a99cea555ab2c88f96f654ee3497b8598d2fcfcee6cb1b11e5fd33fc8eadc2f": {
    "soal": "Which command starts the services defined in the 'docker-compose.yaml' file?",
    "jawaban": "The correct answer is: sudo docker-compose up -d"
  },
  "c96ef68680855b0f49db140dfa682c20f49c2cd17c54d34b3f688bc9dcda7490": {
    "soal": "What can process mining techniques reveal about organizational processes?",
    "jawaban": "The correct answer is: Actual performance and compliance issues"
  },
  "75c1e573b4a5bdb4b500483491cca3406d04951c99924f8ec6b134ae7c75f813": {
    "soal": "Data attributes in Orange have the type discrete, continuous or ..................(1)............. The attribute type is marked with a symbol that appears in front of the attribute name (D, C, ...............(2)..............).\n\n\n",
    "jawaban": "The correct answer is: (2) \u2192 S, (1) \u2192 character string"
  },
  "c63a27f7709f062490a43342a91860dabc89c1926586e789f1b1d6e8704f243e": {
    "soal": "In the Distributions widget, for continuous attributes, the graph displayed by the widget shows how many times (e.g., how many instances) each attribute value appears in the data. If the data contains a class variable, the class distribution for each attribute value will also be displayed (as shown in the snapshot below). To create this graph, we used the Zoo dataset.\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "fd64a6522a4615eede052af07522a5a12cebe87fd18d3c0868e14852f9d2c06d": {
    "soal": "R-programming is a programming language used for processing big data. This programming language is open-source, meaning it can be used for free and modified by anyone. Its open-source nature encourages active user contributions to develop R-programming.\nSome advantages of R-programming:\n- R programming can integrate with other programming languages, such as SQL.\n- It is used for data cleansing and manipulation, spatial analysis, data analysis, model creation, data visualization, and text analysis with natural language processing.\n- It has many functions and packages that make it easier for data practitioners.",
    "jawaban": "The correct answer is 'True'."
  },
  "4e63136e7e7ede300162f670917782d2f5f588c9230c7f014bbc823c26e7b1dd": {
    "soal": "Widget Dataset retrieves the selected dataset from the server and sends it to the output. The file is downloaded to local memory and is thus immediately available even without an internet connection. Each dataset is accompanied by a description and information about data size, number of instances, number of variables, target, and tags.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "245d60597f690b4abf7253b6c6cfd688ca6e529c72b7ab2690cb1f6f5f0cc640": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nWhat are the dimensions of the images in the CIFAR-100 dataset?",
    "jawaban": "The correct answer is: 32x32 pixels"
  },
  "1ac20436c689b1d03e33001f9f821add6c9d8b38878a93a66d7f2efc7f835422": {
    "soal": "In the Scatter Plot Widget - Selection can be used to define a top group manually defined in the data. Use the Shift modifier when selecting data instances to place them into a new group. Shift + Ctrl (or Shift + Cmd in macOS) adds instances to the last group.\n\nSignal data outputs a data table with an additional column containing the group index.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "4ea3277a20b297c90616c1fa8eac92004ad1c7537e5cb1b5f267cb6ce0b5606c": {
    "soal": "The Corpus widget can load a collection of text documents, optionally tagged with ..................., or convert input data to a corpus.\n",
    "jawaban": "The correct answer is: kategori"
  },
  "e8c485aa17e41ddefc34c98fa159868c130a4b22094aae682ecab35c07964629": {
    "soal": "Why is continuous monitoring of a deployed model's performance necessary?",
    "jawaban": "The correct answer is: To detect and address performance degradation over time."
  },
  "65ba6edccd999addf13ae84fe1d03c37b8405ee9b25ad2943316b474257fdb66": {
    "soal": "The Predictions widget displays ..................... predictions on the data.\n",
    "jawaban": "The correct answer is: predictions"
  },
  "7fb5eafa1f5097a60527b0090ca3c2d0a734439117e7590799db832b55f103b1": {
    "soal": "In the Line Chart widget, we can input forecast input signals from the forecast model. The forecast is drawn with a dashed line and confidence intervals as the range area.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "f2813a1cc6da5c9f6a0aecf394210417f065b6d1a5318d72c13d8d514b6015f3": {
    "soal": "Dalam contoh klasifikasi tipikal, kita akan menggunakan widget Contant untuk membandingkan skor algoritma pembelajaran lainnya (seperti kNNN) dengan skor default. Gunakan dataset iris dan connect ke Test & Score. Kemudian hubungkan Constant dan kNN ke Test & Score dan amati seberapa baik kinerja kNNN terhadap baseline Constant.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "bfc38e6a502cb9f8a1391cfb97aabca482759e3db595ab3e6aec8fb69771dbef": {
    "soal": "Pada Widget Constant ORANGE - untuk classification, ketika memprediksi nilai class dengan Prediction, widget ini akan menghasilkan frekuensi min dari class yang ada di training set. Jika ada dua atau lebih majority class, classifier akan memilih secara random dari predicted class, tapi akan selalu menghasilkan class yang sama untuk contoh tersebut.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "44de6874898c1eb786b5a41bbe67d22b2dd42f20b1baf2495580b0b1a628405a": {
    "soal": "In this example, we will just check what a bag of words model looks like. Load book-excerpts.tab using the Corpus widget and connect it to the Bag of Words widget. Here, we deliberately use the default parameters - the simplest count is term frequency. Check the output of the Bag of Words widget using the ..................... Data Table widget. The last column represents the term frequency of each document.\n\n\n",
    "jawaban": "The correct answer is: Data Table"
  },
  "c12736f602be9ce3683fc7231d48fbafa67a6d6fda06370a6873626aeddd3a14": {
    "soal": "This workflow example shows how Network Analysis can enrich the workflow. We have used the lastfm.net data as input network from Network File and sent it to Network Analysis. We can decide to calculate the degree, degree centrality, and closeness centrality at node-level.\n\nWe can then visualize the network in Network Explorer. In the Network Explorer widget, we color it with the best tags, such as the default for this dataset. But now we can also set the node size to match the degree centrality calculation results. This is a good way to visualize network performance.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "0c3210340a30eaae6d59452987675bfdc18c07a595ad725b679dcf8f1654bfe2": {
    "soal": "The Corpus widget can load a collection of text documents, (mandatory) tagged with categories, or convert input data into a corpus.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "afbf075d922d59ad74f5f41bc7da545d71f6166792f6432052327c930dcd6004": {
    "soal": "The core of the Apache Hadoop framework consists of the following modules:\n- Hadoop Common - contains libraries and utilities needed by other Hadoop modules;\n- Hadoop Distributed File System (\nAnswer Question 86\n) - a distributed file system that stores data on commodity machines, providing very high aggregate bandwidth in the cluster;\n- Hadoop YARN - a resource management platform responsible for managing computational resources in the cluster and using them for scheduling user applications;\n- Hadoop MapReduce - a programming model for processing large-scale data.",
    "jawaban": "The correct answer is: HDFS"
  },
  "e992c4af434c6ad1c6c0cfa8ba53f3ffcd97539cf881aff8e2d76494a71f0c7a": {
    "soal": "R-programming is a programming language used in big data processing. The nature of this programming language is open-source, meaning it can be used for free and modified by anyone. Its \nAnswer Question 93\n source nature allows many active users to contribute to the development of R-programming.\nSome advantages of R-programming include:\n- R programming can integrate with other programming languages, such as SQL.\n- It is used for data cleansing and manipulation, spatial analysis, data analysis and model building, data visualization, and even text analysis with natural language processing.\n- It has many functions and packages that make it easier for data practitioners.",
    "jawaban": "The correct answer is: open"
  },
  "ce1132aa2990d9abf14df2475c13ddc64bb5155190829434f7bfb5fceace607e": {
    "soal": "The core of Apache Hadoop consists of a storage part (\nAnswer Question 94\n Distributed File System (HDFS)) and a processing part (MapReduce). Hadoop divides files into large blocks and distributes them among nodes in the cluster. To process data, Hadoop MapReduce transfers code to nodes for parallel processing, based on the data that needs to be processed at each node. This approach takes advantage of data locality\u2014nodes manipulate the data they hold\u2014to allow faster and more efficient data processing compared to more conventional supercomputer architectures relying on parallel file systems where computation and data are connected via high-speed networks.",
    "jawaban": "The correct answer is: Hadoop"
  },
  "68a989b8cd3183992878f881a09e0f979ffbf32aacc5bb08b6ec780d8e9f6fad": {
    "soal": "Column attribute names can be preceded by a label followed by a hash. Use c for class and m for meta attributes, i for ignoring columns, w for weight (of a column), and C , D , T , S for continuous attribute types ........... , discrete, time, and string. Example: C#mph, mS#name, i#dummy.\n\nWhere ........ is,\n",
    "jawaban": "The correct answer is: continuous"
  },
  "c7e51f64019b723feb74e0d400151eb63cba5d30c64808d51dc26fad893b685b": {
    "soal": "The Predictions widget shows the probabilities and the final decision of the prediction model. The output of the Predictions widget is another dataset, where the predictions are added as a new ............. attribute. We can choose which features we want to remove (original data, predictions, probabilities). The results can be observed in the Data Table. If the predicted data includes the actual class value, the prediction results can also be observed in the Confusion Matrix Widget.\n",
    "jawaban": "The correct answer is: meta"
  },
  "1e2ca43f0d9dfdc41b598aee770bf389382a171ec4f4450a7c5003d7346c6014": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#-xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\ndfg, start_activities, end_activities = pm4py.discover_dfg(log)\npm4py.view_dfg(dfg, start_activities, end_activities)\nWhat does 'dfg' stand for in the code?",
    "jawaban": "The correct answer is: Directly-Follows Graph -Data Flow Graph -Digital Frequency Graph -Directed Flow Graph -Direct File Generator -Dynamic File Generator"
  },
  "9d8c03811741371ec40eff65ac8ffeed529fd4bf784eddf2a68f505483a09499": {
    "soal": "The Import Documents widget takes ..........(1)................. files from a folder and creates a corpus. The Import Documents widget can read ...........(2).........., .docx, .odt, .pdf, and .xml. If there are subfolders in the folder, they can be used to label classes.\n",
    "jawaban": "The correct answer is: (2) \u2192 txt, (1) \u2192 text"
  },
  "3e8ef54b9a5a62a33a405393b09144f8a5f3c1b691edc6fda99715128e7510a4": {
    "soal": "The Moving Transform widget can apply the rolling window function to ............................... Use the Moving Transform widget to get the average value of the series.\n",
    "jawaban": "The correct answer is: time series"
  },
  "daf5c8d69530a5e9bd18ab2c13cc5adc79b2fde57afe437be13d681d262ad045": {
    "soal": "The Tree Widget uses the Tree algorithm with the ability to perform .................... pruning.\n",
    "jawaban": "The correct answer is: forward"
  },
  "3c9fd302e15a356e74395d87d753d90e867c16822da4083f11e7d29b4976ef42": {
    "soal": "import pandas as pd\nfrom pm4py.objects.conversion.log import converter as log_converter\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom pm4py.visualization.petrinet import visualizer as pn_visualizer\nfrom pm4py.objects.log.util import dataframe_utils\n# csv_filename='Insurance_claims_event_log.csv'\n# csv_filename='Business Travel Permits.csv'\n# csv_filename='Parent-Child Union Output.csv'\n# csv_filename='PM4PY_Dummy_Event_Log_100_Cases_pm4py.csv'\n# csv_filename='PM4Py_Dummy_Event_Log-congested-business-process.csv'\n# 1. Load CSV event log\ndf = pd.read_csv(csv_filename)\n# df = pd.read_csv(csv_filename,sep=';')\n# print(df.head())\n# 2. Format timestamps properly\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n# 3. Ensure column names match PM4Py expectations\ndf = dataframe_utils.convert_timestamp_columns_in_df(df)\n# 4. Convert to event log object\nevent_log = log_converter.apply(df, parameters=)\n# 5. Discover process model using Alpha Miner\nnet, initial_marking, final_marking = alpha_miner.apply(event_log)\n# 6. Visualize Petri Net\ngviz = pn_visualizer.apply(net, initial_marking, final_marking)\npn_visualizer.view(gviz)\nWhat does the 'case_id' column represent in a CSV event log for PM4Py?\n{\n~Timestamp of activity\n~Activity duration\n~Activity name\n=Unique identifier for each process instance\n~Process cost\n~Process result\n}",
    "jawaban": "The correct answer is: log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case_id'"
  },
  "7036f019a707c5669a06cc0f3a3c1232b8ea8eeec7a6a0b522f843a9db2eda88": {
    "soal": "In the workflow below, the Tree widget will train a model and evaluate it against Logistic Regression. The accuracy comparison of the Random Tree widget and the Logistic Regression widget is done through the Test & Score widget. The results of the Test & Score widget can be viewed through the Confusion Matrix widget to compare the classification errors that occurred.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "277f6ddccd52f6394dc1bc27695e729fa21f70dfc4b8211f95eb017a4e362654": {
    "soal": "The best strategy for selecting PM4Py methods involves:",
    "jawaban": "The correct answer is: Experimenting and comparing multiple methods"
  },
  "26b0f21cedf4c398f84e40f946aed0b44c9d46077c8a85201de689e0a9600b71": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#-xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\ndfg, start_activities, end_activities = pm4py.discover_dfg(log)\npm4py.view_dfg(dfg, start_activities, end_activities)\nWhat does 'import pm4py' do?",
    "jawaban": "The correct answer is: Includes the pm4py library into the script -Exports data -Creates visualizations -Cleans data -Sets parameters -Reads files"
  },
  "20dd4d11c53f0d0e17461ef2fdb73746c953cebb3e90168df1146b9ad707429f": {
    "soal": "How do you restart the Ollama server after making configuration changes?",
    "jawaban": "The correct answer is: ollama serve --restart"
  },
  "125a94091ae96aefac0c41beca2dc803e55720127f689c37bd65e00b54a385c2": {
    "soal": "Line plot, a type of plot that displays data as a series of points, connected by straight line segments. Line Plot is used for data ...................... Meanwhile, for categorical data, Line Plot can be used for grouping data points.\n",
    "jawaban": "The correct answer is: numeric"
  },
  "45303ab54aeaa93bd9b829faccd5bab062a738d54148796371c7d48846b6fe84": {
    "soal": "In the workflow below, the Tree widget will train a model and evaluate it against Logistic Regression. The accuracy comparison between the Tree widget and Logistic Regression widget is done through the ..................... widget. The results from the Test & Score widget can be viewed through the Confusion Matrix widget to compare the classification errors that occurred.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Test & Score"
  },
  "8be6c3b51482f479fb7d864e5b3631e693350094ab2dfc6b682984d868bca2cb": {
    "soal": "Overgeneralization risk is highest with:",
    "jawaban": "The correct answer is: Inductive Miner"
  },
  "398ac3f317e232423c47cd81fc1aad0b3d2ffbec93330f7e5784ca1d86a1f398": {
    "soal": "Widget ...................... can be used for statistical analysis of network data.\n",
    "jawaban": "The correct answer is: Network Analysis"
  },
  "ac97f00f6a25dd1709aec07482698a21ec308202dc2475725ceec2d55f2ed786": {
    "soal": "The Test & Score widget tests the learning algorithm. Different sampling schemes are available, including using separate test data. The widget does two things. First, it shows a table with performance measures of different classifiers, such as classification accuracy and area under the curve. Second, the output of the evaluation, which can be used by other widgets to analyze the performance of classifiers, such as ROC Analysis or Confusion .............. .\n",
    "jawaban": "The correct answer is: matrix"
  },
  "ff5a98b2435f4946d23bb028f75733bab1ef1c48fe16fd1cc1cb41b4439f7c99": {
    "soal": "The Tree algorithm is a simple algorithm that can split data into nodes based on class purity (category/................(1).............). The Tree algorithm is the predecessor of the Random Forest algorithm. The Tree widget in Orange was designed in-house and can handle .............(2)............ and continuous datasets.\n",
    "jawaban": "The correct answer is: (1) \u2192 class, (2) \u2192 discrete"
  },
  "05ac2e6487d4eac411f03f4c7e45c25715ff5274d35796eb2265a6306eaa3a10": {
    "soal": "When analyzing TikTok comments for an initial experiment, how many comments are recommended?",
    "jawaban": "The correct answer is: 1000\u20133000 comments"
  },
  "842b996fde8cd9e52fc791a16cc4a0bc9d6c8e0f051ed03b882d77c16ae61c87": {
    "soal": "\n\n\nWorkflow di atas melakukan test / evaluasi CN2 Rule Induction dan Tree di Widget Test & Score. Terlihat Tree\u00a0 lebih ......................\n\n\n\n",
    "jawaban": "The correct answer is: buruk"
  },
  "a295f7b8364973c5b816461fb2805b8ec577fd20b3387b976f6044e013ef7d3d": {
    "soal": "Network Generator Widget can create an example network graph.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "b7b1592dd2ea7619049d9123ea522039c70cc759dc82fe08f2ad9f7ccb38c4ab": {
    "soal": "In the workflow below, we compare the statistics of two widgets ....................... one with information from the entire dataset and the other with information from the selected subset (manually) from the Scatter Plot widget. Here, we use the Iris dataset.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Data Info"
  },
  "9a75924486d3f292b193efe47e49f7f37fce51677afa979e3741ac6e7f633019": {
    "soal": "The easiest way to use the Venn Diagram widget is to select .................. data subsets with the Select Rows widget - the first subset is breast cancer patients aged between 40 and 49 years, and the second is patients with tumor sizes between 20 and 29. The Venn diagram helps us find examples that fit both criteria, which can be found at the intersection of the two circles.\n\n\n",
    "jawaban": "The correct answer is: two"
  },
  "d22535b7446e91e573a4bc0e239c8d8957bf94c9fe8efa723f4053216312b3f3": {
    "soal": "The example workflow below shows the very standard use of the Distance Matrix widget. We calculate the distance between rows in a sample from the Iris dataset and output it into the Distance Map. It is not surprising that Iris Virginica and Iris Setosa are the farthest apart.\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "0b0cd6f2901ea0721c6de22cd85b4745a8fb5e13d3b64404ff6e8414f57d9a5f": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nHow does the Dropout layer contribute to a model's ability to generalize to new data?",
    "jawaban": "The correct answer is: By preventing the model from relying too heavily on specific features"
  },
  "ab9254e566139d69207a8c0a260df2809dc773d869ed9ddd3346ef559a856d6b": {
    "soal": "Which activation function is used in the first Dense layer?",
    "jawaban": "The correct answer is: ReLU"
  },
  "1bce3b263c2a68d2b7f6cda75f43c0752b81e8b0347455e1f3c9cb71d89ed634": {
    "soal": "The .................(1)................ widget is used to manually organize the data domain. Users can decide which attributes to use and how to use them. Orange differentiates between ordinary attributes, ...............(2).............. attributes (optional), and meta attributes. For example, to build a classification model, the domain will consist of a set of attributes and a discrete class attribute. Meta attributes are not used in modeling, but some widgets can use them to label instances.\n",
    "jawaban": "The correct answer is: (1) \u2192 Select Columns, (2) \u2192 class"
  },
  "4f104e44b34fa821eab8372e1e4a3ad4cb9c5aa90fa5f182f4b789fa64de0d5f": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.rand(100) # 100 nilai target\n# Inisialisasi model regresi linier\nmodel = LinearRegression()\n# Tentukan jumlah fold (misalnya, 5-fold cross-validation)\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n# Evaluasi model menggunakan cross-validation\nscores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n# Menampilkan skor R^2 untuk setiap fold dan rata-ratanya\nprint(\"Skor R^2 untuk setiap fold:\", scores)\nprint(\"Rata-rata skor R^2:\", np.mean(scores))\nWhat is the purpose of the `np.mean(scores)` statement in the code?",
    "jawaban": "The correct answer is: To compute the average R\u00b2 score across all folds."
  },
  "83b1c4cb18042d820ea0d1f6fefa389abc75fb7dc23338270b84ef548f4eaf16": {
    "soal": "Network Explorer Widget allows us to visually explore the network and its properties.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "9ed54070f0464dc771afc26328a383f50920e84fff408f3c8eaf6f30f7fccb33": {
    "soal": "R-programming is a programming language used in big data processing. The nature of this programming language is \nAnswer Question 85\n source, meaning it can be used for free and modified by anyone. Its open-source nature allows many active users to contribute to the development of R-programming.\nSome advantages of R-programming include:\n- R programming can integrate with other programming languages, such as SQL.\n- It is used for data cleansing and manipulation, spatial analysis, data analysis and model building, data visualization, and even text analysis with natural language processing.\n- It has many functions and packages that make it easier for data practitioners.",
    "jawaban": "The correct answer is: open"
  },
  "2578f9f626ae67c55ffef1ba6649d50a03a162c56c8290892063bce22fb83367": {
    "soal": "Column attribute names can be preceded by a label followed by a hash. Use c for class and m for meta attributes, i for ..........., w for weight (of a column), and C, D, T, S for continuous, discrete, time, and string attribute types. Example: C#mph, mS#name, i#dummy.\n\nWhere ........ is,\n",
    "jawaban": "The correct answer is: i"
  },
  "9df2a376bd6e2c96946bf12a7dc1b45f25bc7030a9c39ac713317db16358b004": {
    "soal": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n# Contoh: Membuat dataset dummy dengan 100 sampel, 10 fitur, dan 1 target\nnp.random.seed(42)\nX = np.random.rand(100, 10) # 100 sampel, 10 fitur\ny = np.random.randint(2, size=100) # 100 nilai target biner (0 atau 1)\n# Membagi dataset menjadi training (80%) dan testing (20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model, misalnya RandomForestClassifier\nmodel = RandomForestClassifier(random_state=42)\n# Melatih model pada data training\nmodel.fit(X_train, y_train)\n# Memprediksi data testing\ny_pred = model.predict(X_test)\n# Mengevaluasi akurasi model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model pada data testing: ')\nWhat type of values are contained in the target array `y`?\n{\n~Continuous values ranging from 0 to 1.\n~Categorical values representing multiple classes.\n=Binary values of 0 and 1.\n~Integer values from 1 to 10.\n~Floating-point values.\n~String labels.\n}",
    "jawaban": "The correct answer is: accuracy:.2f"
  },
  "9971ccc8218c601b530112d832f0bd13658f5f56fc73560b50c7f24f3dc7d007": {
    "soal": "One URL location for Orange development/use examples is\n\n\u00a0https://www.youtube.com/user/realannoyingorange\n",
    "jawaban": "The correct answer is 'False'."
  },
  "2784042dece9f5f424b66176c670aabe60b6c78064db89f7db1db7584db2fb03": {
    "soal": "The core of Apache Hadoop consists of the storage part (Hadoop Distributed File System (HDFS)) and the processing part (MapReduce). Hadoop splits files into large blocks and distributes them across nodes in the cluster. To process the data, Hadoop \nAnswer Question 96\n transfers code to the nodes to process in parallel, based on the data that needs to be processed at each node. This approach takes advantage of data-node locality, manipulating the data they hold in hand to enable faster and more efficient processing than more conventional supercomputer architectures relying on parallel file systems where computation and data are linked through high-speed networks.",
    "jawaban": "The correct answer is: MapReduce"
  },
  "98db2829850e825babdd960c2f91314f6a8e6938d3ba6d82c74ac1545059cbd8": {
    "soal": "The kNN widget predicts based on the largest training instance.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "bd4852eb91f2d49256fd359854575101c44c8d8256caecc08683a387ab5c61c4": {
    "soal": "The selected data in the first .................. widget is passed to the second Data Table widget. Observe that we can choose which dataset to view (iris or glass). Switching from one dataset to another changes the data instance selection that is communicated if Commit on any change is selected.\n\n\n",
    "jawaban": "The correct answer is: Data Table"
  },
  "f1ab2c95a15471929ca530115b184d2e2bb019816971570296fc44bce4942c84": {
    "soal": "What is an advantage of the Alpha Miner?",
    "jawaban": "The correct answer is: Simple and easy to understand"
  },
  "2c80949192bb203e5e774a006d8aeca0bf702467e95bdf3df6ff699780e544e3": {
    "soal": "In the Moving Transform widget, to integrate the time series\u2019 difference from the Difference widget, use Cumulative sum aggregation on the .......................... window wide enough to capture the entire series.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: window"
  },
  "f545ae911aa73b0f548cd72e2c3aa49a2a42c6380e5f91d001c03d90d0806f54": {
    "soal": "The Distance Transform widget transforms ....................... that are present in the dataset.\n",
    "jawaban": "The correct answer is: distances"
  },
  "5f2be9f1549b972dda386140bb590473f66706288cf74e53877f479df16b10d5": {
    "soal": "In the Import Documents Widget, to retrieve ................., select (click) the folder icon on the right side of the widget. Select (choose) the folder that we want to use as a corpus. After loading is complete, we will see how many documents were successfully retrieved by the Import Documents Widget. To observe the obtained corpus, connect the Import Documents widget to the Corpus Viewer Widget. In this workflow, we are using a set of speeches by President Kennedy in plain text format.\n\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is: data"
  },
  "043b2c21d2edec65335a96ca72f24f540a10247b29200ccf517849ef1274718e": {
    "soal": "model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(10)\n])\nWhat is the function of the final Dense layer in the model?",
    "jawaban": "The correct answer is: Produces the raw prediction scores for each class"
  },
  "f5ed7c7beba4653b5fd2855e22cb43953181a948c455e2d9ff3c279da00b2f7f": {
    "soal": "Widget ........................... tries to find clusters in a network. Network Clustering works with two algorithms, one from Raghavan et al. (2007), which uses label propagation to find the appropriate clusters, and one from Leung et al. (2009), which builds on the work of Raghavan and adds hop attenuation as a parameter for cluster formation.\n",
    "jawaban": "The correct answer is: Network Clustering"
  },
  "4f2efa2d943b060470ec8a0ac0c7f3e27a42769b91bad1bb02ee320ab0a7cf43": {
    "soal": "import pandas as pd\nfrom pm4py.objects.conversion.log import converter as log_converter\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom pm4py.visualization.petrinet import visualizer as pn_visualizer\nfrom pm4py.objects.log.util import dataframe_utils\n# csv_filename='Insurance_claims_event_log.csv'\n# csv_filename='Business Travel Permits.csv'\n# csv_filename='Parent-Child Union Output.csv'\n# csv_filename='PM4PY_Dummy_Event_Log_100_Cases_pm4py.csv'\n# csv_filename='PM4Py_Dummy_Event_Log-congested-business-process.csv'\n# 1. Load CSV event log\ndf = pd.read_csv(csv_filename)\n# df = pd.read_csv(csv_filename,sep=';')\n# print(df.head())\n# 2. Format timestamps properly\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n# 3. Ensure column names match PM4Py expectations\ndf = dataframe_utils.convert_timestamp_columns_in_df(df)\n# 4. Convert to event log object\nevent_log = log_converter.apply(df, parameters=)\n# 5. Discover process model using Alpha Miner\nnet, initial_marking, final_marking = alpha_miner.apply(event_log)\n# 6. Visualize Petri Net\ngviz = pn_visualizer.apply(net, initial_marking, final_marking)\npn_visualizer.view(gviz)\nWhich Python library must be installed for CSV manipulation before using PM4Py?\n{\n~Graphviz\n~Numpy\n=Pandas\n~Seaborn\n~Matplotlib\n~Plotly\n}",
    "jawaban": "The correct answer is: log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case_id'"
  },
  "38073e353fc439fb0dc09b46d68cbd1dfb39fc0acfbd13b3f48112bb283c9bf7": {
    "soal": "Data attributes in Orange have types discrete, .................... or character string. The attribute type is marked by a symbol that appears before the attribute name (D, C, S).\n\n\n\nAnswer: \nQuestion 44",
    "jawaban": "The correct answer is: continuous"
  },
  "6a606b7f34c729071942af7b2b243ef7dcb382587dfaaf52b1e74addfa568d44": {
    "soal": "In the Interpolate widget, which is part of the Time Series add-on, the interpolation type - Linear interpolation replaces missing values with linearly-spaced values between three nearest and defined data points.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "f1c41c70de0e1c8ff7d20b34c67f17e25a7d4cba05713659162f3154d51f92b5": {
    "soal": "The core of Apache Hadoop consists of the storage part (Hadoop Distributed File \nAnswer Question 59\n (HDFS)) and the processing part (MapReduce). Hadoop splits files into large blocks and distributes them across nodes in the cluster. To process the data, Hadoop MapReduce transfers code to the nodes to process in parallel, based on the data that needs to be processed at each node. This approach takes advantage of data-node locality, manipulating the data they hold in hand to enable faster and more efficient processing than more conventional supercomputer architectures relying on parallel file systems where computation and data are linked through high-speed networks.",
    "jawaban": "The correct answer is: System"
  },
  "3e1806881ecdd2ce00fd63179bbbf42e947868c766765be0a891a163408ab6ee": {
    "soal": "import tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nplt.imshow(x_train[0], cmap='gray')\nplt.title(f\"Label: \")\nplt.show()\nmodel = keras.Sequential([\nkeras.layers.Flatten(input_shape=(28, 28)),\nkeras.layers.Dense(128, activation='relu'),\nkeras.layers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nWhat is the purpose of dividing pixel values by 255.0?\n{\n= To normalize the pixel values to a 0-1 range\n~ To increase the brightness of the image\n~ To convert images to grayscale\n~ To sharpen the image\n~ To augment the dataset\n~ To apply dropout regularization\n}",
    "jawaban": "The correct answer is: y_train[0]"
  },
  "26aac1b5e4d27c01d58c717b2c70e518d0632e814e57081ec821fef11fcfa274": {
    "soal": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n# Contoh: Memuat dataset dari file CSV\n# Gantilah 'data.csv' dengan path ke dataset Anda\ndf = pd.read_csv('data.csv')\n# Memisahkan fitur (X) dan target \n# Asumsikan kolom terakhir adalah target\nX = df.iloc[:, :-1]\ny = df.iloc[:, -1]\n# Membagi dataset menjadi data latih dan data uji (80% : 20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model Decision Tree\nmodel = DecisionTreeClassifier()\n# Melatih model dengan data latih\nmodel.fit(X_train, y_train)\n# Melakukan prediksi pada data uji\ny_pred = model.predict(X_test)\n# Menghitung akurasi\nakurasi = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model: ')\n# Menampilkan laporan klasifikasi yang mencakup presisi, recall, dan F1-score\nprint(classification_report(y_test, y_pred))\nWhich method is used to train the `DecisionTreeClassifier` model?\n{\n~train()\n~start()\n=fit()\n~learn()\n~educate()\n~prepare()\n}",
    "jawaban": "The correct answer is: akurasi:.2f"
  },
  "e748180cb7de7448b4fef3ccc1841436c4f46d3419e6144002f9a7b6f5de3421": {
    "soal": "import tensorflow as tf\nfrom tensorflow.keras import layers, models\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nmodel = models.Sequential([\nlayers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(64, (3,3), activation='relu'),\nlayers.MaxPooling2D((2,2)),\nlayers.Conv2D(128, (3,3), activation='relu'),\nlayers.Flatten(),\nlayers.Dense(128, activation='relu'),\nlayers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5, batch_size=64)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Akurasi model pada data uji:\", test_acc)\nWhat is the batch size used during training?",
    "jawaban": "The correct answer is: 64"
  },
  "bb5e6bfd5b65f4432d4751b52492ca90fd36b9868d8a8d4cb74a065299e1201d": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nWhat is the shape of the `x_train` and `x_test` arrays in the CIFAR-10 dataset?",
    "jawaban": "The correct answer is: (number_of_samples, 32, 32, 3)"
  },
  "5a3ba78069d398542e6a49e867dde53c5a8f25330548f77e55b3a6986d50689c": {
    "soal": "\n\n\n\n\n\nSupervised Learning uses labeled data.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "a5bee42d3e1b396e2b6e0c5c6e77a344813a4530a8dd293dc125cca855118478": {
    "soal": "The Save Data widget can consider the dataset provided in the input channel and save it to a data file with a specified name. The Save Data widget can save the data as a file with data separated by ........................ or separated by commas.\n",
    "jawaban": "The correct answer is: tab"
  },
  "c4494c201a1dba4699684d315f54115e38649b471a2e19bacaf382397c7f0b49": {
    "soal": "Widget Constant ORANGE biasanya digunakan sebagai model utama untuk learner lainnya.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "490c3fa422f152ada519cd1449d685488476815cba53d0c853e8b954fc1b7e16": {
    "soal": "If you want to download a model without immediately running it, which command should you use?",
    "jawaban": "The correct answer is: ollama pull model"
  },
  "14a9c89ea861ffb2d8a94a1d4e7f7a4dca2f454441795a4a34917af668faf2a7": {
    "soal": "According to the official Zoho website, Zoho Analytics is a complete, reliable, and scalable analytics platform. Developers and system integrators (\nAnswer Question 3\n) can use this platform to develop and implement custom analytics applications and integrations.\nAnother advantage of Zoho Analytics is its user-friendliness, making it easy for users to upload and control data. By using Zoho Analytics, data practitioners can create multifaceted and custom dashboards. This platform is easy to use and implement.",
    "jawaban": "The correct answer is: SI"
  },
  "ac518c6a6eb1a8944d64d88601b2695f2da28be09edb9f96deb213b95b47bff6": {
    "soal": "Line plot, a type of plot that displays data as a series of points connected by straight line segments. Line Plot works for text data. For categorical data, Line Plot can be used for grouping data points.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "f65338aa890f079d2b75f6ebb9b2457017a0352f0bbccc86d730db9404e03936": {
    "soal": "The Box Plot widget can show / illustrate the distribution of ................. values.\n\n\n\n",
    "jawaban": "The correct answer is: attribute"
  },
  "62b7c1b51e8a1b758f437accfe88da83999136e47a54829ac7cf0de959071a6f": {
    "soal": "The ...................... widget can apply the rolling window function to the time series. Use the Moving Transform widget to get the average value of the series.\n",
    "jawaban": "The correct answer is: Moving Transform"
  },
  "164f517ed3597ccfe88b196229ed0ee130002920723a3ffb05a806704594a2ee": {
    "soal": "The Save Data widget does not save data every time it receives a new signal in the input because this would continuously (and mostly, accidentally) overwrite the file. Instead, the data is saved only after the ................... is set or the user presses the Save button.\n",
    "jawaban": "The correct answer is: filename"
  },
  "cfd39ea4278128216573b801210a81527d3c0a9f8e884954bdf0ca2b83e24145": {
    "soal": "Widget Calibrated Learner menghasilkan sebuah model yang mengkalibrasi distribusi dari class probabilities dan meng-optimasi decision threshold. Widget ini hanya bekerja untuk logistics task.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "954b58ab0d8b22f9b082621fa07b4bf9e845aa5d76db06756aa6fb043abd661a": {
    "soal": "In the workflow below, the Tree widget will train a model and evaluate it against Logistic Regression. The accuracy comparison between the Tree widget and the Logistic Regression widget is done through the Test & Score widget. The results from the Test & Score widget can be viewed through the ...................... widget to compare the classification errors that occurred.\n\n\n\n\n\n",
    "jawaban": "The correct answer is: Confusion Matrix"
  },
  "f42a1e576bf91dd6f0e4eab024860fc4b3a62064f08fd4aa10874f1ed8450ebe": {
    "soal": "To avoid confusion, Orange supports dates and/or times formatted in one of the ISO 8601 formats. For example, the following values are all valid:\n\n2016\n2016-12-27\n2016-12-27 14:20:51\n16:20\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "a1e71a34740bf059dc282d598cdf44277669bca843778c628ac25c708964afaa": {
    "soal": "\n\n\n\n\n\nThe following Workflow can be used to perform feature ranking of the data against each desired category.\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "ccb6b9a16941bb54886b66b06374a6487e4980c31fdc83f78365183476bed46c": {
    "soal": "Widget ................. can help us uncover clusters and highly connected groups in a network. First, we will use the Widget Network File to load the lastfm.net dataset. Then we will send the network to the Widget Network Clustering. The Network Clustering widget finds 79 clusters in the network. To visualize the results, we use the Widget Network Explorer and set the Color attribute to Cluster. This will color the network nodes with the appropriate cluster colors - this is a good way to visualize highly connected groups in a dense network.\n\nKeep in mind that the Widget Network Explorer here will color the 10 largest clusters and color the rest as 'Other'.\n\n\n\n",
    "jawaban": "The correct answer is: Network Clustering"
  },
  "f75e28af1803eb969ae96b79854446c3aa5a67c0acc50030f378756c67ed3fdc": {
    "soal": "An example of using the kNN Widget for ............................ task is shown in the workflow below. This workflow demonstrates how to use the Learner output. For this example, we use the housing dataset. We input the kNN prediction model into Predictions and observe the predicted values.\n\n\n\n",
    "jawaban": "The correct answer is: regression"
  },
  "25db414fc1fc010f0e198226b62e721fdba704d6ed822551fe9847238c425725": {
    "soal": "import pandas as pd\ndf = pd.read_excel('path_ke_file.xlsx', sheet_name=0)\ndf_selected = df[['Kolom1', 'Kolom2']]\nprint(df_selected.head())\nHow do you read columns 'A' through 'C' from an Excel file?",
    "jawaban": "The correct answers are: pd.read_excel('file.xlsx', usecols, 'A to C'), pd.read_excel('file.xlsx', usecols, 'A,C'), df, pd.read_excel('file.xlsx', usecols, 'A:C'), pd.read_excel('file.xlsx', usecols, 'A-C'), pd.read_excel('file.xlsx', usecols, ['A', 'B', 'C']), pd.read_excel('file.xlsx', usecols, 'A;B;C')"
  },
  "b4bbf8c2dff09d2472e7425ae7238b9ded06b0a116c5b46045fa527910f09799": {
    "soal": "from googlesearch import search\nimport csv\nimport time\ndef load_keywords(filename):\nwith open(filename, 'r', encoding='utf-8') as f:\nreturn [line.strip() for line in f if line.strip()]\ndef google_scrape_bulk(keywords, num_results=10, output_file='results.csv'):\nwith open(output_file, mode='w', newline=, encoding='utf-8') as file:\nwriter = csv.writer(file)\nwriter.writerow(['Keyword', 'Rank', 'URL'])\nfor keyword in keywords:\nprint(f\"\ud83d\udd0d Searching for: \")\ntry:\nresults = search(keyword, num_results=num_results)\nfor i, url in enumerate(results):\nwriter.writerow([keyword, i+1, url])\ntime.sleep(1) # Delay agar tidak dianggap bot\nexcept Exception as e:\nprint(f\"\u274c Error while searching '{keyword}': {e}\")\nprint(f\"\n\u2705 All results saved to '{output_file}'\")\n# Main\nif __name__ == '__main__':\nkeywords = load_keywords('keywords.txt')\ngoogle_scrape_bulk(keywords, num_results=10)\nBy default, how many search results does the script retrieve for each keyword?\n{\n=10\n~5\n~20\n~30\n~40\n~50\n}",
    "jawaban": "The correct answer is: keyword"
  },
  "50c59e8f20e02febdf3b028592a278fd7485f17fa31ef32123869dfbd5414300": {
    "soal": "We can measure the distance between embedded images and see which ones are the most similar. We can use ..................... widget to measure the distance. Typically, cosine distance works best for images. We send the distance matrix to Hierarchical Clustering to visualize similar pairs in the dendrogram.\n\n\n",
    "jawaban": "The correct answer is: Distances"
  },
  "e0782b1c8882870f9cc473ee57b86d6c05b56ae41b6c7d60587eacabf9724cb5": {
    "soal": "The SQL widget accesses data stored in an SQL database. The SQL widget can connect to ..............(1)............... (requires the psycopg2 module) or .............(2)................ (requires the pymssql module).\n",
    "jawaban": "The correct answer is: (2) \u2192 SQL Server, (1) \u2192 PostgreSQL"
  },
  "d51031cdb95ee0aabd03a3e4bb14273a29431eaab5d894c36a97b20bbcf7ec11": {
    "soal": "Some formats supported by ORANGE3 (based on its add-ons):\n\n.............................. : Network File from Network add-on\n",
    "jawaban": "The correct answer is: network"
  },
  "677464f0ae223d542c3cf3ed7c6438fee6b147177ed63c443833c6752e39d1ee": {
    "soal": "The Calibration Plot widget can show the match between predicted probability by the classifier and the actual class probability.\n",
    "jawaban": "The correct answer is 'True'."
  },
  "0b8224882c37e4945872aaee77d67f424bbef810e395212792e4234b34056b7a": {
    "soal": "In which subsets should stratification be applied to maintain class proportions?",
    "jawaban": "The correct answer is: Training, validation, and testing sets."
  },
  "01c68c1ce14a528742d76c619696b28c08aa54764a4dd1feac6a3c4282b1c2d0": {
    "soal": "Column attribute names can be preceded by a label followed by a hash. Use c for class and m for meta attributes, i for ignoring columns, w for weight (of a column), and C , .......... , T, S for continuous, discrete, time, and string attribute types. Example: C#mph, mS#name, i#dummy.\n\nWhere ........ is,\n",
    "jawaban": "The correct answer is: C"
  },
  "d63bd880e4c76d8f28c7ef21e15c158088e9caca6b77079fc44f13af322e42fd": {
    "soal": "The basic framework of Apache Hadoop consists of the following modules:\n- Hadoop Common - contains libraries and utilities required by other Hadoop modules;\n- Hadoop Distributed File System (HDFS) - a distributed file system that stores data on commodity machines, providing very high aggregate bandwidth in clusters;\n- Hadoop YARN - a resource management platform responsible for managing computing resources in the cluster and using them for user application scheduling; and\n- Hadoop MapReduce - a programming model for large-scale data processing.\nThe term \"Hadoop\" has come to refer not only to the basic modules above but also to \"\nAnswer Question 18\n\", or a collection of additional software packages that can be installed on top of or alongside Hadoop, such as Apache Pig, Apache Hive, Apache HBase, Apache Phoenix, Apache Spark, Apache Zookeeper, Impala, Apache Flume, Apache Sqoop, Apache Oozie, Apache Storm, and others.",
    "jawaban": "The correct answer is: ecosystem"
  },
  "ff7fdd433a306e6cd1b59eac22794c72f344aea7ca19ad9c8427379cfe60399d": {
    "soal": "A simple example with Calibrated Learner. We use the Titanic dataset because this widget requires binary class values (in this case, they are 'survived' or 'not survived').\n\nWe use Logistic Regression as the base learner, which will be calibrated with the default settings, with sigmoid optimization from the value distribution and optimized with CA.\n\nBy comparing the results of the uncalibrated Logistic Regression model, we will clearly see that the calibrated model is better.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "a0a867345f761c34b74b6b793f7e96af867ac8dbc0dfbc35264b1cab61a0bd9d": {
    "soal": "PM4Py visualizer best for rapid initial analysis:",
    "jawaban": "The correct answer is: Directly-Follows Graph (DFG)"
  },
  "92b5d834786f2e3bf605f595e77b880005d514e196b1f521187162d8e01953ea": {
    "soal": "The Box Plot widget shows .......................... attribute values. It is good practice to examine any new data with this widget to quickly spot anomalies, such as duplicate values (e.g., gray or grey), outliers, and the like.\n",
    "jawaban": "The correct answer is: distribution"
  },
  "d73d8f56673ee54d400cea9eae4266977d775ba7b622b981c6526851b853fc85": {
    "soal": "The Data Info widget is a simple widget that presents information about the dataset size, feature, target, attributes ................., and location.\n",
    "jawaban": "The correct answer is: meta"
  },
  "5be119b360fb9ea9d985acaac4b5a5ad180ae8dcfc9db6b547d87c81132ad9ec": {
    "soal": "\n\n\nIn the machine learning model creation process, data is usually split, for example, 80%:20%, where 80% is used for the training dataset to obtain the model, while the remaining 20% is used for the test dataset. Training data is used to evaluate model performance.\n\n\n\n\n\n\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "51fe5383b91627ae778989176749810323dfe6f7574f1a6d00ed7fb79167e4d7": {
    "soal": "The Heat Map widget can plot a heat map for a pair of classes.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "fc158bd6576e1fe4182b162d04645206fa2b7cf946f79f23bbd71f1c61f8460a": {
    "soal": "import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nHow many images are allocated for training in the CIFAR-100 dataset?",
    "jawaban": "The correct answer is: 50,000"
  },
  "c52f39ffe8cfc326200ea0207532758afb42337e561bd306045f7b1a0b9de4ab": {
    "soal": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n# Contoh: Memuat dataset dari file CSV\n# Gantilah 'data.csv' dengan path ke dataset Anda\ndf = pd.read_csv('data.csv')\n# Memisahkan fitur (X) dan target \n# Asumsikan kolom terakhir adalah target\nX = df.iloc[:, :-1]\ny = df.iloc[:, -1]\n# Membagi dataset menjadi data latih dan data uji (80% : 20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Inisialisasi model Decision Tree\nmodel = DecisionTreeClassifier()\n# Melatih model dengan data latih\nmodel.fit(X_train, y_train)\n# Melakukan prediksi pada data uji\ny_pred = model.predict(X_test)\n# Menghitung akurasi\nakurasi = accuracy_score(y_test, y_pred)\nprint(f'Akurasi model: ')\n# Menampilkan laporan klasifikasi yang mencakup presisi, recall, dan F1-score\nprint(classification_report(y_test, y_pred))\nIf the printed accuracy is `0.85`, what does this indicate?\n{\n~That 85% of the test data was misclassified.\n~That the model is 85% uncertain about its predictions.\n=That the model correctly predicted 85% of the test instances.\n~That the model's error rate is 85%.\n~That 85% of the training data was used.\n~That the model's predictions are 85% random.\n}",
    "jawaban": "The correct answer is: akurasi:.2f"
  },
  "f941956ab89b9934cdee9db30ae06a56e429ae3d28c677920be4735ab280879c": {
    "soal": "\n\n\n\n\n\nProses klasifikasi dilakukan menggunakan algoritma / modul data regression yang hasilnya langsung dinilai keakuratannya oleh modul Test & Score\n\n\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "467e1a17fcf02e4a4d016887cd22a71a73195270424643b81b1078e37f014027": {
    "soal": "The basic framework of Apache Hadoop consists of the following modules:\n- Hadoop Common - contains libraries and utilities required by other Hadoop modules;\n- Hadoop Distributed File System (HDFS) - a distributed file system that stores data on commodity machines, providing very high aggregate bandwidth in the cluster;\n- Hadoop YARN - a resource management platform responsible for managing computational resources in the cluster and using them for scheduling user applications; and\n- Hadoop Reduce - a programming model for large-scale data processing.",
    "jawaban": "The correct answer is 'False'."
  },
  "f35294b178052045084d8db3127f56c7a13d77070374122e54124a420459ebf8": {
    "soal": "The Bag of Words widget can create a ....................... from the input corpus.\n",
    "jawaban": "The correct answer is: Bag of Words"
  },
  "882412f2cf07268eb74b6fc37284400d70521755a627fc923e448085f3e9406b": {
    "soal": "\n\n\nThe workflow above performs testing/evaluation of CN2 Rule Induction and Tree in the Test & Score Widget. It appears that Tree is ......................\n\n\n\n",
    "jawaban": "The correct answer is: worse"
  },
  "bd20def2b3d3e230d3816dc12cd24b4b315e53f2f43e2e0060bf27aa75d84903": {
    "soal": "The Select Columns widget is used to manually organize the data domain. Users can decide which attributes to use and how to use them. Orange differentiates between ordinary attributes, class attributes (optional), and meta attributes. For example, to build a classification model, the domain will consist of a set of attributes and a discrete class attribute. Meta attributes are not used in modeling, but some widgets can use them to label instances.\n",
    "jawaban": "The correct answer is: (1) \u2192 manual, (2) \u2192 discrete"
  },
  "43ff5859646d3f999898ebb151e8d3ff6e730e602a84cd522be90291b074e0ad": {
    "soal": "As an example of using kNN for classification, we use the iris dataset. We compare the results of k-Nearest Neighbors with the default Constant model, which will predict the majority class. It appears kNN is better.\n\n\n",
    "jawaban": "The correct answer is 'True'."
  },
  "97e7c7a73b4b4f09f6db0ef366331df052643490960393731343be0e968fefc4": {
    "soal": "In the workflow below, the ................. widget will train a model and evaluate it against Logistic Regression. The accuracy comparison between the Tree widget and Logistic Regression widget is done through the Test & Score widget. The results from the Test & Score widget can be viewed through the Confusion Matrix widget to compare the classification errors that occurred.\n\n\n",
    "jawaban": "The correct answer is: Tree"
  },
  "a47498c9883da895ece14e06b410fa5ca7585943db7593c73465639cf476da40": {
    "soal": "The Calibrated Learner widget generates a model that calibrates the distribution of class probabilities and optimizes the decision threshold. This widget only works for regression tasks.\n",
    "jawaban": "The correct answer is 'False'."
  },
  "4f99c5875b6335705d147d58a5a09009d237325fba4e595c34b4e8dc110b95e6": {
    "soal": "The basic framework of Apache Hadoop consists of the following modules:\n- Hadoop Common - contains libraries and utilities required by other Hadoop modules;\n- Hadoop Distributed File System (HDFS) - a distributed file system that stores data on commodity machines, providing very high aggregate bandwidth in the cluster;\n- Hadoop YARN - a resource management platform responsible for managing computational resources in the cluster and using them for scheduling user applications; and\n- Hadoop MapReduce - a programming model for large-scale data processing.\nThe term \"Hadoop\" has come to refer not only to the basic modules above but also to the \"ecosystem,\" or collection of additional software packages that can be installed above or alongside Hadoop, such as Apache Dog, Apache Bird, Apache Hive, Apache HBase, Apache Phoenix, Apache Spark, Apache Zookeeper, Impala, Apache Flume, Apache Sqoop, Apache Oozie, Apache Storm, and others.",
    "jawaban": "The correct answer is 'False'."
  },
  "9868725ade92d9f303f341f4d4e36a57624d119c684b5c60a95ab4ab0221973a": {
    "soal": "In a dataset, what type of data might a 'Sentiment' column contain?",
    "jawaban": "The correct answer is: Categorical data indicating opinions like 'Positive' or 'Negative'."
  },
  "c20d69a551519d95898fa9443a90619ee895e3a51622ed97ee6f14090a1c1410": {
    "soal": "Some formats supported by ORANGE3 (based on its add-ons):\n\n.............................. : Corpus or Import Documents from Text add-on\n",
    "jawaban": "The correct answer is: text/corpus"
  },
  "2437d7adc3dfc93085a665bcf9a869ff4e593a8b9e4d12a7af52b436dd443fab": {
    "soal": "The Tree widget can be used for classification tasks and ....................\n",
    "jawaban": "The correct answer is: regression"
  },
  "3d1e69509a8af0b0cd954f0da227c5f92c3d4b40fbf65306176a1c958d36234b": {
    "soal": "The Distance Matrix widget can visualize the measurement results of distances in many distance matrices.\n\n",
    "jawaban": "The correct answer is 'False'."
  },
  "65ac3b24617a13313819c3c627e83ae596820d862e32fc966b75c27a4a840567": {
    "soal": "Which area is NOT listed as an LLM application?",
    "jawaban": "The correct answer is: Image classification"
  },
  "6505313320c8ca1238606f29dccaf6ef98c6031797b14810c68bfb205883361a": {
    "soal": "Data Science is an interdisciplinary field that uses methods, ................, algorithms, and scientific systems to extract knowledge and insights from data in various forms, both structured and unstructured, similar to data mining.\n",
    "jawaban": "The correct answer is: process"
  },
  "8a98b9b3cf7f54c9a582a2a569e24558d41b96a8a898475c1ae36fd6f7c2ffd8": {
    "soal": "The core of Apache Hadoop consists of the storage part (Hadoop Distributed \nAnswer Question 85\n System (HDFS)) and the processing part (MapReduce). Hadoop splits files into large blocks and distributes them across nodes in the cluster. To process the data, Hadoop MapReduce transfers code to the nodes to process in parallel, based on the data that needs to be processed at each node. This approach takes advantage of data-node locality, manipulating the data they hold in hand to enable faster and more efficient processing than more conventional supercomputer architectures relying on parallel file systems where computation and data are linked through high-speed networks.",
    "jawaban": "The correct answer is: File"
  },
  "eaf1b6a86be80674fa6a8508abf081a3ac1338bccebeee5db87a8021b3c9b2d2": {
    "soal": "Why is stratification particularly important when working with imbalanced datasets?",
    "jawaban": "The correct answer is: It prevents the model from being biased toward majority classes by maintaining class proportions."
  },
  "ce3f516b967521b2473b3fd7affac120df43d50025c71572fa998e403bcae791": {
    "soal": "import pm4py\nif __name__ == \"__main__\":\nxes_filename = 'BPIC_2012_A.xes'\n# xes_filename = 'BPIC_2012_W.xes'\n# xes_filename = 'BPIC_2012_O.xes'\n# xes_filename = 'BPI_Challenge_2019.xes'\n# xes_filename = 'PM4PY-running-example.xes'\n# xes_filename = 'Cross_Hospital.xes'\n# xes_filename = 'excercise.xes'\n#_xes_filename = 'Production.xes'\n#-xes_filename = 'training_log_1.xes'\n#-xes_filename = 'training_log_3.xes'\n#-xes_filename = 'training_log_8.xes'\nlog = pm4py.read_xes(xes_filename)\nmap = pm4py.discover_heuristics_net(log)\npm4py.view_heuristics_net(map)\nWhich of the following is NOT a potential filename listed in the script comments?",
    "jawaban": "The correct answer is: BPI_Challenge_2021.xes -BPIC_2012_A.xes -BPIC_2012_W.xes -BPI_Challenge_2019.xes -PM4PY-running-example.xes -training_log_1.xes"
  },
  "4de386fd9d3ae81eabd79002a6bfdc2df28a6b648ae74335e0392fdf5d57a5eb": {
    "soal": "Data Mining has the power to transform businesses; however, implementing processes that meet the needs of all stakeholders often hinders successful data mining investments \u2014 78% of respondents say they struggle to find the right data mining strategies or solutions.\nDespite these obstacles, businesses that are able to effectively mine widespread data have several key similarities. Successful companies:\n- Know the core needs of their business, both tactical and strategic, that can be met by data mining;\n- Identify and evaluate data sources to be used by data mining tools for accuracy and relevance;\n- Define applications, including Business Intelligence (BI) systems, where data mining tools should be interoperable;\n- Identify available data mining solutions that meet the entire scope of the organization's requirements, from budget to end-user technical capabilities;\n- Use three hundred standard data mining tools that meet the needs of IT, data scientists, and analysts, while also meeting the consumption and visualization needs of business users.",
    "jawaban": "The correct answer is 'False'."
  },
  "2794cd69c5197db907e19dfab51e3bc5d40c335349f3a751d9e4370052a857ac": {
    "soal": "The CSV File Import widget reads comma-separated files and sends the dataset to the output channel. The separator can be .............., semicolon, space, tab, or manually-defined delimiters. The history of the recently opened files is maintained in the widget.\n\n\n\n",
    "jawaban": "The correct answer is: comma"
  },
  "30d5d63472864c9ec28d76d473fd28cc0633a6379cfb2124dbadb68485ce4895": {
    "soal": "Which of the following is NOT typically derived from process mining?",
    "jawaban": "The correct answer is: Employee personal information"
  },
  "37531d52718a81d34eda27a52daecac3e5c1e44c3672a847b336157fcdd54fe3": {
    "soal": "The core of Apache Hadoop consists of the storage part (Hadoop Distributed File \nAnswer Question 34\n (HDFS)) and the processing part (MapReduce). Hadoop splits files into large blocks and distributes them across nodes in the cluster. To process the data, Hadoop MapReduce transfers code to the nodes to process in parallel, based on the data that needs to be processed at each node. This approach takes advantage of data-node locality, manipulating the data they hold in hand to enable faster and more efficient processing than more conventional supercomputer architectures relying on parallel file systems where computation and data are linked through high-speed networks.",
    "jawaban": "The correct answer is: System"
  }
}